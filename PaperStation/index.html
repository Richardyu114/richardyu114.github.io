<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2"><meta name="google-site-verification" content="true"><meta name="baidu-site-verification" content="true"><link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1"><link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.1.1",sidebar:{position:"left",display:"post",offset:12,onmobile:!1,dimmer:!1},back2top:!0,back2top_sidebar:!1,fancybox:!0,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="baidu-site-verification" content="0bqk4mbBLD"><meta name="google-site-verification" content="FvMbHVgnH7FO0GFRzTmOevjSaFwvzIQZv94LdufMMPY"><meta name="description" content="About记录自己看到的与计算机视觉，视觉SLAM，机器人，机器学习等相关的论文。如果是比较重要的和自己感兴趣的论文会另开一篇post详细介绍。2019Visual SLAM几篇综述基于单目视觉的同时定位与地图构建方法综述摘 要: 增强现实是一种在现实场景中无缝地融入虚拟物体或信息的技术, 能够比传统的文字、图像和视频等方式更高效、直观地呈现信息，有着非常广泛的应用. 同时定位与地图构建作为增强现"><meta property="og:type" content="website"><meta property="og:title" content="PaperStation"><meta property="og:url" content="http://densecollections.top/PaperStation/index.html"><meta property="og:site_name" content="自拙集"><meta property="og:description" content="About记录自己看到的与计算机视觉，视觉SLAM，机器人，机器学习等相关的论文。如果是比较重要的和自己感兴趣的论文会另开一篇post详细介绍。2019Visual SLAM几篇综述基于单目视觉的同时定位与地图构建方法综述摘 要: 增强现实是一种在现实场景中无缝地融入虚拟物体或信息的技术, 能够比传统的文字、图像和视频等方式更高效、直观地呈现信息，有着非常广泛的应用. 同时定位与地图构建作为增强现"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_Thoracic%20Disease%20Identification%20and%20Localization%20with%20Limited%20Supervisio.PNG"><meta property="og:image" content="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_FCN.PNG"><meta property="og:image" content="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/BoxSup_model.PNG"><meta property="og:updated_time" content="2020-04-19T14:18:13.966Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="PaperStation"><meta name="twitter:description" content="About记录自己看到的与计算机视觉，视觉SLAM，机器人，机器学习等相关的论文。如果是比较重要的和自己感兴趣的论文会另开一篇post详细介绍。2019Visual SLAM几篇综述基于单目视觉的同时定位与地图构建方法综述摘 要: 增强现实是一种在现实场景中无缝地融入虚拟物体或信息的技术, 能够比传统的文字、图像和视频等方式更高效、直观地呈现信息，有着非常广泛的应用. 同时定位与地图构建作为增强现"><meta name="twitter:image" content="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_Thoracic%20Disease%20Identification%20and%20Localization%20with%20Limited%20Supervisio.PNG"><link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml"><link rel="canonical" href="http://densecollections.top/PaperStation/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>PaperStation | 自拙集</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">自拙集</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Work cures everything</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-paperstation menu-item-active"><a href="/PaperStation/" rel="section"><i class="menu-item-icon fa fa-fw fa-edit"></i><br>PaperStation</a></li><li class="menu-item menu-item-mindwandering"><a href="/MindWandering/" rel="section"><i class="menu-item-icon fa fa-fw fa-paper-plane"></i><br>MindWandering</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="post-block page"><header class="post-header"><h2 class="post-title" itemprop="name headline">PaperStation</h2><div class="post-meta"></div></header><div class="post-body"><h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>记录自己看到的与计算机视觉，视觉SLAM，机器人，机器学习等相关的论文。如果是比较重要的和自己感兴趣的论文会另开一篇post详细介绍。</p><h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><h3 id="Visual-SLAM几篇综述"><a href="#Visual-SLAM几篇综述" class="headerlink" title="Visual SLAM几篇综述"></a>Visual SLAM几篇综述</h3><ul><li><a href="http://www.cad.zju.edu.cn/home/gfzhang/projects/JCAD2016-SLAM-survey.pdf" target="_blank" rel="noopener">基于单目视觉的同时定位与地图构建方法综述</a></li></ul><blockquote><p>摘 要: 增强现实是一种在现实场景中无缝地融入虚拟物体或信息的技术, 能够比传统的文字、图像和视频等方式更高效、直观地呈现信息，有着非常广泛的应用. 同时定位与地图构建作为增强现实的关键基础技术, 可以用来在未知环境中定位自身方位并同时构建环境三维地图, 从而保证叠加的虚拟物体与现实场景在几何上的一致性. 文中首先简述基于视觉的同时定位与地图构建的基本原理; 然后介绍几个代表性的基于单目视觉的同时定位与地图构建方法并做深入分析和比较; 最后讨论近年来研究热点和发展趋势, 并做总结和展望。</p></blockquote><p>中文综述目前看的比较舒服的一篇，对于visual SLAM有一定了解的人看起来很快，同时也梳理得比较整洁紧凑。</p><ul><li><a href="https://www.researchgate.net/profile/Jose_Ascencio/publication/234081012_Visual_Simultaneous_Localization_and_Mapping_A_Survey/links/55383e610cf247b8587d3d58/Visual-Simultaneous-Localization-and-Mapping-A-Survey.pdf" target="_blank" rel="noopener">Visual simultaneous localization and mapping: a survey</a></li></ul><blockquote><p>Abstract : Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, construct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to provide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art.</p></blockquote><p>这是一篇非常棒的综述，对于入门的人非常友好，几乎没有数学公式，只是对slam中的各个问题和模块进行了分解，词汇也不复杂，读起来很快。但是缺点是不是很新，而且深度不够。</p><ul><li><a href="https://arxiv.org/pdf/1606.05830.pdf" target="_blank" rel="noopener">Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age</a></li></ul><blockquote><p>Abstract—Simultaneous Localization and Mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors’ take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?</p></blockquote><p>这个综述相对难说难度点，但是写的非常好，毕竟作者都是有名的大佬。比较难得的是，这篇综述不仅梳理了slam的发展历程和技术现状，还提出了一些”open problem”，表明了自己的的观点，详细地阐述了视觉SLAM现在的挑战以及未来可能的应对办法，虽然有些问题是显而易见的。此外，该综述中也提供了很多参考文献，尤其是对于场景识别中的感知混叠以及滤波器优化和非线性优化的比较，以及因子图的功效，后续都值得研究一下。</p><ul><li><a href="https://arxiv.org/abs/1803.11288" target="_blank" rel="noopener">FutureMapping: The Computational Structure of Spatial AI Systems,Andrew J. Davison</a></li></ul><blockquote><p>We discuss and predict the evolution of Simultaneous Localisation and Mapping (SLAM) into a general geometric and semantic ‘Spatial AI’ perception capability for intelligent embodied devices. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or consumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments.</p></blockquote><p><a href="https://richardyu114.github.io/2019/04/16/FutureMapping-by-A-J-Davison/" target="_blank" rel="noopener">blog</a></p><ul><li><a href="https://arxiv.org/pdf/1910.14139.pdf" target="_blank" rel="noopener">FutureMapping 2: Gaussian Belief Propagation for Spatial AI</a></li></ul><p><a href="https://richardyu114.github.io/2019/11/17/FutureMapping2/" target="_blank" rel="noopener">blog</a></p><ul><li><a href="https://www.researchgate.net/profile/Ruihao_Li4/publication/327531951_Ongoing_Evolution_of_Visual_SLAM_from_Geometry_to_Deep_Learning_Challenges_and_Opportunities/links/5c8650db92851c1d5e156d7f/Ongoing-Evolution-of-Visual-SLAM-from-Geometry-to-Deep-Learning-Challenges-and-Opportunities.pdf" target="_blank" rel="noopener">Ongoing Evolution of Visual SLAM from Geometry to Deep Learning: Challenges and Opportunities</a></li></ul><p>这篇综述主要关注的是深度学习在slam中的应用，先介绍了几种常见的模型，即CNN, RNN和encoder, decoder，然后列举了一些slam常用的dataset，包括KITTI, TUM, NYU等，接着分块介绍深度学习在depth estimation, pose estimation, ego-motion estimation, relocalization, sensor fusion, semantic mapping方面的应用概况，总的来说在位姿估计，深度尺度估计，回环检测重定位，地图构建这几个方面着手，论文最后提出了一些存在的挑战和思路，总体来说介绍的还是挺全的，列举的文章也很经典。但是总觉得深度不够，像是一种在知乎上回答问题的方式。虽然值得看，不过等到以后发现了更好的综述再来替换吧。</p><ul><li><a href="https://arxiv.org/abs/1909.05214" target="_blank" rel="noopener">A Survey of Simultaneous Localization and Mapping</a></li></ul><h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><ul><li><a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">paper</a></li><li><a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="noopener">code</a></li></ul><p>loss的具体形式为：$criterion= \alpha(1-a)^{\gamma}y \ln a + (1-\alpha)a^{\gamma}(1-y) \ln (1-a)$，主要的作用就是提高对假阴性的惩罚力度，在<a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">论文</a>中作者指出，对于设计的RetinaNet，超参数$\alpha=2, \gamma=0.25$效果最好（分类目标检测阶段的前景和背景分离），在我实际的二分类使用中，效果并不是十分突出，参数的调节是个技术活，否则很容易使得假阳性很高，不过这可能也是和数据集有关。</p><p>pytorch代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">#自定义的模型和loss都要继承nn.Module类</span><br><span class="line">class BFocalLoss(nn.Module):</span><br><span class="line">      def __init__(self, gamma=2, alpha=0.25):</span><br><span class="line">          super(BFocalLoss, self).__init__()</span><br><span class="line">          self.gamma = gamma</span><br><span class="line">          self.alpha = alpha</span><br><span class="line">      def forward(self, inputs, targets):</span><br><span class="line">          pt = nn.Softmax(input, dim=1)</span><br><span class="line">          p = pt[:,1]</span><br><span class="line">          loss = -self.alpha * (1-p) ** self.gamma*(target*torch.log(p+1e-12)) - \</span><br><span class="line">                 (1-self.alpha)*p**self.gamma*((1-target)*torch.log(1-p+1e-12))</span><br><span class="line">          return loss.mean()</span><br></pre></td></tr></table></figure><p>medium上一篇<a href="https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de" target="_blank" rel="noopener">blog</a>对flocal loss进行了阐释。</p><h3 id="Deep-Learning-in-Tumor-Metastatic-on-Medicine-Image"><a href="#Deep-Learning-in-Tumor-Metastatic-on-Medicine-Image" class="headerlink" title="Deep Learning in Tumor Metastatic on Medicine Image"></a>Deep Learning in Tumor Metastatic on Medicine Image</h3><p>两篇深度学习在乳腺癌细胞转移检测的论文：</p><ul><li><a href="https://arxiv.org/pdf/1606.05718.pdf" target="_blank" rel="noopener">Deep Learning for Identifying Metastatic Breast Cancer</a></li><li><a href="https://arxiv.org/pdf/1703.02442.pdf" target="_blank" rel="noopener">Detecting Cancer Metastases on Gigapixel Pathology Images</a></li></ul><p>医学图像处理与自然图像处理不同，一般来说由于设备的原因，可能病灶特征不是特别容易区分，也不是很明显，因此ImageNet上的预训练模型可能不是很有用。医学图像方面由于图像数量少，标注成本高，所以用的tricks比较多，要根据具体的要求和数据采集情况分析，比如痰涂片载玻片图像，一般得到的数据集可能是组与组之间是连续的特征，就像视频中连续帧的图像，差别不会很大，因此标注的时候可能只需要根据采集的组进行少量标注就可以，进行弱监督训练，也可能达到很不错的分类精度。</p><ul><li>特定的数据增强，RGB-HSV转换，color normalization</li><li>slide选取patches放大不同尺度，多尺度输入</li><li>原始样本旋转90，180，270度，left-right flip之后再旋转90，180，270度，这样就扩增到了8倍大小。然后进行图像色调的调整，包括对比度，亮度，饱和度等。</li><li>FROC 而不是ROC和AUC（performance衡量标准）</li><li>减少计算，移除背景patches</li><li>随机森林提取heatmap特征</li></ul><h3 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h3><ul><li><a href="https://arxiv.org/abs/1710.09412" target="_blank" rel="noopener">paper</a></li><li><a href="https://github.com/facebookresearch/mixup-cifar10" target="_blank" rel="noopener">code</a></li></ul><p>adversarial examples， ERM（经验风险最小化）准则不能很好的适用，数据增强，VRM（近邻风险最小化），插值生成对抗样本和标签</p><script type="math/tex;mode=display">\widetilde x   = \lambda x_{i} + (1-\lambda)x_{j}\\
\widetilde y  = \lambda y_{i} + (1-\lambda)y_{j} \\</script><p>对交叉熵损失函数和Focal Loss而言，可以直接取出$y_{i}, y_{j}$对其损失函数进行插值(数学上可以推导)</p><script type="math/tex;mode=display">loss = \lambda \cdot criterion(\widetilde x, y_{i}) + (1-\lambda) \cdot criterion(\widetilde x, y_{j})</script><blockquote><p>The mixup vicinal distribution can be understood as a form of data augmentation that encourages the model f to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam’s razor, since it is one of the simplest possible behaviors.</p></blockquote><p>论文指出，mixup可以控制模型复杂度，也就是说模型在ERM情况下不断训练会记住training data，导致泛化能力差，而mixup通过随机pairing插值融合，生成对抗样本可以有效的缓解这种情况。而且通过大量的实验，证明mixup确实有效果，而且在各个领域都还不错，此外可以和dropout等控制模型复杂度方法相结合。</p><blockquote><p>We have shown that mixup is a form of vicinal risk minimization, which trains on virtual examples<br>constructed as the linear interpolation of two random examples from the training set and their labels. Incorporating mixup into existing training pipelines reduces to a few lines of code, and introduces little or no computational overhead. Throughout an extensive evaluation, we have shown that mixup improves the generalization error of state-of-the-art models on ImageNet, CIFAR, speech, and tabular datasets. Furthermore, mixup helps to combat memorization of corrupt labels, sensitivity to adversarial examples, and instability in adversarial training.</p></blockquote><p>在图像上的训练trick: 训练时每个epoch都采用mixup，$\lambda$的取值由$Beta(\alpha,\alpha)$函数随机指定，$\alpha$是hyper-parameter，论文中指出在imagNet上的值在[0.1, 0.4]之间，在CIFAR上取的是1，此外，网络结构加深和训练周期的加长都会使得最终的泛化效果比较好。但是论文中没有将为什么选用$beta$函数去生成$\lambda$，优化器选的是带动量的SGD，其中learning rate会随着指定的epoch范围进行下降，且没有使用drop out。</p><p>numpy.random.beta()是对beta分布进行随机采样，下式是beta分布的概率密度函数，当$\alpha$的取值越大时，取样的值基本就会往0.5靠近，这时候似乎就退化成sample pairing。</p><script type="math/tex;mode=display">\lambda = f(x ; a, b)=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1} \\
B(\alpha, \beta)=\int_{0}^{1} t^{\alpha-1}(1-t)^{\beta-1} dt</script><p>mixup与IBM的一篇文章<a href="https://arxiv.org/abs/1801.02929" target="_blank" rel="noopener">sample pairing</a>的想法很类似，而且提出的时间都差不多，不过sample pairing是随机将两幅图片平均插值，但是label不变，等于是引入噪声，而且训练的trick也比较多，可以参考这篇<a href="https://jsideas.net/samplepairing/" target="_blank" rel="noopener">博客</a>的实验。</p><p>如果加入warmup，学习率随指定epoch下降，weight decay=$10^{-4}$（对于mixup，小的weight decay效果 更好)，此外超参$\alpha$的取值越大，训练集的loss会越大，但是泛化能力就会越好。但是具体的数据集可能training loss变化趋势不同，可能随着$\alpha$的增加急剧增加，也可能不怎么变化，因此最佳的位置，作者也提出了疑问，放在了discussion中，他们猜测可能大容量模型可能会对大取值$\alpha$的适应度好点。</p><blockquote><p>In our experiments, the following trend is consistent: with increasingly large $\alpha$, the training error on<br>real data increases, while the generalization gap decreases.</p></blockquote><hr><p>2020.3，另有两篇images mixture的文章，分别是关于有监督和无监督领域的：</p><p><a href="https://arxiv.org/pdf/2003.05034.pdf" target="_blank" rel="noopener">SuperMix: Supervising the Mixing Data Augmentation</a></p><p><a href="https://arxiv.org/pdf/2003.05438.pdf" target="_blank" rel="noopener">Rethinking Image Mixture for Unsupervised Visual Representation Learning</a></p><h3 id="Thoracic-Disease-Identification-and-Localization-with-Limited-Supervision"><a href="#Thoracic-Disease-Identification-and-Localization-with-Limited-Supervision" class="headerlink" title="Thoracic Disease Identification and Localization with Limited Supervision"></a>Thoracic Disease Identification and Localization with Limited Supervision</h3><p><a href="https://arxiv.org/pdf/1711.06373v6.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/romanovar/evaluation_MIL" target="_blank" rel="noopener">code</a></p><p>这是平安科技和李飞飞合作的论文，主要是通过图片级label标注和少量框标注来达到病灶regions显示的目的，生成heatmap和类别判定。</p><blockquote><p>Given images with disease labels and limited bounding box information, we aim to design a unified model that simultaneously produces disease identification and localization.</p></blockquote><p><img src="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_Thoracic%20Disease%20Identification%20and%20Localization%20with%20Limited%20Supervisio.PNG" alt="model_overview"></p><p>大致的网络结构是利用resnet backbone进行下采样，生成$P \times P$的feature map，然后对应原图的空间尺寸，有框的话就进行像素级的比对预测，没框但是有图片级的label的话那么这张图片中肯定存在至少一个patch对应于这种疾病，就直接根据概率进行预测。（多实例学习）</p><ul><li>利用resnet结构，去掉global pooling和fc层，利用卷积block下采样图片，得到feature map</li><li>不同size的input图片最后得到的feature map尺寸不同，为了得到设定的patch大小，大的feature map进行maxpooling进行下采样，小的利用bilinear interpolation（双线性插值）扩大</li><li>feature map送入全卷积网络，先通过$3 \times 3$的卷积层，卷积核大小$c^{*}=512$，然后在送入$1 \times 1$的卷积层，生成$P \times P \times K$的feature map，也就是$K$个类别的预测图，根据每个图上预测的概率值进行判断属于哪个类别。（output map与target map进行loss定位回归）</li></ul><p>损失函数就是利用权重$\lambda$调节框监督和无框标注的重要程度，利用二元参数$\eta$来选择图片的监督信息对应的loss函数。</p><p>对于每个patch在之前的过程中都预测出了对应的score，在定位阶段设置一个阈值，只要score大于该值即可认为它是positive patch，论文中的阈值设置为0.5。之前论文中提到，并不预测出严格的位置来，而是一个大概区域。因为经过阈值判定的结果并不会精确地分布在一个规则的矩形框之中。</p><p>个人觉得这篇文章的两个亮点在于：</p><ul><li>医疗影像处理，论文实验做的也不少</li><li>不同的监督信息混合，多实例学习，得到区域分割和类别判断的效果</li></ul><p>但是通篇读下来觉得收获并不是很大，废话比较多，让人耳目一新的东西并不多，如果不是Feifei Li挂名的话，可能登不上CVPR。不过实习的项目想拿这篇文章的思想来做些东西，于是便选择读了。</p><h3 id="FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation"><a href="#FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation" class="headerlink" title="FCN: Fully Convolutional Networks for Semantic Segmentation"></a>FCN: Fully Convolutional Networks for Semantic Segmentation</h3><p><a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/pytorch/vision/tree/master/torchvision/models/segmentation" target="_blank" rel="noopener">code</a></p><p>开始之前先了解下上采样中的反卷积(deconvoltion)方法（一般有双线性插值，反卷积和反池化三种方法）。</p><p>实际上，转置卷积(transposed convolution)这种叫法可能更为合适。因为反卷积的数学含义，通过反卷积可以将通过卷积的输出信号，完全还原输入信号，而事实是，转置卷积只能还原shape大小，不能还原value. <a href="https://zhuanlan.zhihu.com/p/48501100" target="_blank" rel="noopener">知乎专栏</a>, <a href="https://www.zhihu.com/question/43609045" target="_blank" rel="noopener">知乎</a></p><blockquote><p>转置卷积是一种特殊的正向卷积，先按照一定的比例通过补0来扩大输入图像的尺寸，接着旋转卷积核，再进行正向卷积。</p><p>输入元素矩阵$X$，输出元素矩阵$Y$，正向卷积的矩阵为$C$，则正常的卷积操作过程为：$Y = CX$，反卷积的操作就是要对这个矩阵运算过程进行逆运算，即: $X=C^{T}Y$。但是此操作只会恢复矩阵大小，并不会恢复元素值。</p><p>转置卷积的公式：</p><script type="math/tex;mode=display">\begin{align}
& o = \text{size of output} \\
&i = \text{size of input} \\
&p = \text{padding} \\
&s = \text{strides}
\end{align}</script><p>对于正常卷积：$o=[(i+2p-k)/s]+1$</p><p>对于转置卷积：</p><p>如果$(o+2p-k) \% s =0$，则可以通过$0 = s(i-1)-2p+k $来确定卷积核参数，</p><p>如果$(o+2p-k) \% s \not =0$，则可以通过$o = s(i-1)-2p+k+(0+2p-k)\% s $来确定卷积核参数</p></blockquote><p>FCN是深度学习+语义分割领域的的milestone论文。语义分割的基本思想和分类相同，只不过是预测图像中的每个像素的所属类别（对类别进行数字编码，加上无关背景，总共是$n_{classes}+1$类，然后每个类别可以用不同的颜色显示），然后计算像素级别的loss，再进行back propagation。FCN也是这种训练方式，最后生成的特征图的个数是类别个数，所有特征图的每个像素进行log_softmax预测属于哪种类别，最后解码成设定的RGB图像。</p><p><strong>architecture+implementation</strong>:</p><p><img src="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_FCN.PNG" alt="architecture of FCN"></p><p>原文对backbone的选择，以及训练集选择，数据预处理，训练数据采样，训练过程中的超参等进行了实验，因此实验内容还是十分丰富的，最终表现好的backbone 是VGG-16，然后实验之后认为利用卷积层提取特征，pooling层下采样5次后预测的精度比较高，结构中亮点在于skip connections，采样较少的feature map尺寸大，保留了比较多的local信息，对小物体表式比较好，采样较多的feature map尺寸小，特征层级高，体现的是global信息，然而对小物体表示比较差，因此lower layer和higher layer融合可以比较好的全面表示图像的语义信息，这个结构提出了这个思考，同时也进行了实验，分别给出不融合，融合一层，融合两层的结构，看各自的表现如何。实验也表明FCN-8s的精度最好，而且越融合精度提升的点不大，但是会使得最后的结果更加smooth。</p><p>训练过程中权重初始化是个很重要的trick，论文是先利用在ImageNet上预训练过的模型去掉最后的全连接层来初始化权重，然后进行后面三个pooling过的feature map对应融合再去最后的上采样（full卷积，即反卷积）得到$c\times h \times w$的图像。FCN中的full卷积方式是先对特征图进行padding，用0填充，然后再进行正常的卷积操作，得到上采样的结果，反卷积层的权重初始化也是个很重要的部份，论文中说使用随机初始化效果不怎么样，这里可以使用<code>bilinear kernel</code>，此函数在pytorch中也有，下面的代码讲解中也有具体介绍。对于最后的loss计算，图像的每个像素点有21（针对pascal VOC数据集，最后网络输出的是$c\times h \times w$个特征图，$c=21$）个归一化概率值，最大的索引代表类别数字，也就是说是每个像素会进行一个loss计算，每个图像有$h\times w$个像素值，最会全部加起来进行backpropagation，等于是图片级分类问题中的一个”batch”操作。</p><p><a href="https://zhuanlan.zhihu.com/p/32506912" target="_blank" rel="noopener">pytorch实现FCN代码讲解</a></p><p><a href="https://www.cnblogs.com/gujianhan/p/6030639.html" target="_blank" rel="noopener">blog详解FCN网络</a></p><p><strong>metric</strong>：</p><p>假设$n_{ij}$是将类别$i$的像素预测成类别$j$的像素的个数，$n_{cl}$是物体的类别数，$t_{i}=\sum _{j}n_{ij}$是预测成像素$i$的总个数（包含正确预测的和把其他类别预测成$i$的）</p><ul><li><p>pixel accuracy: $ \sum_{i} n_{i i} / \sum_{i} t_{i}$</p></li><li><p>mean accuracy: $\left(1 / n_{\mathrm{cl}}\right) \sum_{i} n_{i i} / t_{i}$</p></li><li><p>mean IU(region intersection over union): $\left(1 / n_{\mathrm{cl}}\right) \sum_{i} n_{i i} /\left(t_{i}+\sum_{j} n_{j i}-n_{i i}\right)$</p></li><li><p>frequency weighted IU: $\left(\sum_{k} t_{k}\right)^{-1} \sum_{i} t_{i} n_{i i} /\left(t_{i}+\sum_{j} n_{j i}-n_{i i}\right)$</p></li></ul><p>FCN在语义分割（semantic segmentation）和场景理解（scene parsing）领域中是一项非常重要的工作，模型思想很简单，同时也是非常具有逻辑性的，但是论文读起来个人感觉不是特别顺畅，可能是自己的原因，觉得讲得比较乱，可能是因为作者做了很多的实验性尝试去试着提高精度，但是都没什么太大的用处，但是又不得不写出来突出工作量的缘故。抛开论文不谈，FCN的意义是重大的，毕竟是从0到1的工作，而且在代码上也有不少工作，尤其是相对图像级别的分类任务而言，有很多流程和细节部份需要注意，否则训练可能会得不到好的效果，比如数据读取数据预处理部份（图像级label-RGB值编码-类别数字编码对应抽取对应，为了进行batch训练对图像和label对应crop等），以及权重初始化，卷积核设计等，自己完整写一遍FCN的代码并且调试出好的结果肯定会大有裨益。</p><h3 id="BoxSup-Exploiting-Bounding-Boxes-to-Supervise-Convolutional-Networks-for-Semantic-Segmentation"><a href="#BoxSup-Exploiting-Bounding-Boxes-to-Supervise-Convolutional-Networks-for-Semantic-Segmentation" class="headerlink" title="BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation"></a>BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</h3><p><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf" target="_blank" rel="noopener">paper</a></p><p>这篇文章的思路和后面的<a href="https://arxiv.org/pdf/1605.07866.pdf" target="_blank" rel="noopener">deepcut</a>处理医学图像分割的思路很像（boundingbox supervised），此外伯克利大学Jitendra Malik等人2014的一篇论文<a href="https://arxiv.org/pdf/1407.1808.pdf" target="_blank" rel="noopener">SDS</a>也跟此论文的结构很类似，也使用MCG进行预选mask提取。</p><p>作者利用bounding box进行弱监督学习semantic segmentation，最后的结果令人满意，如果进行半监督（1/10的数据使用像素级标注，9/10的数据使用bounding box进行标注）训练，最后的结果甚至可以达到STOA的水平（2015年）。作者认为效果好的原因在于bounding box监督的方式使得网络提高了物体识别的准确率，这样的话就会容易将foreground和background分开，网络也会将特征学习集中在instance上面，得到的特征质量也会比较高。</p><p>模型的大致框架是：先利用region proposal(MCG, Multiscale Combinatorial Grouping)得到candidate segmentation mask，然后利用FCN训练得到coarse的分割结果，过CRF进行轮廓平滑，然后再利用这个estimation作为新的label丢进网络进行训练，iterative training得到最后的分割结果。</p><p><img src="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/BoxSup_model.PNG" alt="BoxSup_model overview"></p><p>模型实现的重点在于一是利用MCG等region proposal方法生成候选mask，然后进行随机sampling，分割训练backbone用的是FCN，之后经过CRF（不是denseCRF）后处理，一个epoch遍历完所有图片后，预测生成的mask重新作为原图的label，迭代进行训练。等于是两个网络进行end-to-end训练学习</p><p><strong>loss function</strong>:</p><p>$\min _{\theta,\left\{l_{S}\right\}} \sum\left(\mathcal{E}_{o}+\lambda \mathcal{E}_{r}\right)$</p><p>$\mathcal{E}_{o}=\frac{1}{N} \sum_{S}(1-\operatorname{IoU}(B, S)) \delta\left(l_{B}, l_{S}\right)$</p><p>$\mathcal{E}_{r}=\sum_{p} e\left(X_{\theta}(p), l_{S}(p)\right)$</p><p>第一个loss$\mathcal{E}_{0}$是region proposal部份生成的候选mask的惩罚函数，采用的是交并比IoU，取值在[0,1]之间，$l_{B}, l_{S}$</p><p>分别代表bounding box和semantic mask的标签，因为可能有多个类别，只计算每个类别的bounding box和semantic mask代表同样类别的交并比，因此，如果是同类$\delta$就是1，否则是0，$1/N$是为了归一化（或者正则化，whatever）。</p><p>第二个loss是标准的semantic segmentation中的像素级loss，每个像素进行类别上的cross-entropy，其中$X_{\theta}(p)$代表FCN网络预测的标签，$l_{s}(p)$代表用来全监督的label，第一次是MCG生成的候选mask，后面则是网络预测并进行CRF后处理后覆盖的mask。</p><p>最后将两个网络的loss加在一起，并设置权重系数$\lambda$来调节两者对整体模型的贡献度，按直觉说，应该是FCN等backbone预测pixel标签部分更为重要，因此$\lambda$应当大于1，论文中此超参给的值是3。</p><p><strong>模型的核心点除了迭代标签之外，就是候选mask的生成以及分配相应的label给candidate segments，这些问题需要弄清楚</strong>：</p><blockquote><p>The candidate segments are used to update the deep convolutional network. The semantic features learned by the network are then used to pick better candidates. This procedure is iterated.</p><p>As a pre-processing, we use a region proposal method to generate segmentation masks. We adopt Multiscale Combinatorial Grouping (MCG) by default, while other methods are also evaluated. For all experiments we use 2k candidates on average per image as a common practice. The proposal candidate masks are fixed throughout the training procedure. But during training, each candidate<br>mask will be assigned a label which can be a semantic category or background. The labels assigned to the masks will be updated.</p></blockquote><p>分配标签问题文中是利用贪心迭代算法（greedy iterative solution）来寻找局部最优值：</p><blockquote><p>With the network parameters $\theta$ fixed, we update the semantic labeling $\left\{l_{s}\right\}$for all candidate segments. As mentioned above, we only consider the case in which one ground-truth bounding box can “activate” (i.e., assign anon-background label to) one and only one candidate. As such, we can simply update the semantic labeling by selecting a single candidate segment for each ground-truth bounding box, such that its cost $\mathcal{E_{0}}+\lambda \mathcal{E_{r}}$is the smallest among all candidates. The selected segment is assigned the groundtruth semantic label associated with that bounding box. All other pixels are assigned the background label.</p><p>The above winner-takes-all selection tends to repeatedly use the same or very similar candidate segments, and the optimization procedure may be trapped in poor local optima. To increase the sample variance for better stochastic training, we further adopt a random sampling method to select the candidate segment for each ground-truth bounding box. Instead of selecting the single segment with the smallest cost $\mathcal{E_{0}}+\lambda \mathcal{E_{r}}$, we randomly sample a segment from the first $k$ segments with the smallest costs. In this paper we use $k = 5$. This random sampling strategy improves the accuracy by about 2% on the validation set.</p></blockquote><p>然而我还没研究到MCG，此论文也没有开源代码，我只能粗略理解为先是通过MCG为每张图片生成2K的candidate segment region proposal，然后对像素标签，训练完并进行了CRF 后处理的时候根据损失函数去找比较小的几个candidate，随机抽取K个进行重新assign标签，然后再迭代训练。这里有两个疑问待解决：一个是生成的2k个proposal应该是在每个类别上都有重复的，这样的话怎么一个bounding box对应一个segment进行监督学习；第二个是原来的update 方法是利用损失函数最小的那个框对去更新segment，，然后再训练，这个很直观，可以理解，但是论文中说为了防止”winner takes all”的误区，也就是陷入局部最优值，随机抽取了k个最小的segments，然后进行迭代，这样的话就又等于是拿出了多个candidate去监督吗？目前我还没弄明白具体的实现细节。。</p><p>增加带bounding box标注的数据显著提高了模型的performance：</p><p>recognition error that is due to confusions of recognizing object categories;</p><p>boundary error that is due to misalignments of pixel-level labels on object boundaries</p><p>主要原因还是增大了数据量使得模型对特征提取更加健壮，能够更好地进行object recognition</p><h3 id="Simple-Does-It-Weakly-Supervised-Instance-and-Semantic-Segmentation"><a href="#Simple-Does-It-Weakly-Supervised-Instance-and-Semantic-Segmentation" class="headerlink" title="Simple Does It: Weakly Supervised Instance and Semantic Segmentation"></a>Simple Does It: Weakly Supervised Instance and Semantic Segmentation</h3><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/philferriere/tfwss" target="_blank" rel="noopener">github1</a></p><p><a href="https://github.com/johnnylu305/Simple-does-it-weakly-supervised-instance-and-semantic-segmentation" target="_blank" rel="noopener">github2</a></p><p>这篇文章在BoxSup之后，细节部份写得要比前者详细的多，虽然方法相似，但是很值得一读，可以得到不少收获。</p><p>直接box或者其他region proposal生成segments，DeepMask作为instance segmentation参考模型，重构semantic segmentation model DeepLabv2去进行instance segmentation，训练部分常规操作，最后还对集中segment生成方法进行了比较。</p><p><strong>From boxes to semantic labels</strong>:</p><p>C1 Background 未被框包围的像素认为是background</p><p>C2 Object extend 框包围了整个object，但是物体的形状可以作为另外一个先验信息，比如椭圆形状的病菌就会看起来更像一个细长的bar，模型会将这个size信息用在模型学习中</p><p>C3 Objectness 除了框带来的区域和范围信息，还有两个典型的先验，即空间连续信息和相较于background对比明显的物体轮廓信息。通常情况下可以通过segment proposal(GrabCut，MCG等)来枚举并排序一些比较像的大部分处于框内的物体形状。</p><p><strong>methods</strong>:</p><p>naive的转换标签是将box里面的全部像素assign为对应类别，外面的为背景，如果两个框交叉了，那么重合的部分是交叉框的标签（也可以从图片上直观看出），之后进行recursive training，预测的结果去update label，论文中给的结果看起来比想象中要好不少，主要原因是training被去噪和标注以及先验信息进行加强了，主要是对模型预测的结果采用了三个后处理方法：预测部份pixel在bounding box之外的全部置为background；预测部份和bounding box计算IoU，如果小于50%，重新把整个box变成像素标签（等于是回到一开始的label）；预测部份过DenseCRF平滑轮廓（use DenseCRF with the DeepLabv1 parameters），这是很重要的一步，对提升performance很有帮助。以上方法作为论文的两个baseline，名叫<strong>Naive（不带post-processing)</strong>和<strong>Box</strong></p><p>另外一个利用矩形标注的方法是$Box^{i}$，引入了ignore regions，也就是将bounding box内20%的区域进行pixel标注，其他区域作为忽视区域，这样的话可能会减少原始输入标签的噪声，同时这20%的标签也会尽可能overlap对应的object，后续也会经过上文中的三个post-processing阶段。</p><p><strong>GrabCut+</strong>方法是参考Grabcut这个region proposal 技巧去生成较bounding box更为精细的分割，论文是用了HED边缘检测方法（ICCV2015，<a href="https://arxiv.org/pdf/1504.06375.pdf" target="_blank" rel="noopener">paper</a>, <a href="https://github.com/s9xie/hed" target="_blank" rel="noopener">code</a>）而不是典型的RGB颜色差异去分割预选segments，因此论文称为<strong>GrabCut+</strong>方法，同时也仿照了$Box^{i}$方法提出了另一个baseline$GrabCut^{i}$（区域的比例阈值设定所有不同）。此外，也采取了另一种当时比较流行的region proposal方法MCG，这也是BoxSup中采用的方法，一个baseline是单用<strong>MCG</strong>方法，另一个是和<strong>GrabCut+</strong>结合，即$MCG \cap GrabCut+$，等于是简单的ensemble投票方法。</p><p>语义分割方面使用的是Deeplab模型，实例分割用的是DeepMask模型，论文也对这些模型实现方法进行了修改，以适合本情况。论文在单数据集，多数据集，以及前处理生成segments上做了很多对比实验，总体来说操作上要比BoxSup简单些，而且效果也很有竞争力。</p><h3 id="FutureMapping"><a href="#FutureMapping" class="headerlink" title="FutureMapping"></a>FutureMapping</h3><p>SLAM领域大佬Andrew Davison的新作</p><p><a href="https://densecollections.top/2019/04/16/FutureMapping-by-A-J-Davison/">FutureMapping</a></p><p><a href="https://densecollections.top/2019/11/17/FutureMapping2/">FutureMapping2</a></p><h3 id="目标检测R-CNN系列"><a href="#目标检测R-CNN系列" class="headerlink" title="目标检测R-CNN系列"></a>目标检测R-CNN系列</h3><p><a href="https://densecollections.top/2019/11/30/RCNN-series-in-object-detection/">R-CNN—-&gt;SPPNet—&gt;Fast R-CNN—&gt;Faster R-CNN—&gt;OHEM</a></p><p><a href="https://densecollections.top/2020/01/10/RCNN-series-in-object-detection-续/">R-FCN, FPN</a></p><h2 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h2><h3 id="Deep-GrabCut-for-Object-Selection"><a href="#Deep-GrabCut-for-Object-Selection" class="headerlink" title="Deep GrabCut for Object Selection"></a>Deep GrabCut for Object Selection</h3><p><a href="https://arxiv.org/pdf/1707.00243.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/jfzhang95/DeepGrabCut-PyTorch" target="_blank" rel="noopener">code</a></p><p>image+Euclidean distance map</p><h3 id="Holistically-Nested-Edge-Detection"><a href="#Holistically-Nested-Edge-Detection" class="headerlink" title="Holistically-Nested Edge Detection"></a>Holistically-Nested Edge Detection</h3><p><a href="https://arxiv.org/pdf/1504.06375.pdf" target="_blank" rel="noopener">paper</a></p><p>code: <a href="https://github.com/tensorpack/tensorpack/tree/master/examples/HED" target="_blank" rel="noopener">tf</a>, <a href="https://github.com/s9xie/hed" target="_blank" rel="noopener">caffe</a>, <a href="https://github.com/meteorshowers/hed-pytorch" target="_blank" rel="noopener">pytorch</a></p><h3 id="Seed-Expand-and-Constrain-Three-Principles-for-Weakly-Supervised-Image-Segmentation"><a href="#Seed-Expand-and-Constrain-Three-Principles-for-Weakly-Supervised-Image-Segmentation" class="headerlink" title="Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation"></a>Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation</h3><p><a href="https://arxiv.org/pdf/1603.06098.pdf" target="_blank" rel="noopener">paper</a></p><h3 id="Weakly-Supervised-Semantic-Segmentation-by-Iteratively-Mining-Common-Object-Features"><a href="#Weakly-Supervised-Semantic-Segmentation-by-Iteratively-Mining-Common-Object-Features" class="headerlink" title="Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features"></a>Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features</h3><p><a href="https://arxiv.org/pdf/1806.04659v1.pdf" target="_blank" rel="noopener">paper</a></p><h3 id="U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation"><a href="#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="U-Net: Convolutional Networks for Biomedical Image Segmentation"></a>U-Net: Convolutional Networks for Biomedical Image Segmentation</h3><p><a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/LeeJunHyun/Image_Segmentation" target="_blank" rel="noopener">code1</a></p><p><a href="https://github.com/milesial/Pytorch-UNet" target="_blank" rel="noopener">code2</a></p><p><a href="https://github.com/ShawnBIT/UNet-family" target="_blank" rel="noopener">U-Net family</a></p><p>U-Net是医学图像分割领域效果很好的一个网络架构，对称的下采样和上采样操作以及skip connections保证了特征图既包含了低层级的特征，也包含了高层级的语义特征，适合单一和图像梯度复杂的医学图像，实际在自然图像中表现也不错。（<a href="https://www.zhihu.com/question/269914775" target="_blank" rel="noopener">知乎：为什么U-Net在医学图像分割领域表现不错</a>）</p><h3 id="U-Net-A-Nested-U-Net-Architecture-for-Medical-Image-Segmentation"><a href="#U-Net-A-Nested-U-Net-Architecture-for-Medical-Image-Segmentation" class="headerlink" title="U-Net++: A Nested U-Net Architecture for Medical Image Segmentation"></a>U-Net++: A Nested U-Net Architecture for Medical Image Segmentation</h3><p><a href="https://arxiv.org/abs/1807.10165" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/MrGiovanni/UNetPlusPlus" target="_blank" rel="noopener">code</a></p><p><a href="https://www.jianshu.com/p/3d9df4aa69bb" target="_blank" rel="noopener">blog</a></p><h3 id="CAM—Learning-Deep-Features-for-Discriminative-Localization"><a href="#CAM—Learning-Deep-Features-for-Discriminative-Localization" class="headerlink" title="CAM—Learning Deep Features for Discriminative Localization"></a>CAM—Learning Deep Features for Discriminative Localization</h3><ul><li><a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener">paper</a></li><li>[code]</li></ul><p>卷积单元即使在没有监督下也可以detect object，但是这种能力会在后面的全连接层下消失，通过global average pooling可以保持这种能力。</p><p><em>class activation mapping</em></p><h3 id="CRFasRNN"><a href="#CRFasRNN" class="headerlink" title="CRFasRNN"></a>CRFasRNN</h3><p>[paper]</p><h3 id="Efficient-Inference-in-Fully-Connected-CRFs-with-Gaussian-Edge-Potentials"><a href="#Efficient-Inference-in-Fully-Connected-CRFs-with-Gaussian-Edge-Potentials" class="headerlink" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials"></a>Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</h3><p><a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/lucasb-eyer/pydensecrf" target="_blank" rel="noopener">code</a></p><p>$E(x)=\sum_{i} \psi_{u}\left(x_{i}\right)+\sum_{i&lt;j} \psi_{p}\left(x_{i}, x_{j}\right)$</p><p>$\psi_{p}\left(x_{i}, x_{j}\right)=\mu\left(x_{i}, x_{j}\right) \sum_{m=1}^{K} w^{(m)} k^{(m)}\left(f_{i}, f_{j}\right)$</p><p>$k^{(m)}\left(f_{i}, f_{j}\right)=w^{(1)} \exp \left(-\frac{\left|p_{i}-p_{j}\right|^{2}}{2 \theta_{\alpha}^{2}}-\frac{\left|I_{i}-I_{j}\right|^{2}}{2 \theta_{\beta}^{2}}\right)+w^{(2)} \exp \left(-\frac{\left|p_{i}-p_{j}\right|^{2}}{2 \theta_{\gamma}^{2}}\right)$</p><h3 id="Multiscale-Combinatorial-Grouping-for-Image-Segmentation-and"><a href="#Multiscale-Combinatorial-Grouping-for-Image-Segmentation-and" class="headerlink" title="Multiscale Combinatorial Grouping for Image Segmentation and"></a>Multiscale Combinatorial Grouping for Image Segmentation and</h3><p>Object Proposal Generation<br><a href="https://arxiv.org/pdf/1503.00848.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/jponttuset/mcg" target="_blank" rel="noopener">code</a></p><h3 id="Weakly-and-Semi-Supervised-Learning-of-a-Deep-Convolutional-Network-for-Semantic-Image-Segmentation"><a href="#Weakly-and-Semi-Supervised-Learning-of-a-Deep-Convolutional-Network-for-Semantic-Image-Segmentation" class="headerlink" title="Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation"></a>Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation</h3><p><a href="https://arxiv.org/pdf/1502.02734.pdf" target="_blank" rel="noopener">paper</a></p><h3 id="Learning-to-Reweight-Examples-for-Robust-Deep-Learning"><a href="#Learning-to-Reweight-Examples-for-Robust-Deep-Learning" class="headerlink" title="Learning to Reweight Examples for Robust Deep Learning"></a>Learning to Reweight Examples for Robust Deep Learning</h3><p><a href="https://arxiv.org/pdf/1803.09050.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/danieltan07/learning-to-reweight-examples" target="_blank" rel="noopener">code</a></p><p>batch-reweight</p><h3 id="Detecting-Lesion-Bounding-Ellipses-With-Gaussian-Proposal-Networks-GPN"><a href="#Detecting-Lesion-Bounding-Ellipses-With-Gaussian-Proposal-Networks-GPN" class="headerlink" title="Detecting Lesion Bounding Ellipses With Gaussian Proposal Networks(GPN)"></a>Detecting Lesion Bounding Ellipses With Gaussian Proposal Networks(GPN)</h3><p>椭圆标注的目标检测，比如医学图像中deep-lesion，人脸检测。</p><p>一种直觉的方法是通过三个点（中心点，长轴点，短轴点）和一个旋转角度$\tan\theta$，仿照faster r-cnn来进行回归，但是长短轴比例ration较大或者接近与1的时候，就会要求角度预测的很准确或者不需要准确，而且two-stage效率比较低。</p><p>GPN源于二维高斯分布，通过ground truth的概率分布图的等高线（椭圆）来和预测的做距离上的regression，这样就大大降低了参数量，对于旋转的情况，利用rotation matrix来进行坐标系的平移（关键词：协方差矩阵，KLD损失函数，概率密度函数，像素密度<foreground ,backgroud>)</foreground></p><h3 id="Class-Balanced-Loss-Based-on-Effective-Number-of-Samples"><a href="#Class-Balanced-Loss-Based-on-Effective-Number-of-Samples" class="headerlink" title="Class-Balanced Loss Based on Effective Number of Samples"></a>Class-Balanced Loss Based on Effective Number of Samples</h3><p><a href="https://arxiv.org/pdf/1901.05555.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/vandit15/Class-balanced-loss-pytorch" target="_blank" rel="noopener">code</a></p><h3 id="Tell-Me-Where-to-Look-Guided-Attention-Inference-Network"><a href="#Tell-Me-Where-to-Look-Guided-Attention-Inference-Network" class="headerlink" title="Tell Me Where to Look: Guided Attention Inference Network"></a>Tell Me Where to Look: Guided Attention Inference Network</h3><p><a href="https://arxiv.org/pdf/1802.10171.pdf" target="_blank" rel="noopener">paper</a></p><p><a href="https://github.com/AustinDoolittle/Pytorch-Gain" target="_blank" rel="noopener">code</a></p><h3 id="Dice-Loss"><a href="#Dice-Loss" class="headerlink" title="Dice Loss"></a>Dice Loss</h3><h3 id="Deep-Learning-Visual-Odometry"><a href="#Deep-Learning-Visual-Odometry" class="headerlink" title="Deep Learning + Visual Odometry"></a>Deep Learning + Visual Odometry</h3><ul><li>[deepvo]</li><li>[code]</li></ul><ul><li>[sfmlearner]</li><li>[code]</li></ul><ul><li>[undeepvo]</li><li>[code]</li></ul><ul><li><a href="https://arxiv.org/pdf/1809.05786.pdf" target="_blank" rel="noopener">GANvo</a></li><li>[code]</li></ul><ul><li>[SGANvo](</li></ul></div></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/header.jpg" alt="Richard YU"><p class="site-author-name" itemprop="name">Richard YU</p><div class="site-description motion-element" itemprop="description">Today everything exists to end in a photograph</div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/richardyu114" title="GitHub &rarr; https://github.com/richardyu114" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Yu1145635107" title="Twitter &rarr; https://twitter.com/Yu1145635107" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a> </span><span class="links-of-author-item"><a href="https://instagram.com/d.h.richard" title="Instagram &rarr; https://instagram.com/d.h.richard" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a> </span><span class="links-of-author-item"><a href="https://weibo.com/u/5211687990" title="Weibo &rarr; https://weibo.com/u/5211687990" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a> </span><span class="links-of-author-item"><a href="https://www.douban.com/people/161993653/" title="豆瓣 &rarr; https://www.douban.com/people/161993653/" rel="noopener" target="_blank"><i class="fa fa-fw fa-paperclip"></i>豆瓣</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#About"><span class="nav-number">1.</span> <span class="nav-text">About</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019"><span class="nav-number">2.</span> <span class="nav-text">2019</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Visual-SLAM几篇综述"><span class="nav-number">2.1.</span> <span class="nav-text">Visual SLAM几篇综述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Focal-Loss"><span class="nav-number">2.2.</span> <span class="nav-text">Focal Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Learning-in-Tumor-Metastatic-on-Medicine-Image"><span class="nav-number">2.3.</span> <span class="nav-text">Deep Learning in Tumor Metastatic on Medicine Image</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mixup"><span class="nav-number">2.4.</span> <span class="nav-text">Mixup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Thoracic-Disease-Identification-and-Localization-with-Limited-Supervision"><span class="nav-number">2.5.</span> <span class="nav-text">Thoracic Disease Identification and Localization with Limited Supervision</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation"><span class="nav-number">2.6.</span> <span class="nav-text">FCN: Fully Convolutional Networks for Semantic Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BoxSup-Exploiting-Bounding-Boxes-to-Supervise-Convolutional-Networks-for-Semantic-Segmentation"><span class="nav-number">2.7.</span> <span class="nav-text">BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Simple-Does-It-Weakly-Supervised-Instance-and-Semantic-Segmentation"><span class="nav-number">2.8.</span> <span class="nav-text">Simple Does It: Weakly Supervised Instance and Semantic Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FutureMapping"><span class="nav-number">2.9.</span> <span class="nav-text">FutureMapping</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#目标检测R-CNN系列"><span class="nav-number">2.10.</span> <span class="nav-text">目标检测R-CNN系列</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2020"><span class="nav-number">3.</span> <span class="nav-text">2020</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-GrabCut-for-Object-Selection"><span class="nav-number">3.1.</span> <span class="nav-text">Deep GrabCut for Object Selection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Holistically-Nested-Edge-Detection"><span class="nav-number">3.2.</span> <span class="nav-text">Holistically-Nested Edge Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Seed-Expand-and-Constrain-Three-Principles-for-Weakly-Supervised-Image-Segmentation"><span class="nav-number">3.3.</span> <span class="nav-text">Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Weakly-Supervised-Semantic-Segmentation-by-Iteratively-Mining-Common-Object-Features"><span class="nav-number">3.4.</span> <span class="nav-text">Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation"><span class="nav-number">3.5.</span> <span class="nav-text">U-Net: Convolutional Networks for Biomedical Image Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#U-Net-A-Nested-U-Net-Architecture-for-Medical-Image-Segmentation"><span class="nav-number">3.6.</span> <span class="nav-text">U-Net++: A Nested U-Net Architecture for Medical Image Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CAM—Learning-Deep-Features-for-Discriminative-Localization"><span class="nav-number">3.7.</span> <span class="nav-text">CAM—Learning Deep Features for Discriminative Localization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CRFasRNN"><span class="nav-number">3.8.</span> <span class="nav-text">CRFasRNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Efficient-Inference-in-Fully-Connected-CRFs-with-Gaussian-Edge-Potentials"><span class="nav-number">3.9.</span> <span class="nav-text">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multiscale-Combinatorial-Grouping-for-Image-Segmentation-and"><span class="nav-number">3.10.</span> <span class="nav-text">Multiscale Combinatorial Grouping for Image Segmentation and</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Weakly-and-Semi-Supervised-Learning-of-a-Deep-Convolutional-Network-for-Semantic-Image-Segmentation"><span class="nav-number">3.11.</span> <span class="nav-text">Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-to-Reweight-Examples-for-Robust-Deep-Learning"><span class="nav-number">3.12.</span> <span class="nav-text">Learning to Reweight Examples for Robust Deep Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Detecting-Lesion-Bounding-Ellipses-With-Gaussian-Proposal-Networks-GPN"><span class="nav-number">3.13.</span> <span class="nav-text">Detecting Lesion Bounding Ellipses With Gaussian Proposal Networks(GPN)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Class-Balanced-Loss-Based-on-Effective-Number-of-Samples"><span class="nav-number">3.14.</span> <span class="nav-text">Class-Balanced Loss Based on Effective Number of Samples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tell-Me-Where-to-Look-Guided-Attention-Inference-Network"><span class="nav-number">3.15.</span> <span class="nav-text">Tell Me Where to Look: Guided Attention Inference Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dice-Loss"><span class="nav-number">3.16.</span> <span class="nav-text">Dice Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Learning-Visual-Odometry"><span class="nav-number">3.17.</span> <span class="nav-text">Deep Learning + Visual Odometry</span></a></li></ol></li></ol></div></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Richard YU</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">230k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">3:29</span></div><div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.1</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script><script src="/js/utils.js?v=7.1.1"></script><script src="/js/motion.js?v=7.1.1"></script><script src="/js/affix.js?v=7.1.1"></script><script src="/js/schemes/pisces.js?v=7.1.1"></script><script src="/js/scrollspy.js?v=7.1.1"></script><script src="/js/post-details.js?v=7.1.1"></script><script src="/js/next-boot.js?v=7.1.1"></script><script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>var GUEST=["nick","mail","link"],guest="nick,mail,link";guest=guest.split(",").filter(function(e){return-1<GUEST.indexOf(e)}),new Valine({el:"#comments",verify:!0,notify:!0,appId:"duGoywhUmLrx9pM6qQhmf47c-gzGzoHsz",appKey:"m7fc8w4Di5qnAXXaJ5Gp3Pgg",placeholder:"欢迎评论！请留下你的邮箱",avatar:"mm",meta:guest,pageSize:"10",visitor:!0,lang:"zh-cn"})</script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });</script><script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>$(".highlight").not(".gist .highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea"),o=window.pageYOffset||document.documentElement.scrollTop;n.style.top=o+"px",n.style.position="absolute",n.style.opacity="0",n.readOnly=!0,n.value=e,document.body.appendChild(n),n.select(),n.setSelectionRange(0,e.length),n.readOnly=!1,document.execCommand("copy")?$(this).text("复制成功"):$(this).text("复制失败"),n.blur(),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script></body></html>