<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2"><meta name="google-site-verification" content="true"><meta name="baidu-site-verification" content="true"><link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1"><link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.1.1",sidebar:{position:"left",display:"post",offset:12,onmobile:!1,dimmer:!1},back2top:!0,back2top_sidebar:!1,fancybox:!0,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="baidu-site-verification" content="0bqk4mbBLD"><meta name="google-site-verification" content="FvMbHVgnH7FO0GFRzTmOevjSaFwvzIQZv94LdufMMPY"><meta name="description" content="​About这篇论文（链接）是印度的几位教授写的，以计算机视觉的角度来阐述目前在全监督学习和无监督学习之间的几种训练方式，写得比较简略，但是适合我这种刚入门连名词都不是很清楚的人，可以让我用来梳理整个领域的技术发展脉络和现状。同时该论文也给出了大量的参考文献，也方便进行下一步的研究。实际上，严格地说，这篇文章不能算一篇合格的综述，只能说是那种“扫盲”的阅读材料。出现上述技术发展的主要原因是数据集比"><meta name="keywords" content="survey,supervised learning,domain adaptation,deep neural network"><meta property="og:type" content="article"><meta property="og:title" content="Beyond Supervised Learning-A Computer Vision Perspective"><meta property="og:url" content="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/index.html"><meta property="og:site_name" content="自拙集"><meta property="og:description" content="​About这篇论文（链接）是印度的几位教授写的，以计算机视觉的角度来阐述目前在全监督学习和无监督学习之间的几种训练方式，写得比较简略，但是适合我这种刚入门连名词都不是很清楚的人，可以让我用来梳理整个领域的技术发展脉络和现状。同时该论文也给出了大量的参考文献，也方便进行下一步的研究。实际上，严格地说，这篇文章不能算一篇合格的综述，只能说是那种“扫盲”的阅读材料。出现上述技术发展的主要原因是数据集比"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/new%20AI%20cake.jpg"><meta property="og:image" content="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/degree%20of%20supervision.png"><meta property="og:image" content="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/conventional%20methods%20for%20DA.PNG"><meta property="og:image" content="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/incomplete-inexact-inaccurate%20supervision.PNG"><meta property="og:image" content="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/comparison%20of%20supervised%20learning%20with%20ZSL.PNG"><meta property="og:image" content="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/supervised-weakly-supervised-self-supervised%20learning.PNG"><meta property="og:updated_time" content="2020-02-03T02:28:47.261Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Beyond Supervised Learning-A Computer Vision Perspective"><meta name="twitter:description" content="​About这篇论文（链接）是印度的几位教授写的，以计算机视觉的角度来阐述目前在全监督学习和无监督学习之间的几种训练方式，写得比较简略，但是适合我这种刚入门连名词都不是很清楚的人，可以让我用来梳理整个领域的技术发展脉络和现状。同时该论文也给出了大量的参考文献，也方便进行下一步的研究。实际上，严格地说，这篇文章不能算一篇合格的综述，只能说是那种“扫盲”的阅读材料。出现上述技术发展的主要原因是数据集比"><meta name="twitter:image" content="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/new%20AI%20cake.jpg"><link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml"><link rel="canonical" href="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>Beyond Supervised Learning-A Computer Vision Perspective | 自拙集</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">自拙集</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Work cures everything</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-paperstation"><a href="/PaperStation/" rel="section"><i class="menu-item-icon fa fa-fw fa-edit"></i><br>PaperStation</a></li><li class="menu-item menu-item-mindwandering"><a href="/MindWandering/" rel="section"><i class="menu-item-icon fa fa-fw fa-paper-plane"></i><br>MindWandering</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Richard YU"><meta itemprop="description" content="Today everything exists to end in a photograph"><meta itemprop="image" content="/uploads/header.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="自拙集"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Beyond Supervised Learning-A Computer Vision Perspective</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-03-02 20:34:28" itemprop="dateCreated datePublished" datetime="2019-03-02T20:34:28+08:00">2019-03-02</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-03 10:28:47" itemprop="dateModified" datetime="2020-02-03T10:28:47+08:00">2020-02-03</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文阅读/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a></span> </span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">评论数：</span> <a href="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/" itemprop="commentCount"></span> </a></span><span id="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/" class="leancloud_visitors" data-flag-title="Beyond Supervised Learning-A Computer Vision Perspective"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">15k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">13 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>​</p><h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>这篇论文（<a href="https://link.springer.com/article/10.1007/s41745-019-0099-3" target="_blank" rel="noopener">链接</a>）是印度的几位教授写的，以计算机视觉的角度来阐述目前在全监督学习和无监督学习之间的几种训练方式，写得比较简略，但是适合我这种刚入门连名词都不是很清楚的人，可以让我用来梳理整个领域的技术发展脉络和现状。同时该论文也给出了大量的参考文献，也方便进行下一步的研究。实际上，严格地说，这篇文章不能算一篇合格的综述，只能说是那种“扫盲”的阅读材料。</p><p>出现上述技术发展的主要原因是数据集比较庞大，但是完全被标注好的数据却很少，而训练好的网络在面对现实多变的复杂场景时依然会出现问题，因此这两者之间出现了合成数据（synthetic data)，迁移学习(transfer learning)，弱监督学习(weakly supervised learning)，少量学习（Few-shot learning/K-shot learning)以及自监督学习(self-supervised learning)，边界当然就是无监督学习了(unsupervised learning)。其中需要注意，self-supervised learning和unsupervised learning不是一回事，前者仍然是有监督学习的一种，正如<a href="https://www.quora.com/What-is-the-difference-between-self-supervised-and-unsupervised-learning" target="_blank" rel="noopener">Quora</a>中的回答：</p><blockquote><p>Self-supervised, imposes classification by itself, derivative from the input data.</p><p>Unsupervised, does not. At all.</p><p>Meaning, that self-supervised, STILL has a measurement of terms of right contra wrong, as in, terms of classification.</p><p>Unsupervised, does not.</p></blockquote><a id="more"></a><ul><li>而目前，self-supervised learning正是Yann lecun在2019的ISSCC上所说的最新“蛋糕”组成成分。</li></ul><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/new AI cake.jpg" alt="new AI cake"></p><p>不过在阅读之前，首先解决一个问题是：<strong>什么是machine learning中的“label”和“feature”?</strong></p><ul><li><p>简短地说，<strong>feature就是输入，label就是输出</strong>。一个feature就是我们输入网络数据中的一列，比如说，我们要分类猫和鸟，我们输入特征可能包括颜色，大小，重量等，label就是输出的结果，就是图片的动物是猫还是鸟。</p></li><li><p>Quora上的一个<a href="https://www.quora.com/What-distinguishes-a-feature-from-a-label-in-machine-learning" target="_blank" rel="noopener">回答</a>比较系统点，这里直接贴出来：</p></li></ul><blockquote><p>Imagine how a toddler might learn to recognize things in the world. The parent, often sits with her and they read a picture book, with photos of animals. The parent teaches the toddler but pointing to the pictures and <strong>labeling</strong> them: <em>“this is a dog”, “this is a cat”, “this is a tree”, “this is a house”</em>, …</p><p>After she has learned enough examples, the toddler gets a grasp on what are the key features of a dog, vs these of a cat or of a house or of a tree. So when the toddler and her dad walk in the park, and the father points to a thing she has never seen before, and asks her “<em>what’s that?</em>“, she is capable of generalising her learning, and correctly answer “<em>it’s a dog!</em>“. What might have happened behind the scenes is that consciously or subconsciously, the toddler extracted and examined the <strong>features</strong> of the “thing” - four legs, fluffy white hair, sticking tounge, round black eyes, etc - which aligned nicely with her preception of a “dog”, and did not match any of the other entity types she knows.</p><p><strong>So labels were the ground truth categories provided explicitly by her father during learning. And features are what she inferred from these implicitly, and then extracted and examined at “run time” in the park</strong>.</p><p>Similarly, in supervised learning, the NN learns by examples: an experts gives it many training examples, each example explicitly labled with the ground truth answer. The NN tries to predict these labels by modifying the values of the parameters of the NN. When an input comes, different areas of the network becomes active (I.e receive high value), depending on the input. These represent the features of the specific input. The output of the network (e.g classification decision) is a function of which of the parameters are active.</p><p><strong>So the labels are explicitly given by the trainer during the training, and the features is the configuration of the network - which is implicitly learned by the network, as guided by the labels</strong>.</p><p>It’s these features that then define the output of the network for a given input in runtime.<br>Want to learn more or see how these features are represented? See this great article: <a href="https://distill.pub/2017/feature-visualization" target="_blank" rel="noopener">Feature Visualization</a></p></blockquote><p>整个supervised learning到unsupervised learning的发展就是人工标注的工作量太大，因为“label”这个东西需要人去手工注解，而且越精细，越准确就使训练结果越好，但是同时也会带来不适应现实数据变化的问题，鲁棒性不好。另外一方面，未标注的数据，“粗糙”的原始数据却很多，同时也比较容易抓取，所以慢慢地会向unsupervised learning发展。</p><h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>该论文的整个写作框架如下：</p><ul><li><p>Abstract</p></li><li><p>Introduction</p><ul><li>Notations and Definitions</li><li>Success of Supervised Learning</li></ul></li><li><p>Effectiveness of Synthetic Data</p></li><li><p>Domain Adaption and Transfer Learning</p></li><li><p>Weakly Supervised Learning</p><ul><li>Incomplete Supervision</li><li>Inexact Supervision</li><li>Inaccurate Supervision</li></ul></li><li><p>K-Shot Learning</p></li><li><p>Self-Supervised Learning</p></li><li><p>Conclusion and Discussion</p></li></ul><p>从整体上看，文章按照监督强度来写的，然后介绍了一些方法出现的原因，同时提及了一些关键的技术，对于更深的探讨没有做进一步地表述，而是直接给出了参考文献。</p><p>论文的内容：First, we summarize the <strong>relevant techniques that fall between the paradigm of supervised and unsupervised learning</strong>. Second, we take autonomous navigation as a running example to explain and compare different models. Finally, we highlight some shortcomings of current methods and suggest future directions.</p><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><h3 id="Reason"><a href="#Reason" class="headerlink" title="Reason"></a>Reason</h3><ul><li><p>Supervised deep learning-based techniques <strong>require a large amount of human-annotated training data</strong> to learn an adequate model. It is not viable to do so for every domain and task. Particularly, for problems in health care and autonomous navigation, collecting an exhaustive data set is either very expensive or all but impossible. (训练数据集大)</p></li><li><p>Even though supervised methods excel at learning from a large quantity of data, results show that they are particularly <strong>poor in generalizing the learned knowledge to new task or domain</strong>. This is because a majority of learning techniques assume that both the train and test data are sampled from the same distribution. (功能单一性太强)</p></li><li><p>Two bottlenecks of fully supervised deep learning methods—(1) <strong>lack of labeled data in a particular domain</strong>; (2) <strong>unavailability of direct supervision for a particular task in a given domain</strong>.</p></li></ul><h3 id="Categories-of-Methods"><a href="#Categories-of-Methods" class="headerlink" title="Categories of Methods"></a>Categories of Methods</h3><p>1.Data-centric techniques which solve the problem by <strong>generating a large amount of data similar</strong> to the one present in the original data set.</p><ul><li>Data-centric techniques include data augmentation which involves <strong>tweaking the data samples with some pre-defined transformations to increase the overall size of the data set</strong>. Another method is to use techniques borrowed from computer graphics to <strong>generate synthetic data</strong> which is used along with the original data to train the model.</li></ul><p>2.Algorithm-centric techniques which <strong>tweak the learning method to harness the limited data efficiently through various techniques</strong> like on-demand human intervention, exploiting the inherent structure of data, capitalizing on freely available data on the web or solving for an easier but related surrogate task.</p><ul><li>Algorithm-centric techniques try to <strong>relax the need of perfectly labeled data by altering the model requirements to acquire supervision through inexact , inaccurate , and incomplete labels</strong>. Another set of methods exploit the knowledge gained while <strong>learning from a related domain or task</strong> by efficiently transferring it to the test environment.</li></ul><p>3.Hybrid techniques which combine ideas from both the data and algorithm-centric methods.</p><ul><li>Hybrid methods incorporate techniques which focus on improving the performance of the model at both the data and algorithm level. （比如自动驾驶中的城市道路场景理解任务中，先用合成数据进行训练，然后进行domain shift去适应真实的场景理解任务）</li></ul><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/degree of supervision.png" alt="degree of supervision"></p><h3 id="Simple-Math-Definition"><a href="#Simple-Math-Definition" class="headerlink" title="Simple Math Definition"></a>Simple Math Definition</h3><p>假设${\cal X}$和${\cal Y}$分别是输入和标签空间（也就是输出了），在一般的机器学习问题中，我们假设要从数据集中学习N个objects。我们从这些objects提取特征去训练模型，即$X = ({\cal x}_{1}, {\cal x}_{2}, \cdots, {\cal x}_{N})$,$P(X)$是$X$上的边缘概率（marginal probability）。在fully supervised learning中，通常假设有相对应的标签$Y = ({\cal y}_{1}, {\cal y}_{2}, \cdots, {\cal y}_{N})$。</p><p>学习算法就是在假设空间（hypothesis space）${\cal F}$中寻找函数$f: {\cal X} \rightarrow {\cal Y}$，同时在空间${\cal L}$定义了损失函数（loss function)​ $l: {\cal Y} \times {\cal Y} \rightarrow \mathbb {R}^{\geq 0}$来衡量函数的适用性。同时机器学习算法也会最小化误差函数（错误预测）$R$来提升函数的正确性：</p><script type="math/tex;mode=display">R = \frac{1}{N}\sum ^{N} _{n=0}l({\cal y}_{i}, f({\cal x}_{i}))</script><p>对synthetic data来说，输入空间发生了变换，设为${\cal X}_{synth}$，标签空间不变，此外，由于输入特征空间个边缘概率分布都发生了变化，因此利用新的domain${\cal D}_{synth}=\lbrace {\cal X}_{synth}, P({\cal X}_{synth}) \rbrace$来代替原来的real domain${\cal D}=\lbrace {\cal X}, P({\cal X}) \rbrace$。因此，我们也不可能用之前的预测函数$f_{synth}: {\cal X}_{synth} \rightarrow {\cal Y}$ 来构建${\cal X}$到${\cal Y}$的映射。</p><p>迁移学习（transfer learning）就是用于解决domain adaptation(DA)问题的技术，其不仅可以用于不同domain之间，也可以用于不同的task之间。根据input feature space 在source and target input distribution的分布是否相同，即 ${\cal X}_{s}$是否等于${\cal X}_{t}$，DA可以分为homogeneous DA和heterogeneous DA（同质和异质），显然heterogeneous DA问题要更为复杂些。</p><p>通常情况下，训练时，supervised learning认为所有的feature sets ${\cal x}_{i}$会有相应的标签${\cal y}_{i}$与之对应，实际情况是，这些标签在实际场景中可能是 $ {\tt incomplete, inexact, inaccurate}$的，因此可能就要在weakly supervised learning技术框架下训练，比如说针对那些在网络抓取的数据而言。针对incomplete的标签场景而言，定义feature set $X = ({\cal x}_{1}, {\cal x}_{2}, \cdots, {\cal x}_{l}, {\cal x}_{l+1}, \cdots, {\cal x}_{n})$，其中$X_{labeled} = ({\cal x}_{1}, {\cal x}_{2}, \cdots, {\cal x}_{l})$会有对应的标签$Y_{labeled} = ({\cal y}_{1}, {\cal y}_{2}, \cdots, {\cal y}_{l})$供其训练，但是$X_{unlabeled} = ({\cal x}_{l+1} \cdots, {\cal x}_{n})$就没有任何对应的标签了。此外，一些其他的weakly supervised模型包含一些有着多种标签的单个实例或者多个实例共享一个标签（multiple-instace single-label)。这个时候会对feature set中的${\cal x}_{i}$进行打包处理，即${\cal x}_{i,j}, j=1,2,\cdots, m$。</p><p>尽管上述技术框架对应着不同程度的supervision，但是都需要大量的实例instances $X$来训练模型。如果某些class没有足够的instances的话就会使得训练不理想，因此出现了Few-shot learning(少量学习)和Zero-shot learning(ZSL)。</p><p>如果没有了监督信号（supervision signal），可以利用instances的内在结构（inherent structure）去训练模型。假设$X$和$Y$分别是feature set和label set，此时$P(Y|X)$无法得出，也无法确立任务${\cal T} = \lbrace {\cal Y}, P(Y|X) \rbrace$，不过我们可以定义一个proxy task${\cal T}_{proxy} = \lbrace Z, P(Z|X)\rbrace$，label set$Z$可以自己从数据中提取。For computer vision problems, proxy tasks have been defined based on spatial and temporal alignment, color, and motion cues.</p><h3 id="Effectiveness-of-Synthetic-Data（个人感觉还是与RL相关的）"><a href="#Effectiveness-of-Synthetic-Data（个人感觉还是与RL相关的）" class="headerlink" title="Effectiveness of Synthetic Data（个人感觉还是与RL相关的）"></a>Effectiveness of Synthetic Data（个人感觉还是与RL相关的）</h3><p>supervised learning在很多方面都取得了成功，但是存在的问题是需要大量的标注数据进行训练，即使是训练好了，也难以适应现实环境。</p><p>随着计算机图形学的发展，synthetic data十分易得，而且提供了精确的ground truth，同时数据的可操作性可以让其模拟任何真实的环境场景，甚至是真实环境下难以发生的。</p><p>In the visual domain, synthetic data have been used mainly for two purposes: (1) evaluation of the generalizability of the model due to the large variability of synthetic test examples, and (2) aiding the training through data augmentation for tasks where it is difficult to obtain ground truth, e.g., optical flow or depth perception.(测试模型的普适性和增强训练过程)</p><p>此外，还有直接在真实图像中加入synthetic data，比如在KITTI数据集中加入车辆3D模型，以辅助模型训练。</p><p>One drawback of using syntheticdata for training a model is that it gives rise to <a href="https://www.lyrn.ai/2018/12/30/sim2real-using-simulation-to-train-real-life-grasping-robots/" target="_blank" rel="noopener">“sim2real” </a>domain gap. 这个sim2real是与RL相关的概念，我这里找了一个CMU的<a href="http://www.andrew.cmu.edu/course/10-703/slides/Lecture_sim2realmaxentRL.pdf" target="_blank" rel="noopener">PPT</a>，可以看下。Recently, a stream of works in domain randomization claims to generate synthetic data with sufficient variations, such that the <strong>model views real data as just another variation of the synthetic data set.</strong> 将真实的数据视为合成数据的一个变体。</p><p>One of the major challenges in using synthetic data for training is the domain gap between real and synthetic data sets. 而迁移学习它提供了一些解决办法。</p><h3 id="Domain-Adaptation-and-Transfer-Learning"><a href="#Domain-Adaptation-and-Transfer-Learning" class="headerlink" title="Domain Adaptation and Transfer Learning"></a>Domain Adaptation and Transfer Learning</h3><p>A model trained on source domain does not perform well on a target domain with <strong>different distribution.</strong> Domain adaptation(DA) is a technique which addresses this issue by reusing the knowledge gained through the source domain for the target domain.</p><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/conventional methods for DA.PNG" alt="conventional methods for DA"></p><p>DA techniques根据三个不同的标准进行分类，这三个标准（criteria）是：</p><ul><li><p>distance between domains</p></li><li><p>presence of supervision in the source and target domain</p></li><li><p>type of domain divergences</p></li></ul><p>Prevalent literature also classifies DA in supervised, semi-supervised, and unsupervised setting according to the presence of labels in source and target domain.</p><p>Earlier works categorized the domain adaptation problem into homogeneous and heterogeneous settings. 现在，随着深度学习的热度提高，DA开始引入DNN和GAN的思想。DA使用DNN 去learn representations invariant to the domain，Adversarial methods encompass a framework which consists of a label classifier trained adversarially to the domain classifier.</p><h3 id="Weakly-Supervised-Learning"><a href="#Weakly-Supervised-Learning" class="headerlink" title="Weakly Supervised Learning"></a>Weakly Supervised Learning</h3><p>Weakly supervised learning is an umbrella term covering the <strong>predictive models which are trained under incomplete, inexact, or inaccurate labels</strong>. Apart from saving annotation cost and time, weakly supervised methods have proven to be robust to change in the domain during testing.</p><h4 id="Incomplete-Supervision-标签不全"><a href="#Incomplete-Supervision-标签不全" class="headerlink" title="Incomplete Supervision(标签不全)"></a>Incomplete Supervision(标签不全)</h4><p>Weakly supervised techniques pertaining incomplete labels make use of either semi-supervised or active learning methods. The conventional semi-supervised approaches include self-training, co-training, and graph-based methods.</p><h4 id="Inexact-Supervision-标签注解程度，比如一幅图像的bounding-box标注和pixel-level标注"><a href="#Inexact-Supervision-标签注解程度，比如一幅图像的bounding-box标注和pixel-level标注" class="headerlink" title="Inexact Supervision(标签注解程度，比如一幅图像的bounding box标注和pixel-level标注)"></a>Inexact Supervision(标签注解程度，比如一幅图像的bounding box标注和pixel-level标注)</h4><p>Apart from dealing with partially labeled data sets, weakly supervised techniques also help <strong>relax the degree of annotation</strong> needed to solve a structured prediction problem.</p><p>A popular approach to harness inexact labels is to formulate the problem in <strong>multiple-instance learning (MIL) framework</strong>. In MIL, the image is interpreted as a <strong>bag of patches.</strong> If one of the patches within the image contains the object of interest, the image is labeled as a positive instance, otherwise negative. Learning scheme alternates between estimating object appearance model and predicting the patches within positive image.</p><h4 id="Inaccurate-Supervision-策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。-“webly”-supervised-scenario"><a href="#Inaccurate-Supervision-策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。-“webly”-supervised-scenario" class="headerlink" title="Inaccurate Supervision(策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。 “webly” supervised scenario)"></a>Inaccurate Supervision(策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。 “webly” supervised scenario)</h4><p>Broadly, we categorize the techniques into two sets—the first approach resorts to <strong>treating the noisy instances as outliers and discard them during training</strong>. Another stream of methods focus on <strong>building algorithms robust to noise</strong> by devising noise-tolerant loss functions or adding appropriate regularization terms.</p><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/incomplete-inexact-inaccurate supervision.PNG" alt="incomplete-inexact-inaccurate supervision"></p><h3 id="K-Shot-Learning-样本少-Few-shot-learning和Zero-shot-learning"><a href="#K-Shot-Learning-样本少-Few-shot-learning和Zero-shot-learning" class="headerlink" title="K-Shot Learning(样本少-Few-shot learning和Zero-shot learning)"></a>K-Shot Learning(样本少-Few-shot learning和Zero-shot learning)</h3><p>Few-shot learning techniques attempt to adapt the current machine learning methods to perform well under a scenario where only a few training instances are available per class. More recent efforts into a few-shot learning techniques can be broadly categorized into <strong>metric-learning</strong> and <strong>meta-learning-based </strong>methods.</p><ul><li><p>Metric-learning aims to design techniques for<strong>embedding the input instances to a feature space</strong> beneficial to few-shot settings. A common approach is to find a good similarity metric in the new feature space applicable to novel categories.</p></li><li><p>Meta-learning entails a class of approaches which quickly adapt to a new task using only a few data instances and training iterations. To achieve this, the model is <strong>trained on a set of tasks</strong>, such that it transfers the “learning ability” to a novel task. In other words, <strong>meta-learners treat the tasks as training examples</strong>.</p></li></ul><p>Another set of methods for few-shot learning relies on<strong>efficient regularization techniques </strong>to <strong>avoid over-fitting</strong> on the small number ofinstances.</p><p>Literature pertaining to Zero-Shot Learning (ZSL) focuses on finding the representation of a novel category without any instance. <strong>Methods used to address ZSL are distinct from few-shot learning.</strong> A major assumptiontaken in this setting is that<strong> the classes observed by model during training are semantically related to the unseen classes encountered during testing.</strong>This semantic relationship is often captured through class-attributes containing shape, color, pose, etc., of the object which are <strong>either labeled by experts or obtained through knowledge sources </strong>such as Wikipedia, Flickr, etc.</p><p>In ZSL, a joint embedding space is learned during training where <strong>both the visual features and semantic vectors are projected</strong>. During testing on unseen classes, <strong>nearest-neighbor search</strong> is performed in this embedding space to match the projection of visual feature vector against a novel object type.</p><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/comparison of supervised learning with ZSL.PNG" alt="comparison of supervised learning with ZSL"></p><h3 id="Self-supervised-Learning-without-any-external-supervision"><a href="#Self-supervised-Learning-without-any-external-supervision" class="headerlink" title="Self-supervised Learning(without any external supervision)"></a>Self-supervised Learning(without any external supervision)</h3><p>Explicit annotation pertaining to the main task is avoided by<strong>defining an auxiliary task that provides a supervisory signal in self-supervised learning</strong>. The assumption is that successful training of the model on the auxiliary task will inherently make it <strong>learn semantic concepts such as object classes and boundaries</strong>. This makes it possible to share knowledge between two tasks.</p><p>However, unlike transfer learning, it does not require a large amount of annotated data from another domain or task.</p><p>The existing literature pertaining self-supervision relies on <strong>using the spatial and temporal context of an entity for “free” supervision signal.</strong></p><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/supervised-weakly-supervised-self-supervised learning.PNG" alt="supervised learning/weakly-supervised learning/self-supervised learning"></p><p>上图比较了supervised learning / weakly-supervised learning / self-supervised learning之间的区别，supervised learning需要bounding box作为label进行训练 ；weakly-supervised learning使用图像级别的标题，语义描述嵌入等进行神经网络预训练；self-supervised learning使用pretext task来学习物体的表示方法。</p><h3 id="Conclusion-and-Discussion"><a href="#Conclusion-and-Discussion" class="headerlink" title="Conclusion and Discussion"></a>Conclusion and Discussion</h3><p>The space between fully supervised and unsupervised learning can be qualitatively divided on the basis of the degree of supervision needed to learn the model.</p><ul><li><p>While synthetic data are cost effective and flexible alternative to real-world data sets, the models learned using it still need to be adapted to the real-world setting.</p></li><li><p>Transfer learning techniques address this issue by explicitly aligning different domains through discrepancy-based or adversarial approaches. However, <strong>both of these techniques require “strict” annotation pertaining to the task which hinders the generalization capability of the model.</strong></p></li><li><p>Weakly supervised algorithms <strong>relax the need of exact supervision by making the learning model tolerant of incomplete, inexact, and inaccurate supervision.</strong> This helps the model to harness the huge amount of data available on the web.</p></li><li><p>Even when a particular domain contains an insufficient number of instances, methods in k-shot learning try to build a reasonable model using parameter regularization or meta-learning techniques.</p></li><li><p>Finally, self-supervised techniques <strong>completely eliminate the need of annotation as they define a proxy task for which annotation is implicit within the data instances.</strong></p></li><li><p>Despite their success, recent models weigh heavily on deep neural networks for their performance. Hence they carry both the pros and cons of using these models; <strong>cons being lack of interpretability and outcomes which largely depend on hyperparameters.</strong></p></li></ul></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Richard YU</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/" title="Beyond Supervised Learning-A Computer Vision Perspective">http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/survey/" rel="tag"><i class="fa fa-tag"></i> survey</a> <a href="/tags/supervised-learning/" rel="tag"><i class="fa fa-tag"></i> supervised learning</a> <a href="/tags/domain-adaptation/" rel="tag"><i class="fa fa-tag"></i> domain adaptation</a> <a href="/tags/deep-neural-network/" rel="tag"><i class="fa fa-tag"></i> deep neural network</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/" rel="next" title="A Brief Review of Object Detection and Semantic Segmentation"><i class="fa fa-chevron-left"></i> A Brief Review of Object Detection and Semantic Segmentation</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2019/03/12/visual-SLAM-by-Gaoxiang-1/" rel="prev" title="visual SLAM by Gaoxiang(1)">visual SLAM by Gaoxiang(1) <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/header.jpg" alt="Richard YU"><p class="site-author-name" itemprop="name">Richard YU</p><div class="site-description motion-element" itemprop="description">Today everything exists to end in a photograph</div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">11</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">51</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/richardyu114" title="GitHub &rarr; https://github.com/richardyu114" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Yu1145635107" title="Twitter &rarr; https://twitter.com/Yu1145635107" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a> </span><span class="links-of-author-item"><a href="https://instagram.com/d.h.richard" title="Instagram &rarr; https://instagram.com/d.h.richard" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a> </span><span class="links-of-author-item"><a href="https://weibo.com/u/5211687990" title="Weibo &rarr; https://weibo.com/u/5211687990" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a> </span><span class="links-of-author-item"><a href="https://www.douban.com/people/161993653/" title="豆瓣 &rarr; https://www.douban.com/people/161993653/" rel="noopener" target="_blank"><i class="fa fa-fw fa-paperclip"></i>豆瓣</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#About"><span class="nav-number">1.</span> <span class="nav-text">About</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Structure"><span class="nav-number">2.</span> <span class="nav-text">Structure</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Content"><span class="nav-number">3.</span> <span class="nav-text">Content</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reason"><span class="nav-number">3.1.</span> <span class="nav-text">Reason</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Categories-of-Methods"><span class="nav-number">3.2.</span> <span class="nav-text">Categories of Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Simple-Math-Definition"><span class="nav-number">3.3.</span> <span class="nav-text">Simple Math Definition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Effectiveness-of-Synthetic-Data（个人感觉还是与RL相关的）"><span class="nav-number">3.4.</span> <span class="nav-text">Effectiveness of Synthetic Data（个人感觉还是与RL相关的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Domain-Adaptation-and-Transfer-Learning"><span class="nav-number">3.5.</span> <span class="nav-text">Domain Adaptation and Transfer Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Weakly-Supervised-Learning"><span class="nav-number">3.6.</span> <span class="nav-text">Weakly Supervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Incomplete-Supervision-标签不全"><span class="nav-number">3.6.1.</span> <span class="nav-text">Incomplete Supervision(标签不全)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inexact-Supervision-标签注解程度，比如一幅图像的bounding-box标注和pixel-level标注"><span class="nav-number">3.6.2.</span> <span class="nav-text">Inexact Supervision(标签注解程度，比如一幅图像的bounding box标注和pixel-level标注)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inaccurate-Supervision-策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。-“webly”-supervised-scenario"><span class="nav-number">3.6.3.</span> <span class="nav-text">Inaccurate Supervision(策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。 “webly” supervised scenario)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-Shot-Learning-样本少-Few-shot-learning和Zero-shot-learning"><span class="nav-number">3.7.</span> <span class="nav-text">K-Shot Learning(样本少-Few-shot learning和Zero-shot learning)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-supervised-Learning-without-any-external-supervision"><span class="nav-number">3.8.</span> <span class="nav-text">Self-supervised Learning(without any external supervision)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conclusion-and-Discussion"><span class="nav-number">3.9.</span> <span class="nav-text">Conclusion and Discussion</span></a></li></ol></li></ol></div></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Richard YU</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">240k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">3:38</span></div><div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.1</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script><script src="/js/utils.js?v=7.1.1"></script><script src="/js/motion.js?v=7.1.1"></script><script src="/js/affix.js?v=7.1.1"></script><script src="/js/schemes/pisces.js?v=7.1.1"></script><script src="/js/scrollspy.js?v=7.1.1"></script><script src="/js/post-details.js?v=7.1.1"></script><script src="/js/next-boot.js?v=7.1.1"></script><script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>var GUEST=["nick","mail","link"],guest="nick,mail,link";guest=guest.split(",").filter(function(e){return-1<GUEST.indexOf(e)}),new Valine({el:"#comments",verify:!0,notify:!0,appId:"duGoywhUmLrx9pM6qQhmf47c-gzGzoHsz",appKey:"m7fc8w4Di5qnAXXaJ5Gp3Pgg",placeholder:"欢迎评论！请留下你的邮箱",avatar:"mm",meta:guest,pageSize:"10",visitor:!0,lang:"zh-cn"})</script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });</script><script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>$(".highlight").not(".gist .highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea"),o=window.pageYOffset||document.documentElement.scrollTop;n.style.top=o+"px",n.style.position="absolute",n.style.opacity="0",n.readOnly=!0,n.value=e,document.body.appendChild(n),n.select(),n.setSelectionRange(0,e.length),n.readOnly=!1,document.execCommand("copy")?$(this).text("复制成功"):$(this).text("复制失败"),n.blur(),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script></body></html>