<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2"><meta name="google-site-verification" content="true"><meta name="baidu-site-verification" content="true"><link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1"><link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.1.1",sidebar:{position:"left",display:"post",offset:12,onmobile:!1,dimmer:!1},back2top:!0,back2top_sidebar:!1,fancybox:!0,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="baidu-site-verification" content="0bqk4mbBLD"><meta name="google-site-verification" content="FvMbHVgnH7FO0GFRzTmOevjSaFwvzIQZv94LdufMMPY"><meta name="description" content="​About目标检测（object detection）和语义分割（semantic segmentation）是计算机视觉的两个重要研究内容，在人脸检测，视频监控和自动驾驶中都有很多的应用，它们对于机器理解环境也具有一定的作用。在视觉SLAM上，已经有科研工作者将这些研究成果加入到现有的SLAM系统中，以提高系统的鲁棒性，同时促进语义SLAM的发展。因此，需要对这些工作进行一些调研总结和学习理解"><meta name="keywords" content="computer vision,deep learning,object detection,semantic segmentation"><meta property="og:type" content="article"><meta property="og:title" content="A Brief Review of Object Detection and Semantic Segmentation"><meta property="og:url" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/index.html"><meta property="og:site_name" content="自拙集"><meta property="og:description" content="​About目标检测（object detection）和语义分割（semantic segmentation）是计算机视觉的两个重要研究内容，在人脸检测，视频监控和自动驾驶中都有很多的应用，它们对于机器理解环境也具有一定的作用。在视觉SLAM上，已经有科研工作者将这些研究成果加入到现有的SLAM系统中，以提高系统的鲁棒性，同时促进语义SLAM的发展。因此，需要对这些工作进行一些调研总结和学习理解"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/focus_of_object_detection_based_on_DL.PNG"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/task%20definition.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/developing_route_of_object_detection.jpeg"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/recent%20progress%20in%20object%20detection.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/two-stage%20detector.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/single-stage%20detector.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/one_two_stage_comparison.PNG"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/architecture_of_refinedet.PNG"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/RPN.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/alternating%20training.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/joint%20training.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/four%20ways%20of%20FPN.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/pipeline%20of%20FPN.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/mask%20branch.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/cascade%20architecture.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/RetinaNet.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/relation%20network.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/SNIP.png"><meta property="og:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/cornerNet.png"><meta property="og:updated_time" content="2020-02-03T02:28:47.260Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="A Brief Review of Object Detection and Semantic Segmentation"><meta name="twitter:description" content="​About目标检测（object detection）和语义分割（semantic segmentation）是计算机视觉的两个重要研究内容，在人脸检测，视频监控和自动驾驶中都有很多的应用，它们对于机器理解环境也具有一定的作用。在视觉SLAM上，已经有科研工作者将这些研究成果加入到现有的SLAM系统中，以提高系统的鲁棒性，同时促进语义SLAM的发展。因此，需要对这些工作进行一些调研总结和学习理解"><meta name="twitter:image" content="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/focus_of_object_detection_based_on_DL.PNG"><link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml"><link rel="canonical" href="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>A Brief Review of Object Detection and Semantic Segmentation | 自拙集</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">自拙集</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Work cures everything</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-paperstation"><a href="/PaperStation/" rel="section"><i class="menu-item-icon fa fa-fw fa-edit"></i><br>PaperStation</a></li><li class="menu-item menu-item-mindwandering"><a href="/MindWandering/" rel="section"><i class="menu-item-icon fa fa-fw fa-paper-plane"></i><br>MindWandering</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Richard YU"><meta itemprop="description" content="Today everything exists to end in a photograph"><meta itemprop="image" content="/uploads/header.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="自拙集"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">A Brief Review of Object Detection and Semantic Segmentation</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-02-27 14:58:38" itemprop="dateCreated datePublished" datetime="2019-02-27T14:58:38+08:00">2019-02-27</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-03 10:28:47" itemprop="dateModified" datetime="2020-02-03T10:28:47+08:00">2020-02-03</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/科研记录/" itemprop="url" rel="index"><span itemprop="name">科研记录</span></a></span> </span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">评论数：</span> <a href="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/" itemprop="commentCount"></span> </a></span><span id="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/" class="leancloud_visitors" data-flag-title="A Brief Review of Object Detection and Semantic Segmentation"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">7k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">6 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>​</p><h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>目标检测（object detection）和语义分割（semantic segmentation）是计算机视觉的两个重要研究内容，在人脸检测，视频监控和自动驾驶中都有很多的应用，它们对于机器理解环境也具有一定的作用。在视觉SLAM上，已经有科研工作者将这些研究成果加入到现有的SLAM系统中，以提高系统的鲁棒性，同时促进语义SLAM的发展。因此，需要对这些工作进行一些调研总结和学习理解。由于对深度学习不了解，还未系统地进行学习，所以写的内容可能幼稚了些，后续会再进行更新。</p><p>值得注意的是，目标检测在经典计算机视觉中也已经提出了一些特征描述的方法，在此不涉及，只涉及基于深度学习的目标检测方法，毕竟神经网络在图片特征的学习上比较擅长。</p><p>本博客将基于文末给出的参考文献来进行总结和学习，尽力把这些问题和技术梳理清楚，同时加入一些自己的思考和不成熟的想法。</p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/focus_of_object_detection_based_on_DL.PNG" alt="focus on method based on deep learning"></p><h2 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h2><ol><li>image classification—multi level 图像分类，这个任务指的是给出一张图片，要识别出哪张图片属于哪个类别；</li><li>object detection—what categories and location 物体检测，这个任务除了需要识别出图片属于哪个类别，还需要对相应的物体进行具体位置的定位，我们通常用矩形框来框出这个物体；</li><li>semantic segmentation—pixel wise, but does not distinct everything in one category 语义分割，这个任务是指对图片中的每个 pixel 打上标签，比如这里要给它们打上 person、sheep、dog 的标签，需要进行非常精细的分类；</li><li>instance segmentation—object detection+semantic segmentation 实例分割，可以理解为进行物体检测后，对每个矩形框中的物体进行语义分割，该任务除了需要找到物体的类别和位置之外，还需要分辨出不同物体的 pixel；</li></ol><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/task definition.png" alt="task definition"></p><a id="more"></a><h2 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h2><h3 id="Progress-一步法single-stage-两步法two-stage"><a href="#Progress-一步法single-stage-两步法two-stage" class="headerlink" title="Progress (一步法single-stage, 两步法two stage)"></a>Progress (一步法<font color="DeepSkyBlue">single-stage</font>, 两步法two stage)</h3><p>最近有一篇很好的综述<a href="https://arxiv.org/pdf/1809.02165v2.pdf" target="_blank" rel="noopener">[5]</a>详细地梳理了最近5年来的generic object detection技术发展和比较的综述。</p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/developing_route_of_object_detection.jpeg" alt="object detection developing route"></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/recent progress in object detection.png" alt="progress"></p><p>上图绿色的字体表示的是 Two-stage Detector 的发展历程，当然还有很多其他的方法，这里列出的是一些比较有代表性的方法。</p><ul><li><p>2014 年有一项很重要的工作是 R-CNN，它是将物体检测首次应用于深度学习中的一篇论文，它的主要思路是将物体检测转化为这么一个问题：首先找到一个 region（区域），然后对 region 做分类。之后作者又提出了 Fast R-CNN，它是一个基于 R-CNN 的算法，运算速度显著提高。</p></li><li><p>2015 年，这一群人又提出了 Faster R-CNN，它在速度上相比 Fast R-CNN 有了更大的提高，主要是改进了怎样在 Fast R-CNN 和 R-CNN 中找 region 的过程，Faster R-CNN 也是用深度学习的方法得到一些 region（称之为 proposal），然后再用这些 proposal 来做分类。虽然距离 Faster R-CNN 的提出已经三年多了，但它依旧是使用非常广泛的一种算法。</p></li><li><p>2016 年，代季峰等人提出了 R-FCN，它在 Faster R-CNN 的基础上进行了改进，当时它在性能和速度都有非常大的提高。</p></li><li><p>2017 年有两篇影响力非常大的论文，FPN 和 Mask R-CNN。FPN 也就是 Feature Pyramid Network，何恺明大神的论文，它相当于生成了 feature pyramid，然后再用多个 level 的 feature 来做 prediction。Mask R-CNN 这篇论文获得了 ICCV 2017 的最佳论文，也是何恺明大神的作品，其在 Faster R-CNN 基础上增加了 mask branch，可以用来做实例分割，同时因为有 multi-task learning，因此它对物体框的性能也有很大的提高。（另外，今年年初，2019.1月，何恺明将自己的Mask R-CNN和FPN合在了一起，效果也不错）。</p></li><li><p>2018 年，沿着 Faster R-CNN 这条路线提出的方法有 Cascade R-CNN，它将 cascade 结构用在了 Faster R-CNN 中，同时也解决了一些 training distribution 的一些问题，因此它的性能是比较高的。另外还有两篇比较重要的论文——Relaiton Network 和 SNIP。</p></li></ul><p>上图蓝色字体表示的 Single-stage Detector 的发展历程：2014 年的 MultiBox 是非常早期的工作；2016 年提出了 SSD 和 YOLO，这两篇论文是 Single-stage Detector 中的代表作；2017 年提出了 RetinaNet（当时 Single-stage Detector 中性能最高的方法）和 YOLO v2；2018 年有一个新的思路，提出了 CornerNet，把物体检测看成一堆点的检测，然后将这些点 group 起来。</p><h3 id="General-Pipeline-of-Object-Detection"><a href="#General-Pipeline-of-Object-Detection" class="headerlink" title="General Pipeline of Object Detection"></a>General Pipeline of Object Detection</h3><p>参考文献<a href="https://www.cnblogs.com/skyfsm/p/6806246.html" target="_blank" rel="noopener">[2]</a>很好地解释了深度学习进行object detection的思路是什么。</p><p>如前所述，基于深度学习的Object Detection的方法主要有两条发展脉络，一个是<strong>single-stage detector</strong>，另一个是<strong>two-stage detector</strong>.</p><p><strong>Two-stage detector</strong></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/two-stage detector.png" alt="single-stage detector"></p><ul><li><p>第一部分是 Feature generation：首先图片经过 backbone（分类网络）后，会生成 feature； 之后 feature 或者直接进行 prediction，或者再过一个 neck 进行修改或增强；</p></li><li><p>第二部分是 Region proposal：<strong>这个部分是 Two-stage Detector 的第一个 stage</strong>，其中会有一堆 sliding window anchor（预先定义好大小的框），之后对这些框做 dense 的分类和回归；接着再筛除大部分 negative anchor，留出可能是物体的框，这些框称之为 proposal；</p></li><li><p>第三个部分是 Region recognition：有了 proposal 后，在 feature map 上利用 Rol feature extractor 来提取出每一个 proposal 对应的 feature（Rol feature），最后会经过 task head；</p></li></ul><p><strong>Single-stage detector</strong></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/single-stage detector.png" alt="single-stage detector"></p><ul><li>图片首先经过 feature generation 模块，这里也有 sliding window anchor，但是它们不是用来生成 proposal 的，而是直接用于做最终的 prediction，通过 dense 的分类和回归，能直接生成最终的检测结果。</li></ul><p><strong>一步法和两步法的区别到底在哪里？</strong></p><p>实际上，现在的二步法object detection的框架都是基于Faster R-CNN的，一步法中的比较全面的算法是SSD，这两个在后面我都会进行详细地阐述。</p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/one_two_stage_comparison.PNG" alt="comparison of one stage and two stage"></p><p>对于两步法的基础Faster R-CNN，两个阶段分别在RPN和R-CNN中完成，第一阶段是预设一系列不同大小和比例的anchors，然后将整张图传入CNN提取特征，最后利用PRN对anchors进行分类和回归，得到候选区域（proposals）；第二阶段是利用RolPooling扣取每个候选区域的特征，接着把扣取特征的特征送入后续R-CNN网络，最后对候选区域进一步分类和回归，得到最终的检测结果。</p><p>由上图也可以看出，两步法是相对于一步法多了二阶段的分类，回归和特征，因而精度会更好，但是会使得算法运行的时间加长。也就是说，<strong>一步法也就是类似于只有RPN阶段，而二步法多了R-CNN</strong>，使得分类更加精细，同时再对候选区域进行回归。</p><p><strong>那么一步法是否可以仿照这样的思路或者利用其他的方法达到二步法的精度，却又不损失运行效率？</strong></p><p>在参考文献<a href="https://mp.weixin.qq.com/s/84JG1ZGFKb6Xp3WQFjtxZw" target="_blank" rel="noopener">[3]</a>里面，张士峰博士分享了他们的工作<a href="https://github.com/sfzhang15/RefineDet" target="_blank" rel="noopener">Single-Shot Refinement Neural Network for Object Detection</a>，在SSD中加入一些类似R-CNN作用的模块，来提高检测的精度，同时也保持了原有的运行效率。</p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/architecture_of_refinedet.PNG" alt="RefineDet"></p><p>整个结构包含ARM模块，TCB连接和ODM模块，其中ARM模块和ODM模块分别对应二步法中的RPN和R-CNN的功能，TCB的主要作用是转换ARM特征，融合高层特征，因为两步法中的特征不相同，第二阶段会提取新的特征，因此需要转换和融合。<strong>该方法与二步法的区别在于没有RoIPooling</strong>，因此运行时间不会边长。该方法的精度大概较SSD提升了两个点，速度上会更快些，因为输入图像的分辨率可以是320x320的。具体的检测框架和测试结果可以看<a href="https://arxiv.org/pdf/1711.06897v3.pdf" target="_blank" rel="noopener">原论文</a>。</p><hr><p>借此提下object detection近年来的发展趋势应当不再是该网络调参来刷速度和正确率，而是回归到框架本身的反思和设计上，毕竟现有的算法都是从Faster R-CNN来的，因此是否可以反思该框架的问题，然后对其进行改善，或者直接提出新的检测思路，比如说在前处理阶段的anchors和后处理阶段的NMS都是手动设置的，这个是否可以进行自动调节，也是一个值得研究的点。</p><p>另外，在视觉SLAM和很多其他的应用场景上，输入的都是视频流的形式，而不是单个的图像，这一方面对算法的实时性，硬件的运算速度提出了要求，另一方面也对视频物体检测提出了要求，如何利用帧与帧之间的信息来加速物体检测，也是有待研究的。</p><p>最后一点就是，视觉SLAM的最终梦想是能在现实大环境下进行运行，然后给出环境地图，以此与人进行交互。现实场景很复杂，充满动态的，不确定性的因素，因此这样的功能要想实现还有很长的路要走。不过可以预见的是，视觉SLAM需要结合这些新的，基于学习方法的计算机视觉的研究成果，以帮助机器人来理解环境，提高系统的鲁棒性，同时各个模块和任务之间也将是相互联系相互促进的，多任务联合（multi-task），以此提升系统的容错性 和运行时间。</p><hr><h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p>Faster R-CNN 是 Two-stage Detector 工作的基础，它主要提出了两个东西:</p><ul><li><p><strong>RPN</strong>：即 Region Proposal Network，目前是生成 proposal 的一个标准方式;</p></li><li><p><strong>Traning pipeline</strong>：主要讲的是 Two-stage Detector 应该怎样 train，论文里给出的是一种交替(alternating training)的方法;</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/RPN.png" alt="RPN"></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/alternating training.png" alt="alternating training"></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/joint training.png" alt="joint training"></p><p>因为深度学习的发展，大家都不想一步一步来回调参，因此会有了<strong>joint training</strong>这个东西。</p><h3 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h3><ul><li><p>FPN（Feature Pyramid Network）主要也是提出了两项重要的思路：<strong>Top-down pathway 和 Multi-level prediction。</strong></p></li><li><p>下图中的 4 张图代表了基于单个或多个 feature map 来做预测的 4 种不同的套路：</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/four ways of FPN.png" alt="four ways of FPN"></p><ul><li>具体实现如下图所示：</li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/pipeline of FPN.png" alt="pipeline of FPN"></p><h3 id="MASK-R-CNN"><a href="#MASK-R-CNN" class="headerlink" title="MASK R-CNN"></a>MASK R-CNN</h3><p>Mask R-CNN 主要也有两点贡献：<strong>RoIAlign</strong>和<strong>Mask branch</strong></p><ul><li><p>RoIAlign：在 Mask R-CNN 之前，大家用得比较多的是 Rol Pooling，实现过程是：给出一个框，在 feature map 上 pool 出一个固定大小的 feature，比如要 pool 一个 2×2 的 feature，首先把这个框画出 2×2 的格子，每个格子映射到 feature map，看它覆盖了多少个点，之后对这些点做 max pooling，这样就能得出一个 2×2 的 feature。它的劣势是如果框稍微偏一点，得出的 feature 却可能是一样的，存在截断误差。RolAlign 就是为了解决这一问题提出的。Rol Align 并不是直接对框内的点做 max pooling，而是用双线性插值的方式得到 feature。其中还有一步是：在 2×2 的每个框中会选择多个点作为它的代表，这里选择了 4 个点，每个点分别做双线性插值，之后再让这 4 个点做 max/average pooling</p></li><li><p>Mask branch：它是一个非常精细的操作，也有额外的监督信息，对整个框架的性能都有所提高。它的操作过程如下图所示:</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/mask branch.png" alt="mask branch"></p><h3 id="Cascade-R-CNN"><a href="#Cascade-R-CNN" class="headerlink" title="Cascade R-CNN"></a>Cascade R-CNN</h3><ul><li><p>Cascade R-CNN 是目前 Faster R-CNN 这条线中较新的方法。这个方法也提出了两点贡献：一是提出了使用 <strong>cascade architecture</strong>；二是提出了怎样来适应 <strong>training distribution</strong>。</p></li><li><p>Cascade architecture ：这个框架不是特别新的东西，之前也有类似的结构。下图左边是 Faster R-CNN，右边是 Cascade R-CNN。其中 I 代表图像或者图像生成的 feature map，H0 是 RPN，B 是 bounding box regression，C 是 classification。经过 RPN 得到的 proposal 再做 pooling 后，会有分类和回归这两个 prediction。</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/cascade architecture.png" alt="cascade architecture"></p><ul><li>Cascade R-CNN 的结构是，在经过第一次分类和回归之后，会用得到 bounding box 再来做一次 pooling，然后对这些框做下一阶段的分类和回归，这个过程可以重复多次。但如果仅仅使用 Cascade R-CNN 而不做其他改变，Cascade R-CNN 带来的提高是非常有限的。 Cascade R-CNN 提出了一个很好的 motivation，这是它比较有意义的一个地方。它研究了一下采用不同的 IoU 阈值来进行 training 的情况下，Detector 和 Regressor 的性能分布。</li></ul><h3 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h3><ul><li><p>RetinaNet 是 Singe-stage Detector 目前比较重要的一项工作，它可以看做是由 FPN+Focal Loss 组成的，其中 FPN 只是该论文中用到的架构，而 Focal Loss 则是本论文主要提出的工作。</p></li><li><p>RetinaNet 结构如下：</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/RetinaNet.png" alt="RetinaNet"></p><ul><li><p>RetinaNet 的结构 和 SSD 非常类似，只不过它用的是 ResNet，并在 ResNet 上加入了 FPN 的结构，每一层都有两个分支：一个用来做分类；另一个用来做框回归。此外它的每个 head 都比 SSD 和 Faster R-CNN 都要大一点，这样的话，它的参数量比较大，计算速度也比较慢。</p></li><li><p>而 Focal Loss 试图解决的问题是 class imbalance。针对 class imbalance 的问题，Two-stage Detector 一般是通过 proposal、mini-batch sampaling 两种方式来解决的；SSD 是通过 hard negative mining 来解决的；而 RetinaNet 则通过 Focal Loss 来解决该问题。</p></li><li><p>Focal loss 的核心思路是：对于 high confidence 的样本，给一个小的 loss——这是因为正负样本不平衡，或者说是由于 class imbalance 导致了这样的问题：比如说正负样本的比例是 1:1000，虽然负样本的 loss 都很小，但数目非常多，这些负样本的 loss 加起来的话，还是比正样本要多——这样的话，负样本就会主导整个框架。</p></li></ul><h3 id="Relation-Network"><a href="#Relation-Network" class="headerlink" title="Relation Network"></a>Relation Network</h3><ul><li>在 Relation Network 之前的大部分 detectcor，在做 prediction 或 training 的时候通常只考虑到当前这一个框，而 Relation Network 提出还要考虑这个框周围的框，并基于此提出了一个 relation module，可以插在网络的任何位置，相当于是 feature refinement。Relation module 如下图所示：</li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/relation network.png" alt="Relation Network"></p><ul><li>它的核心思想是：当前框的 feature 除了由当前框决定之外，还要考虑当前框和周围框及其它框的关系</li></ul><h3 id="SNIP"><a href="#SNIP" class="headerlink" title="SNIP"></a>SNIP</h3><ul><li><p>SNIP（Scale Normalization for Image Pyramids）另一篇比较有启发性的工作。它提出的问题是：在 train 分类器的时候，究竟是要 scale specific 还是 scale invariant。传统的 detector 通常会选择 scale invariant，但 SNIP 研究了一下之前的方法后，发现之前的训练方式得到的 feature 对 scale 并没不是很 robust，因而提出要尽量减少 scale 的 variance，让训练时的 scale 尽可能地相似。</p></li><li><p>SNIP 结构图如下:</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/SNIP.png" alt="SNIP"></p><h3 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h3><ul><li><p>CornerNet，是 Singe-stage Detector 中比较新的方法，其与其他方法最不一样的地方是：之前的方法会在图像上选出框，再对框做分类的问题；CornerNet 则是在图中找到 pair 的关键点，这个点就代表物体。它的 pipeline 包括两步：第一步是检测到这样的 corner，即 keypoint；第二步是 group corner，就是说怎样将同一个物体的左上顶点和右下顶点框到一起。</p></li><li><p>其算法结构如下：</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/cornerNet.png" alt="CornerNet"></p><h3 id="mmdetection"><a href="#mmdetection" class="headerlink" title="mmdetection"></a><a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener">mmdetection</a></h3><p>港中文多媒体实验室的开源物体检测框架。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1]. <a href="http://www.mooc.ai/open/course/604" target="_blank" rel="noopener">物体检测算法的近期发展及开源框架介绍-陈恺</a><br>[2]. <a href="https://www.cnblogs.com/skyfsm/p/6806246.html" target="_blank" rel="noopener">基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN</a><br>[3]. <a href="https://mp.weixin.qq.com/s/84JG1ZGFKb6Xp3WQFjtxZw" target="_blank" rel="noopener">基于深度学习的物体检测算法对比探索 - 张士峰</a><br>[4]. <a href="https://blog.csdn.net/Gentleman_Qin/article/details/84421435" target="_blank" rel="noopener">基于深度学习的目标检测算法近5年发展历史（综述）</a><br>[5]. <a href="https://arxiv.org/pdf/1809.02165v2.pdf" target="_blank" rel="noopener">Deep Learning for Generic Object Detection: A Survey</a><br>[6]. <a href="https://zhuanlan.zhihu.com/p/48169867" target="_blank" rel="noopener">目标检测算法中检测框合并策略技术综述</a><br>[7]. <a href="https://mp.weixin.qq.com/s/mu_4kNGZuExxUK2JFTdDFw" target="_blank" rel="noopener">综述 | CVPR2019目标检测方法进展</a></p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Richard YU</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/" title="A Brief Review of Object Detection and Semantic Segmentation">http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/computer-vision/" rel="tag"><i class="fa fa-tag"></i> computer vision</a> <a href="/tags/deep-learning/" rel="tag"><i class="fa fa-tag"></i> deep learning</a> <a href="/tags/object-detection/" rel="tag"><i class="fa fa-tag"></i> object detection</a> <a href="/tags/semantic-segmentation/" rel="tag"><i class="fa fa-tag"></i> semantic segmentation</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/02/18/Ubuntu16-04下安装NVIDIA最新驱动以及深度学习环境配置/" rel="next" title="Ubuntu16.04下安装NVIDIA最新驱动以及深度学习环境配置"><i class="fa fa-chevron-left"></i> Ubuntu16.04下安装NVIDIA最新驱动以及深度学习环境配置</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/" rel="prev" title="Beyond Supervised Learning-A Computer Vision Perspective">Beyond Supervised Learning-A Computer Vision Perspective <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/header.jpg" alt="Richard YU"><p class="site-author-name" itemprop="name">Richard YU</p><div class="site-description motion-element" itemprop="description">Today everything exists to end in a photograph</div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">22</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">11</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">52</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/richardyu114" title="GitHub &rarr; https://github.com/richardyu114" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Yu1145635107" title="Twitter &rarr; https://twitter.com/Yu1145635107" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a> </span><span class="links-of-author-item"><a href="https://instagram.com/d.h.richard" title="Instagram &rarr; https://instagram.com/d.h.richard" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a> </span><span class="links-of-author-item"><a href="https://weibo.com/u/5211687990" title="Weibo &rarr; https://weibo.com/u/5211687990" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a> </span><span class="links-of-author-item"><a href="https://www.douban.com/people/161993653/" title="豆瓣 &rarr; https://www.douban.com/people/161993653/" rel="noopener" target="_blank"><i class="fa fa-fw fa-paperclip"></i>豆瓣</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#About"><span class="nav-number">1.</span> <span class="nav-text">About</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Task-Definition"><span class="nav-number">2.</span> <span class="nav-text">Task Definition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Object-Detection"><span class="nav-number">3.</span> <span class="nav-text">Object Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Progress-一步法single-stage-两步法two-stage"><span class="nav-number">3.1.</span> <span class="nav-text">Progress (一步法single-stage, 两步法two stage)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#General-Pipeline-of-Object-Detection"><span class="nav-number">3.2.</span> <span class="nav-text">General Pipeline of Object Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Faster-R-CNN"><span class="nav-number">3.3.</span> <span class="nav-text">Faster R-CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FPN"><span class="nav-number">3.4.</span> <span class="nav-text">FPN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MASK-R-CNN"><span class="nav-number">3.5.</span> <span class="nav-text">MASK R-CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cascade-R-CNN"><span class="nav-number">3.6.</span> <span class="nav-text">Cascade R-CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RetinaNet"><span class="nav-number">3.7.</span> <span class="nav-text">RetinaNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Relation-Network"><span class="nav-number">3.8.</span> <span class="nav-text">Relation Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SNIP"><span class="nav-number">3.9.</span> <span class="nav-text">SNIP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CornerNet"><span class="nav-number">3.10.</span> <span class="nav-text">CornerNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mmdetection"><span class="nav-number">3.11.</span> <span class="nav-text">mmdetection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Richard YU</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">255k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">3:52</span></div><div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.1</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script><script src="/js/utils.js?v=7.1.1"></script><script src="/js/motion.js?v=7.1.1"></script><script src="/js/affix.js?v=7.1.1"></script><script src="/js/schemes/pisces.js?v=7.1.1"></script><script src="/js/scrollspy.js?v=7.1.1"></script><script src="/js/post-details.js?v=7.1.1"></script><script src="/js/next-boot.js?v=7.1.1"></script><script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>var GUEST=["nick","mail","link"],guest="nick,mail,link";guest=guest.split(",").filter(function(e){return-1<GUEST.indexOf(e)}),new Valine({el:"#comments",verify:!0,notify:!0,appId:"duGoywhUmLrx9pM6qQhmf47c-gzGzoHsz",appKey:"m7fc8w4Di5qnAXXaJ5Gp3Pgg",placeholder:"欢迎评论！请留下你的邮箱",avatar:"mm",meta:guest,pageSize:"10",visitor:!0,lang:"zh-cn"})</script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });</script><script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>$(".highlight").not(".gist .highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea"),o=window.pageYOffset||document.documentElement.scrollTop;n.style.top=o+"px",n.style.position="absolute",n.style.opacity="0",n.readOnly=!0,n.value=e,document.body.appendChild(n),n.select(),n.setSelectionRange(0,e.length),n.readOnly=!1,document.execCommand("copy")?$(this).text("复制成功"):$(this).text("复制失败"),n.blur(),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script></body></html>