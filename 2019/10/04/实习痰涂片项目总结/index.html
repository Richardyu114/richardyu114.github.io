<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2"><meta name="google-site-verification" content="true"><meta name="baidu-site-verification" content="true"><link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1"><link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.1.1",sidebar:{position:"left",display:"post",offset:12,onmobile:!1,dimmer:!1},back2top:!0,back2top_sidebar:!1,fancybox:!0,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="baidu-site-verification" content="0bqk4mbBLD"><meta name="google-site-verification" content="FvMbHVgnH7FO0GFRzTmOevjSaFwvzIQZv94LdufMMPY"><meta name="description" content="classification在上篇博客提到，该任务就是将原始数据的每张图片（256x256）进行grid级别的label预测，思路很简单，就是最后卷出的feature map是4x4的，不要过average global pooling layer，直接拉成1x16的向量过sigmoid激活函数即可（label也要变成16个字符，有点类似OCR)。dataset原始数据给的标注是json格式的框标"><meta name="keywords" content="computer vision,deep learning,CNN,medical image analysis"><meta property="og:type" content="article"><meta property="og:title" content="实习痰涂片项目总结"><meta property="og:url" content="http://densecollections.top/2019/10/04/实习痰涂片项目总结/index.html"><meta property="og:site_name" content="自拙集"><meta property="og:description" content="classification在上篇博客提到，该任务就是将原始数据的每张图片（256x256）进行grid级别的label预测，思路很简单，就是最后卷出的feature map是4x4的，不要过average global pooling layer，直接拉成1x16的向量过sigmoid激活函数即可（label也要变成16个字符，有点类似OCR)。dataset原始数据给的标注是json格式的框标"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://densecollections.top/2019/10/04/实习痰涂片项目总结/1.jpg"><meta property="og:image" content="http://densecollections.top/2019/10/04/实习痰涂片项目总结/2.jpg"><meta property="og:image" content="http://densecollections.top/2019/10/04/实习痰涂片项目总结/3.png"><meta property="og:image" content="http://densecollections.top/2019/10/04/实习痰涂片项目总结/4.png"><meta property="og:image" content="http://densecollections.top/2019/10/04/实习痰涂片项目总结/5.png"><meta property="og:image" content="http://densecollections.top/2019/10/04/实习痰涂片项目总结/6.png"><meta property="og:image" content="http://densecollections.top/2019/10/04/实习痰涂片项目总结/7.png"><meta property="og:updated_time" content="2020-02-03T02:28:47.281Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="实习痰涂片项目总结"><meta name="twitter:description" content="classification在上篇博客提到，该任务就是将原始数据的每张图片（256x256）进行grid级别的label预测，思路很简单，就是最后卷出的feature map是4x4的，不要过average global pooling layer，直接拉成1x16的向量过sigmoid激活函数即可（label也要变成16个字符，有点类似OCR)。dataset原始数据给的标注是json格式的框标"><meta name="twitter:image" content="http://densecollections.top/2019/10/04/实习痰涂片项目总结/1.jpg"><link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml"><link rel="canonical" href="http://densecollections.top/2019/10/04/实习痰涂片项目总结/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>实习痰涂片项目总结 | 自拙集</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">自拙集</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Work cures everything</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-paperstation"><a href="/PaperStation/" rel="section"><i class="menu-item-icon fa fa-fw fa-edit"></i><br>PaperStation</a></li><li class="menu-item menu-item-mindwandering"><a href="/MindWandering/" rel="section"><i class="menu-item-icon fa fa-fw fa-paper-plane"></i><br>MindWandering</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://densecollections.top/2019/10/04/实习痰涂片项目总结/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Richard YU"><meta itemprop="description" content="Today everything exists to end in a photograph"><meta itemprop="image" content="/uploads/header.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="自拙集"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">实习痰涂片项目总结</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-10-04 15:47:17" itemprop="dateCreated datePublished" datetime="2019-10-04T15:47:17+08:00">2019-10-04</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-03 10:28:47" itemprop="dateModified" datetime="2020-02-03T10:28:47+08:00">2020-02-03</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工作总结/" itemprop="url" rel="index"><span itemprop="name">工作总结</span></a></span> </span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">评论数：</span> <a href="/2019/10/04/实习痰涂片项目总结/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/10/04/实习痰涂片项目总结/" itemprop="commentCount"></span> </a></span><span id="/2019/10/04/实习痰涂片项目总结/" class="leancloud_visitors" data-flag-title="实习痰涂片项目总结"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">18k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">16 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="classification"><a href="#classification" class="headerlink" title="classification"></a>classification</h2><p>在上篇博客提到，该任务就是将原始数据的每张图片（256x256）进行grid级别的label预测，思路很简单，就是最后卷出的feature map是4x4的，不要过average global pooling layer，直接拉成1x16的向量过sigmoid激活函数即可（label也要变成16个字符，有点类似OCR)。</p><h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p>原始数据给的标注是json格式的框标注，但是框不是杆菌的具体位置，而是代表这个grid里面存在杆菌：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;frames&quot;:&#123;&quot;0_grid.png&quot;:[&#123;&quot;x1&quot;:162.42542787286064,&quot;y1&quot;:25.34963325183374,&quot;x2&quot;:170.24938875305622,&quot;y2&quot;:34.11246943765281,&quot;width&quot;:256,&quot;height&quot;:256,&quot;box&quot;:&#123;&quot;x1&quot;:162.42542787286064,&quot;y1&quot;:25.34963325183374,&quot;x2&quot;:170.24938875305622,&quot;y2&quot;:34.11246943765281&#125;,&quot;UID&quot;:&quot;fd548124&quot;,&quot;id&quot;:0,&quot;type&quot;:&quot;Rectangle&quot;,&quot;tags&quot;:[&quot;TB01&quot;],&quot;name&quot;:1&#125;,&#123;&quot;x1&quot;:214.3765281173594,&quot;y1&quot;:24.410757946210268,&quot;x2&quot;:224.07823960880197,&quot;y2&quot;:34.11246943765281,&quot;width&quot;:256,&quot;height&quot;:256,&quot;box&quot;:&#123;&quot;x1&quot;:214.3765281173594,&quot;y1&quot;:24.410757946210268,&quot;x2&quot;:224.07823960880197,&quot;y2&quot;:34.11246943765281&#125;,&quot;UID&quot;:&quot;5318dd81&quot;,&quot;id&quot;:1,&quot;type&quot;:&quot;Rectangle&quot;,&quot;tags&quot;:[&quot;TB01&quot;],&quot;name&quot;:2&#125;,...&#125;</span><br></pre></td></tr></table></figure><p>部分标注内容如上，主要包含了对应的文件夹下有哪些图片，图片上有无杆菌，杆菌的位置在哪个格子（要自己判断），以及一张图片有杆菌的话共有几个（”name”）。</p><a id="more"></a><p>首先找出哪些是positive的图片，并且根据坐标位置写出标签:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def find_write_positive_imgs(src_json_path, src_imgs_path, dst_csv_path, dst_imgs_path):</span><br><span class="line">    </span><br><span class="line">    data_csv = open(dst_csv_path, &apos;a+&apos;, newline=&apos;&apos;)</span><br><span class="line">    csv_writer = csv.writer(data_csv)</span><br><span class="line">    csv_writer.writerow([&quot;ImageName&quot;, &quot;LabelId&quot;])</span><br><span class="line">    </span><br><span class="line">    with open(src_json_path,&apos;r&apos;) as load_json:</span><br><span class="line">         load_dict = json.load(load_json)</span><br><span class="line"></span><br><span class="line">         img_names = load_dict[&apos;visitedFrames&apos;]</span><br><span class="line"></span><br><span class="line">         for img_name in img_names:</span><br><span class="line">         </span><br><span class="line">             #n_name represents the boxes quantities of the img &lt;&quot;name&quot; attribute in .json file&gt;</span><br><span class="line">             n_name=len(load_dict[&apos;frames&apos;][img_name])</span><br><span class="line">             </span><br><span class="line">             if n_name &gt; 0:</span><br><span class="line">                src_img_path = os.path.join(src_imgs_path, img_name)</span><br><span class="line">                img = cv2.imread(src_img_path)</span><br><span class="line">                H = img.shape[0]</span><br><span class="line">                W = img.shape[1]</span><br><span class="line">                dst_img_path = os.path.join(dst_imgs_path, img_name.replace(&apos;.png&apos;, &apos;_22.png&apos;))</span><br><span class="line">                cv2.imwrite(dst_img_path, img)</span><br><span class="line">                </span><br><span class="line">                labelid = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] </span><br><span class="line">                for i in range(0,n_name):</span><br><span class="line">                    x1 = load_dict[&apos;frames&apos;][img_name][i][&apos;x1&apos;]</span><br><span class="line">                    y1 = load_dict[&apos;frames&apos;][img_name][i][&apos;y1&apos;]</span><br><span class="line">                    area_h0 = 0</span><br><span class="line">                    area_w0 = 0</span><br><span class="line">                    for area_h1 in range(H//4, H+1, H//4):</span><br><span class="line">                        if y1 &gt; area_h0 and y1 &lt; area_h1:</span><br><span class="line">                           row_id = (area_h1 * 4 / H) - 1</span><br><span class="line">                           for area_w1 in range(W//4, W+1, W//4):</span><br><span class="line">                               if x1 &gt; area_w0 and x1 &lt; area_w1:</span><br><span class="line">                                  col_id = (area_w1 * 4 / W) - 1</span><br><span class="line">                                  id = int(col_id + 4 * row_id)</span><br><span class="line">                                  labelid[id] = 1</span><br><span class="line">                                  break</span><br><span class="line">                               else:</span><br><span class="line">                                    area_w0 = area_w1</span><br><span class="line">                           break</span><br><span class="line">                        else:</span><br><span class="line">                            area_h0 = area_h1</span><br><span class="line">                  </span><br><span class="line">                csv_writer.writerow([img_name.replace(&apos;.png&apos;, &apos;_22.png&apos;), </span><br><span class="line">                                     &apos;&apos;.join(str(k) for k in labelid)])</span><br></pre></td></tr></table></figure><p>此外，由于最后找出的positive图片很少（好像只有320张），我又对其进行了数据扩增，先是原始旋转一圈，然后right-left翻转后又旋转了一圈，因此总共扩增到了8倍大小。之后进行一下train-val-test set的划分，一般生成随机数就可以按自己的意愿划分，也有专门的库，具体划分代码就不上了。</p><p>另外数据增强方面也考虑过rgb转hsv或者ycrcb的，但是我试了一个样例之后效果不是很好，毕竟这样做的目的就是为了将主要的前景和特征显示出来，奈何我的数据太差了些，不好操作，于是作罢。</p><p>准备好数据之后，要对数据进行抽取，我用的是pytorch，直接继承Dataset类就好：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class SSDataset(Dataset):</span><br><span class="line">      </span><br><span class="line">      def __init__(self, imgs_path, csv_path, </span><br><span class="line">                    img_transform=None, loader=default_loader):</span><br><span class="line">          with open(csv_path, &apos;r&apos;) as f:</span><br><span class="line">               #这里一定要按字符串读取，否则前面的0会丢掉</span><br><span class="line">               #类似于OCR的labe读取</span><br><span class="line">               data_info = pd.read_csv(f, dtype=str) </span><br><span class="line">               #第一列是image name</span><br><span class="line">               self.img_list = list(data_info.iloc[:,0])</span><br><span class="line">               #第二类是labelid</span><br><span class="line">               self.label_list = list(data_info.iloc[:,1])</span><br><span class="line">          self.img_transform = img_transform</span><br><span class="line">          #loader用PIL.Image.open()</span><br><span class="line">          #不要用cv2.imread()</span><br><span class="line">          #pytorch默认PIL格式</span><br><span class="line">          self.loader = loader</span><br><span class="line">          self.imgs_path = imgs_path</span><br><span class="line">      </span><br><span class="line">      def __getitem__(self, index):</span><br><span class="line">          img_path = os.path.join(self.imgs_path, self.img_list[index])</span><br><span class="line">          img = self.loader(img_path)</span><br><span class="line">          label = self.label_list[index]</span><br><span class="line">          if self.img_transform is not None:</span><br><span class="line">             img = self.img_transform(img)</span><br><span class="line">          return img, label</span><br><span class="line"></span><br><span class="line">      def __len__(self):</span><br><span class="line">         return len(self.label_list)</span><br></pre></td></tr></table></figure><p>但是定义的labelid是str，还需要转成tensor去计算loss:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def labelid_switch(labels_str):</span><br><span class="line">    b_s = len(labels_str)</span><br><span class="line">    pad_label = []</span><br><span class="line">    for i in range(0, b_s):</span><br><span class="line">        temp_label = [0]* 16</span><br><span class="line">        temp_label[:16] = labels_str[i]</span><br><span class="line">        temp_label = list(map(int, temp_label))</span><br><span class="line">        pad_label.append(temp_label)</span><br><span class="line">    pad_label = torch.Tensor(pad_label)</span><br><span class="line">    labels_float = pad_label.view(b_s, 16)</span><br><span class="line">    return labels_float</span><br></pre></td></tr></table></figure><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><p>训练模型是主要用的是resnet和vgg，这部分代码可以直接参考torchvision，然后改改后面的layer就好了。</p><p>loss function上我试了binary cross entropy和focal loss（毕竟整体上positive grids还是少于negative grids的），此外我也试了下<a href="https://github.com/facebookresearch/mixup-cifar10" target="_blank" rel="noopener">mixup</a>，就是随机把batch里面的图片两两混合，计算loss的时候按照混合的比例分别计算相加，这也是一种应对过拟合，降低模型复杂度的办法（还有一种类似的方法叫sample pairing，只混合图片，不管label，我也试了，不过实际好像没mixup顶用）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class BFocalLoss(nn.Module):</span><br><span class="line"> </span><br><span class="line">    def __init__(self, gamma=1,alpha=0.8):</span><br><span class="line">        super(BFocalLoss, self).__init__()</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">    def forward(self, inputs, targets):</span><br><span class="line">        p = inputs</span><br><span class="line">        loss = -self.alpha*(1-p)**self.gamma*(targets*torch.log(p+1e-12))-\</span><br><span class="line">               (1-self.alpha)*p**self.gamma*((1-targets)*torch.log(1-p+1e-12))</span><br><span class="line">        loss = torch.sum(loss)</span><br><span class="line">        return loss</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def mixup_data(in_img, in_label, alpha=1.0):</span><br><span class="line">    #alpha in [0.1,0.4] in paper has better gain(for imagenet)</span><br><span class="line">    #for cifar-10 is 1.</span><br><span class="line">    if alpha &gt; 0:</span><br><span class="line">       lam = np.random.beta(alpha, alpha)</span><br><span class="line">    else:</span><br><span class="line">       lam = 1</span><br><span class="line">    </span><br><span class="line">    Batch_Size = in_img.size()[0]</span><br><span class="line">    Index = torch.randperm(Batch_Size)</span><br><span class="line">    mixed_x = lam * in_img + (1 - lam) * in_img[Index, :]</span><br><span class="line">    y_a, y_b = in_label, in_label[Index]</span><br><span class="line">    return mixed_x, y_a, y_b, lam</span><br><span class="line">    </span><br><span class="line">#计算loss  </span><br><span class="line">loss_mixup =  lam * criterion(pred, labels_a) + \</span><br><span class="line">                   (1 - lam) * criterion(pred, labels_b)</span><br></pre></td></tr></table></figure><p>接下来的事就是调参，对比实验，开tensorboard看loss的趋势了。（这里有一个现象，前期有一部分时间loss难以下降，总是在一个范围内波动，我猜想可能是因为数据扩增的原因。）</p><h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>这部分就是加载模型，一张张图片测试，然后写出预测的csv即可，然后给出grid acc</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">#部分代码如下：</span><br><span class="line">file_pre = open(PRE_TEST_CSV, &apos;w&apos;, newline=&apos;&apos;)</span><br><span class="line">pre_writer = csv.writer(file_pre)</span><br><span class="line">pre_writer.writerow([&quot;ImageName&quot;, &quot;LabelId&quot;])</span><br><span class="line"></span><br><span class="line">with open(SRC_TEST_CSV, &apos;r&apos;) as f_test:</span><br><span class="line">     test_data = pd.read_csv(f_test, dtype=str)</span><br><span class="line">     img_name = list(test_data.iloc[:,0])</span><br><span class="line">     labelid = list(test_data.iloc[:,1])</span><br><span class="line">     test_data_len = len(test_data.index)</span><br><span class="line"></span><br><span class="line">     num_right = 0 </span><br><span class="line">     positive_num = 0 </span><br><span class="line">     positive_num_right = 0</span><br><span class="line">     for i in range(0,test_data_len):</span><br><span class="line">         img_path = os.path.join(TEST_DATA_PATH, img_name[i])</span><br><span class="line">         img = Image.open(img_path)</span><br><span class="line">         img_tensor = transformations(img).float()</span><br><span class="line">         img_tensor = img_tensor.unsqueeze_(0)</span><br><span class="line">         </span><br><span class="line">         temp_label = [0]*16</span><br><span class="line">         temp_label[:16] = labelid[i]</span><br><span class="line">         temp_label = list(map(int, temp_label))</span><br><span class="line">         for temp in temp_label:</span><br><span class="line">             if temp &gt; 0:</span><br><span class="line">                positive_num += 1</span><br><span class="line">         label = torch.FloatTensor(temp_label)</span><br><span class="line">         label = label.view(1, 16)</span><br><span class="line">          </span><br><span class="line">         input = Variable(img_tensor)</span><br><span class="line">         input = input.to(device)</span><br><span class="line">         pred = net(input).data.cpu() #在CPU中比较</span><br><span class="line">         output = pred</span><br><span class="line">         pred_len = pred.size()[1]</span><br><span class="line">         out = []</span><br><span class="line">         for j in range(0, pred_len):</span><br><span class="line">             if pred[0][j] &lt; 0.5:</span><br><span class="line">                output[0][j] = 0</span><br><span class="line">                out.append(0)</span><br><span class="line">                if output[0][j] == label[0][j]:</span><br><span class="line">                   num_right += 1</span><br><span class="line">             else:</span><br><span class="line">                 output[0][j] = 1</span><br><span class="line">                 out.append(1)</span><br><span class="line">                 if output[0][j] == label[0][j]:</span><br><span class="line">                    num_right += 1</span><br><span class="line">                    positive_num_right += 1</span><br><span class="line">         pre_writer.writerow([img_name[i],&apos;&apos;.join(str(k) for k in out)]) </span><br><span class="line">    </span><br><span class="line">     print(&apos;test acc is: &apos;, num_right/(test_data_len*16))</span><br><span class="line">     print(&apos;postivite acc is: &apos;, positive_num_right, &apos;/&apos;, positive_num)</span><br></pre></td></tr></table></figure><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><p>实际上这个代码下来，调参还是挺费劲的，尤其是对我这种刚开始搞深度学习，经验还不够的新手来说，着实走了不少弯路。可是数据集实在太差，实在是想不出什么招。。所以硬撑了快两个月（实际上前大半个月我是直接分割grid成单独的图片，然后全部丢进去训练的。。这样搞不仅正负样本差距极大，而且切断了图片的连续性，效果奇差也在意料之中了，基本训不动，即使加了focal loss也没什么卵用）最后最高也才得到90%的acc。</p><h2 id="weakly-semantic-segmentation"><a href="#weakly-semantic-segmentation" class="headerlink" title="weakly semantic segmentation"></a>weakly semantic segmentation</h2><p>好歹8月下旬那会找到了一个公开的sputum smear的数据集，还带着框的标注：</p><ul><li>Makerere University, Uganda<ul><li><a href="http://air.ug/microscopy/" target="_blank" rel="noopener">homepage</a></li><li><a href="http://proceedings.mlr.press/v56/Quinn16.pdf" target="_blank" rel="noopener">paper</a></li><li><a href="https://github.com/jqug/microscopy-object-detection/blob/master/CNN%20training%20%26%20evaluation%20-%20tuberculosis.ipynb" target="_blank" rel="noopener">code</a></li></ul></li></ul><p>跟CTO交流后，他觉得这数据集质量不错，干脆就提议做弱监督分割，毕竟object detection现在都做烂了，而且开源这数据集的小哥自己也把object detection的acc刷的不错了，所以没必要再调包重复同样的事情了。我当时其实没啥思路，但是觉得应该挺有意思的，于是就接了下来。</p><p>后来通过调研发现，原来在自然图像上早就有人做了weakly segmentation(又是我恺明哥那些人…)，而且效果还不错，唯一可惜的就是完整的代码基本没人开源，不过后来参考GitHub上的一些相关代码也慢慢搭建出了整个框架。</p><p>整个项目思路主要参考的是这两篇论文：戴季峰的<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf" target="_blank" rel="noopener">BoxSup</a>和Max Planck Institute的<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Simple Does It</a>，主要的思路就是先设定几个从bounding box annotations生成segment proposals的方法（主要是opencv中GrabCut），然后利用此label去进行supervised training，最后过一下<a href="https://github.com/lucasb-eyer/pydensecrf" target="_blank" rel="noopener">denseCRF</a>优化一下，让boundary更加丝滑。当然也可以试试递归训练，让performance不错的model去预测生成新的training set中的label，然后进行下一轮的训练。</p><p>因为代码比较庞杂，分块不好展示，完整的代码就直接放在我的<a href="https://github.com/Richardyu114/weakly-segmentation-with-bounding-box" target="_blank" rel="noopener">github</a>上。</p><h3 id="pre-processing"><a href="#pre-processing" class="headerlink" title="pre-processing"></a>pre-processing</h3><p>原始的数据集中有1217张阳性图片，此外这些图片的标注还有47张莫名奇妙多了些20x20的框（可能是标的时候手抖了），因此要先一个个去掉。</p><p><img src="/2019/10/04/实习痰涂片项目总结/1.jpg" alt="看到了吗，左下角和右上角都多了个小框"></p><p>之后，对这些图片进行大致masks的生成，我这里给了三种方法：</p><ul><li><em>Box_segments</em>: 把整个box里面的像素都认为是杆菌（要把box的坐标都转成int，得对上像素）</li><li><em>Sbox_segments</em>:取box里面的80%的矩形框，认为该框里面的像素都是杆菌（同样，坐标都是int类型）</li><li><em>GrabCut_segments</em>: 利用经典的计算机视觉方法<a href="https://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf" target="_blank" rel="noopener">GrabCut</a>来得到杆菌的分割区域，但是该方法一般对图片的里面的单个的大物体比较友好，而杆菌又细又长，同时又包含着染色质，所以利用颜色分布的GrabCut分割出的杆菌要么会大点，要么就没有。大点的我不管，没有的我在这里就直接用<em>Box_segments</em>代替了。</li></ul><p>GrabCut部分代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">def grabcut(img_name):</span><br><span class="line">        masks = [] </span><br><span class="line">        # one image has many object that need to grabcut</span><br><span class="line">        for i, ann_info in enumerate(ANNS[img_name], start=1):</span><br><span class="line">               img = cv.imread((img_dir +img_name).rstrip()+&apos;.jpg&apos;)</span><br><span class="line">               grab_name = ann_info[1]</span><br><span class="line">               xmin = ann_info[3]</span><br><span class="line">               ymin = ann_info[2]</span><br><span class="line">               xmax = ann_info[5]</span><br><span class="line">               ymax = ann_info[4]</span><br><span class="line">               &quot;&quot;&quot;get int box coor&quot;&quot;&quot;</span><br><span class="line">               img_w = img.shape[1]</span><br><span class="line">               img_h = img.shape[0]</span><br><span class="line">               xmin, ymin, xmax, ymax = get_int_coor(xmin, ymin, xmax, ymax, img_w, img_h)           </span><br><span class="line">               box_w = xmax - xmin</span><br><span class="line">               box_h = ymax - ymin</span><br><span class="line">               # cv.grabcut&apos;s para</span><br><span class="line">               mask = np.zeros(img.shape[:2], np.uint8)</span><br><span class="line">               # rect is the tuple</span><br><span class="line">               rect = (xmin, ymin, box_w, box_h)</span><br><span class="line">               bgdModel = np.zeros((1, 65), np.float64)</span><br><span class="line">               fgdModel = np.zeros((1, 65), np.float64)</span><br><span class="line">               #for small bbox:</span><br><span class="line">               if box_w * box_h &lt; MINI_AREA:</span><br><span class="line">                   img_mask = mask[ymin:ymax, xmin:xmax] = 1</span><br><span class="line">                # for big box that area == img.area(one object bbox is just the whole image)</span><br><span class="line">               elif box_w * box_h == img.shape[1] * img.shape[0]:</span><br><span class="line">                      rect = [RECT_SHRINK, RECT_SHRINK, box_w - RECT_SHRINK * 2, box_h - RECT_SHRINK * 2]</span><br><span class="line">                      cv.grabCut(img, mask, rect, bgdModel,fgdModel, ITER_NUM, cv.GC_INIT_WITH_RECT)</span><br><span class="line">                      # astype(&apos;uint8&apos;) keep the image pixel in range[0,255]</span><br><span class="line">                      img_mask =  np.where((mask == 0) | (mask == 2), 0, 1).astype(&apos;uint8&apos;)</span><br><span class="line">                # for normal bbox:</span><br><span class="line">               else:</span><br><span class="line">                       cv.grabCut(img, mask, rect, bgdModel,fgdModel, ITER_NUM, cv.GC_INIT_WITH_RECT)</span><br><span class="line">                       img_mask = np.where((mask == 0) | (mask == 2), 0, 1).astype(&apos;uint8&apos;)</span><br><span class="line">                       # if the grabcut output is just background(it happens in my dataset)</span><br><span class="line">                       if np.sum(img_mask) == 0:</span><br><span class="line">                           img_mask = np.where((mask == 0), 0, 1).astype(&apos;uint8&apos;)</span><br><span class="line">                        # couting IOU</span><br><span class="line">                        # if the grabcut output too small region, it need reset to bbox mask</span><br><span class="line">                       box_mask = np.zeros((img.shape[0], img.shape[1]))</span><br><span class="line">                       box_mask[ymin:ymax, xmin:xmax] = 1</span><br><span class="line">                       sum_area = box_mask + img_mask</span><br><span class="line">                       intersection = np.where((sum_area==2), 1, 0).astype(&apos;uint8&apos;)</span><br><span class="line">                       union = np.where((sum_area==0), 0, 1).astype(&apos;uint8&apos;)</span><br><span class="line">                       IOU = np.sum(intersection) / np.sum(union)</span><br><span class="line">                       if IOU &lt;= IOU_THRESHOLD:</span><br><span class="line">                           img_mask = box_mask</span><br><span class="line">                # for draw mask on the image later           </span><br><span class="line">               img = cv.cvtColor(img, cv.COLOR_BGR2RGB) </span><br><span class="line">               masks.append([img_mask, grab_name, rect])</span><br><span class="line">        </span><br><span class="line">        num_object = i</span><br><span class="line">        &quot;&quot;&quot;for multi-objects intersection and fix the label &quot;&quot;&quot;</span><br><span class="line">        masks.sort(key=lambda mask: np.sum(mask[0]), reverse=True)</span><br><span class="line">        for j in range(num_object):</span><br><span class="line">              for k in range(j+1, num_object):</span><br><span class="line">                      masks[j][0] = masks[j][0] - masks[k][0]</span><br><span class="line">              masks[j][0] = np.where((masks[j][0]==1), 1, 0).astype(&apos;uint8&apos;)</span><br><span class="line">              &quot;&quot;&quot;get class name  id&quot;&quot;&quot;</span><br><span class="line">              grab_name = masks[j][1]</span><br><span class="line">              class_id = grab_name.split(&apos;_&apos;)[-1]</span><br><span class="line">              class_id = int(class_id.split(&apos;.&apos;)[0])</span><br><span class="line"></span><br><span class="line">              #set the numpy value to class_id</span><br><span class="line">              masks[j][0] = np.where((masks[j][0]==1), class_id, 0).astype(&apos;uint8&apos;)</span><br><span class="line">              # save grabcut_inst(one object in a image)</span><br><span class="line">              scipy.misc.toimage(masks[j][0], cmin=0, cmax=255, pal=tbvoc_info.colors_map,</span><br><span class="line">                                                      mode=&apos;P&apos; ).save((grabcut_dir).rstrip()+masks[j][1])</span><br><span class="line">        </span><br><span class="line">        &quot;&quot;&quot;merge masks&quot;&quot;&quot;</span><br><span class="line">        # built array(img.shape size)</span><br><span class="line">        mask_ = np.zeros(img.shape[:2])</span><br><span class="line">        for mask in masks:</span><br><span class="line">                mask_ = mask_ + mask[0]</span><br><span class="line">        # save segmetation_label(every object in a image)</span><br><span class="line">        scipy.misc.toimage(mask_, cmin=0, cmax=255, pal=tbvoc_info.colors_map,</span><br><span class="line">                                                mode=&apos;P&apos;).save((segmentation_label_dir+img_name).rstrip()+&apos;.png&apos;)</span><br></pre></td></tr></table></figure><p>这里面我是用scipy来保存masks的，我用的版本是0.19.0，超过这个版本的scipy就没有toimage()这个函数了，据说PIL有可以替代的函数，但是我看两个的功效好像不一样，就没去折腾了。</p><p><img src="/2019/10/04/实习痰涂片项目总结/2.jpg" alt="原图，带框标注"></p><p><img src="/2019/10/04/实习痰涂片项目总结/3.png" alt="GrabCut生成的segmentation label"></p><p>读取数据部分进行了resize处理，原图尺寸是1632x1224，1224不能被32整除，五次下采样和上采样的时候会出现feature map维度不匹配的错误，因此resize成了1632x1216。这里要注意，原图是利用双线性插值进行resize的，masks图是利用最近邻进行resize的（实际上我是生成好masks后训练时才意识到这个问题，实际上可以在最开始就把dataset的数据resize好，这样masks的误差可能就小点），PIL和cv2里面都有类似的函数。</p><p>数据读取部分代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class TBDataset(Dataset):</span><br><span class="line">      def __init__(self, txt_dir, width, height,  transform=None):</span><br><span class="line">          self.img_names = []</span><br><span class="line">          with open(txt_dir, &apos;r&apos;) as f_txt:</span><br><span class="line">               for img_name in f_txt:</span><br><span class="line">                   self.img_names.append(img_name)</span><br><span class="line">          </span><br><span class="line">          self.transform = transform</span><br><span class="line">          self.txt_dir = txt_dir</span><br><span class="line">          self.width = width</span><br><span class="line">          self.height = height</span><br><span class="line">                   </span><br><span class="line">      def __getitem__(self, index):</span><br><span class="line">          img_name = self.img_names[index]</span><br><span class="line">          img = Image.open(os.path.join(img_dir, img_name).rstrip()+&apos;.jpg&apos;)</span><br><span class="line">          # the resize function like bilinear</span><br><span class="line">          img = img.resize((self.width, self.height), Image.LANCZOS)</span><br><span class="line">          img = np.array(img)</span><br><span class="line">          label = Image.open(os.path.join(label_dir, img_name).rstrip()+&apos;.png&apos;)</span><br><span class="line">          # for consider class_id is not consecutive and just fixed by user</span><br><span class="line">          label = label.resize((self.width, self.height), Image.NEAREST)</span><br><span class="line">          label = np.array(label)</span><br><span class="line">          if self.transform is not None:</span><br><span class="line">             img = self.transform(img)</span><br><span class="line">          #img = torch.FloatTensor(img)</span><br><span class="line">          label = torch.FloatTensor(label)</span><br><span class="line">          return img, label</span><br><span class="line">                             </span><br><span class="line">      def __len__(self):</span><br><span class="line">          return len(self.img_names)</span><br></pre></td></tr></table></figure><h3 id="train-1"><a href="#train-1" class="headerlink" title="train"></a>train</h3><p>训练部分模型用的是FCN和UNet，因为考虑到只有二分类，后面也可以考虑deeplab，UNet++等等。FCN用的是VGG-16 backbone，下采样5次，UNet下采样4次，都是按照论文来的，没做什么改动。模型最后输出的是一个1632x1216的feature map，然后直接过sigmoid激活函数，再和1632x1216的mask图片（读进来的是一个二维0-1矩阵，代表每个像素点的label）进行loss计算，然后BP，更新参数学习。loss也用了交叉熵和focal loss.</p><h3 id="post-processing"><a href="#post-processing" class="headerlink" title="post-processing"></a>post-processing</h3><p>对模型预测出的结果再过一遍denseCRF，优化分割的同时也会去掉一些false-positive</p><p>部分代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">def run_densecrf(img_dir, img_name, masks_pro):</span><br><span class="line">        height = masks_pro.shape[0]</span><br><span class="line">        width = masks_pro.shape[1]</span><br><span class="line"></span><br><span class="line">        # must use cv2.imread()</span><br><span class="line">        # if use PIL.Image.open(), the algorithm will break</span><br><span class="line">        #TODO --need to fix the image problem</span><br><span class="line">        img = cv.imread(os.path.join(img_dir, img_name).rstrip()+&apos;.jpg&apos;)</span><br><span class="line">        img = cv.resize(img, (1632,1216), interpolation = cv.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line">        # expand to [1,H,W]</span><br><span class="line">        masks_pro = np.expand_dims(masks_pro, 0)</span><br><span class="line">        # masks_pro = masks_pro[:, :, np.newaxis]</span><br><span class="line">        # append to array---shape(2,H,W)</span><br><span class="line">        # one depth represents the class 0, the other represents the class 1</span><br><span class="line">        masks_pro = np.append(1-masks_pro, masks_pro, axis=0)</span><br><span class="line">        #[Classes, H, W]</span><br><span class="line">        # U needs to be flat</span><br><span class="line">        U = masks_pro.reshape(2, -1)</span><br><span class="line">        # deepcopy and the order is C-order(from rows to colums)</span><br><span class="line">        U = U.copy(order=&apos;C&apos;)</span><br><span class="line">        # for binary classification, the value after sigmoid may be very small</span><br><span class="line">        U = np.where((U &lt; 1e-12), 1e-12, U)</span><br><span class="line">        d = dcrf.DenseCRF2D(width, height, 2)</span><br><span class="line"></span><br><span class="line">        # make sure the array be c-order which will faster the processing speed</span><br><span class="line">        # reference: https://zhuanlan.zhihu.com/p/59767914</span><br><span class="line">        U = np.ascontiguousarray(U)</span><br><span class="line">        img = np.ascontiguousarray(img)</span><br><span class="line"></span><br><span class="line">        d.setUnaryEnergy(-np.log(U))</span><br><span class="line">        d.addPairwiseGaussian(sxy=3, compat=3)</span><br><span class="line">        d.addPairwiseBilateral(sxy=80, srgb=13, rgbim=img, compat=10)</span><br><span class="line">        Q = d.inference(5)</span><br><span class="line">        # compare each value between two rows by colum</span><br><span class="line">        # and inference each pixel belongs to which class(0 or 1)</span><br><span class="line">        map = np.argmax(Q, axis=0).reshape((height, width))</span><br><span class="line">        proba = np.array(map)</span><br><span class="line"></span><br><span class="line">        return proba</span><br></pre></td></tr></table></figure><p>这里主要用到了二元势pairwise potential，比较每个像素和其他像素的关系，具体原理可以去看看原代码和论文。</p><p>此外，我还顺手进行了下迭代训练。实际上，对于我这个数据集，基本上用GrabCut生成label训练一遍效果就不错了，不过为了看下更新label再训练一轮会不会得到更好的结果，在固定的epoch结束后将训练好得模型设为eval模式，然后预测train set的数据，然后再返回train模式继续训练。需要注意的是，更新label的时候，可能会有漏诊和误诊，我就直接将预测的mask和<em>Box_segments</em>得到的mask相加，只取为2的部分，这样就去掉了假阳性，然后漏诊的部分再用box补回来。</p><p>从实验结果来看，一般我这个是更新3次label（每10个epoch更新一次）就差不多了，再多也没什么提升。总体上来说，这个操作可以提高单张图片同时存在多个杆菌的分割效果，但是提升力度也没什么太令人满意的地方。可能是我的更新姿势不对？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">def update_label(predict_model, device):</span><br><span class="line">       </span><br><span class="line">       &quot;&quot;&quot;load train_pairs.txt info for check the missed diagnosis objects&quot;&quot;&quot;</span><br><span class="line">       #ann_info:[image name, image name_num_ class_id.png, bbox_ymin,</span><br><span class="line">       #                    bbox_xmin,bbox_ymax, bbox_xmax, class_name]</span><br><span class="line">       print(&apos;start to update...&apos;)</span><br><span class="line">       ANNS = &#123;&#125;</span><br><span class="line">       with open(dataset_pairs_dir, &apos;r&apos;) as da_p_txt:</span><br><span class="line">                 for ann_info in da_p_txt:</span><br><span class="line">                        # split the string line, get the list</span><br><span class="line">                        ann_info = ann_info.rstrip().split(&apos;###&apos;)</span><br><span class="line">                        if ann_info[0].rstrip()  not in ANNS:</span><br><span class="line">                            ANNS[ann_info[0].rstrip()] = []</span><br><span class="line">                        ANNS[ann_info[0].rstrip()].append(ann_info)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       predict_model.eval()</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">       # define the same image transformations</span><br><span class="line">       transformations = transforms.Compose([</span><br><span class="line">                                             transforms.ToTensor(),</span><br><span class="line">                                             transforms.Normalize(mean=[0.485, 0.456, 0.406], </span><br><span class="line">                                             std=[0.229, 0.224, 0.225])</span><br><span class="line">                                             ])</span><br><span class="line"></span><br><span class="line">       update_num = 0</span><br><span class="line">       print(&apos;updating progress:&apos;)</span><br><span class="line">       with open(dataset_txt_dir, &apos;r&apos;) as da_txt:</span><br><span class="line">                 # don&apos;t use the code line below</span><br><span class="line">                 # or it will close the file and the whole programm end here (I guess)</span><br><span class="line">                 # I debug here for two hours......</span><br><span class="line">                 #lines = len(da_txt.readlines())</span><br><span class="line">                 for update_name in da_txt:</span><br><span class="line">                         update_num += 1</span><br><span class="line">                         # in RGB [W, H, depth]</span><br><span class="line">                         img = Image.open(os.path.join(img_dir, update_name).rstrip()+&apos;.jpg&apos;)</span><br><span class="line">                         img_w = img.size[0]</span><br><span class="line">                         img_h = img.size[1]</span><br><span class="line">                         img = img.resize((1632, 1216), Image.LANCZOS)</span><br><span class="line">                         input_ = transformations(img).float()</span><br><span class="line">                         # add batch_size dimension</span><br><span class="line">                         #[3, H, W]--&gt;[1, 3, H, W]</span><br><span class="line">                         input_ = input_.unsqueeze_(0)</span><br><span class="line">                         input_ = input_.to(device)</span><br><span class="line">                         pred = predict_model(input_).view([1216, 1632]).data.cpu()</span><br><span class="line">                         #pred.shape[H,W]</span><br><span class="line">                         pred = np.array(pred)</span><br><span class="line">                         &quot;&quot;&quot;crf smooth prediction&quot;&quot;&quot;</span><br><span class="line">                         crf_pred = run_densecrf(img_dir, update_name,  pred)</span><br><span class="line"></span><br><span class="line">                         &quot;&quot;&quot;start to update&quot;&quot;&quot;</span><br><span class="line">                         last_label = Image.open(os.path.join(label_dir, update_name).rstrip()+&apos;.png&apos;)</span><br><span class="line">                         last_label = last_label.resize((1632, 1216), Image.NEAREST)</span><br><span class="line">                         last_label = np.array(last_label)</span><br><span class="line"></span><br><span class="line">                         # predicted label without false-positive segments</span><br><span class="line">                         updated_label = crf_pred + last_label</span><br><span class="line">                         updated_label = np.where((updated_label==2), 1, 0).astype(&apos;uint8&apos;)</span><br><span class="line">                         # predicted label with missed diagnosis </span><br><span class="line">                         # we just use the box segments as missed diagnosis for now</span><br><span class="line">                         info4check = ANNS[update_name.rstrip()]</span><br><span class="line">                         masks_missed = np.zeros((1216, 1632), np.uint8)</span><br><span class="line">                         for box4check in info4check:</span><br><span class="line">                                xmin = box4check[3]</span><br><span class="line">                                ymin = box4check[2]</span><br><span class="line">                                xmax = box4check[5]</span><br><span class="line">                                ymax = box4check[4]</span><br><span class="line">                                xmin, ymin, xmax, ymax = get_int_coor(xmin, ymin, </span><br><span class="line">                                                                       xmax, ymax, img_w, img_h)</span><br><span class="line">                                xmin = int(xmin * 1632 / img_w)</span><br><span class="line">                                xmax = int(xmax * 1632 / img_w)</span><br><span class="line">                                ymin = int(ymin * 1216 / img_h)</span><br><span class="line">                                ymax = int(ymax * 1216 / img_h)</span><br><span class="line">                                if np.sum(updated_label[ymin:ymax, xmin:xmax]) == 0:</span><br><span class="line">                                    masks_missed[ymin:ymax, xmin:xmax] = 1</span><br><span class="line"></span><br><span class="line">                         updated_label = updated_label + masks_missed</span><br><span class="line">                         scipy.misc.toimage(updated_label, cmin=0, cmax=255, pal=colors_map, </span><br><span class="line">                                                            mode=&apos;P&apos;).save(os.path.join(label_dir, </span><br><span class="line">                                                                           update_name).rstrip()+ &apos;.png&apos;)</span><br><span class="line">                         print(&apos;&#123;&#125; / &#123;&#125;&apos;.format(update_num, len(ANNS)), end=&apos;\r&apos;)</span><br></pre></td></tr></table></figure><h3 id="metric"><a href="#metric" class="headerlink" title="metric"></a>metric</h3><p>一般的segmentation论文都是用IoU来进行比较的，但是这个数据集没有segmentation groundtruth，所以我就自己定义了个检测的acc：预测的mask和框有交叉(np.sum(region of box)!=0)，就认为检测出了一个，然后算average acc，通过这个指标和test set上的预测结果来大致衡量哪些方法组合在一起不错。最后总结下来，还是GrabCut+FCN+FL($\alpha=0.75,\gamma=1$)更好些，不过我没加大UNet的深度和通道数，否则的话我猜想可能UNet会占上风。</p><p>篇幅有限，放几个还不错的预测结果：</p><p><img src="/2019/10/04/实习痰涂片项目总结/4.png" alt="GrabCut+FCN+FL"></p><p><img src="/2019/10/04/实习痰涂片项目总结/5.png" alt="GrabCut+FCN+FL"></p><p><img src="/2019/10/04/实习痰涂片项目总结/6.png" alt="GrabCut+UNet+FL，UNet的结果似乎要圆润一些"></p><p><img src="/2019/10/04/实习痰涂片项目总结/7.png" alt="GrabCut+FCN+FL更新3次label，效果。。也就马马虎虎吧"></p><h3 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h3><p>总的来说，最后的弱监督分割还是收获挺多的，尤其是自己的工程能力得到了锻炼，代码组织和书写也得到了一定地提升，最后相关成果也写成论文投了ISBI会议，如果能中的话，还是很舒服的^-^</p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Richard YU</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://densecollections.top/2019/10/04/实习痰涂片项目总结/" title="实习痰涂片项目总结">http://densecollections.top/2019/10/04/实习痰涂片项目总结/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/computer-vision/" rel="tag"><i class="fa fa-tag"></i> computer vision</a> <a href="/tags/deep-learning/" rel="tag"><i class="fa fa-tag"></i> deep learning</a> <a href="/tags/CNN/" rel="tag"><i class="fa fa-tag"></i> CNN</a> <a href="/tags/medical-image-analysis/" rel="tag"><i class="fa fa-tag"></i> medical image analysis</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/10/03/实习见闻及其他/" rel="next" title="实习见闻及其他"><i class="fa fa-chevron-left"></i> 实习见闻及其他</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2019/11/17/FutureMapping2/" rel="prev" title="FutureMapping2">FutureMapping2 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/header.jpg" alt="Richard YU"><p class="site-author-name" itemprop="name">Richard YU</p><div class="site-description motion-element" itemprop="description">Today everything exists to end in a photograph</div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">11</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">51</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/richardyu114" title="GitHub &rarr; https://github.com/richardyu114" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Yu1145635107" title="Twitter &rarr; https://twitter.com/Yu1145635107" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a> </span><span class="links-of-author-item"><a href="https://instagram.com/d.h.richard" title="Instagram &rarr; https://instagram.com/d.h.richard" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a> </span><span class="links-of-author-item"><a href="https://weibo.com/u/5211687990" title="Weibo &rarr; https://weibo.com/u/5211687990" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a> </span><span class="links-of-author-item"><a href="https://www.douban.com/people/161993653/" title="豆瓣 &rarr; https://www.douban.com/people/161993653/" rel="noopener" target="_blank"><i class="fa fa-fw fa-paperclip"></i>豆瓣</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#classification"><span class="nav-number">1.</span> <span class="nav-text">classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dataset"><span class="nav-number">1.1.</span> <span class="nav-text">dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#train"><span class="nav-number">1.2.</span> <span class="nav-text">train</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#test"><span class="nav-number">1.3.</span> <span class="nav-text">test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary"><span class="nav-number">1.4.</span> <span class="nav-text">summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#weakly-semantic-segmentation"><span class="nav-number">2.</span> <span class="nav-text">weakly semantic segmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pre-processing"><span class="nav-number">2.1.</span> <span class="nav-text">pre-processing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#train-1"><span class="nav-number">2.2.</span> <span class="nav-text">train</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#post-processing"><span class="nav-number">2.3.</span> <span class="nav-text">post-processing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#metric"><span class="nav-number">2.4.</span> <span class="nav-text">metric</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-1"><span class="nav-number">2.5.</span> <span class="nav-text">summary</span></a></li></ol></li></ol></div></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Richard YU</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">240k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">3:38</span></div><div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.1</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script><script src="/js/utils.js?v=7.1.1"></script><script src="/js/motion.js?v=7.1.1"></script><script src="/js/affix.js?v=7.1.1"></script><script src="/js/schemes/pisces.js?v=7.1.1"></script><script src="/js/scrollspy.js?v=7.1.1"></script><script src="/js/post-details.js?v=7.1.1"></script><script src="/js/next-boot.js?v=7.1.1"></script><script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>var GUEST=["nick","mail","link"],guest="nick,mail,link";guest=guest.split(",").filter(function(e){return-1<GUEST.indexOf(e)}),new Valine({el:"#comments",verify:!0,notify:!0,appId:"duGoywhUmLrx9pM6qQhmf47c-gzGzoHsz",appKey:"m7fc8w4Di5qnAXXaJ5Gp3Pgg",placeholder:"欢迎评论！请留下你的邮箱",avatar:"mm",meta:guest,pageSize:"10",visitor:!0,lang:"zh-cn"})</script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });</script><script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>$(".highlight").not(".gist .highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea"),o=window.pageYOffset||document.documentElement.scrollTop;n.style.top=o+"px",n.style.position="absolute",n.style.opacity="0",n.readOnly=!0,n.value=e,document.body.appendChild(n),n.select(),n.setSelectionRange(0,e.length),n.readOnly=!1,document.execCommand("copy")?$(this).text("复制成功"):$(this).text("复制失败"),n.blur(),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script></body></html>