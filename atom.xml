<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>自拙集</title>
  
  <subtitle>Work cures everything</subtitle>
  <link href="http://densecollections.top/atom.xml" rel="self"/>
  
  <link href="http://densecollections.top/"/>
  <updated>2021-01-02T11:53:19.661Z</updated>
  <id>http://densecollections.top/</id>
  
  <author>
    <name>Richard YU</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SSD-RefineDet论文阅读</title>
    <link href="http://densecollections.top/posts/ssd-refinedet-paper/"/>
    <id>http://densecollections.top/posts/ssd-refinedet-paper/</id>
    <published>2020-08-03T13:25:33.000Z</published>
    <updated>2021-01-02T11:53:19.661Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><a href="https://arxiv.org/pdf/1512.02325.pdf">paper</a></p><p>参考：<a href="https://zhuanlan.zhihu.com/p/33544892">1</a></p><p>SSD是一篇写得非常好，读起来也非常舒服的文章。</p><hr><p>写于fater rcnn之后，所以anchor不是新思想，新思想是多个尺寸的feature map然后分别设置不同尺寸的anchor去预测和回归，有特征金字塔的萌芽</p><blockquote><p>Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/ssd-refinedet-paper/SSD.JPG" alt></p><a id="more"></a><p>backbone使用VGG16，但是把fc6和fc7换成了卷积，其中conv5_出来后的maxpool5的$2\times2$, $s=2$换成了$3\times3$, $s=1$的，没有改变尺寸大小，fc6换成了卷积层并使用使用了空洞卷积，增大感受野，去掉了fc8层，直接接上卷积层进行特征抽取和预测。为什么要在maxpool5和fc6这里做如此处理呢？作者在论文的实验分析部分指出，如果单纯只是用原始vgg16 conv5_3来作预测，效果没有太大变化，但是速度会比使用空洞卷积慢20%左右，这里可能是为了速度上的工程考量。在后续自己的工作中，似乎可以考虑实验下空洞卷积带来的作用。</p><p>此外，由于用了VGG16种的conv4_3作为第一个预测层，相比于其他层比较靠前，论文中用了<a href="https://arxiv.org/abs/1506.04579">parseNet</a>的L2 normalization对每个像素在channel层面上做了归一化（跟layer norm不一样，在图像上layer norm是CHW做了归一化，保留N维度）。这样处理是想匹配特征数据范围，便于模型收敛。但是具体原因应该还是实际实验发现的，不然也不会只L2 NORM一个层，其具体计算公式为：</p><script type="math/tex; mode=display">y_{i}=\frac{x_{i}}{\sqrt{\sum_{i=1}^{D} x_{i}^{2}}}</script><p>计算完之后还需要乘上指定的scale参数进行缩放。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">norm &#x3D; conv4_3_feats.pow(2).sum(dim&#x3D;1, keepdim&#x3D;True).sqrt()+1e-10  # (N, 1, 38, 38)</span><br><span class="line">conv4_3_feats &#x3D; conv4_3_feats &#x2F; norm  # (N, 512, 38, 38)</span><br><span class="line">conv4_3_feats &#x3D; conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)</span><br></pre></td></tr></table></figure><p>更多相关代码示例见<a href="https://blog.csdn.net/flyfish1986/article/details/105586716">此</a>，以及作者对于SSD中对Conv4_3做归一化，加入variance的<a href="https://zhuanlan.zhihu.com/p/39399799">解释</a>。</p><p>利用卷积来预测框的偏移量和类别置信度，所以最后得卷积核数量为$(C+4)k$，其中k为anchor的预设数量，C包含了背景类。</p><p>如何获得GT:</p><blockquote><p>We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]). Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5).</p></blockquote><p>anchor box和GT取IOU，大于阈值0.5的就匹配上，负责预测该GT（可以一个GT匹配多个anchor box但是反过来不行，如果出现这样的情况，那么该anchor box只会取与某个GT有最大IOU的）</p><p>但是anchor box不是通过该feature map的感受野去匹配的，而是人为指定的scale和中心，原文是最浅的层scale是0.2，最深的层scale是0.9（相对与原图而言），anchor scale为{1, 2,3,1/2,1/3,1’}，这个1’是每个特征图单独指定的，与1对应的区域大小不一样。虽然anchor scale设置了6个，对应的大小也有相应的公式，但是并不是每一个feature map上都用了全部的anchor scale,大小的公式推导也只是针对后面新增的特征图，vgg16 head里用来预测的是单独设置的，原文中也对此做了详细说明：</p><blockquote><p>Figure 2 shows the architecture details of the SSD300 model. We use <strong>conv4_3, conv7 (fc7), conv8_2, conv9_2, conv10_2, and conv11_2 to predict</strong> both location and confidences. <strong>We set default box with scale 0.1 on conv4_3</strong>  . We initialize the parameters for all the newly added convolutional layers with the ”xavier” method [20]. <strong>For conv4_3, conv10_2 and conv11_2, we only associate 4 default boxes</strong> at each feature map location – omitting aspect ratios of $\frac{1}{3}$ and 3. For all other layers, we put 6 default boxes as described in Sec. 2.2. Since, as pointed out in [12], conv4_3 has a different feature scale compared to the other layers, we use the <strong>L2 normalization</strong> technique introduced in [12] to scale the feature norm at each location in the feature map to 20 and learn the scale during back propagation.</p></blockquote><p>为了更清楚整体的流程和一些细节，我这里直接贴出参考的博客<a href="https://zhuanlan.zhihu.com/p/33544892">1</a>中的解释：</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/ssd-refinedet-paper/SSD_anchor1.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/ssd-refinedet-paper/SSD_anchor2.JPG" alt></p><p>由于特征图每个cell内部都设置了几个同样的大小和长宽比的anchor，其对应的中心就是特征图cell的中心放大该特征图下采样倍数对应到原图区域的中心位置。</p><p>损失函数与faster rcnn相同，offset的预测类型也是一样的，回归用smooth L1，分类用softmax加交叉熵，两者之间的权重通过交叉验证设置为1</p><p>hard negative mining，通过对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，尽量控制正负样本比例在1：3左右</p><p>关于为何选择多尺度feature map而不是利用多尺度图片进行训练：</p><blockquote><p>To handle different object scales, some methods [4,9] suggest processing the image at different sizes and combining the results afterwards. However, by utilizing feature maps from several different layers in a single network for prediction we can mimic the same effect, while also sharing parameters across all object scales.</p></blockquote><p>对小物体检测效果差，因为结构上是从浅层去检测，语义信息不够充分。</p><blockquote><p>Figure 4 shows that SSD is very sensitive to the bounding box size. In other words, it has much worse performance on smaller objects than bigger objects. This is not surprising because those small objects may not even have any information at the very top layers.</p></blockquote><p>相较于faster rcnn，对data augumentation（random crop去zoom in图像类似的操作，其实在这一点上，我觉得yolo v4的“马赛克”数据增强手段有异曲同工之妙，可以说是增强普适版，这种数据增强操作对一阶的object detection似乎是标配）的依赖很重，原文的结果表示加了数据增强会比不加高8-9个点，作者认为可能是faster rcnn feature pooling这一步会让模型对物体的translation更加鲁棒，”use a feature pooling step during classification that is relatively robust to object translation by design”</p><p>最后总结了当时的目标检测思路，将SSD与主流的方法进行了比较，其中我认为关键的地方在于作者指出，SSD其实就是相当于faster rcnn中的RPN:</p><blockquote><p>Our SSD is very similar to the region proposal network (RPN) in Faster R-CNN in that we also use a fixed set of (default) boxes for prediction, similar to the anchor boxes in the RPN. But instead of using these to pool features and evaluate another classifier, we simultaneously produce a score for each object category in each box. Thus, our approach avoids the complication of merging RPN with Fast R-CNN and is easier to train, faster, and straightforward to integrate in other tasks.</p></blockquote><h2 id="RefineDet"><a href="#RefineDet" class="headerlink" title="RefineDet"></a>RefineDet</h2><p><a href="https://arxiv.org/abs/1711.06897">paper</a>, <a href="https://github.com/sfzhang15/RefineDet">code</a>，<a href="https://github.com/lzx1413/PytorchSSD">pytorch版本</a></p><p>参考：<a href="https://zhuanlan.zhihu.com/p/50917804">1</a></p><p><strong>目的：</strong>借鉴一阶检测算法类似SSD的高效率推断和二阶检测算法类似Faster RCNN的高检测率，构建anchor两次回归的二阶段回归模型，但是没有用到图像proposal，只是针对特征图来做。</p><p><strong>结构/思想部分：</strong></p><p>第一阶段：</p><blockquote><p>(1)  filter  out  negative anchors to reduce search space for the classifier, and (2)  coarsely  adjust  the  locations  and  sizes  of  anchors  to provide  better  initialization  for  the  subsequent  regressor.</p></blockquote><p>第二阶段：</p><blockquote><p>The  latter  module  takes  the  refined  anchors  as  the  input from the former to further improve the regression and predict  multi-class  label. </p></blockquote><p>第一阶段与faster rcnn类似，分出positive的anchor box，也就是包含物体，非background，并作一些修正，第二阶段用的特征图会在第一阶段的基础上继续做一些特征提取和融合操作，以增强多分类和回归效果。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/ssd-refinedet-paper/refinedet.JPG" alt="refinedet结构图，可以看出，大致是RPN+FPN+RPN堆叠起来的。论文中给的三个模块名称分别为ARM+TCB+ODM"></p><p>第一阶段，每个特征图的grid cell固定了几个anchor box，然后预测四个offset，针对原始设定的anchor而言，并且给出foreground置信度</p><p>ARM和ODM的特征图维度相同</p><p>ARM中negative box的negative 置信度大于阈值（0.99）就丢掉，预测的时候也是，ARM只传递hard negative anchor boxes和修正过的positive anchor boxes给ODM</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/ssd-refinedet-paper/TCB.JPG" alt="TCB的结构图，逐级与高层特征图进行侧向连接"></p><p><strong>训练/推理部分：</strong></p><p>数据增强：crop，expand, flipping等，与SSD类似</p><p>backbone：采用vgg16或者resnet101，以VGG16为例，把最后两层全连接层fc_6, fc_7去掉，加入两个卷积层conv_fc6, conv_fc7，接在pool5后面的卷积层conv_fc6和SSD一样做了空洞卷积（dilation=6），在conv_fc7之后又接了两个卷积层，其中最后一个用了stride=2进行下采样，预测的特征图是Conv4_3, Conv_5_3, Conv6_1, Conv6_2，前面两个也做了跟SSD一样的L2 norm操作。所欲预测的特征图分别对原图下采样了8，16，32，64倍，每个特征图安排一个大小（4乘以下采样倍数）的和三个不同长宽比（0.5，1，2）的anchors，匹配原则：首先让每个GT匹配与其有最大IOU的anchor，然后再让anchor匹配与其IOU大于0.5的GT，这一点与SSD相同。</p><blockquote><p>Specifically, we first match each ground truth to the anchorbox with the best overlap score, and then match the anchorboxes to any ground truth with overlap higher than0.5</p></blockquote><p>hard negative mining：与SSD相同，选择那些负样本分类置信度小的，也就是loss大的，进行排序，保持正负样本在1：3左右或者更高，</p><p>loss function: 分ARM和ODM两部分。类似faster rcnn的rpn和fast rcnn, 分类都采用交叉熵，回归采用smooth L1，论文中四个损失函数之间没有权重设置，都是等比例贡献，如下所示。N代表positive anchor的数量，只对正样本回归，但是分类对正负样本都会进行，且如前所述，ARM会把难分的负样本传给ODM继续分类。论文中还提到，如果ARM或者ODM中的N为0，那么分类和回归损失都置为0。</p><script type="math/tex; mode=display">\begin{array}{l}\mathcal{L}\left(\left\{p_{i}\right\},\left\{x_{i}\right\},\left\{c_{i}\right\},\left\{t_{i}\right\}\right)=\frac{1}{N_{\text {arm }}}\left(\sum_{i} \mathcal{L}_{\mathrm{b}}\left(p_{i},\left[l_{i}^{*} \geq 1\right]\right)\right. \left.+\sum_{i}\left[l_{i}^{*} \geq 1\right] \mathcal{L}_{\mathrm{r}}\left(x_{i}, g_{i}^{*}\right)\right)+  \frac{1}{N_{\text {odm }}}\left(\sum_{i} \mathcal{L}_{\mathrm{m}}\left(c_{i}, l_{i}^{*}\right)\right. \left.+\sum_{i}\left[l_{i}^{*} \geq 1\right] \mathcal{L}_{\mathrm{r}}\left(t_{i}, g_{i}^{*}\right)\right)\end{array}</script><p>训练：采用pretrained model，batch_size=32, 新增的卷积层采用“Xavier”方法初始化权重，采用带动量（0.9）的SGD训练，weight decay为0.0005，初始学习率为0.001，后续会随着迭代次数增加而调整。</p><p>推理：ARM先去掉置信度高的负样本（跟阈值比较），然后对剩下的anchors进行回归refine，输送给ODM，ODM进一步分类和refine输出top400的框，然后利用NMS（阈值0.45）筛选，然后最多留下200的框产生最后的结果</p><p>结果：300的图片可以达到0.8的mAP (PASCAL VOC)，Titan X上可以达到40FPS，512的输入尺度，速度掉到了24FPS，几乎一半。</p><p>这里根据博客1提到的pytorch源码，其实一阶段对anchor的抑制并没有做，而是全部送到了ODM，然后ODM根据ARM的置信度和当前的分类置信度一起筛选anchor.</p><blockquote><p><strong>这个操作可能是我理解有误，在pytorch_refindet未有体现，实际上ARM中所有anchor都直接传到了ODM中，但ARM中所有anchor确实完成了1st-stage的refine，ODM中为每个anchor进一步预测2nd-stage的bbox reg + cls，结合1st-stage的objectness得分 + 2nd-stage的bbox cls（具体类别）得分，一起筛选有效anchor，并结合2nd-stage的bbox reg预测结果，完成2nd-stage的refine后，最后NMS输出结果；</strong></p></blockquote><p>refinedet结构pytorch<a href="https://github.com/lzx1413/PytorchSSD/blob/master/models/RefineSSD_vgg.py">代码</a>（以VGG16为backbone）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">from layers import *</span><br><span class="line">from .base_models import vgg, vgg_base</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def vgg(cfg, i&#x3D;3, batch_norm&#x3D;False):</span><br><span class="line">    layers &#x3D; []</span><br><span class="line">    in_channels &#x3D; i</span><br><span class="line">    for v in cfg:</span><br><span class="line">        if v &#x3D;&#x3D; &#39;M&#39;:</span><br><span class="line">            layers +&#x3D; [nn.MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2)]</span><br><span class="line">        elif v &#x3D;&#x3D; &#39;C&#39;:</span><br><span class="line">            layers +&#x3D; [nn.MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2, ceil_mode&#x3D;True)]</span><br><span class="line">        else:</span><br><span class="line">            conv2d &#x3D; nn.Conv2d(in_channels, v, kernel_size&#x3D;3, padding&#x3D;1)</span><br><span class="line">            if batch_norm:</span><br><span class="line">                layers +&#x3D; [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace&#x3D;True)]</span><br><span class="line">            else:</span><br><span class="line">                layers +&#x3D; [conv2d, nn.ReLU(inplace&#x3D;True)]</span><br><span class="line">            in_channels &#x3D; v</span><br><span class="line">    pool5 &#x3D; nn.MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0)</span><br><span class="line">    conv6 &#x3D; nn.Conv2d(512, 1024, kernel_size&#x3D;3, padding&#x3D;6, dilation&#x3D;6)</span><br><span class="line">    conv7 &#x3D; nn.Conv2d(1024, 1024, kernel_size&#x3D;1)</span><br><span class="line">    layers +&#x3D; [pool5, conv6,</span><br><span class="line">               nn.ReLU(inplace&#x3D;True), conv7, nn.ReLU(inplace&#x3D;True)]</span><br><span class="line">    return layers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vgg_base &#x3D; &#123;</span><br><span class="line">    &#39;320&#39;: [64, 64, &#39;M&#39;, 128, 128, &#39;M&#39;, 256, 256, 256, &#39;C&#39;, 512, 512, 512, &#39;M&#39;,</span><br><span class="line">            512, 512, 512],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RefineSSD(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;Single Shot Multibox Architecture</span><br><span class="line">    The network is composed of a base VGG network followed by the</span><br><span class="line">    added multibox conv layers.  Each multibox layer branches into</span><br><span class="line">        1) conv2d for class conf scores</span><br><span class="line">        2) conv2d for localization predictions</span><br><span class="line">        3) associated priorbox layer to produce default bounding</span><br><span class="line">           boxes specific to the layer&#39;s feature map size.</span><br><span class="line">    See: https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1512.02325.pdf for more details.</span><br><span class="line">    Args:</span><br><span class="line">        phase: (string) Can be &quot;test&quot; or &quot;train&quot;</span><br><span class="line">        base: VGG16 layers for input, size of either 300 or 500</span><br><span class="line">        extras: extra layers that feed to multibox loc and conf layers</span><br><span class="line">        head: &quot;multibox head&quot; consists of loc and conf conv layers</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, size, num_classes, use_refine&#x3D;False):</span><br><span class="line">        super(RefineSSD, self).__init__()</span><br><span class="line">        self.num_classes &#x3D; num_classes</span><br><span class="line">        # TODO: implement __call__ in PriorBox</span><br><span class="line">        self.size &#x3D; size</span><br><span class="line">        self.use_refine &#x3D; use_refine</span><br><span class="line"></span><br><span class="line">        # SSD network</span><br><span class="line">        self.base &#x3D; nn.ModuleList(vgg(vgg_base[&#39;320&#39;], 3))</span><br><span class="line">        # Layer learns to scale the l2 normalized features from conv4_3</span><br><span class="line">        self.L2Norm_4_3 &#x3D; L2Norm(512, 10)</span><br><span class="line">        self.L2Norm_5_3 &#x3D; L2Norm(512, 8)</span><br><span class="line">        self.last_layer_trans &#x3D; nn.Sequential(nn.Conv2d(512, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1),</span><br><span class="line">                                              nn.ReLU(inplace&#x3D;True),</span><br><span class="line">                                              nn.Conv2d(256, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1),</span><br><span class="line">                                              nn.Conv2d(256, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1))</span><br><span class="line">        self.extras &#x3D; nn.Sequential(nn.Conv2d(1024, 256, kernel_size&#x3D;1, stride&#x3D;1, padding&#x3D;0), nn.ReLU(inplace&#x3D;True), \</span><br><span class="line">                                    nn.Conv2d(256, 512, kernel_size&#x3D;3, stride&#x3D;2, padding&#x3D;1), nn.ReLU(inplace&#x3D;True))</span><br><span class="line"></span><br><span class="line">        if use_refine:</span><br><span class="line">            self.arm_loc &#x3D; nn.ModuleList([nn.Conv2d(512, 12, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                          nn.Conv2d(512, 12, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                          nn.Conv2d(1024, 12, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                          nn.Conv2d(512, 12, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                          ])</span><br><span class="line">            self.arm_conf &#x3D; nn.ModuleList([nn.Conv2d(512, 6, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                           nn.Conv2d(512, 6, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                           nn.Conv2d(1024, 6, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                           nn.Conv2d(512, 6, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                           ])</span><br><span class="line">        self.odm_loc &#x3D; nn.ModuleList([nn.Conv2d(256, 12, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                      nn.Conv2d(256, 12, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                      nn.Conv2d(256, 12, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                      nn.Conv2d(256, 12, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                      ])</span><br><span class="line">        self.odm_conf &#x3D; nn.ModuleList([nn.Conv2d(256, 3*num_classes, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                       nn.Conv2d(256, 3*num_classes, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                       nn.Conv2d(256, 3*num_classes, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                       nn.Conv2d(256, 3*num_classes, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1), \</span><br><span class="line">                                       ])</span><br><span class="line">        self.trans_layers &#x3D; nn.ModuleList([nn.Sequential(nn.Conv2d(512, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1),</span><br><span class="line">                                                         nn.ReLU(inplace&#x3D;True),</span><br><span class="line">                                                         nn.Conv2d(256, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1)), \</span><br><span class="line">                                           nn.Sequential(nn.Conv2d(512, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1),</span><br><span class="line">                                                         nn.ReLU(inplace&#x3D;True),</span><br><span class="line">                                                         nn.Conv2d(256, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1)), \</span><br><span class="line">                                           nn.Sequential(nn.Conv2d(1024, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1),</span><br><span class="line">                                                         nn.ReLU(inplace&#x3D;True),</span><br><span class="line">                                                         nn.Conv2d(256, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1)), \</span><br><span class="line">                                           ])</span><br><span class="line">        self.up_layers &#x3D; nn.ModuleList([nn.ConvTranspose2d(256, 256, kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0),</span><br><span class="line">                                        nn.ConvTranspose2d(256, 256, kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0),</span><br><span class="line">                                        nn.ConvTranspose2d(256, 256, kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0), ])</span><br><span class="line">        self.latent_layrs &#x3D; nn.ModuleList([nn.Conv2d(256, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1),</span><br><span class="line">                                           nn.Conv2d(256, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1),</span><br><span class="line">                                           nn.Conv2d(256, 256, kernel_size&#x3D;3, stride&#x3D;1, padding&#x3D;1),</span><br><span class="line">                                           ])</span><br><span class="line"></span><br><span class="line">        self.softmax &#x3D; nn.Softmax()</span><br><span class="line"></span><br><span class="line">    def forward(self, x, test&#x3D;False):</span><br><span class="line">        &quot;&quot;&quot;Applies network layers and ops on input image(s) x.</span><br><span class="line">        Args:</span><br><span class="line">            x: input image or batch of images. Shape: [batch,3*batch,300,300].</span><br><span class="line">        Return:</span><br><span class="line">            Depending on phase:</span><br><span class="line">            test:</span><br><span class="line">                Variable(tensor) of output class label predictions,</span><br><span class="line">                confidence score, and corresponding location predictions for</span><br><span class="line">                each object detected. Shape: [batch,topk,7]</span><br><span class="line">            train:</span><br><span class="line">                list of concat outputs from:</span><br><span class="line">                    1: confidence layers, Shape: [batch*num_priors,num_classes]</span><br><span class="line">                    2: localization layers, Shape: [batch,num_priors*4]</span><br><span class="line">                    3: priorbox layers, Shape: [2,num_priors*4]</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        arm_sources &#x3D; list()</span><br><span class="line">        arm_loc_list &#x3D; list()</span><br><span class="line">        arm_conf_list &#x3D; list()</span><br><span class="line">        obm_loc_list &#x3D; list()</span><br><span class="line">        obm_conf_list &#x3D; list()</span><br><span class="line">        obm_sources &#x3D; list()</span><br><span class="line"></span><br><span class="line">        # apply vgg up to conv4_3 relu</span><br><span class="line">        for k in range(23):</span><br><span class="line">            x &#x3D; self.base[k](x)</span><br><span class="line"></span><br><span class="line">        s &#x3D; self.L2Norm_4_3(x)</span><br><span class="line">        arm_sources.append(s)</span><br><span class="line"></span><br><span class="line">        # apply vgg up to conv5_3</span><br><span class="line">        for k in range(23, 30):</span><br><span class="line">            x &#x3D; self.base[k](x)</span><br><span class="line">        s &#x3D; self.L2Norm_5_3(x)</span><br><span class="line">        arm_sources.append(s)</span><br><span class="line"></span><br><span class="line">        # apply vgg up to fc7</span><br><span class="line">        for k in range(30, len(self.base)):</span><br><span class="line">            x &#x3D; self.base[k](x)</span><br><span class="line">        arm_sources.append(x)</span><br><span class="line">        # conv6_2</span><br><span class="line">        x &#x3D; self.extras(x)</span><br><span class="line">        arm_sources.append(x)</span><br><span class="line">        # apply multibox head to arm branch</span><br><span class="line">        if self.use_refine:</span><br><span class="line">            for (x, l, c) in zip(arm_sources, self.arm_loc, self.arm_conf):</span><br><span class="line">                arm_loc_list.append(l(x).permute(0, 2, 3, 1).contiguous())</span><br><span class="line">                arm_conf_list.append(c(x).permute(0, 2, 3, 1).contiguous())</span><br><span class="line">            arm_loc &#x3D; torch.cat([o.view(o.size(0), -1) for o in arm_loc_list], 1)</span><br><span class="line">            arm_conf &#x3D; torch.cat([o.view(o.size(0), -1) for o in arm_conf_list], 1)</span><br><span class="line">        x &#x3D; self.last_layer_trans(x)</span><br><span class="line">        obm_sources.append(x)</span><br><span class="line"></span><br><span class="line">        # get transformed layers</span><br><span class="line">        trans_layer_list &#x3D; list()</span><br><span class="line">        for (x_t, t) in zip(arm_sources, self.trans_layers):</span><br><span class="line">            trans_layer_list.append(t(x_t))</span><br><span class="line">        # fpn module</span><br><span class="line">        trans_layer_list.reverse()</span><br><span class="line">        arm_sources.reverse()</span><br><span class="line">        for (t, u, l) in zip(trans_layer_list, self.up_layers, self.latent_layrs):</span><br><span class="line">            x &#x3D; F.relu(l(F.relu(u(x) + t, inplace&#x3D;True)), inplace&#x3D;True)</span><br><span class="line">            obm_sources.append(x)</span><br><span class="line">        obm_sources.reverse()</span><br><span class="line">        for (x, l, c) in zip(obm_sources, self.odm_loc, self.odm_conf):</span><br><span class="line">            obm_loc_list.append(l(x).permute(0, 2, 3, 1).contiguous())</span><br><span class="line">            obm_conf_list.append(c(x).permute(0, 2, 3, 1).contiguous())</span><br><span class="line">        obm_loc &#x3D; torch.cat([o.view(o.size(0), -1) for o in obm_loc_list], 1)</span><br><span class="line">        obm_conf &#x3D; torch.cat([o.view(o.size(0), -1) for o in obm_conf_list], 1)</span><br><span class="line"></span><br><span class="line">        # apply multibox head to source layers</span><br><span class="line"></span><br><span class="line">        if test:</span><br><span class="line">            if self.use_refine:</span><br><span class="line">                output &#x3D; (</span><br><span class="line">                    arm_loc.view(arm_loc.size(0), -1, 4),  # loc preds</span><br><span class="line">                    self.softmax(arm_conf.view(-1, 2)),  # conf preds</span><br><span class="line">                    obm_loc.view(obm_loc.size(0), -1, 4),  # loc preds</span><br><span class="line">                    self.softmax(obm_conf.view(-1, self.num_classes)),  # conf preds</span><br><span class="line">                )</span><br><span class="line">            else:</span><br><span class="line">                output &#x3D; (</span><br><span class="line">                    obm_loc.view(obm_loc.size(0), -1, 4),  # loc preds</span><br><span class="line">                    self.softmax(obm_conf.view(-1, self.num_classes)),  # conf preds</span><br><span class="line">                )</span><br><span class="line">        else:</span><br><span class="line">            if self.use_refine:</span><br><span class="line">                output &#x3D; (</span><br><span class="line">                    arm_loc.view(arm_loc.size(0), -1, 4),  # loc preds</span><br><span class="line">                    arm_conf.view(arm_conf.size(0), -1, 2),  # conf preds</span><br><span class="line">                    obm_loc.view(obm_loc.size(0), -1, 4),  # loc preds</span><br><span class="line">                    obm_conf.view(obm_conf.size(0), -1, self.num_classes),  # conf preds</span><br><span class="line">                )</span><br><span class="line">            else:</span><br><span class="line">                output &#x3D; (</span><br><span class="line">                    obm_loc.view(obm_loc.size(0), -1, 4),  # loc preds</span><br><span class="line">                    obm_conf.view(obm_conf.size(0), -1, self.num_classes),  # conf preds</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">    def load_weights(self, base_file):</span><br><span class="line">        other, ext &#x3D; os.path.splitext(base_file)</span><br><span class="line">        if ext &#x3D;&#x3D; &#39;.pkl&#39; or &#39;.pth&#39;:</span><br><span class="line">            print(&#39;Loading weights into state dict...&#39;)</span><br><span class="line">            self.load_state_dict(torch.load(base_file, map_location&#x3D;lambda storage, loc: storage))</span><br><span class="line">            print(&#39;Finished!&#39;)</span><br><span class="line">        else:</span><br><span class="line">            print(&#39;Sorry only .pth and .pkl files supported.&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def build_net(size&#x3D;320, num_classes&#x3D;21, use_refine&#x3D;False):</span><br><span class="line">    if size !&#x3D; 320:</span><br><span class="line">        print(&quot;Error: Sorry only SSD300 and SSD512 is supported currently!&quot;)</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    return RefineSSD(size, num_classes&#x3D;num_classes, use_refine&#x3D;use_refine)</span><br></pre></td></tr></table></figure><h2 id="RefineDet-1"><a href="#RefineDet-1" class="headerlink" title="RefineDet++"></a>RefineDet++</h2><p><a href="https://ieeexplore.ieee.org/document/9058700">paper</a>；</p><p><a href="https://www.bilibili.com/video/BV1Dt411p7TN?from=search&amp;seid=12388514684040581306">video</a>；</p><p>原作者对CVPR论文进行了丰富，整体结构没什么区别，主要不同是在第二阶段的分类和回归中使用AlignConv来进行操作。</p><p>这样设计的原因是修正后的anchor和特征之间存在不对齐的问题。回顾下经典的two-stage目标检测器faster rcnn，先利用anchor找出修正原图的proposal，然后利用ROI Pooling在特征图上抠出修正后的proposal对应的特征区域，RPN和fast rcnn阶段没有直接的联系。这样的话可以保证每次回归都是在对应的特征上进行，不容易被干扰。one-stage检测器由于没有这个区域抠取的操作，只是在同样的特征图上进行，分类和回归，每个特征图上的像素点可能设置了不同大小的anchor，而该特征图对应的感受野是一定的，所以不一定可以让检测效果好，SSD的多尺度特征图预测以及FPN可以在一定程度上解决该问题。在refinedet这里，第二阶段利用的特征图来自于第一阶段，在位置上没做改变，所以还是在原始的位置进行预测，但是第一阶段已经对anchor进行修正，它所对应的特征可能已经偏移了，所以依然没解决特征不对齐的问题。后来图森的AlignDet对二阶段的卷积做了处理，优化了refinedet的检测结果，其中相关的知乎问题解释了refinedet的这个问题，链接在<a href="https://www.zhihu.com/question/338959309/answer/780051681">此</a>，有关AlignDet和RePoints后面会接着讲。</p><p>实际上，目标检测中的Feature Alignment是目前提及比较多的问题，参考<a href="https://blog.csdn.net/weixin_42096202/article/details/105178855">1</a>的总结，主要分为两个部分：</p><p>1.分类和回归的不匹配，大意是分类和回归我们不应该用同样的特征，毕竟不是同一个任务，两者对特征的精细度要求也不同，像faster rcnn最后拿了ROI Pooling的特征之后共享给了分类和回归任务；</p><p>2.anchor based方法由于预设了不同尺寸，对于一个特征图来说，感受野固定，特征不对齐；一阶的级联回归，如果使用同样位置的特征图，修正后的anchor对应的区域已经发生改变，此时第二阶段的特征区域却没有对应改变；</p><p>这篇<a href="https://zhuanlan.zhihu.com/p/114700229">博客</a>根据以上两个问题做了一个论文综述。在这里具体细节和方法暂时先不展开谈，等后续看完相关论文，搞清楚了这些问题再具体开一篇博客详说。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/ssd-refinedet-paper/refinedet++.JPG" alt="refinedet++结构和alignconv操作细节"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/ssd-refinedet-paper/AlignConv.JPG" alt="alignconv设计背景"></p><p>论文中给出的AlignConv具体实现步骤如下：</p><blockquote><p>To this end, we design an alignment convolution operation(AlignConv), which uses the aligned feature from the refined anchors  to  predict  multi-class  labels  and  regress  accurate object locations, as shown in Fig. 3(c). Specifically, the newly designed AlignConv operation conducts convolution operation based on computed offsets from the refined anchors. Denoting each refined anchor with a four-tuple (x,y,h,w) that specifies the  top-left  corner  (x,y)  and  height  and  width  (h,w),  theAlignConv is conducted as follows. <strong>First, after taking the re-fined anchors from ARM, we equally divide the regions of the refined anchors into K×K parts</strong>, where K is the kernel size of convolution operation. The center of each part is computed as: for the part at i-th row and j-th column, the center locationis$(x+\frac{(2i-1)<em>w}{2K}，y+\frac{(2i-1)</em>h}{2K})$. <strong>Second, we multiply the feature values at the K×K part centers in refined anchors with the corresponding parameters of the convolution filter</strong>, see Fig. 1.In  this  way,  we  successfully  extract  more  accurate  features that  are  aligned  to  the  refined  anchors  for  object  detection.In contrast to existing deformable convolution methods [65]–[67] that learn the offsets by convolution operation with extra parameters, our AlignConv conducts the convolution with the guidance from the refined anchors of the ARM, which is more suitable for RefineDet++ and produces better performance.</p></blockquote><p>大意就是根据ARM修正后的anchor区域，来对ODM的卷积偏移一个offset，根据修正的anchor区域来调整卷积区域，不再是原来的的区域，类似可变形卷积，但是比较奇怪的是，作者在论文中也用可变形卷积代替，结果反而更差了。加了这个操作后，速度下降明显，毕竟割裂了连续的卷积操作，精度大概涨了1个点左右。</p><h2 id="AlignDet"><a href="#AlignDet" class="headerlink" title="AlignDet"></a>AlignDet</h2><p>repoints</p><p>Cascade RetinaNet解释了这个问题</p><p><a href="https://zhuanlan.zhihu.com/p/78026765">https://zhuanlan.zhihu.com/p/78026765</a></p><p><a href="https://zhuanlan.zhihu.com/p/114700229">https://zhuanlan.zhihu.com/p/114700229</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;SSD&quot;&gt;&lt;a href=&quot;#SSD&quot; class=&quot;headerlink&quot; title=&quot;SSD&quot;&gt;&lt;/a&gt;SSD&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1512.02325.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33544892&quot;&gt;1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;SSD是一篇写得非常好，读起来也非常舒服的文章。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;写于fater rcnn之后，所以anchor不是新思想，新思想是多个尺寸的feature map然后分别设置不同尺寸的anchor去预测和回归，有特征金字塔的萌芽&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/ssd-refinedet-paper/SSD.JPG&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
    <category term="object detection" scheme="http://densecollections.top/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>侯捷c++STL体系结构与内核分析</title>
    <link href="http://densecollections.top/posts/houjieC++STL/"/>
    <id>http://densecollections.top/posts/houjieC++STL/</id>
    <published>2020-06-12T08:24:02.000Z</published>
    <updated>2021-01-02T12:49:50.350Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p><a href="https://www.bilibili.com/video/BV1db411q7B8">videos</a>;</p><p><a href="https://pan.baidu.com/s/1DkCctgH1BO2Cs0NuUcyHhg">课件，密码kr24</a>;</p><h2 id="认识headers、版本、重要资源"><a href="#认识headers、版本、重要资源" class="headerlink" title="认识headers、版本、重要资源"></a>认识headers、版本、重要资源</h2><p>以STL为目标探讨泛型编程。</p><blockquote><p>使用一个东西，却不明白它的道理，不高明！</p></blockquote><ul><li><p>level 0: 使用C++标准库；</p></li><li><p>level 1: 认识C++标准库（胸中自有丘壑，体系结构应当建立起来）；</p></li><li><p>level 2: 良好使用C++标准库；</p></li><li><p>level 3: 扩充C++标准库；</p></li></ul><p>Standard Template Library (<a href="https://blog.csdn.net/qq_44770155/article/details/97882816">6大部件</a>：Container(容器) 各种基本数据结构；Adapter(适配器) 可改变containers、Iterators或Function object接口的一种组件；Algorithm(算法) 各种基本算法如sort、search…等；Iterator(迭代器) 连接containers和algorithms；Function object(函数对象)；Allocator(分配器)) 加上其他一些零碎的东西构成了C++ Standard Library.</p><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/STL.JPG" alt></p><h2 id="STL体系结构基础介绍"><a href="#STL体系结构基础介绍" class="headerlink" title="STL体系结构基础介绍"></a>STL体系结构基础介绍</h2><p>STL六大部件（component）：</p><p>数据在容器（内存的事情我们不必管，由分配器支持），处理数据在算法，连接之间的桥梁是迭代器（泛化的指针）。仿函数可以处理类之间的一些操作，适配器转换。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/six_component.JPG" alt="STL六大部件"></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int stl_component()</span><br><span class="line">&#123;</span><br><span class="line">  int ia[6] &#x3D; &#123;27, 210, 12, 47, 109, 83&#125;;</span><br><span class="line">  &#x2F;&#x2F;用到了容器和分配器定义一个vector</span><br><span class="line">  vector &lt;int, allocator&lt;int&gt; &gt; vi(ia, ia+6);</span><br><span class="line">  &#x2F;&#x2F;用到了算法，迭代器，函数适配器，函数对象</span><br><span class="line">  &#x2F;&#x2F; vi.begin()和vi.end()迭代指向vi中的元素</span><br><span class="line">  &#x2F;&#x2F; not1是negator，否定less的作用，变成了大于等于40的数</span><br><span class="line">  &#x2F;&#x2F; bind2nd绑定了less比较的第二个对象，固定在40</span><br><span class="line">  cout &lt;&lt; count_if(vi.begin(), vi.end(),</span><br><span class="line">         not1(bind2nd(less&lt;int&gt;(), 40))) &lt;&lt; endl;</span><br><span class="line"> </span><br><span class="line">  return 0;  </span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>简单提及时间复杂度。</p><p>“前闭后开”曲间，标准库规定容器迭代器的最后一个元素取不到，即[iter.begin(), iter.end() )，此外容器也不一定是连续空间，比如链表。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Container&lt;T&gt; c;</span><br><span class="line">...</span><br><span class="line">Container&lt;T&gt;::iterator ite &#x3D; c.begin();</span><br><span class="line">&#x2F;&#x2F; for语句的语法规定，括号里面一定要有两个分号，分开三个句子。</span><br><span class="line">&#x2F;&#x2F;第一个句子是初始化用的，如果没有初始化的必要，就视为空语句，加上分号；</span><br><span class="line">&#x2F;&#x2F;第二个句子作为判断条件，如果没有判断条件，也视为空语句，后加一个分号。这种情况，会无限循环，相当于while(1)。如果for的执行部分，就是&#123;&#125;之间有专break语句，可以退出；</span><br><span class="line">&#x2F;&#x2F;第三个句子是执行部分执行完毕再执行的语句；无则视为属空语句；此时不用再加分号</span><br><span class="line">for (; ite !&#x3D; ite.end(); ++ite)</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure><p>建议使用range-based for statement (since c++11):</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for (decl:coll)&#123;</span><br><span class="line">statement</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for (int i : &#123;2,3,5,7,9,13&#125;) &#123;</span><br><span class="line">  std::cout&lt;&lt;i&lt;&lt;std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="容器与分类之各种测试（1）"><a href="#容器与分类之各种测试（1）" class="headerlink" title="容器与分类之各种测试（1）"></a>容器与分类之各种测试（1）</h2><p>容器-结构与分类</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/containers_classes.JPG" alt="标红的是C++11新加入的特性"></p><p>1.sequence containers 序列式：</p><p>array（不可扩充）, vector（分配器动态扩容）, deque（前后快速插入）, list（双向循环链表）, forward-list（单向链表）；</p><p>2.associative containers 关联式，便于查找：</p><p>set/multiset, map/multimap, 两者编译器所带标准库都用红黑树实现。set只带key，map带有key和value，两者意思与python的set和dict类似。multiset/multimap代表key是可以重复的。</p><p>3.unordered containers (since c++11)：</p><p>不定序，hash table，其实也可以属于一种associative containers.内部采用链表法解决哈希冲突。</p><hr><p>使用容器array示例：</p><p>array必须指定固定大小，不能是变量大小；</p><p><code>array.data()</code>首地址；</p><p><code>clock()</code>得到的是ms；</p><p><code>snprintf()</code>是c标注库函数，在这里是将数字转成字符串，具体可参考<a href="https://blog.csdn.net/qq_34707315/article/details/77895735#commentBox">此</a>；</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/array_test1.JPG" alt="在固定大小的array中随机生成数并出初始化填充，然后快排再二分查找指定的数字，并打印出消耗的时间"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/array_test2.JPG" alt></p><h2 id="容器与分类之各种测试（2）"><a href="#容器与分类之各种测试（2）" class="headerlink" title="容器与分类之各种测试（2）"></a>容器与分类之各种测试（2）</h2><p>侯捷老师给的一些建议：</p><p>1.通过给每个测试定义不同的namespace，使得每一部分都是独立的，方便测试。</p><p>2.用到变量的时候去定义，然后缩排方便寻找。</p><hr><p>使用容器vector：</p><p><code>vector.push_back()</code>把元素放到尾巴；</p><p>vector 2倍扩容增长空间；</p><p><code>vecotr.size()</code>元素个数；</p><p><code>vector.capacity()</code>容器容量，一般比size的结果大；</p><p><code>try, catch</code>目的是了解当前是否可以抓取到指定的内存大小；</p><p><code>::find</code>中双冒号代表全局的，find()是模板函数（循序查找），返回的是iterator;</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/vector_test1.JPG" alt="定义一个vector，然后不断填充随机数初始化，然后利用模板函数find和二分查找去搜索指定的数，比较两者的时间差异"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/vector_test2.JPG" alt></p><h2 id="容器与分类之各种测试（3）"><a href="#容器与分类之各种测试（3）" class="headerlink" title="容器与分类之各种测试（3）"></a>容器与分类之各种测试（3）</h2><p>1.使用容器list:</p><p>容器的第二个参数是分配器，一般我们不写使用默认的默认器分配内容；</p><p><code>list.max_size()</code></p><p><code>list.size()</code></p><p><code>list.front()</code></p><p><code>list.back()</code></p><p><code>list.sort()</code>对其进行排序</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/list_test.JPG" alt></p><p>2.使用容器forward_list:</p><p><code>forward_list.push_front</code>只提供单向的放数据</p><p><code>forward_list.max_size()</code></p><p><code>forward_list.front()</code></p><p>没有<code>forward_list.back()</code>和<code>forward_list.szie()</code></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/forward_list_test.JPG" alt></p><p>3.使用容器slist:</p><p>非C++11标准库；</p><p><code>#include&lt;ext\list&gt;</code>头文件包含。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/slist.JPG" alt></p><p>4.使用容器deque:</p><p>双向开口，可进可出。</p><p>分段连续的buffer构成的，但是给使用者感觉是连续的，内部其实不是，分段连续，buffer内部连续。每次扩充一个buffer.</p><p>vector扩充方便，但是容易浪费，list每次扩充一个节点，但是查找麻烦，deque每次一个buffer,相对折衷。</p><p><code>deque.size()</code></p><p><code>deque.front()</code></p><p><code>deque.back()</code></p><p><code>deque.max_size()</code></p><p>没有自己的sort()函数</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/deque.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/deque_test.JPG" alt></p><p>5.使用容器stack:</p><p><code>stack.pop()</code></p><p><code>stack.push()</code></p><p><code>stack.size()</code></p><p><code>stack.top()</code></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/stack_test.JPG" alt></p><p>6.使用容器queue:</p><p><code>queue.push()</code></p><p><code>queue.pop()</code></p><p><code>queue.front()</code></p><p><code>queue.back()</code></p><p><code>queue.size()</code></p><p>stack和queue用的deque，因此在技术上称容器的adapter, 不提供iterator</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/queue_test.JPG" alt></p><h2 id="容器之分类与各种测试（4）"><a href="#容器之分类与各种测试（4）" class="headerlink" title="容器之分类与各种测试（4）"></a>容器之分类与各种测试（4）</h2><p>关联式容器</p><p>1.使用容器multiset:</p><p>key就是value, 可插入相同元素，内部红黑树组织数据</p><p><code>multiset.insert()</code></p><p><code>multiset.size()</code></p><p><code>multiset.max_size()</code></p><p><code>multiset.find()</code>容器自带find()函数</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/multiset_test.JPG" alt></p><p>2.使用容器multimap:</p><p>，内部使用红黑树，key可以重复，指定key和value的数据类型，<code>multimap&lt;long, string&gt; c;</code></p><p><code>multimap.insert()</code>, <code>c.insert(pair&lt;long,string&gt;(i,buff))</code>，利用pair插入和取出</p><p><code>multimap.find()</code></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/multimap_test.JPG" alt></p><p>3.使用容器unordered_multiset:</p><p>内部使用hash_table, 链表法解决哈希冲突，一个篮子bucket后续可以挂链表</p><p><code>load_facotr</code>载重因子</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/unordered_multiset_test.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/unordered_multiset_test2.JPG" alt></p><p>4.使用容器unordered_multimap:</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/unordered_multimap_test.JPG" alt></p><p>5.使用容器set:</p><p>内部用红黑树实现，元素不允许相同</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/set_test.JPG" alt></p><p>6.使用容器map:</p><p>内部用红黑树实现，key不允许相同</p><p><code>c[i] = string(buf)</code>，与之前的multimap插入元素不同，可以用这种特殊写法，在内部会合成pair</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/map_test.JPG" alt></p><p>7.使用容器unordered_set:</p><p>内部用hash_table</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/unordered_set_test.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/unordered_set_test2.JPG" alt></p><p>8.使用容器unordered_map:</p><p>内部用hash_table</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/unordered_map_test.JPG" alt></p><h2 id="分配器测试"><a href="#分配器测试" class="headerlink" title="分配器测试"></a>分配器测试</h2><p>使用分配器：</p><p>不使用标准库自带的std::allocator, 调用GNU c++编译器的allocator</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/allocator_1.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/allocator_2.JPG" alt></p><h2 id="OOP-vs-GP"><a href="#OOP-vs-GP" class="headerlink" title="OOP vs. GP"></a>OOP vs. GP</h2><p>object-oriented programming and generic programming, 面对对象编程与泛型编程</p><p>OOP企图将data和methods放在一起，GP却将data和methods分开来</p><p>算法通过迭代器操作容器数据</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/GP_1.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/GP_2.JPG" alt></p><h2 id="操作符重载和模板（泛化，全特化，偏特化）"><a href="#操作符重载和模板（泛化，全特化，偏特化）" class="headerlink" title="操作符重载和模板（泛化，全特化，偏特化）"></a>操作符重载和模板（泛化，全特化，偏特化）</h2><p>再次强调和复习operator overloading，templates（类模板，成员模板，函数模板；模板的泛化，全特化和偏特化）。</p><p>1.操作符重载的情况主要是为了满足用户自己定义类的一些操作，可以在相关网站查询，并不是每个操作符都可以重载。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/operator_overloading_1.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/operator_overloading_2.JPG" alt></p><p>2.模板</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/class_template.JPG" alt="类模板，指定类参数类型"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/function_template.JPG" alt="函数模板，自动推导传入函数的参数类型并根据作相应的操作符调用"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/member_template.JPG" alt="成员模板"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/specialization.JPG" alt="在模板中，对于某种类型的做特殊处理（比如此时会有更适合和高效的处理），调用时也会自动调用特化的版本，标志template&lt;&gt;为空声明"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/partial_specialization.JPG" alt="部分模板参数指定为某类型时有特殊处理，只绑定其中一个模板参数，标志template&lt;&gt;为非空声明"></p><h2 id="Allocator"><a href="#Allocator" class="headerlink" title="Allocator"></a>Allocator</h2><p>不建议你单独使用分配器。</p><p><code>operator new()</code>和底层的内存分配函数<code>malloc()</code></p><p><code>malloc()</code>分配给申请内存大小的空间时会额外给一些空间放必要的东西，有一些额外的开销cookie，<code>free</code>释放内存。但是一般使用时，由于申请的size不大，所以这些额外的空间占所有的比例比较大，所以是个缺陷。</p><blockquote><p>P11总结一下，allocator就是用来分配内存的，最重要的两个函数是allocate和deallocate，就是用来申请内存和回收内存的，外部（一般指容器）调用的时候只需要知道这些就够了。内部实现，目前的所有编译器都是直接调用的::operator new()和::operator delete()，说白了就是和直接使用new运算符的效果是一样的，所以老师说它们都没做任何特殊处理。G2.9里的那个更好的allocator，在老师的另一门课《C++内存管理》里讲得特别细，大概从P23开始讲细节，更早的几P也有使用用例。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p11_1.JPG" alt="malloc开辟内存示意"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p11_2.JPG" alt="一般的VC等编译器只是将malloc包装成allocator，然后调用，缺陷没有改变"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p11_3.JPG" alt="GNU2.9使用了alloc这个函数，利用16个链表来分配申请的size的内存，减少了不必要的cookie"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p11_4.JPG" alt="alloc 16个链表示意。每个代表8个字节长度，从8-128个字节"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p11_5.JPG" alt="但是到了GNU4.9时候又抛弃了alloc，转而回到了原始的有缺陷的设计。不过依然可以调用GNU命名空间中的_pool_alloc来使用2.9版本中的alloac函数"></p><h2 id="容器之间的实现关系与分类"><a href="#容器之间的实现关系与分类" class="headerlink" title="容器之间的实现关系与分类"></a>容器之间的实现关系与分类</h2><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p12_1.JPG" alt="容器之间的关联，复合关系，不是继承，只是会有某些容器的功能"></p><h2 id="深度探索list（上）"><a href="#深度探索list（上）" class="headerlink" title="深度探索list（上）"></a>深度探索list（上）</h2><p>最具代表性的容器，双向循环列表，优先介绍。</p><p>定义自己的迭代器iterator， 其中要有typedef和operator reloading（操作符重载）</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p13_1.JPG" alt="list的iterator定义"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p13_2.JPG" alt="提到了迭代器++所体现的智能指针的作用，取出prev或next指针赋给node。其中要特别注意前++和后++的区别，由于C++中整数连续后++是违法的，所以这里操作符重载后++时返回原值，也造成迭代器后++违法的现象"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p13_3.JPG" alt="取出list中的data"></p><h2 id="深度探索list（下）"><a href="#深度探索list（下）" class="headerlink" title="深度探索list（下）"></a>深度探索list（下）</h2><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p14_1.JPG" alt="G2.9和G4.9版本链表源代码的比较，一处修改了iterator模板传入参数个数，一个是修改指针指向自己这个类，而非void然后再强制转换"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p14_2.JPG" alt="容器的一般都是满足左闭右开的特点，也就是最后一个值取不到，所以这里最后再环状末尾加了一个空的节点，但是指向begin节点"></p><h2 id="迭代器的设计原则和Iterator-Traits的作用与设计"><a href="#迭代器的设计原则和Iterator-Traits的作用与设计" class="headerlink" title="迭代器的设计原则和Iterator Traits的作用与设计"></a>迭代器的设计原则和Iterator Traits的作用与设计</h2><p>容器用域存储数据，算法用以处理数据，而迭代器iterator就是沟通容器和算法的桥梁。上一讲提到，容器需要自己定义iterator，并且写出<strong>五个</strong>规定的typedef，这个就是为了算法询问iterator，得出这样的特性然后进行定制的数据操作。但是如果iterator不是类，无法写出typedef的时候，就需要traits这样的中间变量来让算法得出迭代器的特性。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p15_1.JPG" alt="算法询问迭代器的特性，但是迭代器只是指针的话就无法直接询问"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p15_2.JPG" alt="利用traits萃取机来充当中间人的角色"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p15_3.JPG" alt="如果迭代器是类，直接询问得出结果，否则利用偏特化替迭代器回答。其中注意const的使用"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p15_4.JPG" alt="traits传递完整的五个必须的特性"></p><p>除了iterator traits之外，还有各式的traits, 包括type traits, char traits, pointer traits等等。</p><h2 id="Vector深度探索"><a href="#Vector深度探索" class="headerlink" title="Vector深度探索"></a>Vector深度探索</h2><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p16_1.JPG" alt="vector不在原地扩充，而是请求重新分配2倍大小的空间"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p16_2.JPG" alt="申请空间并搬运，这里的insert_aux函数还可能被其他函数调用，所以又重新判断了一次"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p16_3.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p16_4.JPG" alt="vector的iterator就是个指针，因为是连续空间，所以利用traits为算法传递typedef. G4.9版本中对vector做了多层推导，所以可读性不高，也被侯老师吐嘈。"></p><h2 id="array-forward-list深度探索"><a href="#array-forward-list深度探索" class="headerlink" title="array, forward_list深度探索"></a>array, forward_list深度探索</h2><p>把array包装成容器，然后利用指针当iterator进行操作。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p17_1.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p17_2.JPG" alt="单向链表与前面的双向循环链表类似，只不过没有prev的指针，因此迭代器没有--操作"></p><h2 id="deque-queue和stack深度探索（上）"><a href="#deque-queue和stack深度探索（上）" class="headerlink" title="deque, queue和stack深度探索（上）"></a>deque, queue和stack深度探索（上）</h2><p>双向队列deque在上层调用时表现是连续的，但是底层实现是用分段连续的buffer来实现的，通过迭代器和操作符重载来营造连续插入或弹出的假象。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p18_1.JPG" alt="一个迭代内部有四个指针：cur,first, last, node，其中前三个指针是针对buffer而言的，而node指的是当前deque中用到了哪个buffer"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p18_2.JPG" alt="deque class内部有四个数据，start和finish是类似begin()和end()的两个首尾迭代器，一般的容器都会有，map指向这个包含buffer的deque，map_size表示大小，所以整个deque大小是2x16+2x4=40 bytes. 此外，G2.9还可以预设buff_size大小，否则的话按照默认规则设定。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p18_3.JPG" alt="deque的迭代器设计，其类型是可以随意访问的iterator，map pointer是指向指针的指针，因此deque内部存着buffer的node指针"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p18_4.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p18_5.JPG" alt="deque的元素插入，是将左右两边比较短的那一部分进行搬移"></p><h2 id="deque-queue和stack深度探索（下）"><a href="#deque-queue和stack深度探索（下）" class="headerlink" title="deque, queue和stack深度探索（下）"></a>deque, queue和stack深度探索（下）</h2><p>deque内部通过iterator操作符重载并做一些处理来进行表面上连续空间的模拟。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_1.JPG" alt="back代表最后一个元素，而容器中的finish是开的，取不到，所以需要减去1得到最后一个元素"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_2.JPG" alt="dwque元素size计算，前面的buffer后面的buffer元素不一定是满的（双向的特点），只有中间是满的，所以分三步计算"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_3.JPG" alt="迭代器++和--操作符重载"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_4.JPG" alt="迭代器跳着加的操作符重载"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_5.JPG" alt="迭代器跳着减的操作转为跳着加的操作"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_6.JPG" alt="G4.9版本的deque，其中取消了用户对buffsize的预设"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_7.JPG" alt="利用deque模拟队列queue"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_8.JPG" alt="用deque模拟stack"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_9.JPG" alt="queue和stack也可用list模拟，但是默认是deque，因为速度较快"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_10.JPG" alt="queue不可用vector模拟，以为vector没有pop_front操作"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p19_11.JPG" alt="queue和stack都不可用map或set模拟，因为没有该数据结构弹出元素这一操作"></p><h2 id="RB-Tree深度探索"><a href="#RB-Tree深度探索" class="headerlink" title="RB-Tree深度探索"></a>RB-Tree深度探索</h2><p>关联式查找容器的底层支撑结构。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p20_1.JPG" alt="这里红黑树的header不是根节点，是空的，为了容器前闭后开处理方便"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p20_2.JPG" alt="红黑树的五个模板参数，key和value是数据类型，keyofvalue代表如何从value取出key(value可能是key-data组成的pair)，compare代表key的比较函数"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p20_3.JPG" alt="四个模板参数的一个实例，其中key和value都是int类型，identity函数代表取key函数，不是C++标准库函数，是G2.9的函数"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p20_4.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p20_5.JPG" alt="G4.9与2.9版本的红黑树使用上改了一些命名字母"></p><h2 id="Set-Multiset深度探索"><a href="#Set-Multiset深度探索" class="headerlink" title="Set, Multiset深度探索"></a>Set, Multiset深度探索</h2><p>set和multiset底层利用Rb_tree实现，所以基本上它们的事内部都是交给红黑树去完成，所以又被叫成container adapter. 其中需要注意的是它们中的key和value是一样的，所以不能被改变，否则key就会变化。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p21_1.JPG" alt="set和multiset"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p21_2.JPG" alt="set具有三个模板参数，因为value和keyofcompare只有一种默认情况。其中const_iterator的类型只能用于读取容器内的元素，但不能改变其值，对const_iterator类型解引用，得到的是一个指向const对象的引用。"></p><h2 id="Map-Multimap深度探索"><a href="#Map-Multimap深度探索" class="headerlink" title="Map, Multimap深度探索"></a>Map, Multimap深度探索</h2><p><a href="https://www.cnblogs.com/ChinaHook/p/6985518.html">related blog</a></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p22_1.JPG" alt="map和multimap使用红黑树实现key-value的存储，且不允许改变key值"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p22_2.JPG" alt="map的模板参数，其中会将key-data打包成pair存入Rb_tree中当作value，且为了防止key不被修改，在key前加了const关键字"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p22_3.JPG" alt="map可以使用[]操作符和lower_bound函数插入key-value，如果key不存在会返回一个合适的位置，但是这个性质multimap没有，只能用insert函数插入pair(), 应该是因为multimap可以有多个相同的key，所以用[]操作符可能无法加入相同的key."></p><h2 id="Hash-Table深度探索"><a href="#Hash-Table深度探索" class="headerlink" title="Hash Table深度探索"></a>Hash Table深度探索</h2><p>另一个关联式容器哈希表，或散列表，是一种常用的存储较多数据的数据结构（减少查找时间），其中难点在于hasd function的选择和对对哈希碰撞的处理（一般是开放式寻址法或者链表法）。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p23_1.JPG" alt="hash table示意，利用bucket和list来存储数据。其中G2.9硬编码了一系列质数作为扩充后的长度，但是G4.9没有继续沿用了"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p23_2.JPG" alt="hashtable的6个模板参数，其中key指定篮子，value是pair，HashFcn是哈希函数，映射要存的object为key，ExtractKey是从存入的value pair中取出key的方式，同Rb_tree类似，EqualKey是判断key是否相等的函数。hashtable的大小为1+1+1+12+4=19个字节，为了对齐取4的倍数，为20 bytes. 其迭代器包含指向当前元素位置的指针和指向当前hashtable的指针。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p23_3.JPG" alt="hashtable使用示例，其中eqstr是自己写的函数，调用C中比较char的函数strcmp，然后根据其返回的int值（1，0，-1）返回bool值"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p23_4.JPG" alt="c++标准库中偏特化了一些hash函数"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p23_5.JPG" alt="但是G2.9内没有提供针对string的现成的hash函数，只有针对C的字符串的哈希函数。到了G4.9版本，基本的数据类型都有了自己的hash function, 包括string"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p23_6.JPG" alt="找到存放的位置key，这里做了取余处理，也就是对编码出的hash值对bucket长度取余"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p23_7.JPG" alt="上面的示例程序计算hash key示意，注意还有一个取余操作。在这里哈希表是单向链表存储，实际有的可能是双向链表存储"></p><h2 id="Unordered容器概念"><a href="#Unordered容器概念" class="headerlink" title="Unordered容器概念"></a>Unordered容器概念</h2><p>利用hash table实现unordered_set, unordered_multiset, unordered_map, unordered_multimap容器。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p26_1.JPG" alt="四个unordered容器的模板参数"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p26_2.JPG" alt="回顾之前利用unordered_set存储随机数的例子，验证两个事情：1. hashtable的bucket数量比要存的object的数量多；2. 扩容后的长度不一定是之前指定的质数了。"></p><h2 id="算法的形式"><a href="#算法的形式" class="headerlink" title="算法的形式"></a>算法的形式</h2><p>算法通过迭代器联通容器，进行数据处理和函数运行。从语言层面上讲，容器是class template, 而算法是function template, 其中有些算法还可传入你定制的准则criterion, 比如sort函数的比较函数。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p27_1.JPG" alt="STL中算法algorithm的作用"></p><h2 id="迭代器的分类"><a href="#迭代器的分类" class="headerlink" title="迭代器的分类"></a>迭代器的分类</h2><p>STL容器中总共会用到三种种类，分别是random_access_iterator_tag (array, vector, deque), bidirectional_iterator_tag(list, rb_tree), forward_iterator_tag(forward_list, hashtable可能是) 这三种。多出来的两种input_iterator_tag和output_iterator_tag属于istream和ostream. 五种之间是继承关系，同时用这种tag的方式，非简单的12345.</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p28_1.JPG" alt="各种容器的迭代器种类"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p28_2.JPG" alt="打印出各个容器的迭代器种类。右边的容器(typename)::iterator()表示临时对象，传给display_category()函数作为具体的模板参数，然后利用萃取机traits得出出入的迭代器的category，然后利用函数重载传给具体的_display_category()，打印出结果。istream和ostream利用直接指定的方式打印出。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p28_3.JPG" alt="也可直接利用C++的typeid取出迭代器的name属性，打印出类型名称。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p28_4.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p28_5.JPG" alt></p><h2 id="迭代器类别对算法的影响"><a href="#迭代器类别对算法的影响" class="headerlink" title="迭代器类别对算法的影响"></a>迭代器类别对算法的影响</h2><p>STL算法内部会利用迭代器的类别开辟出不同的处理分支，想尽方法达到最快的处理速度。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p29_1.JPG" alt="根据不同的迭代器类别计算其距离。主函数部分的inline语句会预先判断该迭代器是否有difference_type，如果有才会进行下一步。主函数利用萃取机得出的category当作临时对象调用次函数，虽然次函数只写了两种情况，但是其他的迭代器类型会根据继承关系进行调用，这样做就是为了方便，不必写出冗余的代码。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p29_2.JPG" alt="举了advance的例子（前进），与之前的distance计算类似，这里只不过把得出迭代器类型的语句包装成了一个小函数。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p29_3.JPG" alt="举了第三个例子，copy函数。一般的思路是给出三个参数：要拷贝的起点和终点，以及拷贝到的起点，但是这里根据迭代器的类型，极尽所能做了很多的分支，优化到最快。memmove是C语言的底层比较安全的内存复制函数，trivial代表“不重要的‘，由标准库提供的type traits提供，目的为了决定哪一种拷贝赋值比较快。其中对判断语句的first!=last和n&lt;first-last都做了速度衡量，可以说是非常精炼了。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p29_4.JPG" alt="举的第四个例子，destroy函数，与copy类似，也做了很多判断处理。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p29_5.JPG" alt="第五个例子，unique_copy函数"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p29_6.JPG" alt></p><h2 id="算法源码剖析（几个例子）"><a href="#算法源码剖析（几个例子）" class="headerlink" title="算法源码剖析（几个例子）"></a>算法源码剖析（几个例子）</h2><p>举了几个C++标准库中的例子。表明与迭代器结合很紧密。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p30_1.JPG" alt="C++标准库的函数与C函数在形式上有些不同，是一种函数模板，并且与迭代器联系紧密"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p30_2.JPG" alt="第一个例子accumulate,累计函数，有两个版本，一种就是传入三个参数进行累加，另一种可以指定二元操作函数，进行定制的累计操作。在右边的代码示例中，利用了自定义函数传入和函数对象/仿函数重载operator()，两者都可以起作用，作为可被调用的对象。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p30_3.JPG" alt="第二个例子for_each，跟第一个例子类似也使用了自定义函数和函数对象，同时强调C++11可以用range-based for statement进行操作"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p30_4.JPG" alt="第三个例子replace函数，replace单纯地根据相等替换数值,replace_if可以传入判断条件, replace_copy是在copy过程中替换旧值为新值。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p30_5.JPG" alt="第四个例子count函数，count函数简单判断值相等进行加一操作，而count_if可以传入判断条件。在这里侯老师提到了C++标准库的这些函数虽然是全局的，但是不一定适用于所有的容器，有的容器可能有自己定义的同名成员函数，这个时候就要调用容器自己的对应函数。右边的例子表明不带其同名成员函数的容器可以调用C++标准库全局的这个函数，但是有了同名成员函数就要最好调用自己的。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p30_6.JPG" alt="第五个例子find函数，这个在最开始容器使用测试时候使用过，属于全局函数，有些容器比如那几个关联式容器就有自己定制的find()函数"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p30_7.JPG" alt="第六个例子sort函数，其中提到了逆序迭代器。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p30_8.JPG" alt="reverse iterator示意"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p30_9.JPG" alt="第七个例子二分搜索binary_search, 内部调用lower_bound函数取出位置。这里侯老师认为在查找时候应该进行一下开头检查，然后再根据结果选择进不进去查找。"></p><h2 id="仿函数和函数对象"><a href="#仿函数和函数对象" class="headerlink" title="仿函数和函数对象"></a>仿函数和函数对象</h2><p>在前面一些的算法的使用中，已经见识到了函数对象/仿函数传入其中进行特殊的操作。在C++的STL中有三大类functors，分别是算术类，逻辑运算类和相对关系类。他们都继承了某个适当者，这样是为了后续利用适配器进行改造，而前提就是像algorithm对iterator一样利用traits询问问题，继承的作用就是为了融入STL，利用被继承者的typedef来回答问题。此外，GNU中也会有一些仿函数供用户使用，比如前面提到的<code>Identity</code>和<code>Select1st</code></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p31_1.JPG" alt="几个标准库仿函数示例，利用类的对象形式，重载了操作符()，并继承了二元变量操作binary_function"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p31_2.JPG" alt="举例前面自定义的仿函数，由于没有继承，所以后续无法融入STL进行适配"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p31_3.JPG" alt="仿函数的可适配条件就是需要挑选适当者进行继承"></p><h2 id="存在多种适配器"><a href="#存在多种适配器" class="headerlink" title="存在多种适配器"></a>存在多种适配器</h2><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p32_1.JPG" alt="仿函数适配器与仿函数类似于算法与迭代器。把仿函数包成adapter给算法使用"></p><p>适配器可以认为具有一个桥梁作用：把某个底层B包装下做成A交给用户使用，但是内部仍然是调用B来运作。在C++中，适配器采用复合的方式获得某种功能，非继承。</p><p>容器适配器方面，典型的是stack和queue，它们内部都是内含了deque作为其底层容器，然后改造成具有自己特性和接口的容器。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p32_2.JPG" alt></p><h2 id="Binder2nd-not1-bind"><a href="#Binder2nd-not1-bind" class="headerlink" title="Binder2nd/not1/bind"></a>Binder2nd/not1/bind</h2><p>介绍了前面使用过的一个计数语句<code>cout &lt;&lt; count_if(vi.begin(), vi.end(), not1(bind2nd(less&lt;int&gt;, 40)))</code>，其中用到了算法，函数适配器和函数对象。</p><p><code>bind2nd</code>和<code>not1</code>都是函数适配器，用以修饰后面的函数并将其包装。<code>bind2nd</code>绑定第二实参，限制less函数调用时保持第二参数为40不变。修饰后的函数继续被<code>not1</code>修饰，将结果取反，也就是不小于40的数计数。</p><p><code>bind2nd</code>内部使用<code>binder2nd</code>函数，目的是为了简化用户使用难度，自动帮你推导<code>operation</code>模板参数。侯老师提到<code>binder1st</code>, <code>binder2nd</code>这些函数后来都被<code>bind</code>函数取代，但是旧的函数依旧可以使用。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p33_1.JPG" alt="bin2nd函数内部做了很多工作，包括继承一元参数函数unary_function，询问和操作符重载。注意这里的less&lt;int&gt;()一开始是函数对象，属于typename加()的形式，非函数调用"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p34_1.JPG" alt="not1继续修饰bind2nd, 对pred取反"></p><p>在C++2.0版本中，<code>bind</code>函数取代了<code>bind2nd</code>，可以用来绑定函数/函数对象/成员函数和数据成员，同时可以指定绑定哪个参数，哪个参数不用绑定，利用诸如_1这样下划线加数字的形式（palceholder，占位符）代表第几个参数可以被后续调用再指明。<code>bind</code>可以指定一个模板参数，代表返回类型。更多的例子和示意见下面的图片。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p35_1.JPG" alt="右边举了几个用bind绑定的例子。fn_invert中my_divide的第一参数在后面，所以调用时是_1/_2，fn_rounding就指定了返回类型为int，bound_memfn绑定了函数为Mypair类中的multioly函数，允许用户传递数据，bound_memdata和bound_memdata2绑定函数为传出类中成员数据。最后的例子比较了bind2nd和bind使用的区别，其中cbegin中的c是const的意思"></p><h2 id="迭代器适配器reverse-iterator-inserter"><a href="#迭代器适配器reverse-iterator-inserter" class="headerlink" title="迭代器适配器reverse_iterator/inserter"></a>迭代器适配器reverse_iterator/inserter</h2><p>Adapter在容器，仿函数和迭代器上都有使用。侯老师在这里举了两个例子，一个是前面用到过的reverse_iterator，逆序迭代器遍历方向，另一个是inserter，用来插入数据。在这里侯老师再次强调，adapter是把所有需要的东西系起来，打包好，然后供日后再用，起到一种修饰改造作用。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p36_1.JPG" alt="逆向迭代器内部也是调用正向迭代器，进行操作符重载，包装成想要的样子。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p37_1.JPG" alt="inserter配合copy函数，通过对=操作符进行重载，实现在foo中选定位置进行bar数组的安插，而不是原先的覆盖赋值搬移。其中由于是list容器所以利用advance实现起始迭代器加3"></p><p>copy函数内部已经把流程写死了，就是按照指定的result迭代器，一步步搬移前面两个迭代器指定范围内的数据，后续的一些adapter改造操作，通过利用操作符重载各自解读，来实现自己定制的特性。</p><h2 id="X适配器ostream-iterator-istream-iterator"><a href="#X适配器ostream-iterator-istream-iterator" class="headerlink" title="X适配器ostream_iterator/istream_iterator"></a>X适配器ostream_iterator/istream_iterator</h2><p>ostream和istream是两个特殊的迭代器，他们不属于一般的迭代器适配器，容器适配器和仿函数适配器中的任何一个，因此侯老师称之为X适配器，X代表一种未知。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p38_1.JPG" alt="利用out_it迭代器重载操作符=配合copy函数把数据传给cout打印出来"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p39_1.JPG" alt="eos可以认为是一个标志，代表cin的结束标志，没有实际参数，iit才是真正的属于输入数据的迭代器，参数为cin,与eos比较判断是否读入完成"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p39_2.JPG" alt="如果在例2的两个语句中间加“请输出数字”这样的提示语永远都不会出现，因为这个istream_iterator一旦创建就开始read，读一个传一个"></p><h2 id="一个万用的Hash-Function"><a href="#一个万用的Hash-Function" class="headerlink" title="一个万用的Hash Function"></a>一个万用的Hash Function</h2><p>hash function的制定可以没有任何规律，但是效果是一定要够乱，尽量减少碰撞。这里侯老师先以一个比较简单的customerhash函数开场，说明这样虽然简单，但是hash code比较容易重复。之后引出了C++标准库中的一个“万用的hash function”, 其中的重点是<strong>variadic templates</strong>, 这个是C++ 2.0中比较重要的新特性，大意就是可变的模板参数，常用…表示，比如下图中的序号1表示的函数，其中的<code>template&lt;typename...Types&gt;</code>就表示可以接受任意数量的模板参数。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p40_1.JPG" alt="右上角的hash_val函数先调用序号1函数，然后转到了序号2，序号2函数根据4中的hash_combine生成seed，然后再返回2继续拿出seed，不断在序号2和序号4中递归循环，参数会一个一个的拿做当seed，最后只剩一个的时候转去序号3函数，再通过4得到最后的hash code. ^=属于异或操作，0x9e3779b9是根据黄金比例来选取的。"></p><p><img src="/.top//p40_2.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p40_3.JPG" alt="G4.9版本中已经实现对string类型的hash function的偏特化"></p><h2 id="Tuple用例"><a href="#Tuple用例" class="headerlink" title="Tuple用例"></a>Tuple用例</h2><p>Tuple是可以将几个不同的数据结构的变量绑在一起的类型，使用上利用直接声明<code>tuple&lt;&gt;</code>创建，也可以利用<code>make_tuple()</code>实现，或者<code>tie</code>函数快速对变量进行赋值，如下图所示。其中可以对&lt;和=这样的操作符进行重载实现特性上的适配。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p41_1.JPG" alt="tuple的用法示例。最后的tuple_size,::value,tuple_element类似于traits的问答式。此外ppt中tuple的大小为32，侯老师没有给出合适的解释，按照后一页ppt中tuple存在套娃继承的思路，大小应该是4+4+4+16=28，如果最顶的tuple没有被继承，那个就是28+1=29，然后要对齐是4的倍数，结果就是32，但是因为被继承，不是单独的，所以就没有大小，还是28.这里我倾向认为是类似struct的大小计算，最后的大小要是complex大小的倍数，所以是32（但是还没有查证）."></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p41_2.JPG" alt="tuple内部实现了自动递归套娃继承，也是通过使用variadic templats可变模板参数完成。不断地划分head-tail构建父类，而且取相应tail地址的时候也是指向一整个模块，如右下角示意。"></p><h2 id="type-traits"><a href="#type-traits" class="headerlink" title="type traits"></a>type traits</h2><p>type traits主要是为了给用户自己写的类提供一些特征信息，用户自己不需要对其进行偏特化处理就可以自动获取，节省代码工作量，广泛用于C++泛型编程。这些特征信息通过询问的方式进行获取，指导该类后续的相应操作。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p42_1.JPG" alt="type_traits部分内容示例（G2.9版本）"></p><p>上图出现的Plain Old Data (POD)是一个类型（比如class或者struct），但是没有构造函数，析构函数，虚函数等成分。<a href="https://zh.wikipedia.org/wiki/POD_(%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1">维基百科</a>_)的解释是：</p><blockquote><p>POD类类型就是指class、struct、union，且不具有用户定义的构造函数、析构函数、拷贝算子、赋值算子；不具有继承关系，因此没有基类；不具有虚函数，所以就没有虚表；非静态数据成员没有私有或保护属性的、没有引用类型的、没有非POD类类型的（即嵌套类都必须是POD）、没有指针到成员类型的（因为这个类型内含了this指针）。</p></blockquote><p>stackoverflow有也相关<a href="https://stackoverflow.com/questions/146452/what-are-pod-types-in-c">问题</a>：</p><blockquote><p>In short, it is all built-in data types (e.g. <code>int</code>, <code>char</code>, <code>float</code>, <code>long</code>, <code>unsigned char</code>, <code>double</code>, etc.) and all aggregation of POD data. Yes, it’s a recursive definition. ;)</p><p>To be more clear, a POD is what we call “a struct”: a unit or a group of units that just store data.</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p42_2.JPG" alt="C++2.0的type_traits非常强大，包含不少特性信息，更多见：http://www.cplusplus.com/reference/type_traits/"></p><p>一个类如果没有没有涉及到指针，就可以不必写析构函数，使用编译器分配的默认的即可。像string内部带了指针，就必须写析构函数。此外，如果不是父类base，没有覆写的虚函数，也不必写virtual destructor.</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p42_3.JPG" alt="利用自定义的类Zoo来测试type traits, 其中刚开始的冒号语句为初始化列，后面四句分别为拷贝构造，搬移构造，拷贝赋值，搬移赋值，&amp;&amp;代表C++2.0新特性move,后面会专门谈到。delete也是2.0新特性，表示写出来但是摧毁掉，default表示使用默认方式，即使你不写也是使用编译器给的默认方式。中间的打印结果中，is_polymorphic为1代表这个类继承了虚函数，所以左边写了虚函数destructor, 相应的右边的结果也显示为1."></p><p>type traits通过模板实现，对类型进行操作，比如下面的例子，利用泛化，偏特化，传回真假值。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p43_1.JPG" alt="type traits实现is_void，拿掉cv，再借助helper，给出0/1值"></p><p>此外有些type traits实现的特性未曾出现于C++源码标准库。侯老师说这种情况可能就是编译器编译的时候来回答支持，不以源代码的方式呈现。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p43_2.JPG" alt></p><p><a href="https://blog.csdn.net/mogoweb/article/details/79264925">traits的理解</a></p><p><a href="https://blog.csdn.net/lihao21/article/details/55043881">type  traits 理解</a></p><h2 id="cout的操作符重载"><a href="#cout的操作符重载" class="headerlink" title="cout的操作符重载"></a>cout的操作符重载</h2><p>cout为何可以接收各式各样的object？因为其内部对各类型都做了操作符重载。注意其返回类型为ostream&amp;，为了可以继续接收，非void返回类型。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p44_1.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p44_2.JPG" alt="G4.9版本中cout的操作符重载示例。sub_match代表正则表达式（字符串处理），bitset代表处理二进制对象的类型，类似位图。"></p><h2 id="movable的使用"><a href="#movable的使用" class="headerlink" title="movable的使用"></a>movable的使用</h2><p>move是C++2.0一个比较重要的特性，其内部就是拷贝指针，属于浅拷贝，在type traits中见到的&amp;&amp;符号就代表move功能，而拷贝构造则是深拷贝。move的速度很快，侯捷老师也在课程中比较了加不加move功能对容器元素拷贝速度的影响，同时也给出了一些测试例子，但是在使用move时需要自己对元素的使用有清晰的认识，因为move是拷贝指针，等于是两个指针指向同一个位置，这个是危险的，C++内部会在move时做一些处理去规避这个行为带来的危险情况，所以move完之后的原始对象就最好不要再用了。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p45_1.JPG" alt="move的速度一般比拷贝构造快很多。这里侯捷老师测试了不少容器，他提到自己都是用insert函数来完成插值，但是像红黑树这样的容器是自动选择位置的，不允许用户手动指定位置。在这样的情况下insert函数也可以使用，但是具体的位置不一定是用户指定的。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p45_2.JPG" alt="从下面的程序中可以看出，为了避免delete错误，move的对象在执行时会指向NULL，所以最好不要再去使用之前被拷贝的对象了。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p45_3.JPG" alt="对于临时创建的对象，如果有move函数操作，编译器就会自动调用，像Mc11(c1)这样的语句，由于c1不是临时对象，编译器还是老老实实用的深拷贝，但是用户自己可以利用下一行的std::move进行安排编译器去用move操作，但是move之后，用户就要确保不要用到c1这个对象了。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p45_4.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++STL/p45_5.JPG" alt="vector的move拷贝动作，利用swap函数对三个指针操作，所以相比较默认的拷贝构造速度会快上很多。"></p><p><a href="https://www.cnblogs.com/catch/p/3500678.html">C++的左值和右值</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1db411q7B8&quot;&gt;videos&lt;/a&gt;;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pan.baidu.com/s/1DkCctgH1BO2Cs0NuUcyHhg&quot;&gt;课件，密码kr24&lt;/a&gt;;&lt;/p&gt;
&lt;h2 id=&quot;认识headers、版本、重要资源&quot;&gt;&lt;a href=&quot;#认识headers、版本、重要资源&quot; class=&quot;headerlink&quot; title=&quot;认识headers、版本、重要资源&quot;&gt;&lt;/a&gt;认识headers、版本、重要资源&lt;/h2&gt;&lt;p&gt;以STL为目标探讨泛型编程。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;使用一个东西，却不明白它的道理，不高明！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;level 0: 使用C++标准库；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;level 1: 认识C++标准库（胸中自有丘壑，体系结构应当建立起来）；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;level 2: 良好使用C++标准库；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;level 3: 扩充C++标准库；&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Standard Template Library (&lt;a href=&quot;https://blog.csdn.net/qq_44770155/article/details/97882816&quot;&gt;6大部件&lt;/a&gt;：Container(容器) 各种基本数据结构；Adapter(适配器) 可改变containers、Iterators或Function object接口的一种组件；Algorithm(算法) 各种基本算法如sort、search…等；Iterator(迭代器) 连接containers和algorithms；Function object(函数对象)；Allocator(分配器)) 加上其他一些零碎的东西构成了C++ Standard Library.&lt;/p&gt;</summary>
    
    
    
    <category term="课程记录" scheme="http://densecollections.top/categories/%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="侯捷" scheme="http://densecollections.top/tags/%E4%BE%AF%E6%8D%B7/"/>
    
    <category term="c++11" scheme="http://densecollections.top/tags/c-11/"/>
    
    <category term="STL" scheme="http://densecollections.top/tags/STL/"/>
    
  </entry>
  
  <entry>
    <title>yolo系列论文阅读笔记</title>
    <link href="http://densecollections.top/posts/yolopaperreading/"/>
    <id>http://densecollections.top/posts/yolopaperreading/</id>
    <published>2020-06-04T01:53:54.000Z</published>
    <updated>2021-01-02T12:22:58.821Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>YOLO系列是one-stage目标检测种的重要成果，兼具速度与精度，同时包含了很多与时俱进的tricks，是理论与工程实践完美结合的产物。虽然Andrew Ng说YOLO论文比较难读，但是为了更好的梳理检测这一模块，理解实践这项工作必须要进行下去。</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&amp;mid=2247485030&amp;idx=1&amp;sn=585864ef65bef3aa21c953506aae67f3&amp;chksm=9f80bcf0a8f735e680e552d973247d4577f974b3a5eff23d89779c3416cb8f360fa0ee6ccb4c&amp;scene=21#wechat_redirect">Pytorch YOLO项目推荐</a>。</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA4MDExMDEyMw==&amp;mid=2247489178&amp;idx=1&amp;sn=c680b306f52f8a14b6dbd95dcf825cd9&amp;chksm=9fa861dea8dfe8c805849ea244a9d5a41c5800c2420ebe5ab59015e0e347106009a1b7503262&amp;mpshare=1&amp;scene=1&amp;srcid=0801DMwwN4jG1mIgJtRHCBbB&amp;sharer_sharetime=1596243161365&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=519b8bbaea533f94d6ea97b2ce92dec8fa25f479b15fcec13c3d4a8a930c7d67bcf1e2a1b5f384022df13a66b501b7961fcaa1e6369148ca1a02a41553f72a4f4f1c5caba43b01d896bfa5448880d86e&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=62090529&amp;lang=zh_CN&amp;exportkey=A0Fqg9Zs%2F5n9SoSbTTvLSBM%3D&amp;pass_ticket=OjPkHeLi1VRb2pZNqGnzwsGeG%2BPZ8Pp95ltGB1Cixid44X1S6yC8NrZdAH%2BnttEd">YOLO算法家族全景图</a>。</p><p>推荐阅读：<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247504485&amp;idx=1&amp;sn=1edfe96858eca65764f5ac0b73932d87&amp;chksm=ec1c3f9cdb6bb68ae198dd24e0bc301a397a5f06f63377d914184ce02ee90e122b587e245d89&amp;scene=0&amp;xtrack=1&amp;key=8723f2e63c89806b337320fe9cd49f907a0e73a074f3fd6ea5c1f54a03f0653490e9551365b96282019e49773aee9250e35e1c4bbab89999b7be583fc38af84c9ff7b535fd459d6c0b8632c6c92a56fa974d9a260ab86a01dc9ef13c97ef814758185a3cd02aaebb6c1df5870cd2f6d711040f58b53fe7f993591f088a2fc1b0&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=62090529&amp;lang=zh_CN&amp;exportkey=Ayc7FmFAK8Edscesr09jGpA%3D&amp;pass_ticket=4Y3TKSx0fE%2Bxg%2B7QWVbNmub5ArN7rwIuPbCxsop%2BTZM1j9YSNFZ65ULuJvjYrYUJ">YOLO v1-v5全解读</a></p><a id="more"></a><h2 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h2><p><a href="https://arxiv.org/abs/1506.02640">paper</a>;</p><p>参考博客：<a href="https://blog.csdn.net/liuxiaoheng1992/article/details/81983280">1</a>, <a href="https://zhuanlan.zhihu.com/p/37850811">2</a>, <a href="https://zhuanlan.zhihu.com/p/32525231">3</a></p><p>检测思想：利用卷积提取特征，然后得到最终的$S\times S$的特征图，这里面每一个cell会预测B个框（这里为什么要每个cell预测多个框，有点类似anchor的思想，猜想可能是为了应对尺度问题，后面回归的时候也是选择跟GT接近的那个框去回归，但是实际上yolo v1对小物体的检测效果并不是很好，所以这样隐式设置，网络应该不好学习到），每个框预测offset和置信度（综合是否有物体以及IOU-预测的质量），根据CNN的位置不变性，原图也可以划分$S\times S$的cell，看原图上GT的中心落在哪个cell上，对应到最后的特征图上的那个cell就负责预测该物体。再加上预测的类别置信度，总共预测的变量为$S\times S \times(B\times5 + C)$，由FC给出，其中为了处理方便，预测的向量中[0 :$S \times S \times C$]为类别概率部分，[$S \times S \times C$:$S \times S \times (C+B)$]是框置信度部分，[$S \times S \times (C+B)$:]是边界框预测部分。</p><p>yolo将整个目标检测都看成回归问题，输出$7 \times 7 \times 30$的tensor(针对pascal VOC)，采用均方差损失函数。</p><p>CNN的空间位置不变性，原图$224 \times 224$ resize到$448 \times 448$，downstream 下采样到划分$7 \times 7$的grid栅格，每个栅格预测2个bounding box，预测变量为$(x, y, w, h, C)$，$C = p_{object} * IoU$，存在物体，$p_{object}=1$，否则为0</p><p>除了最后一层，其他层用leakly ReLU激活函数，loss函数，分配权重。</p><script type="math/tex; mode=display">\begin{array}{l}\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text {obj }}\left[\left(x_{i}-\hat{x}_{i}\right)^{2}+\left(y_{i}-\hat{y}_{i}\right)^{2}\right] \\\quad+\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text {obj }}\left[(\sqrt{w_{i}}-\sqrt{\hat{w}_{i}})^{2}+(\sqrt{h_{i}}-\sqrt{\hat{h}_{i}})^{2}\right] \\\quad+\sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{obj}}\left(C_{i}-\hat{C}_{i}\right)^{2}+\lambda_{\mathrm{noobj}} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{noobj}}\left(C_{i}-\hat{C}_{i}\right)^{2}\\\quad+\sum_{i=0}^{S^{2}} \mathbb{1}_{i}^{\mathrm{obj}} \sum_{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_{i}(c)\right)^{2}\end{array}</script><p>$\lambda_{coord}=5,\lambda_{noobj}=0.5$，利用平方根宽高是为了让小物体框对尺寸变化更加敏感。其中C代表的是框的置信度，是为了衡量是否存在物体和框的质量，p才是类别置信度。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/yolopaperreading/yolov1_backbone.JPG" alt="v1的backbone"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/yolopaperreading/regress_detail.png" alt="回归变量的计算方式，图来自博客1"></p><blockquote><p>YOLO predicts multiple bounding boxes per grid cell. At <strong>training time</strong> we only want <strong>one bounding box predictor</strong> to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the <strong>highest current IOU with the ground truth.</strong> This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall.</p><p>Note that the loss function <strong>only penalizes classification error if an object is present in that grid cell</strong> (hence the conditional class probability discussed earlier). It also only <strong>penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box</strong> (i.e. has the highest IOU of any predictor in that grid cell).</p></blockquote><p>trick: warm up, drop out fc, dataset random scaling、translations (up to 20%), adjust exposure and saturation (factor 1.5) in HSV color space.</p><p>普适性的架构，在artwork数据集上检测效果也不错，不局限于自然图像</p><p>一个grid cell只检测一个物体，如果是重合的多个物体只能检测一个，精度不高，但是由于是统一的架构，在一个网络上检测，信息比faster rcnn的region proposal充分，所以误检率低</p><p>预测部分：</p><p>在博客3中给出了源码中的预测输出，大致是输出结果，置信度低于阈值置0，NMS之后输出，也就是先NMS再给类别，而不是先给类别置信度再NMS</p><p>根据类别NMS，还是全部NMS？一般都是预测框全部放在一起然后NMS，结果似乎没有很大区别。</p><p>这里给出先判断类别再NMS的预测方案，代码来自博客3：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def _build_detector(self):</span><br><span class="line">        &quot;&quot;&quot;Interpret the net output and get the predicted boxes&quot;&quot;&quot;</span><br><span class="line">        # the width and height of orignal image</span><br><span class="line">        self.width &#x3D; tf.placeholder(tf.float32, name&#x3D;&quot;img_w&quot;)</span><br><span class="line">        self.height &#x3D; tf.placeholder(tf.float32, name&#x3D;&quot;img_h&quot;)</span><br><span class="line">        # get class prob, confidence, boxes from net output</span><br><span class="line">        idx1 &#x3D; self.S * self.S * self.C</span><br><span class="line">        idx2 &#x3D; idx1 + self.S * self.S * self.B</span><br><span class="line">        # class prediction</span><br><span class="line">        class_probs &#x3D; tf.reshape(self.predicts[0, :idx1], [self.S, self.S, self.C])</span><br><span class="line">        # confidence</span><br><span class="line">        confs &#x3D; tf.reshape(self.predicts[0, idx1:idx2], [self.S, self.S, self.B])</span><br><span class="line">        # boxes -&gt; (x, y, w, h)</span><br><span class="line">        boxes &#x3D; tf.reshape(self.predicts[0, idx2:], [self.S, self.S, self.B, 4])</span><br><span class="line"></span><br><span class="line">        # convert the x, y to the coordinates relative to the top left point of the image</span><br><span class="line">        # the predictions of w, h are the square root</span><br><span class="line">        # multiply the width and height of image</span><br><span class="line">        boxes &#x3D; tf.stack([(boxes[:, :, :, 0] + tf.constant(self.x_offset, dtype&#x3D;tf.float32)) &#x2F; self.S * self.width,</span><br><span class="line">                          (boxes[:, :, :, 1] + tf.constant(self.y_offset, dtype&#x3D;tf.float32)) &#x2F; self.S * self.height,</span><br><span class="line">                          tf.square(boxes[:, :, :, 2]) * self.width,</span><br><span class="line">                          tf.square(boxes[:, :, :, 3]) * self.height], axis&#x3D;3)</span><br><span class="line"></span><br><span class="line">        # class-specific confidence scores [S, S, B, C]</span><br><span class="line">        scores &#x3D; tf.expand_dims(confs, -1) * tf.expand_dims(class_probs, 2)</span><br><span class="line"></span><br><span class="line">        scores &#x3D; tf.reshape(scores, [-1, self.C])  # [S*S*B, C]</span><br><span class="line">        boxes &#x3D; tf.reshape(boxes, [-1, 4])  # [S*S*B, 4]</span><br><span class="line"></span><br><span class="line">        # find each box class, only select the max score</span><br><span class="line">        box_classes &#x3D; tf.argmax(scores, axis&#x3D;1)</span><br><span class="line">        box_class_scores &#x3D; tf.reduce_max(scores, axis&#x3D;1)</span><br><span class="line"></span><br><span class="line">        # filter the boxes by the score threshold</span><br><span class="line">        filter_mask &#x3D; box_class_scores &gt;&#x3D; self.threshold</span><br><span class="line">        scores &#x3D; tf.boolean_mask(box_class_scores, filter_mask)</span><br><span class="line">        boxes &#x3D; tf.boolean_mask(boxes, filter_mask)</span><br><span class="line">        box_classes &#x3D; tf.boolean_mask(box_classes, filter_mask)</span><br><span class="line"></span><br><span class="line">        # non max suppression (do not distinguish different classes)</span><br><span class="line">        # ref: https:&#x2F;&#x2F;tensorflow.google.cn&#x2F;api_docs&#x2F;python&#x2F;tf&#x2F;image&#x2F;non_max_suppression</span><br><span class="line">        # box (x, y, w, h) -&gt; box (x1, y1, x2, y2)</span><br><span class="line">        _boxes &#x3D; tf.stack([boxes[:, 0] - 0.5 * boxes[:, 2], boxes[:, 1] - 0.5 * boxes[:, 3],</span><br><span class="line">                           boxes[:, 0] + 0.5 * boxes[:, 2], boxes[:, 1] + 0.5 * boxes[:, 3]], axis&#x3D;1)</span><br><span class="line">        nms_indices &#x3D; tf.image.non_max_suppression(_boxes, scores,</span><br><span class="line">                                                   self.max_output_size, self.iou_threshold)</span><br><span class="line">        self.scores &#x3D; tf.gather(scores, nms_indices)</span><br><span class="line">        self.boxes &#x3D; tf.gather(boxes, nms_indices)</span><br><span class="line">        self.box_classes &#x3D; tf.gather(box_classes, nms_indices)</span><br></pre></td></tr></table></figure><h2 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h2><p><a href="https://arxiv.org/abs/1612.08242">paper</a>;</p><p>参考博客：<a href="https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&amp;mid=2247484076&amp;idx=1&amp;sn=7741125973b350fd99e2a2dd02d31849&amp;chksm=9f80b83aa8f7312c62c65d7c9deea687b1c0b9d5144e5bd14c075fa93f92b162e277807bb04f&amp;scene=21#wechat_redirect">1</a>, <a href="https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&amp;mid=2247484093&amp;idx=1&amp;sn=44ff2ef2c2c7e9c8fbbfa771645fbbec&amp;chksm=9f80b82ba8f7313d7bf951450fe5a278a1dd24310f9113a49f82f89da0cca6d8133c438cf896&amp;scene=21#wechat_redirect">2</a>, <a href="https://zhuanlan.zhihu.com/p/40659490">3</a>, <a href="https://zhuanlan.zhihu.com/p/35325884">4</a></p><p>相比于YOLO v1，v2每个cell通过聚类设置了anchor boxes,这些anchor会预测offset，置信度和类别概率，全卷积预测，除去了FC层</p><p>聚类采用的距离是衡量box和聚类中心之间的IOU，如果IOU大就说明距离近，聚类得出的结果比直接手工设定的anchor相比，前者的平均IOU更大些，这样模型会更容易训练学习。</p><p>回归的设置</p><p>pass through层为了预测小物体，通过拆分特征图来缩小尺寸并增加维度</p><p>先验框怎么匹配的，与v1类似，还是看物体GT的中心落在哪一个cell上，这个cell的anchor boxes就负责预测它，具体是哪一个anchor box预测，需要在训练时确定，选IOU最大（这里计算不考虑位置，只考虑面积）的那个，其他的都不匹配。与ground truth匹配的先验框计算坐标误差、置信度误差以及分类误差，而其它的边界框只计算置信度误差，与v1类似</p><p>reorganization, 跨层连接，自己构建的网络Darknet-19，reorg layer/passthrough</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/yolopaperreading/darknet19.JPG" alt="yolov2采用自搭建的网络，Darknet-19"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/yolopaperreading/yolov2_backbone.png" alt="yolo v2 net, 源自：https://ethereon.github.io/netscope/#/gist/d08a41711e48cf111e330827b1279c31"></p><p>全卷积，多尺度</p><p>加入anchors，提高recall</p><p>anchor尺度聚类代替手工设定，<a href="https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&amp;mid=2247484186&amp;idx=1&amp;sn=6b2f83ab797f5d585b46aa749c8bf169&amp;scene=21#wechat_redirect">如何实现聚类</a></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/yolopaperreading/yolov2_anchor.JPG" alt></p><p>预测bounding box偏移变量$t_{x},t_{y},t_{w},t_{h}$，基于anchor的宽高预测，基于grid cell左上角的位置预测，相比于v1直接预测框的绝对位置，偏移量能更好地被网络学习到，因为比较小且稳定。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/yolopaperreading/yolov2_regress.JPG" alt></p><p>loss函数</p><script type="math/tex; mode=display">\begin{array}{rl}\operatorname{loss}_{t}=\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} & 1_{\text {Max } 100<\text { Thresh }} \lambda_{\text {noobj }} *\left(-b_{i j k}^{o}\right)^{2} \\& +1_{t<12800} \lambda_{\text {prior }} * \sum_{r_{\epsilon}(x, y, w, h)}\left(\text { prior }_{k}^{r}-b_{i j k}^{r}\right)^{2} \\& +1_{k}^{\text {truth }}\left(\lambda_{\text {coord }} * \sum_{r_{\epsilon}(x, y, w, h)}\left(\text { truth }^{r}-b_{i j k}^{r}\right)^{2}\right. & \left.+b_{i j k}^{2}\right)^{2} \\& +\lambda_{\text {obj }} *\left(I O U_{\text {truth }}^{k}\right. \\& \left.+\lambda_{\text {class }} *\left(\sum_{c=1}^{C}\left(t r u t h^{c}-b_{i j k}^{c}\right)^{2}\right)\right)\end{array}</script><p>先验框匹配，样本选择 ，IOU计算只考虑形状大小，不考虑坐标，移到原点计算，每个groudtruth只会分配一个最合适的预测框。前面先让预测框学着预测先验框anchor</p><p>训练，多scale训练，通道数不变，只是特征图的尺寸会变化。先用$224 \times 224$尺寸ImageNet图片对Darknet-19进行第一阶段训练，然后将图片尺寸调整为$448 \times 448$继续进行第二阶段的训练，目的是为了让网络可以适应大尺寸的图片输入。之后移除最后的卷积层，global avgpooling和分类的softmax, 增加一些卷积层改为分类backbone进行训练。输出的通道数为$anchor_nums \times (4+1+num_classes)$，以VOC数据集为例，最后的网络输出shape为$(N,W,H,125)$，先reshape为$(N,W,H,5,25)$，其中$[:,:,:,:,0:4]$为边界框的位置和大小$(t_{x},t_{y},t_{w},t_{h})$，$[:,:,:,:,4]$为边界框的置信度，$[:,:,:,:,5:]$为边界框的类别预测，最后还需要对其进行NMS等后处理</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">Detection ops for Yolov2</span><br><span class="line">codelink:https:&#x2F;&#x2F;github.com&#x2F;xiaohu2015&#x2F;DeepLearning_tutorials&#x2F;tree&#x2F;master&#x2F;ObjectDetections&#x2F;yolo2</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def decode(detection_feat, feat_sizes&#x3D;(13, 13), num_classes&#x3D;80,</span><br><span class="line">           anchors&#x3D;None):</span><br><span class="line">    &quot;&quot;&quot;decode from the detection feature&quot;&quot;&quot;</span><br><span class="line">    H, W &#x3D; feat_sizes</span><br><span class="line">    num_anchors &#x3D; len(anchors)</span><br><span class="line">    detetion_results &#x3D; tf.reshape(detection_feat, [-1, H * W, num_anchors,</span><br><span class="line">                                        num_classes + 5])</span><br><span class="line"></span><br><span class="line">    bbox_xy &#x3D; tf.nn.sigmoid(detetion_results[:, :, :, 0:2])</span><br><span class="line">    bbox_wh &#x3D; tf.exp(detetion_results[:, :, :, 2:4])</span><br><span class="line">    obj_probs &#x3D; tf.nn.sigmoid(detetion_results[:, :, :, 4])</span><br><span class="line">    class_probs &#x3D; tf.nn.softmax(detetion_results[:, :, :, 5:])</span><br><span class="line"></span><br><span class="line">    anchors &#x3D; tf.constant(anchors, dtype&#x3D;tf.float32)</span><br><span class="line"></span><br><span class="line">    height_ind &#x3D; tf.range(H, dtype&#x3D;tf.float32)</span><br><span class="line">    width_ind &#x3D; tf.range(W, dtype&#x3D;tf.float32)</span><br><span class="line">    x_offset, y_offset &#x3D; tf.meshgrid(height_ind, width_ind)</span><br><span class="line">    x_offset &#x3D; tf.reshape(x_offset, [1, -1, 1])</span><br><span class="line">    y_offset &#x3D; tf.reshape(y_offset, [1, -1, 1])</span><br><span class="line"></span><br><span class="line">    # decode</span><br><span class="line">    bbox_x &#x3D; (bbox_xy[:, :, :, 0] + x_offset) &#x2F; W</span><br><span class="line">    bbox_y &#x3D; (bbox_xy[:, :, :, 1] + y_offset) &#x2F; H</span><br><span class="line">    bbox_w &#x3D; bbox_wh[:, :, :, 0] * anchors[:, 0] &#x2F; W * 0.5 # 这里除以0.5是为下面中心点计算左上和右下坐标</span><br><span class="line">    bbox_h &#x3D; bbox_wh[:, :, :, 1] * anchors[:, 1] &#x2F; H * 0.5</span><br><span class="line"></span><br><span class="line">    bboxes &#x3D; tf.stack([bbox_x - bbox_w, bbox_y - bbox_h,</span><br><span class="line">                       bbox_x + bbox_w, bbox_y + bbox_h], axis&#x3D;3)</span><br><span class="line"></span><br><span class="line">    return bboxes, obj_probs, class_probs</span><br></pre></td></tr></table></figure><p>预测，归功于multi-scale training, 测试时，大尺寸的图片mAP很高</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/yolopaperreading/yolov2_improvement.JPG" alt="相比v1做的一些改进和tricks"></p><p>yolo9000, 联合ImageNet和COCO数据集进行检测和分类训练</p><h2 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h2><p><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">paper</a>;</p><p>换了backbone, Darknet-53, 使用仿resnet的skip connection，引入residual module，通过大中小三个特征图尺度和不同的anchor大小，提升小目标的检测精度</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/yolopaperreading/darknet53.JPG" alt="Darknet-53"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/yolopaperreading/yolov3_architecture.jpg" alt="yolov3检测架构，图来自：https://zhuanlan.zhihu.com/p/35325884"></p><p>多尺度融合。FPN，依然是使用k-mean聚类得出9种不同大小比例的anchor，然后均匀的分配到3个不同scales的feature map上去。</p><p>yolov2与yolov3的损失函数，知乎相关<a href="https://www.zhihu.com/question/357005177">回答</a></p><p>网络预测出的$t_{x}, t_{y}$是在0-1之间的，采用了BCE而不是MSE，loss是在输出的t上计算，还是进一步归一化处理后的b上计算？</p><h2 id="YOLO-v4"><a href="#YOLO-v4" class="headerlink" title="YOLO v4"></a>YOLO v4</h2><p><a href="https://arxiv.org/abs/2004.10934">paper</a>;</p><p>参考博客：<a href="https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&amp;mid=2247485776&amp;idx=1&amp;sn=3cc43a8763bf3ef737ecd7f7065b54f2&amp;chksm=9f80b3c6a8f73ad029734bafdf7dd40f2a15af29b70e63bdbd7b2e5cd802ce5f77fbfffb624c&amp;scene=21#wechat_redirect">1</a>, <a href="https://zhuanlan.zhihu.com/p/135980432">2</a></p><p>一个GT可以有多个anchor对应，不再只回归一个，提升正样本比例</p><p>Mosaic（马赛克）数据增强和Self-Adversarial Training(自对抗训练)比较有趣，从风格迁移可视化CNN来的</p><p><a href="https://zhuanlan.zhihu.com/p/172121380">https://zhuanlan.zhihu.com/p/172121380</a></p><p><a href="https://zhuanlan.zhihu.com/p/143747206">https://zhuanlan.zhihu.com/p/143747206</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;YOLO系列是one-stage目标检测种的重要成果，兼具速度与精度，同时包含了很多与时俱进的tricks，是理论与工程实践完美结合的产物。虽然Andrew Ng说YOLO论文比较难读，但是为了更好的梳理检测这一模块，理解实践这项工作必须要进行下去。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&amp;amp;mid=2247485030&amp;amp;idx=1&amp;amp;sn=585864ef65bef3aa21c953506aae67f3&amp;amp;chksm=9f80bcf0a8f735e680e552d973247d4577f974b3a5eff23d89779c3416cb8f360fa0ee6ccb4c&amp;amp;scene=21#wechat_redirect&quot;&gt;Pytorch YOLO项目推荐&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzA4MDExMDEyMw==&amp;amp;mid=2247489178&amp;amp;idx=1&amp;amp;sn=c680b306f52f8a14b6dbd95dcf825cd9&amp;amp;chksm=9fa861dea8dfe8c805849ea244a9d5a41c5800c2420ebe5ab59015e0e347106009a1b7503262&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0801DMwwN4jG1mIgJtRHCBbB&amp;amp;sharer_sharetime=1596243161365&amp;amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;amp;key=519b8bbaea533f94d6ea97b2ce92dec8fa25f479b15fcec13c3d4a8a930c7d67bcf1e2a1b5f384022df13a66b501b7961fcaa1e6369148ca1a02a41553f72a4f4f1c5caba43b01d896bfa5448880d86e&amp;amp;ascene=1&amp;amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;amp;devicetype=Windows+10+x64&amp;amp;version=62090529&amp;amp;lang=zh_CN&amp;amp;exportkey=A0Fqg9Zs%2F5n9SoSbTTvLSBM%3D&amp;amp;pass_ticket=OjPkHeLi1VRb2pZNqGnzwsGeG%2BPZ8Pp95ltGB1Cixid44X1S6yC8NrZdAH%2BnttEd&quot;&gt;YOLO算法家族全景图&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;推荐阅读：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;amp;mid=2247504485&amp;amp;idx=1&amp;amp;sn=1edfe96858eca65764f5ac0b73932d87&amp;amp;chksm=ec1c3f9cdb6bb68ae198dd24e0bc301a397a5f06f63377d914184ce02ee90e122b587e245d89&amp;amp;scene=0&amp;amp;xtrack=1&amp;amp;key=8723f2e63c89806b337320fe9cd49f907a0e73a074f3fd6ea5c1f54a03f0653490e9551365b96282019e49773aee9250e35e1c4bbab89999b7be583fc38af84c9ff7b535fd459d6c0b8632c6c92a56fa974d9a260ab86a01dc9ef13c97ef814758185a3cd02aaebb6c1df5870cd2f6d711040f58b53fe7f993591f088a2fc1b0&amp;amp;ascene=1&amp;amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;amp;devicetype=Windows+10+x64&amp;amp;version=62090529&amp;amp;lang=zh_CN&amp;amp;exportkey=Ayc7FmFAK8Edscesr09jGpA%3D&amp;amp;pass_ticket=4Y3TKSx0fE%2Bxg%2B7QWVbNmub5ArN7rwIuPbCxsop%2BTZM1j9YSNFZ65ULuJvjYrYUJ&quot;&gt;YOLO v1-v5全解读&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
    <category term="object detection" scheme="http://densecollections.top/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>侯捷C++面向对象程序设计</title>
    <link href="http://densecollections.top/posts/houjieC++/"/>
    <id>http://densecollections.top/posts/houjieC++/</id>
    <published>2020-05-28T08:48:27.000Z</published>
    <updated>2021-01-02T12:52:49.113Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>侯捷手把手教学C++</p><p><a href="https://www.bilibili.com/video/BV1aW411H7Xa?p=1">video-part1</a>；</p><p><a href="https://www.bilibili.com/video/BV1sW411J7JQ/?spm_id_from=333.788.videocard.0">video-part2</a>;</p><p><a href="https://github.com/harvestlamb/Cpp_houjie">课件与代码</a>；</p><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h2><p>编写规范的class代码（object based，单一class)；</p><p>学习class之间的关系（object oriented，多个class）—-继承/复合/委托；</p><p>主要学习c++ 98/c++11，以c++ 98为主；</p><p>同时关注C++ 语言和C++标准库；</p><p>推荐书籍：C++ Primer, The C++ Programming Language (C++11), Effective C++ 11, The C++ Standard Library, STL源码剖析；</p><a id="more"></a><h2 id="2-头文件与类声明"><a href="#2-头文件与类声明" class="headerlink" title="2.头文件与类声明"></a>2.头文件与类声明</h2><p>数据和函数包在一起打包成class，与struct有一些差别。</p><p>C++代码的基本形式:</p><ul><li>.h(header files, class声明)；</li><li>.cpp(#include<iostream.h>, #include “own_file.h”, 标准库和自己的引用方式不同)；</iostream.h></li><li>.h(标准库)；</li></ul><p>头文件防卫式声明（任意include头文件，不会重复include）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#ifndef __COMPLEX__</span><br><span class="line">#define __COMPLEX__</span><br><span class="line">.....</span><br><span class="line">前置声明</span><br><span class="line">类-声明</span><br><span class="line">类-定义</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">#endif</span><br></pre></td></tr></table></figure><p>类声明：class head, class body；</p><p>class template(用的时候再指定，比如数据类型)；</p><p><code>template&lt;typename T&gt;</code></p><h2 id="3-构造函数"><a href="#3-构造函数" class="headerlink" title="3.构造函数"></a>3.构造函数</h2><p>inline函数，在class内部定义完成，比较快，但是是否成为inline函数由编译器决定；</p><p>数据指定为private，只给函数内部使用，用的时候利用class内部函数传递，不能直接调用；函数大部分可指定为public，允许给外部使用，也可指定private，class内部使用；</p><p>constructor（ctor，构造函数），创建对象，程序自动调用；</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">complex c1(2,1);</span><br><span class="line">complex c2; &#x2F;&#x2F;默认值</span><br><span class="line">complex* p &#x3D; new complex(4); &#x2F;&#x2F;指针</span><br></pre></td></tr></table></figure><p>变量的初始化和赋值是两个阶段，能初始化的不要放在赋值，效率差一些；</p><p>不带指针的函数多半不用析构函数；</p><p><code>: re (r), im(i)</code></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/%E5%88%9D%E5%80%BC%E5%8C%96.JPG" alt="构造对象与初始化"></p><p>构造函数可以有很多个，overloading重载，也就是说同一名的函数可能不止一个，但是编译器会对其作出区分，实际名称并不相同。但是写的时候要注意，可能会造成冲突，导致编译器不知道调用哪个。</p><h2 id="4-参数传递与返回"><a href="#4-参数传递与返回" class="headerlink" title="4.参数传递与返回"></a>4.参数传递与返回</h2><p>举例Singleton，说明确实有类中的函数是private的。</p><p><code>const</code>关键字，代表函数不会改变数据内容：<code>double real() const &#123;return re;&#125;</code>，如果考虑到函数不改变data，<strong>一定要加上</strong>，避免编译时错误。</p><p><code>const</code>也可以用在对象面前，代表这个对象的指定的值是不可以改变的，<code>const complex c1(2,1);</code></p><p>参数传递：pass by value / pass by reference (to const)，最好参数传递都用引用，如果害怕会引用的值会改变，可以to const，<code>const complex&amp; param</code>，约定引用的值不要改变，否则编译出错。</p><p>返回值传递：return by value / return by reference (to const)，返回值也尽量传reference。</p><p>friend （友元）：<code>friend</code>关键字指定的函数可以取private中的data，比从class内部函数拿会快些。</p><p>相同class的各个object互为friend。</p><p>设计类注意：</p><ul><li>数据放在private，函数尽量放public，可被外部调用；</li><li>参数尽可能传reference，看状况加const；</li><li>返回值尽可能传reference；</li><li>class body中需要加const就去加；</li><li>构造函数的特殊用法，init初始值尽可能去用它；</li></ul><p>return by reference不能使用的一种情况，class body外的各种定义，应该是局部的变量，local variable去返回的时候不要用引用，因为程序一结束，这个空间就释放出去了。</p><h2 id="操作符重载与临时对象"><a href="#操作符重载与临时对象" class="headerlink" title="操作符重载与临时对象"></a>操作符重载与临时对象</h2><p>1.operator overloading(操作符重载，成员函数)，this:</p><p>不能在参数列写this（成员函数都有this pointer），但是函数里面可以用，操作符传递的时候会自动给this传指针。比如<code>c2 += c1</code>中，<code>+=</code>这个操作符重载了，传递的时候c2自动给了this，你设计的时候不需要人为指定传给谁，只需要指定c1给谁就行了。</p><p>连续赋值，<code>c3+=c2+=c1</code>，关注一下返回值，ppt中举这个例子提醒我们操作符<code>+=</code>重载函数返回类型是<code>complex&amp;</code>，非代表无返回值的<a href="https://blog.csdn.net/llxlqy/article/details/79339522"><code>void</code></a>，因此支持了从右到左连续赋值操作。</p><p>2.操作符重载，非成员函数，无this:</p><p>ppt举例为了应对几种可能用法，写出几种函数方式。</p><p>temp object —typename(); 同时表明返回的是局部变量，return by value，而不是reference: <code>return complex(x + real(y), imag(y))</code>，这里的complex()就代表一个临时对象。</p><p>举例关于正负号<code>+, -</code>和运算符<code>+, -</code>编译器通过参数个数区分。</p><p>对于特殊的操作符只能写成全局的非成员函数，比如<code>cout &lt;&lt;</code>中的<code>&lt;&lt;</code>，输出操作符。当然侯捷老师举例说你也可以用成员函数代替，但是这样就变成了<code>c1 &lt;&lt; cout</code>，c1放前面用this指针读取，cout是操作符重载函数的输入参数，虽然不会有啥错误，但是不符合常规习惯。</p><p>这里讲了cout的返回值不是void，因为可能会连续输出，返回值为ostream&amp;，再一次强调好的程序设计思想。</p><p>再一次总结：初始化值；const；参数传递尽量考虑pass by reference；return考虑value或者reference；数据尽量放在private，函数一般放在public中。</p><h2 id="复习Complex类的实现过程"><a href="#复习Complex类的实现过程" class="headerlink" title="复习Complex类的实现过程"></a>复习Complex类的实现过程</h2><ul><li><p>防卫式常数定义；</p></li><li><p>class head + class body；</p></li><li><p>private写数据，public写函数（返回值方式，参数传递方式，初值列，成员函数（前面加class名称），考虑是否加const，利用友元关键字获取private数据）；</p></li><li><p>inline函数（是否成为看编译器），传引用时注意变量是否为local variable；</p></li><li><p>可用临时对象返回；</p></li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#ifndef __COMPLEX__</span><br><span class="line">#define __COMPLEX__</span><br><span class="line">class complex</span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">complex (double r &#x3D; 0, double i &#x3D; 0)</span><br><span class="line">: re (r), im (i)</span><br><span class="line">&#123; &#125;</span><br><span class="line">complex&amp; operator +&#x3D; (const complex&amp;);</span><br><span class="line">double real () const &#123; return re; &#125;</span><br><span class="line">double imag () const &#123; return im; &#125;</span><br><span class="line">private:</span><br><span class="line">double re, im;</span><br><span class="line">friend complex&amp; __doapl (complex*,</span><br><span class="line">const complex&amp;);</span><br><span class="line">&#125;;</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">inline complex&amp;</span><br><span class="line">__doapl(complex* ths, const complex&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">ths-&gt;re +&#x3D; r.re; &#x2F;&#x2F;ths是代表类的指针，所以用-&gt;访问成员</span><br><span class="line">ths-&gt;im +&#x3D; r.im;</span><br><span class="line">return *ths;</span><br><span class="line">&#125;</span><br><span class="line">inline complex&amp;</span><br><span class="line">complex::operator +&#x3D; (const complex&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">return __doapl (this, r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">inline complex</span><br><span class="line">operator + (const complex&amp; x, const complex&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">return complex ( real (x) + real (y),</span><br><span class="line">imag (x) + imag (y) );</span><br><span class="line">&#125;</span><br><span class="line">inline complex</span><br><span class="line">operator + (const complex&amp; x, double y)</span><br><span class="line">&#123;</span><br><span class="line">return complex (real (x) + y, imag (x));</span><br><span class="line">&#125;</span><br><span class="line">inline complex</span><br><span class="line">operator + (double x, const complex&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">return complex (x + real (y), imag (y));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream.h&gt;</span><br><span class="line">ostream&amp;</span><br><span class="line">operator &lt;&lt; (ostream&amp; os,</span><br><span class="line">const complex&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">return os &lt;&lt; &#39;(&#39; &lt;&lt; real (x) &lt;&lt; &#39;,&#39;</span><br><span class="line">&lt;&lt; imag (x) &lt;&lt; &#39;)&#39;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="三大函数：拷贝构造，拷贝赋值，析构"><a href="#三大函数：拷贝构造，拷贝赋值，析构" class="headerlink" title="三大函数：拷贝构造，拷贝赋值，析构"></a>三大函数：拷贝构造，拷贝赋值，析构</h2><p>解析string类，带point numbers指针；</p><p>big three，三个特殊函数；</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class String</span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">String(const char* cstr &#x3D; 0);</span><br><span class="line">String(const String&amp; str); &#x2F;&#x2F;拷贝构造</span><br><span class="line">String&amp; operator&#x3D;(const String&amp; str); &#x2F;&#x2F;拷贝赋值</span><br><span class="line">~String(); &#x2F;&#x2F;析构函数，离开作用域，进行清楚clean</span><br><span class="line">char* get_c_str() const &#123; return m_data; &#125; &#x2F;&#x2F;成员函数，inline function</span><br><span class="line">private:</span><br><span class="line">char* m_data; &#x2F;&#x2F;指定指针，动态分配，未指定大小</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>class with pointer members必须有copy ctor(拷贝构造)和copy op=(拷贝赋值)，浅拷贝与深拷贝的问题。</strong></p><p>拷贝赋值：检测自我赋值，先清空，再开辟，然后复制过来。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 析构</span><br><span class="line">inline</span><br><span class="line">String::~String()</span><br><span class="line">&#123;</span><br><span class="line">delete[] m_data;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 拷贝构造</span><br><span class="line">inline</span><br><span class="line">String::String(const String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">m_data &#x3D; new char[ strlen(str.m_data) + 1 ];</span><br><span class="line">strcpy(m_data, str.m_data);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 拷贝赋值</span><br><span class="line">inline</span><br><span class="line">String&amp; String::operator&#x3D;(const String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">if (this &#x3D;&#x3D; &amp;str)</span><br><span class="line">   return *this;</span><br><span class="line">delete[] m_data;</span><br><span class="line">m_data &#x3D; new char[ strlen(str.m_data) + 1 ];</span><br><span class="line">strcpy(m_data, str.m_data);</span><br><span class="line">return *this;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="堆，栈与内存管理"><a href="#堆，栈与内存管理" class="headerlink" title="堆，栈与内存管理"></a>堆，栈与内存管理</h2><p>stack，是存在于某作用域scope的一块内存空间。比如函数本身就会形成一个stack。</p><p>heap堆，由操作系统提供的一块global内存空间，程序可动态分配，比如<code>new</code>关键字，需要自己释放。</p><p>stack(auto) / static / global object</p><p>注意写法，防止内存泄漏；</p><p>new: 先分配memory, 再调用ctor（拷贝构造）；</p><p>delete: 先调用dtor（析构），再释放memory；</p><p>独家讲解VC编译器动态分配（new）的内存块和分配所得的array，进一步指出array new 一定要搭配 array delete；</p><p><a href="https://www.cnblogs.com/Simulation-Campus/p/8809934.html">C++中::的作用</a>；</p><p><a href="https://blog.csdn.net/love_x_you/article/details/42846993">C++中的数据类型</a>；</p><p>char(unsigned), short, int(unsigned), long(unsigned), bool, float(单精度，保留到小数点后7位), double(双精度，保留到小数点后16位)。</p><h2 id="复习string类的实现过程"><a href="#复习string类的实现过程" class="headerlink" title="复习string类的实现过程"></a>复习string类的实现过程</h2><p>防卫式声明；</p><p>动态分配，利用指针（32位系统4个byte）；</p><p>构造函数，拷贝构造，拷贝赋值，析构函数（把自己清干净，clean）；</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 类声明</span><br><span class="line">class String</span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">String(const char* cstr &#x3D; 0);</span><br><span class="line">String(const String&amp; str);</span><br><span class="line">String&amp; operator&#x3D;(const String&amp; str);</span><br><span class="line">~String();</span><br><span class="line">char* get_c_str() const &#123; return m_data; &#125;</span><br><span class="line">private:</span><br><span class="line">char* m_data;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; ctor构造函数</span><br><span class="line">inline</span><br><span class="line">&#x2F;&#x2F; ::声明作用域，类外写函类的数</span><br><span class="line">String::String(const char* cstr &#x3D; 0)</span><br><span class="line">&#123;</span><br><span class="line">if (cstr) &#123;</span><br><span class="line">m_data &#x3D; new char[strlen(cstr)+1];</span><br><span class="line">strcpy(m_data, cstr);</span><br><span class="line">&#125;</span><br><span class="line">else &#123; &#x2F;&#x2F; 未指定初值</span><br><span class="line">m_data &#x3D; new char[1];</span><br><span class="line">*m_data &#x3D; &#39;\0&#39;;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F;析构函数</span><br><span class="line">inline</span><br><span class="line">String::~String()</span><br><span class="line">&#123;</span><br><span class="line">delete[] m_data;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; copy ctor 拷贝构造函数</span><br><span class="line">inline</span><br><span class="line">String::String(const String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">m_data &#x3D; new char[ strlen(str.m_data) + 1 ];</span><br><span class="line">strcpy(m_data, str.m_data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; copy assignment operator拷贝赋值函数</span><br><span class="line">inline</span><br><span class="line">&#x2F;&#x2F; string后面的&amp;放在typename后面，代表reference</span><br><span class="line">String&amp; String::operator&#x3D;(const String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">&#x2F;&#x2F; str前面的&amp;代表放在object前面，取出地址</span><br><span class="line">&#x2F;&#x2F; this是指针</span><br><span class="line">if (this &#x3D;&#x3D; &amp;str)</span><br><span class="line">return *this;</span><br><span class="line">delete[] m_data;</span><br><span class="line">m_data &#x3D; new char[ strlen(str.m_data) + 1 ];</span><br><span class="line">strcpy(m_data, str.m_data);</span><br><span class="line">&#x2F;&#x2F; 这里用了return，不用void，不做返回值处理与之前的打印cout使用的&lt;&lt;类似，是为了连续赋值的情况出现，否则连续赋值没有对象</span><br><span class="line">&#x2F;&#x2F; 这里*this代表传回的是this指向的值，至于具体传出去什么由函数前的声明Sting&amp;决定，两者没有绝对关系，这里实际应该传出去了引用</span><br><span class="line">return *this;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/weixin_42878758/article/details/82865314">C++中*和&amp;</a></p><h2 id="扩展补充：类模板，函数模板及其他"><a href="#扩展补充：类模板，函数模板及其他" class="headerlink" title="扩展补充：类模板，函数模板及其他"></a>扩展补充：类模板，函数模板及其他</h2><p>static关键字，定义静态变量。静态局部变量保存在全局数据区（在函数内定义，多次调用函数只会初始化一次，但是只受此函数控制），而不是保存在栈中，每次的值保持到下一次调用，直到下次赋新值。静态全局变量可以实现文件隔离，静态全局变量不能被其它文件所用 (全局变量可以)，其它文件中可以定义相同名字的变量，不会发生冲突。（见<a href="https://blog.csdn.net/majianfei1023/article/details/45290467">此</a>）</p><p>singleton，某个类在整个系统中只能有一个实例对象可被获取和使用的代码模式？（见<a href="https://blog.csdn.net/crayondeng/article/details/24853471">此</a>）</p><p>类模板，函数模板；</p><p>namespace std 封装到单元，将库名称封起来，防止冲突。两种用法，一种是using directive: <code>using namespace std;</code>, 另一种是using declaration: <code>using std::cout;</code>, 一般一个文件代码如果不长，只有几百来行，可以直接用第一种。（见<a href="https://blog.csdn.net/fsz520w/article/details/82561795">此</a>）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; using directive方法</span><br><span class="line">#include &lt;iostream.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">cin &lt;&lt; ...;</span><br><span class="line">cout &lt;&lt; ...;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; using declaration</span><br><span class="line">#include &lt;iostream.h&gt;</span><br><span class="line">using std::cout;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">std::cin &lt;&lt; ...;</span><br><span class="line">cout &lt;&lt; ...;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 或者</span><br><span class="line">#include &lt;iostream.h&gt;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">std::cin &lt;&lt; ;</span><br><span class="line">std::cout &lt;&lt; ...;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="组合和继承"><a href="#组合和继承" class="headerlink" title="组合和继承"></a>组合和继承</h2><p>object oriented programming, object oriented desugn (OOP, OOD).</p><p>继承（inheritance），复合（composition），委托（delegation）.</p><p>1.复合：has-a, 意思是说一个类具有比较强大的功能，可以将其某些性能改造包装成一个新的类，adapter. （container—-&gt; component）</p><p>复合下的构造和析构：构造由内而外（打地基），析构由外而内（剥洋葱）。</p><p>2.委托：composition by reference, pimpl: pointer to implementation.(handle/body, 编译防火墙)</p><p>共享，reference counting，当其中某个人想要改变共享的内容，那么会单独拿出一份copy来给他改，不影响原先的共享内容。</p><p>3.<strong>继承</strong>: is-a, <code>: public _List_node_base</code>，属于哪一类</p><p>继承下的构造与析构，基类与派生类。父类是子类的一种组成成分，类似复合，因此构造由内而外，析构由外而内（父类的析构函数必须是virtual的，否则会出现undefined behavior）。</p><h2 id="虚函数与多态"><a href="#虚函数与多态" class="headerlink" title="虚函数与多态"></a>虚函数与多态</h2><p>inheritance with virtual functions.</p><p>non-virtual函数：不希望子类重新定义（override，覆写）它；</p><p>virtual函数：你希望子类重新定义（override覆写）它，且你对它已有默认定义；</p><p>pure virtual函数：你希望子类一定要定义它，你对它没有默认定义；</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Shape&#123;</span><br><span class="line">public:</span><br><span class="line">  virtual void draw() const&#x3D;0; # pure virtual</span><br><span class="line">  &#x2F;&#x2F; impure virtual</span><br><span class="line">  virtual void error(const std::string&amp; msg);</span><br><span class="line">  &#x2F;&#x2F; non-virtual</span><br><span class="line">  int objectID() const;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">class Rectangle:public Shape&#123;...&#125;;</span><br><span class="line">class Epllipse: public Shape&#123;...&#125;;</span><br></pre></td></tr></table></figure><p>举例application framework中的template method（指的某一个函数），不是之前的C++模板（通过this调用子类覆写的虚函数）。</p><p>inheritance+composition关系下的构造和析构，子类既有继承又有component part（子类先构造父类的default构造函数，然后在调用component的default构造函数），或者子类是继承，父类有component part（先复合，再继承）；</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/inheritance_composition_ctor.JPG" alt></p><h2 id="委托相关设计"><a href="#委托相关设计" class="headerlink" title="委托相关设计"></a>委托相关设计</h2><p>delegation+inheritance: </p><p>讲设计模式，并推荐<a href="https://pan.baidu.com/s/1pLIoWDd">Design Patterns Explained Simply</a></p><ul><li><p>file system, 文件系统；</p></li><li><p>prototype, 原型；</p></li></ul><hr><p><strong>复习：</strong></p><p>0.防卫式声明。</p><p>1.inline关键字</p><p>line的使用是有所限制的，inline只适合函数体内代码简单的函数使用，不能包含复杂的结构控制语句例如while、switch，并且不能内联函数本身不能是直接递归函数（即，自己内部还调用自己的函数）。定义在类中的成员函数缺省都是内联的，如果在类定义时就在类内给出函数定义，那当然最好。如果在类中未给出成员函数定义，而又想内联该函数的话，那在类外要加上inline，否则就认为不是内联的。（只是给编译器的建议）</p><p>2.函数构造函数初始化，比赋值快。</p><p>3.const关键字加在为改变数据值的表达式前面，限定为只读属性。</p><p>4.尽量返回引用，但是局部临时变量或者函数结束对象就死亡的不能返回其引用。</p><p>5.友元friend，相同类的各个实例对象互为friend，可以通过彼此的内部方法调用传入参数的内部私有成员变量。</p><p>6.this指针，操作符重载函数，成员函数，编译器自动读取，设计者不可自己指定作为声明，但是可以在函数中显式使用。</p><p>7.构造，拷贝构造，拷贝赋值，其中拷贝赋值记得检测自我赋值，否则内存出错。</p><p>8.new关键字，先分配足够的memory，再进行构造。delete关键字，先调用析构函数，然后释放内存。用了new array，也要注意用相应的delete array.</p><p>9.多类关系，复合，委托，继承，以及继承中的虚函数。</p><h2 id="Part2-导读"><a href="#Part2-导读" class="headerlink" title="Part2-导读"></a>Part2-导读</h2><p>part1是面向对象编程，part2是兼谈对象模型。</p><ul><li>保持良好编程风格和素养基础上，探讨更多技巧；</li><li>泛型编程（generic programming）和面对对象编程（object-oriented programming)虽然分属不同思维，但它们正是C++的技术主线，所以本课程也讨论模板（template）；</li><li>深入探索面向对象之继承关系所形成的对象模型（object model)，包括隐藏于底层的this指针，vptr虚指针，vbtl虚表，virtual mechanism虚机制，以及virtual function虚函数造成的polymorphism多态效果；</li></ul><p>讲解C++11中的三个特性，更多的特性在侯捷老师另一个C++ 2.0教学视频中提及。</p><ul><li>variadic template;</li><li>auto;</li><li>range-based for loop;</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/part2_content.JPG" alt="第二部分将涉及的内容"></p><h2 id="转换函数-conversion-function"><a href="#转换函数-conversion-function" class="headerlink" title="转换函数 conversion function"></a>转换函数 conversion function</h2><p>大意就是类型之间的转换，比如把自定义的类类型转换成内建类型（比如double），后者向相反的方向转。（类转出去，或者把其他东西转换成类）。</p><p>只要设计者认为合理，就可以在一个类中写好几个转换函数。</p><p>转换函数的形式：<code>operator typeName()</code></p><ul><li>转换函数必须是类的成员函数；</li><li>转换函数不能指定返回类型，但是有返回值；</li><li>转换函数没有参数；</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Fraction&#123;</span><br><span class="line">public:</span><br><span class="line">   Fraction(int num, int den&#x3D;1)</span><br><span class="line">     :m_numerator(num), m_denominator(den)&#123;&#125;</span><br><span class="line">   &#x2F;&#x2F; 转换函数，讲分数转成double类型，返回类型已经由operator关键字后面得double()指定</span><br><span class="line">   operator double() const&#123;</span><br><span class="line">     return (double) (m_numerator &#x2F; m_denominator);</span><br><span class="line">   &#125;</span><br><span class="line">private:</span><br><span class="line">   int m_numerator; &#x2F;&#x2F;分子</span><br><span class="line">   int m_denominator; &#x2F;&#x2F;分母</span><br><span class="line">&#125;；</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 例子</span><br><span class="line">&#x2F;&#x2F; Fraction f(3,5);</span><br><span class="line">double d&#x3D;4+f; &#x2F;&#x2F;调用operator double()将f转为0.6</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="non-explicit-one-argument-constructor"><a href="#non-explicit-one-argument-constructor" class="headerlink" title="non-explicit one argument constructor"></a>non-explicit one argument constructor</h2><p>explict关键字，用在构造函数前面，让编译器不去隐式调用变量类型转换，避免在用户不需要类型转换时就自动进行转换。</p><p>具体解释示例见<a href="https://blog.csdn.net/FightFightFight/article/details/79513657">此博客</a>。</p><h2 id="pointer-like-classes"><a href="#pointer-like-classes" class="headerlink" title="pointer-like classes"></a>pointer-like classes</h2><p>1.关于智能指针。</p><p>举例shared_ptr，实际上还有unique_ptr, auto_ptr(c++ 98, 但也可继续使用), weak_ptr等。</p><blockquote><p>为了更加容易（更加安全）的使用动态内存，引入了智能指针的概念。智能指针的行为类似常规指针，重要的区别是它负责自动释放所指向的对象。标准库提供的两种智能指针的区别在于管理底层指针的方法不同，shared_ptr允许多个指针指向同一个对象，unique_ptr则“独占”所指向的对象。标准库还定义了一种名为weak_ptr的伴随类，它是一种弱引用，指向shared_ptr所管理的对象，这三种智能指针都定义在memory头文件中。</p></blockquote><p>必须写操作符重载<code>operator*()</code>和<code>operator-&gt;()</code></p><p>其中箭头符号-&gt;会继续作用下去。</p><p>关于C++11中四种典型的智能指针讲解，见<a href="https://blog.csdn.net/k346k346/article/details/81478223">此</a>。</p><p>2.关于迭代器。</p><p>容器，容器一定带迭代器</p><p>也是智能指针，但是略有不同，操作符重载多了几个，<code>operator*()</code>,<code>operator-&gt;()</code>, <code>operator++()</code>, <code>operator--()</code>，因为指针要移动。</p><blockquote><p>迭代器是为了表示容器中某个元素位置这个概念而产生的，是一种检查容器内元素并遍历元素的数据类型。C++更趋向于使用迭代器而非下标进行操作，因为标准库（STL）为每一种标准容器定义了一种迭代器类型，而只有少数容器支持下标操作访问容器元素。</p></blockquote><p><a href="https://blog.csdn.net/u013719339/article/details/80615217">c++中的容器和迭代器</a>。</p><h2 id="function-like-classes"><a href="#function-like-classes" class="headerlink" title="function-like classes"></a>function-like classes</h2><p>仿函数，functor。</p><p>operator (), 重载小括号</p><blockquote><p>仿函数（Functor）又称为函数对象（Function Object）是一个能行使函数功能的类。仿函数的语法几乎和我们普通的函数调用一样，不过作为仿函数的类，都必须重载 operator() 运算符。因为调用仿函数，实际上就是通过类对象调用重载后的 operator() 运算符。</p><p>在stl中提供了大量有用的仿函数，比如plus，minus，multiplies，divides，modulus，equal_to，not_equal_to，greater…很多很多，根据传入的参数的个数我们可以分为只需要接受一个参数的仿函数（unary_function）和需要接收两个参数的仿函数（binary_function）。</p></blockquote><p>为什么需要仿函数，<a href="https://blog.csdn.net/K346K346/article/details/82818801">这里</a>给出了一个解释。</p><p>仿函数的一些应用见<a href="https://blog.csdn.net/coolwriter/article/details/81533226">此</a>。</p><h2 id="namespace经验谈"><a href="#namespace经验谈" class="headerlink" title="namespace经验谈"></a>namespace经验谈</h2><p>包住你的函数，防止和项目中其他开发者导致类，函数冲突。</p><p><code>using namespace std;</code></p><p><code>namespace jj01(自己取得空间名称)</code></p><p>调用函数时，加上空间名，比如：<code>jj01::test_member_template()</code></p><h2 id="template-模板"><a href="#template-模板" class="headerlink" title="template 模板"></a>template 模板</h2><p>class template，在part1中已经讲过，比如声明类的时候可以指定数据类型。</p><p>function template，与类模板不同的是，函数模板在使用是不需要显式地声明传入参数类型，编译器将自动推导类型（编译器会对function template进行实参推导）。</p><p>member template，成员模板。一个类(无论是普通类还是类模板)可以包含本身是模板的成员函数。这种函数称为成员模板。注意！成员模板不能是虚函数。该语言特性往往被用来支持class template内的成员之间的自动类型转换。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/class_template.JPG" alt="类模板"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/function_template.JPG" alt="函数模板"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/member_template.JPG" alt="成员模板"></p><h2 id="specialization"><a href="#specialization" class="headerlink" title="specialization"></a>specialization</h2><p>模板的泛化与特化。</p><p>还有一种partial specialization, 偏特化——个数上的偏特化，范围上的偏特化。</p><blockquote><p>模板为什么要特化，因为编译器认为，对于特定的类型，如果你对某一功能有更好地实现，那么就该听你的。</p><p>模板分为类模板与函数模板，特化分为全特化与偏特化。全特化就是限定死模板实现的具体类型，偏特化就是模板如果有多个类型，那么就只限定为其中的一部分，其实特化细分为范围上的偏特化与个数上的偏特化。(C++模板全特化之后已经失去了Template的属性了)</p><p>模板的泛化：是指用的时候指定类型。</p></blockquote><p>优先级：全特化&gt;偏特化&gt;主版本模板类。</p><p>具体解释可参考<a href="https://blog.csdn.net/m_buddy/article/details/72973207">1</a>和<a href="https://www.cnblogs.com/xuelisheng/p/9323853.html">2</a>。</p><h2 id="template-template-parameter"><a href="#template-template-parameter" class="headerlink" title="template template parameter"></a>template template parameter</h2><p>模板参数本身也是模板。</p><p>容器需要好几个模板参数。</p><p>函数模板不支持模板的模板参数。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">template&lt;typename T,</span><br><span class="line">          template &lt;typename T&gt;</span><br><span class="line">          class Container &#x2F;&#x2F;class不能被typename代替</span><br><span class="line">         &gt;</span><br><span class="line">class XCls</span><br><span class="line">&#123;</span><br><span class="line">private:</span><br><span class="line">  Container&lt;T&gt; c;</span><br><span class="line">public:</span><br><span class="line">   .....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意，模板的模板参数中，class 不能被替换成typename。在模板中，如果不能区分的哪应该使用typename和class，可以全部都用class替代，typename和class的作用基本相同，而typename出现得比较晚。同时它也支持缺省值（参考此<a href="https://blog.csdn.net/qq_37960222/article/details/83932246">博客</a>）。</p><h2 id="关于C-标准库"><a href="#关于C-标准库" class="headerlink" title="关于C++标准库"></a>关于C++标准库</h2><p>侯捷老师建议全部上手调用一遍，实验一遍。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/about_std.JPG" alt></p><p>可参考侯捷老师的C++标准库STL讲解<a href="https://www.bilibili.com/video/BV1db411q7B8">视频</a>。</p><h2 id="三个主题"><a href="#三个主题" class="headerlink" title="三个主题"></a>三个主题</h2><p>1.variadic templates (since c++11)，数量不定的模板参数。</p><p><code>template &lt;typename T, typename... Types&gt;</code></p><p>…就是一个所谓的pack包，<code>sizeof...(args)</code>可以看出几个arguments.</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/variadic_template.JPG" alt="c++2.0的一个比较大的语言特性"></p><p>2.auto (since c++11)</p><p>让编译器帮忙推导type类型，但是频繁使用会使代码阅读性变差。另外有些情况不适合用auto，因为编译器可能无法判断类型。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/auto.JPG" alt="auto关键字"></p><p>3.range-based for (since c++11)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for (decl : coll)&#123;</span><br><span class="line">    statement</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for (int i: &#123;2,3,5,7,9,13,17,19&#125;) &#123;</span><br><span class="line">  cout&lt;&lt;i&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vector&lt;double&gt; vec;</span><br><span class="line">...</span><br><span class="line">for (auto elem:vec)&#123;</span><br><span class="line">    cout&lt;&lt;elem&lt;&lt;endl; &#x2F;&#x2F;pass by value，对值操作不改变原先的值</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">for (auto&amp; elem:vec)&#123;</span><br><span class="line">    elem *&#x3D; 3 &#x2F;&#x2F;pass by reference,对原先的数值造成影响</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>更多的C++11特性讲解见侯捷老师的这个<a href="https://www.bilibili.com/video/BV1p4411v7Dh">视频</a>。</p><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p>一种漂亮的pointer，多半用在参数传递上面。指针*指向变量的地址，引用&amp;可以理解为变量的另一个名称，代表的就是这个变量，虽然内部依旧是指针实现的。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/reference.JPG" alt="reference和指针*的区别"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/reference_.JPG" alt="reference的用法"></p><h2 id="复合-amp-继承关系下的构造和析构"><a href="#复合-amp-继承关系下的构造和析构" class="headerlink" title="复合&amp;继承关系下的构造和析构"></a>复合&amp;继承关系下的构造和析构</h2><p>复习前面的类与类的关系。</p><p>继承：构造由内而外，析构由外而内。</p><p>复合：拥有关系，构造由内而外，析构由外而内。</p><p>继承+复合：base&lt;——derived——-&gt;component, 具体怎么排base和component看编译器。</p><h2 id="关于vptr和vtbl"><a href="#关于vptr和vtbl" class="headerlink" title="关于vptr和vtbl"></a>关于vptr和vtbl</h2><p>object model对象模型，virtual pointer(vptr)和virtual table(vtbl).</p><p>多个虚函数只有一个指针。</p><p>动态绑定（指针调用，向上转型，调用虚函数），非静态绑定，虚机制，多态（面向对象继承多态的本质）。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/vptr_vtbl.JPG" alt="虚机制，p是this pointer"></p><p>可参考此<a href="https://blog.csdn.net/qq_29003347/article/details/78566304">博客</a>。</p><h2 id="关于this"><a href="#关于this" class="headerlink" title="关于this"></a>关于this</h2><p>除了上面的那个多态，虚函数的另一种用法，template method。此例子在part1也举过，放在这里由于之前解释了虚函数和动态绑定的用法，所以复习之后进一步加深这个设计理念。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/about_this.JPG" alt="this指针指的是子类，同时调用serialize虚函数，满足动态绑定条件"></p><p>后面简单从编译后的汇编码看静态绑定和动态绑定。</p><h2 id="谈谈const"><a href="#谈谈const" class="headerlink" title="谈谈const"></a>谈谈const</h2><p>侯捷老师强调const关键字在设计时非常重要，一定要注意，该加的一定要记得加上。</p><p>const放在成员函数后头，可修饰成员函数。下面PPT的例子中，有两个<code>operator[]</code>函数，由于const也算是函数签名，所以并不会构成ambiguity. 其中前者是针对常量字符串，不必考虑COW(copy on write)，后者需考虑COW，在这里，函数是从一个字符串中取出某个值，可能对其进行变动，返回是reference也预示了这个情况。侯捷老师在这里举例了前面的共享内存的例子，可能多个指针变量指向同一块内存，如果其中某一个需要修改所指的变量值，那么就是单独copy出来一份给他改。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/const.JPG" alt="注意object调用成员函数的匹配问题"></p><p>关于const关键字的简单总结可参考此<a href="https://blog.csdn.net/u011333734/article/details/81294043">博客</a>。</p><h2 id="关于New-Delete"><a href="#关于New-Delete" class="headerlink" title="关于New, Delete"></a>关于New, Delete</h2><p>1.复习</p><p>new先分配memory，再调用ctor.</p><p>delete先调用dtor，再释放memory.</p><p>array new一点更要搭配array delete.</p><p>即<code>new()</code>搭配<code>delete()</code>，<code>new[]</code>搭配<code>delete[]</code>.</p><p>2.重载operator new, operator delete, operator new[], operator delete[] </p><p>从内存分配上看，new[]里面还加了一个counter，4个字节大小，存储你new了几个元素，这样便于delete时销毁。</p><p>如果写上了global scope operator ::，那么调用new和delete会绕过前述所有的overload functions，强迫使用global version.</p><blockquote><p>C++语言内置默认实现了一套全局new和delete的运算符函数以及placement new/delete运算符函数。不管是类还是内置类型都可以通过new/delete来进行堆内存对象的分配和释放的。对于一个类来说，当我们使用new来进行构建对象时，首先会检查这个类是否重载了new运算符，如果这个类重载了new运算符那么就会调用类提供的new运算符来进行内存分配，而如果没有提供new运算符时就使用系统提供的全局new运算符来进行内存分配。内置类型则总是使用系统提供的全局new运算符来进行内存的分配。对象的内存销毁流程也是和分配一致的。</p><p>new和delete运算符既支持全局的重载又支持类级别的函数重载。下面是这种运算符的定义的格式：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;全局运算符定义格式</span><br><span class="line">void * operator new(size_t size [, param1, param2,....]);</span><br><span class="line">void operator delete(void *p [, param1, param2, ...]);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;类内运算符定义格式</span><br><span class="line">class CA</span><br><span class="line">&#123;</span><br><span class="line">  void * operator new(size_t size [, param1, param2,....]);</span><br><span class="line">  void operator delete(void *p [, param1, param2, ...]);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对于new/delete运算符重载我们总有如何下规则：</p><ul><li>new和delete运算符重载必须成对出现。</li><li>new运算符的第一个参数必须是size_t类型的，也就是指定分配内存的size尺寸；delete运算符的第一个参数必须是要销毁释放的内存对象。其他参数可以任意定义。</li><li>系统默认实现了new/delete、new[]/delete[]、 placement new / delete 6个运算符函数。它们都有特定的意义。</li><li>你可以重写默认实现的全局运算符，比如你想对内存的分配策略进行自定义管理或者你想监测堆内存的分配情况或者你想做堆内存的内存泄露监控等。但是你重写的全局运算符一定要满足默认的规则定义。</li><li>如果你想对某个类的堆内存分配的对象做特殊处理，那么你可以重载这个类的new/delete运算符。当重载这两个运算符时虽然没有带static属性，但是不管如何对类的new/delete运算符的重载总是被认为是静态成员函数。</li><li>当delete运算符的参数&gt;=2个时，就需要自己负责对象析构函数的调用，并且以运算符函数的形式来调用delete运算符。</li></ul></blockquote><p>更多详解可参考此<a href="https://www.jianshu.com/p/d2d5cdd7aa1d">博客</a>。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/operator_new1.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/operator_new2.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/operator_new3.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/operator_new4.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/operator_new5.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/operator_new6.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/operator_new7.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/operator_new8.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/houjieC++/operator_new9.JPG" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;侯捷手把手教学C++&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1aW411H7Xa?p=1&quot;&gt;video-part1&lt;/a&gt;；&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1sW411J7JQ/?spm_id_from=333.788.videocard.0&quot;&gt;video-part2&lt;/a&gt;;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/harvestlamb/Cpp_houjie&quot;&gt;课件与代码&lt;/a&gt;；&lt;/p&gt;
&lt;h2 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1.简介&quot;&gt;&lt;/a&gt;1.简介&lt;/h2&gt;&lt;p&gt;编写规范的class代码（object based，单一class)；&lt;/p&gt;
&lt;p&gt;学习class之间的关系（object oriented，多个class）—-继承/复合/委托；&lt;/p&gt;
&lt;p&gt;主要学习c++ 98/c++11，以c++ 98为主；&lt;/p&gt;
&lt;p&gt;同时关注C++ 语言和C++标准库；&lt;/p&gt;
&lt;p&gt;推荐书籍：C++ Primer, The C++ Programming Language (C++11), Effective C++ 11, The C++ Standard Library, STL源码剖析；&lt;/p&gt;</summary>
    
    
    
    <category term="课程记录" scheme="http://densecollections.top/categories/%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="C++11" scheme="http://densecollections.top/tags/C-11/"/>
    
    <category term="侯捷" scheme="http://densecollections.top/tags/%E4%BE%AF%E6%8D%B7/"/>
    
  </entry>
  
  <entry>
    <title>二分图搜无向定长环</title>
    <link href="http://densecollections.top/posts/loopdetectioninBipartitegraph/"/>
    <id>http://densecollections.top/posts/loopdetectioninBipartitegraph/</id>
    <published>2020-05-08T02:41:32.000Z</published>
    <updated>2021-01-02T12:25:08.796Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>这个问题来源于中兴举办的2020优招算法比赛“傅里叶派”，要求是对提供的数据进行二分图建模，然后找出4，6，8，10，12，14的无向图的总数量，同时有时间和禁止多线程操作要求。起先，我是把数据当成随机生成的普通数据，然后利用DFS（B站上有个up主讲的DFS和BFS系列非常浅显易懂，<a href="https://www.bilibili.com/video/BV1Ks411579J?from=search&amp;seid=7572393692341634029">链接</a>）和拼接单链组合成环，最后利用python的set哈希去重的方法来实现的，在一阶段数据上（（256+640，256+640）大小的邻接矩阵）大概可以进1分钟至1分半钟。后面，随着第二阶段数据的增大（（1344+1344，1344+1344）大小的邻接矩阵），运行时间急剧增加，大概到了30分钟。实际上，算法的主要瓶颈在去重时间上（去重数据达到亿级别的字符串），为了降低去重时间，我花了不少时间去Google，包括数据库的一些去重，或者使用bitmap位图等，考虑到硬件限制，试了一下<a href="https://github.com/joseph-fox/python-bloomfilter">bloom filter</a>，但是由于hash碰撞等原因，可能出现一些误判，而且随着查重数量的增大，占用的内存也会增大。最后的测试时间上与set相比没有什么优势。</p><p>最后跟朋友交流，发现数据的一些规律得以解决。原来主办方给的数据是精心设计过的，整个数据中存在固定长度的固定数量sections，每个section中若$A_{i}B_{j}=1$，那么$A_{i+1}B_{j+1}=1$，存在这么一种递推关系，这样就会大大减少了搜索量和查重量。根据固定的节点修改代码，最后运行时间从半个小时降到了50s左右。</p><p>最后的数据规律是快要接近deadline知道的，虽然分析数据也应当是一个基本的素养，但是面对这种题目，我个人感觉还是追求通用性解决方案更有意义一些。比如真正面对庞大的数据量的时候，关系图是不确定的，没有这种很强的先验知识，那么应该设计系统去尽可能快速地查找和统计环的数量。我自己没有进一步深究了，目前的几个naive想法是可以考虑多线程找环查重在一起合并查重，查重获取可以采用一些数据库的技术，比如针对业务建一个服务器专门用来进行通用性查重，代码中直接调API即可。</p><p>具体题目要求，两阶段数据和代码见<a href="https://github.com/Richardyu114/ZTEchallenge-2020-Fourier">github仓库</a></p><a id="more"></a><h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><p><strong>在阶段一中，没有对数据分布进行分析，而是单纯以普通的二分图找环问题进行算法设计</strong>：</p><p>首先读入csv文件数据，将A, B之间关系采用邻接表的形式分别存储，并赋给A, B中每个人以独有的id（A的id为0-A总人数减一，B的id为0加A总人数-B总人数减一加A总人数），然后利用深度优先搜索思想，按id从小到大的顺利，以每个A为起点，不断递归往下深入图，找出符合长度的环，并将其存储，遍历去重，得到属于每个A的环结果。由于每个A都是往下搜索下一个A，所以搜出的环不会与前面的结果重复，最后直接相加每个A搜索出的结果即为最后的答案。</p><p>由于图的尺寸比较大，纯搜索环（以本题为例，尤其环的长度超过8以后搜索时间会迅速增加）和遍历去重比较耗时，为此需要进行优化。</p><p>a). 由于是搜索二分图中的无向环，因此环的长度只能是偶数，为了减少整体搜索时间，将10，12，14长度的环分别切成两条长度相等的单链（长度分别为6，7，8），仅头和尾节点相同的单链可以组合成以上述长度的环，4，6，8长度的环可以在搜索单链时分别反向回溯下一节点得到环的数量；</p><p>b). 遍历去重问题是个人算法设计中最为耗时的一部分，实验发现如果仅按照a)中的思想搜索环而不去重，可以较快得到答案。一种直接的去重方法是以list存放环的组成id，并升序排好嵌入另一个大list中，之后拼出的环在其中遍历比较，不相等即计数加一并将其append进大list中，但是随着环长度的增加，遍历比较的时间爆炸增长，几乎无法很好完成去重任务。因此后续考虑将已升好序的环的id转成字符串进行编码存入set()中，通过内部hash去重，来极大降低去重开销；</p><p><strong>在阶段二中，若直接采用阶段一的思路来处理阶段二的数据，程序运行时间会从95s左右增加到1600s左右，时间复杂度太高</strong>。对这种普适性找环算法在题目限制下优化几乎不太可能，于是将目光转移到数据上。通过分析发现，整体数据可以分成好几个sections，在每个sections内部，A点的朋友关系是递增的，即若$A_{i}B_{j}$是朋友，那么$A_{i+1}B_{j+1}$也是朋友，这种相似性结构会导致sections内部每个点A的环组成也是相似的，且数量相同，类似于动态规划中的重叠子问题。因此，只需要计算每个sections内中开头点的环数量，然后乘以该section的长度，最后相加所有的sections即可。另外，由于是无向环，因此最后还需要将累加得出的每个固定长度环的数量除以A的节点数，去除重复搜索的影响。</p><p><strong>由于两阶段的数据都存在这样的规律，因此可以大大简化搜索算法，只需指定不同section的开头节点和长度便可将其合并</strong>。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import csv</span><br><span class="line">import time</span><br><span class="line">from collections import deque</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">数据之间存在规律：如果A_&#123;i&#125;B_&#123;j&#125;之间是朋友关系，那么A_&#123;i+1&#125;B_&#123;j+1&#125;也是朋友关系</span><br><span class="line">每行数据之间和上一行存在递归关系，这些关系存在于每份数据固定长度的段落中，</span><br><span class="line">因此可以简化搜索点，每个段落只需搜索起始点即可，后面的环数量都相同</span><br><span class="line"></span><br><span class="line">1.通过深度优先搜索DFS在A和B之间反复往下找，直到固定长度或者无法深入为止；</span><br><span class="line">2.由于环的长度越大纯搜索就越耗时，所以利用一半长度加1的单链然后组合拼接；</span><br><span class="line">3.拼接过后的无向环存在重复，利用字符串编码环信息和集合set()加速遍历去重；</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">class Solutions():</span><br><span class="line"></span><br><span class="line">  def __init__(self, input_file, output_file):</span><br><span class="line">      self.input_file &#x3D; input_file  #输入的csv文件名</span><br><span class="line">      self.output_file &#x3D; output_file #输出的文件名</span><br><span class="line">      self.A_num &#x3D; 0 #存储A部落人数</span><br><span class="line">      self.B_num &#x3D; 0  # 存储B部落人数</span><br><span class="line">      self.graphAB &#x3D; &#123;&#125; # 存储A与B的好友关系</span><br><span class="line">      #存储长度为6,7,8的单链，后续组合元素到10,12,14的环</span><br><span class="line">      self.lines &#x3D; &#123;6:deque([]), 7:deque([]), 8:deque([])&#125; </span><br><span class="line">      #存储搜索到的长度为4，6，8, 10, 12, 14的环</span><br><span class="line">      self.circles &#x3D; &#123;4:set(), 6:set(), 8:set(), 10:set(), 12:set(), 14:set()&#125;</span><br><span class="line">      self.count &#x3D; &#123;4:0, 6:0, 8:0, 10:0, 12:0, 14:0&#125; #存储最终答案</span><br><span class="line">      self.sections_len &#x3D; 0 # 存储给定数据中设定的固定段落数长度</span><br><span class="line"></span><br><span class="line"># 读取CSV文件创建字典表示关系图</span><br><span class="line">  def load(self):</span><br><span class="line">      with open(self.input_file, &#39;r&#39;) as f_in:</span><br><span class="line">           lines &#x3D; list(csv.reader(f_in))</span><br><span class="line">           self.A_num &#x3D; len(lines) # A部落人数         </span><br><span class="line">           self.B_num &#x3D; len(lines[0])  # B部落人数</span><br><span class="line">           for m in range(self.A_num + self.B_num):</span><br><span class="line">               self.graphAB[m] &#x3D; deque([])</span><br><span class="line">           for i in range(self.A_num):</span><br><span class="line">               for j in range(self.B_num):</span><br><span class="line">                   if lines[i][j] &#x3D;&#x3D; &#39;1&#39;:</span><br><span class="line">                      # A的id：0 ~ self.A_num - 1</span><br><span class="line">                      # B的id：self.A_num ~ self.A_num + self.B_num - 1</span><br><span class="line">                      self.graphAB[i].append(j + self.A_num)</span><br><span class="line">                      self.graphAB[j + self.A_num].append(i)</span><br><span class="line"></span><br><span class="line"> # 深度优先遍历搜索小于等于8的路径</span><br><span class="line">  def  DFS(self, s,A_head,path):</span><br><span class="line">       &quot;&quot;&quot;</span><br><span class="line">       根据好友关系搜索环，直到深度小于8或没有为止</span><br><span class="line">       并根据对应关系存入4，6，8的环和6，7，8的单链</span><br><span class="line">       s: 搜索起始点</span><br><span class="line">       A_head: 最开始的起点A</span><br><span class="line">       path: 搜索出的路径</span><br><span class="line">       &quot;&quot;&quot;</span><br><span class="line">       # 计算路径的长度</span><br><span class="line">       path_len &#x3D; len(path)</span><br><span class="line">       </span><br><span class="line">       # 4，6，8的路径的下一个朋友是起点，成4，6，8的环</span><br><span class="line">       if (path_len &gt; 2) and (A_head in self.graphAB[s]) :</span><br><span class="line">          path_ &#x3D; path[1:] #深拷贝当前path，丢掉开头</span><br><span class="line">          path_.sort() #升序排列</span><br><span class="line">          # 进行字符串编码</span><br><span class="line">          path_s &#x3D; &#39;&#39;.join(p for p in map(str, path_)) </span><br><span class="line">          # 利用set()和内置hash函数去重</span><br><span class="line">          if path_s not in self.circles[path_len]:</span><br><span class="line">             self.circles[path_len].add(path_s)</span><br><span class="line">             self.count[path_len] +&#x3D; 1</span><br><span class="line">       </span><br><span class="line">       # 6，7，8的路径塞进存储中，后续组合配对成环</span><br><span class="line">       if (path_len &gt; 5) and (path_len &lt; 9):</span><br><span class="line">          self.lines[path_len].append(path[1:])</span><br><span class="line"></span><br><span class="line">      # 深度优先搜索，长度只搜到8</span><br><span class="line">       if path_len &lt; 8:</span><br><span class="line">          for f_next in self.graphAB[s]:</span><br><span class="line">              # 如果一个一个搜，可以这样写，每个环只往下搜避免重复</span><br><span class="line">              # if (f_next not in path) and (f_next &gt; A_head):</span><br><span class="line">              if (f_next not in path): </span><br><span class="line">                 path.append(f_next)</span><br><span class="line">                 self.DFS(f_next, A_head, path)</span><br><span class="line">                 path.pop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   #将找到固定长度的单链两两组合成环并去重</span><br><span class="line">  def lines2circles(self):</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">      把以某个A为起点搜索出的固定长度的单链进行比较</span><br><span class="line">      找出除起终点相同任意元素都不同的单链两两配对成环</span><br><span class="line">      lines: 指定长度的单链集合列表，每个单链开头的元素相同，都是以A中某人为起点</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">      # 依次拼接长度为6，7，8的单链</span><br><span class="line">      # 初始化访问标志位，便于确立是否可以拼接两条单链</span><br><span class="line">      visited &#x3D; [0] * (self.A_num + self.B_num)</span><br><span class="line">      for k,v in zip(self.lines.keys(),self.lines.values()):</span><br><span class="line">          len_lines &#x3D; len(v) #单链的数量</span><br><span class="line">          # 以某个A为起点的固定长度单链可能没有或只有一个，</span><br><span class="line">          # 这个时候无法成环，不用继续运行</span><br><span class="line">          if (len_lines &#x3D;&#x3D; 0) or (len_lines &#x3D;&#x3D; 1):  </span><br><span class="line">             return </span><br><span class="line">          # 开始找环 </span><br><span class="line">          len_circle &#x3D; 2 * k - 2</span><br><span class="line">          # 将lines按照最后一个元素进行升序，降低循环次数</span><br><span class="line">          v &#x3D; sorted(v,key&#x3D;lambda k:k[-1])</span><br><span class="line"></span><br><span class="line">          for t in range(len_lines - 1):</span><br><span class="line">              line1 &#x3D; v[t]</span><br><span class="line">              for s in line1:</span><br><span class="line">                  visited[s] &#x3D; 1</span><br><span class="line">              for k in range(t+1, len_lines):</span><br><span class="line">                  #确保只在起终点相同的line中组合，第二个没有后面一定没有</span><br><span class="line">                  if line1[-1] !&#x3D; v[k][-1]:</span><br><span class="line">                     break </span><br><span class="line">                  #找出两个中相同的元素 </span><br><span class="line">                  line2 &#x3D; v[k][:-1]</span><br><span class="line">                  SAME &#x3D; 0 # 合并双链判断标志</span><br><span class="line">                  # 除了开头和末尾还有其他元素相同，无法配对成环</span><br><span class="line">                  for s in line2:</span><br><span class="line">                      if visited[s] &#x3D;&#x3D; 1:</span><br><span class="line">                         SAME &#x3D; 1</span><br><span class="line">                         break</span><br><span class="line">                  if SAME &#x3D;&#x3D; 0:</span><br><span class="line">                     #去掉相同的头和尾元素，成环并升序</span><br><span class="line">                     circle &#x3D; line1 + line2</span><br><span class="line">                     circle.sort()</span><br><span class="line">                      # 字符串编码环存入set()</span><br><span class="line">                     circle_ &#x3D; &#39;&#39;.join(c for c in map(str, circle))</span><br><span class="line">                     if circle_ not in self.circles[len_circle]:</span><br><span class="line">                        self.circles[len_circle].add(circle_)</span><br><span class="line">                        self.count[len_circle] +&#x3D; 1</span><br><span class="line">              for s in line1:</span><br><span class="line">                  visited[s] &#x3D; 0</span><br><span class="line"></span><br><span class="line">  # 以每个A为起点搜索所有指定长度的不重复无向环                            </span><br><span class="line">  def search_circles(self):</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">      以每个A为起点找环；</span><br><span class="line">      由于数据存在规律，可以简化搜索点；</span><br><span class="line">      否则，如果是随机生成朋友关系，</span><br><span class="line">      需要一个一个点去搜</span><br><span class="line">      </span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">      # 第一阶段数据各段落开头节点</span><br><span class="line">      if self.A_num &#x3D;&#x3D; 256:</span><br><span class="line">         nodes &#x3D; [0, 64, 128, 192]</span><br><span class="line">         self.sections_len &#x3D; 64</span><br><span class="line">      # 第二阶段数据各段落开头节点   </span><br><span class="line">      elif self.A_num &#x3D;&#x3D; 1344:</span><br><span class="line">         nodes &#x3D; [0, 192, 384, 576, 768, 960, 1152]</span><br><span class="line">         self.sections_len &#x3D; 192</span><br><span class="line">      # 如果数据是随机生成的</span><br><span class="line">      else:</span><br><span class="line">         nodes &#x3D; range(self.A_num)</span><br><span class="line">      for i in nodes:</span><br><span class="line">          path &#x3D; [i]</span><br><span class="line">          #找4，6，8长度环和6，7，8长度单链</span><br><span class="line">          self.DFS(i,i,path)</span><br><span class="line">          # 拼接10，12，14长度环</span><br><span class="line">          self.lines2circles()</span><br><span class="line">          </span><br><span class="line">          # 清空，下一次找环做准备</span><br><span class="line">          self.lines &#x3D; &#123;6:deque([]), 7:deque([]), 8:deque([])&#125;</span><br><span class="line">          self.circles &#x3D; &#123;4:set(), 6:set(), 8:set(), 10:set(), 12:set(), 14:set()&#125;</span><br><span class="line"></span><br><span class="line">          # 简单打印程序进度</span><br><span class="line">          if self.sections_len !&#x3D; 0:</span><br><span class="line">             print(&#39;progress:：%d &#x2F; %d&#39;%(i+self.sections_len, self.A_num), end&#x3D;&#39;\r&#39;)</span><br><span class="line">          else:</span><br><span class="line">             print(&#39;progress:：%d &#x2F; %d&#39;%(i+1, self.A_num), end&#x3D;&#39;\r&#39;)</span><br><span class="line">          </span><br><span class="line">  # 写入答案到txt文件        </span><br><span class="line">  def output_ans(self):</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">      把最终的答案写到result.txt文件中</span><br><span class="line">      并打印至终端</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">      with open(self.output_file, &#39;w&#39;) as f_out:    </span><br><span class="line">          for k,v in zip(self.count.keys(), self.count.values()):</span><br><span class="line">              # 每个A节点会被重复遍历，因此需要除以环中包含的A节点数</span><br><span class="line">              # 如果是一个一个搜，改一下DFS最后的if代码，此处不再需要</span><br><span class="line">              # 这里是为了统一数据规律的情况做了折中处理</span><br><span class="line">              v &#x3D; v * self.sections_len &#x2F;&#x2F; (k &#x2F;&#x2F; 2)</span><br><span class="line">              f_out.write(&#39;木托盘上有%d个名字的祭品最多有%d个;\n&#39;%(k,v))</span><br><span class="line">              print(&#39;\n&#39;, v)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">   </span><br><span class="line">   t1 &#x3D; time.time()</span><br><span class="line">   find &#x3D; Solutions(&#39;Example.csv&#39;, &#39;result.txt&#39;)</span><br><span class="line">   find.load()</span><br><span class="line">   find.search_circles()</span><br><span class="line">   find.output_ans()</span><br><span class="line">   t2 &#x3D; time.time()</span><br><span class="line">   print(&#39;Runing Time: %ds&#39;%(t2-t1))</span><br><span class="line">             </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>个人线下IDE 采用VS Code，笔记本电脑运行环境为64位 Win10下的Anaconda3 Python 3.7虚拟环境，CPU为2.3GHz的i5-8300H，总内存为16G。针对第一阶段数据，程序用时大约3s左右，针对第二阶段数据，程序用时大约50s左右。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/loopdetectioninBipartitegraph/1.JPG" alt="一阶段数据结果"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/loopdetectioninBipartitegraph/2.JPG" alt="二阶段数据结果"></p><p>实际上，此份代码是还可以进一步优化的，尤其是去重部分，还可以根据数据做些文章，我自己不是很感兴趣，没再继续下去了。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;这个问题来源于中兴举办的2020优招算法比赛“傅里叶派”，要求是对提供的数据进行二分图建模，然后找出4，6，8，10，12，14的无向图的总数量，同时有时间和禁止多线程操作要求。起先，我是把数据当成随机生成的普通数据，然后利用DFS（B站上有个up主讲的DFS和BFS系列非常浅显易懂，&lt;a href=&quot;https://www.bilibili.com/video/BV1Ks411579J?from=search&amp;amp;seid=7572393692341634029&quot;&gt;链接&lt;/a&gt;）和拼接单链组合成环，最后利用python的set哈希去重的方法来实现的，在一阶段数据上（（256+640，256+640）大小的邻接矩阵）大概可以进1分钟至1分半钟。后面，随着第二阶段数据的增大（（1344+1344，1344+1344）大小的邻接矩阵），运行时间急剧增加，大概到了30分钟。实际上，算法的主要瓶颈在去重时间上（去重数据达到亿级别的字符串），为了降低去重时间，我花了不少时间去Google，包括数据库的一些去重，或者使用bitmap位图等，考虑到硬件限制，试了一下&lt;a href=&quot;https://github.com/joseph-fox/python-bloomfilter&quot;&gt;bloom filter&lt;/a&gt;，但是由于hash碰撞等原因，可能出现一些误判，而且随着查重数量的增大，占用的内存也会增大。最后的测试时间上与set相比没有什么优势。&lt;/p&gt;
&lt;p&gt;最后跟朋友交流，发现数据的一些规律得以解决。原来主办方给的数据是精心设计过的，整个数据中存在固定长度的固定数量sections，每个section中若$A_{i}B_{j}=1$，那么$A_{i+1}B_{j+1}=1$，存在这么一种递推关系，这样就会大大减少了搜索量和查重量。根据固定的节点修改代码，最后运行时间从半个小时降到了50s左右。&lt;/p&gt;
&lt;p&gt;最后的数据规律是快要接近deadline知道的，虽然分析数据也应当是一个基本的素养，但是面对这种题目，我个人感觉还是追求通用性解决方案更有意义一些。比如真正面对庞大的数据量的时候，关系图是不确定的，没有这种很强的先验知识，那么应该设计系统去尽可能快速地查找和统计环的数量。我自己没有进一步深究了，目前的几个naive想法是可以考虑多线程找环查重在一起合并查重，查重获取可以采用一些数据库的技术，比如针对业务建一个服务器专门用来进行通用性查重，代码中直接调API即可。&lt;/p&gt;
&lt;p&gt;具体题目要求，两阶段数据和代码见&lt;a href=&quot;https://github.com/Richardyu114/ZTEchallenge-2020-Fourier&quot;&gt;github仓库&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="算法与实现" scheme="http://densecollections.top/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    
    
    <category term="python" scheme="http://densecollections.top/tags/python/"/>
    
    <category term="algorithm" scheme="http://densecollections.top/tags/algorithm/"/>
    
    <category term="contest" scheme="http://densecollections.top/tags/contest/"/>
    
  </entry>
  
  <entry>
    <title>[MIT]计算机科学课堂中学不到的知识</title>
    <link href="http://densecollections.top/posts/4074/"/>
    <id>http://densecollections.top/posts/4074/</id>
    <published>2020-03-02T09:56:00.000Z</published>
    <updated>2021-01-02T11:35:11.743Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><ul><li><a href="https://missing.csail.mit.edu/">homepage</a>;</li><li><a href="https://www.bilibili.com/video/av86911412?p=1">lecture video;</a></li><li>2020年初始刚放出的公开课，十分实用，千呼万唤始出来。主要是介绍一些CS领域常用的工具和技巧，这些对于平时科研，工作都有很大的帮助。正如在课程介绍和评论中说的，<strong>“It’s not computer science. It is computer literacy.”</strong></li><li>相关资源：阮一峰老师的<a href="https://wangdoc.com/bash/index.html">bash脚本教程</a></li></ul><a id="more"></a><h2 id="Lecture-1-Course-overview-the-shell"><a href="#Lecture-1-Course-overview-the-shell" class="headerlink" title="Lecture 1. Course overview + the shell"></a>Lecture 1. Course overview + the shell</h2><p><a href="https://missing.csail.mit.edu/2020/course-shell/">官方讲义</a>比较系统和简练，其中包含一些练习题。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">date #显示时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># echo 后面跟上要输出的文本</span><br><span class="line">echo Hello</span><br><span class="line">echo &quot;Hello World&quot;</span><br><span class="line">echo Hello\ World</span><br><span class="line">echo $PATH</span><br><span class="line">which echo</span><br><span class="line">#讲解绝对路径和相对路径</span><br><span class="line">pwd # print working diretory</span><br><span class="line">cd # change working path</span><br><span class="line">cd .. #进入当前目录上一目录</span><br><span class="line"># 讲解输入代码要注意相对路径和绝对路径</span><br><span class="line"></span><br><span class="line">ls #当前路径下的文件</span><br><span class="line">ls .. # 当前路径前一层的文件夹下的文件</span><br><span class="line">cd ~ # 返回&#x2F;home&#x2F;username&#x2F;</span><br><span class="line">cd - #往返当前和上一个文件夹路径</span><br><span class="line"># 讲解 - 加在某个命令后增加argument</span><br><span class="line"></span><br><span class="line">ls -l # 显示更加详细的文件信息，比如read, write, excute（对该文件或文件夹执行命令）</span><br><span class="line"></span><br><span class="line"># mv 文件名 文件名将源文件名改为目标文件名</span><br><span class="line"># mv 文件名 目录名将文件移动到目标目录</span><br><span class="line"># mv 目录名 目录名目标目录已存在，将源目录移动到目标目录；目标目录不存在则改名</span><br><span class="line"># mv 目录名 文件名出错</span><br><span class="line">mv filename1 filename2</span><br><span class="line">cp source dest #复制文或目录</span><br><span class="line">rm filename #删除文件或目录，不可恢复的操作</span><br><span class="line">rmdir # 删除空目录，但无法删除非空目录</span><br><span class="line">mkdir # 新建目录</span><br><span class="line">man ls #manual, 功能类似-help，但是更便于阅读，按q退出</span><br><span class="line"># CTRL + l清除面板记录</span><br></pre></td></tr></table></figure><p>在终端有个输入流input stream和输出流output stream，一般输出流就是执行输入的命令，打印在终端上，但是而可以通过&lt; 和 &gt;符号对input stream和output stream进行指定</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo hello &gt; hello.txt # 把文本hello存储在hello.txt中</span><br><span class="line">cat hello.txt or cat &lt; hello.txt # 显示file 内容</span><br><span class="line">cat &lt; hello.txt &gt; hello2.txt # 复制内容到hello2.txt中，而且是从头覆写</span><br><span class="line">cat &lt; hello.txt &gt;&gt; hello2.txt # append, 接着后面写</span><br><span class="line"></span><br><span class="line">ls -l | tail -n1 &gt;ls.txt # 显示最后一行并写入ls.txt</span><br><span class="line"># | 管道命令，是指 | 的左边运行结果 是|右边的 输入条件或者范围</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo su # 加入root权限，后续命令都以super user执行，exit命令推出</span><br><span class="line">cd sys # 进入系统设备，可以查看一些参数</span><br><span class="line"># 进入之后如果想改变某些系统参数，但是又没有使用sudo su，可以使用tee命令</span><br><span class="line">#  tee命令用于读取标准输入的数据，并将其内容输出成文件</span><br><span class="line"># 下面的命令不仅改变了brightness亮度值，还打印出结果在终端</span><br><span class="line">echo 1060 | sudo tee brightness</span><br><span class="line"></span><br><span class="line">xdg-open filename #以默认程序打开文件</span><br></pre></td></tr></table></figure><h2 id="Shell-Tools-and-Scripting"><a href="#Shell-Tools-and-Scripting" class="headerlink" title="Shell Tools and Scripting"></a>Shell Tools and Scripting</h2><p><a href="https://missing.csail.mit.edu/2020/shell-tools/">官方笔记</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">foo&#x3D;bar</span><br><span class="line">echo foo</span><br><span class="line"># 失效</span><br><span class="line">foo &#x3D; bar</span><br><span class="line"># echo单引号和双引号的区别</span><br><span class="line">echo &quot;value is $ foo&quot; # value is bar</span><br><span class="line">echo &#39;value is $foo&#39; # value is $foo</span><br><span class="line">echo &quot;we are in $(pwd)&quot;</span><br></pre></td></tr></table></figure><p>.sh中的$</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># creates a directory and cds into it.</span><br><span class="line">mcd () &#123;</span><br><span class="line">    mkdir -p &quot;$1&quot;</span><br><span class="line">    cd &quot;$1&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>$0</code> - Name of the script</li><li><code>$1</code> to <code>$9</code> - Arguments to the script. <code>$1</code> is the first argument and so on.</li><li><code>$@</code> - All the arguments</li><li><code>$#</code> - Number of arguments</li><li><code>$?</code> - Return code of the previous command(上一次的错误代码，echo $?，返回值为0代表正确)</li><li><code>2$</code> -Process Identification number for the current script (两个美元一起，把’2’换成’$’，我这里显示有问题)</li><li><code>!!</code> - Entire last command, including arguments. A common pattern is to execute a command only for it to fail due to missing permissions, then you can quickly execute it with sudo by doing <code>sudo !!</code></li><li><code>$_</code> - Last argument from the last command. If you are in an interactive shell, you can also quickly get this value by typing <code>Esc</code> followed by <code>.</code></li></ul><p><code>grep</code>命令用于查找文件里符合条件的字符串，<code>grep foobar mcd.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">true</span><br><span class="line">echo $? # 0</span><br><span class="line">false</span><br><span class="line">echo $? #1</span><br><span class="line"></span><br><span class="line">false || echo &quot;Oops, fail&quot;</span><br><span class="line"># Oops, fail</span><br><span class="line"></span><br><span class="line">true || echo &quot;Will not be printed&quot;</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">true &amp;&amp; echo &quot;Things went well&quot;</span><br><span class="line"># Things went well</span><br><span class="line"></span><br><span class="line">false &amp;&amp; echo &quot;Will not be printed&quot;</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">false ; echo &quot;This will always run&quot;</span><br><span class="line"># This will always run，分号代表两个不关联的语句</span><br></pre></td></tr></table></figure><p>example.sh，查找指定的arguments代表的文件中是否有“foobar”这个字符串，没有就加上</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">echo &quot;Starting program at $(date)&quot; # Date will be substituted</span><br><span class="line"></span><br><span class="line">echo &quot;Running program $0 with $# arguments with pid $$&quot;</span><br><span class="line"></span><br><span class="line">for file in $@; do</span><br><span class="line">    grep foobar $file &gt; &#x2F;dev&#x2F;null 2&gt; &#x2F;dev&#x2F;null</span><br><span class="line">    # When pattern is not found, grep has exit status 1</span><br><span class="line">    # We redirect STDOUT and STDERR to a null register</span><br><span class="line">    # since we do not care about them</span><br><span class="line">    if [[ $? -ne 0 ]]; then</span><br><span class="line">        echo &quot;File $file does not have any foobar, adding one&quot;</span><br><span class="line">        echo &quot;# foobar&quot; &gt;&gt; &quot;$file&quot;</span><br><span class="line">    fi</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ls *.py #打印所有含有.py的文件名</span><br><span class="line">ls project? #打印所有&quot;project&quot;后加一个字符的文件名</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">convert image.png image.jpg</span><br><span class="line">conert image.&#123;png,jpg&#125; #大括号有笛卡儿积的作用</span><br><span class="line">touch foo&#x2F;x bar&#x2F;y #touch更改文件时间属性，没有则创建</span><br><span class="line">diff &lt;(ls foo) &lt;(ls bar) #比较两个文件夹的不同</span><br></pre></td></tr></table></figure><p><code>#!/usr/bin/env python</code>加在第一句可以将python按照./file.py执行，提供了解释器的环境路径</p><p>利用<code>shellcheck</code>检查bash脚本错误</p><p>几个shell tools：</p><ul><li><p>利用<a href="https://tldr.sh/">tldr</a>简化命令的使用教程，可代替<code>man</code>命令</p></li><li><p>find命令</p></li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Find all directories named src</span><br><span class="line">find . -name src -type d</span><br><span class="line"># Find all python files that have a folder named test in their path</span><br><span class="line">find . -path &#39;**&#x2F;test&#x2F;**&#x2F;*.py&#39; -type f</span><br><span class="line"># Find all files modified in the last day</span><br><span class="line">find . -mtime -1</span><br><span class="line"># Find all zip files with size in range 500k to 10M</span><br><span class="line">find . -size +500k -size -10M -name &#39;*.tar.gz&#39;</span><br><span class="line"></span><br><span class="line"># 还可以找到特定的文件然后执行命令</span><br><span class="line"># Delete all files with .tmp extension</span><br><span class="line">find . -name &#39;*.tmp&#39; -exec rm &#123;&#125; \;</span><br><span class="line"># Find all PNG files and convert them to JPG</span><br><span class="line">find . -name &#39;*.png&#39; -exec convert &#123;&#125; &#123;.&#125;.jpg \;</span><br></pre></td></tr></table></figure><p>另有根据数据库查找的<code>locate</code>命令，更新数据库<code>updatedb</code></p><ul><li>利用<a href="https://github.com/BurntSushi/ripgrep">rg</a>等代替grep查找代码中的特定语句</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Find all python files where I used the requests library</span><br><span class="line">rg -t py &#39;import requests&#39;</span><br><span class="line"># Find all files (including hidden files) without a shebang line</span><br><span class="line">rg -u --files-without-match &quot;^#!&quot;</span><br><span class="line"># Find all matches of foo and print the following 5 lines</span><br><span class="line">rg foo -A 5</span><br><span class="line"># Print statistics of matches (# of matched lines and files )</span><br><span class="line">rg --stats PATTERN</span><br></pre></td></tr></table></figure><ul><li><p><code>history，history  | grep find</code>, <code>CTRL+R</code>交互搜索命令，推荐<code>fzf</code></p></li><li><p>文件导航，推荐<a href="https://github.com/Canop/broot">broot</a></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://missing.csail.mit.edu/&quot;&gt;homepage&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/av86911412?p=1&quot;&gt;lecture video;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2020年初始刚放出的公开课，十分实用，千呼万唤始出来。主要是介绍一些CS领域常用的工具和技巧，这些对于平时科研，工作都有很大的帮助。正如在课程介绍和评论中说的，&lt;strong&gt;“It’s not computer science. It is computer literacy.”&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;相关资源：阮一峰老师的&lt;a href=&quot;https://wangdoc.com/bash/index.html&quot;&gt;bash脚本教程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="MissingcourseofCS" scheme="http://densecollections.top/categories/MissingcourseofCS/"/>
    
    
    <category term="computer science" scheme="http://densecollections.top/tags/computer-science/"/>
    
    <category term="public course" scheme="http://densecollections.top/tags/public-course/"/>
    
    <category term="MIT" scheme="http://densecollections.top/tags/MIT/"/>
    
    <category term="practical skills" scheme="http://densecollections.top/tags/practical-skills/"/>
    
  </entry>
  
  <entry>
    <title>Stanford CS231n 笔记</title>
    <link href="http://densecollections.top/posts/notesofCS231N/"/>
    <id>http://densecollections.top/posts/notesofCS231N/</id>
    <published>2020-02-29T04:44:28.000Z</published>
    <updated>2021-01-02T12:35:20.376Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><ul><li><a href="http://cs231n.stanford.edu/">homepage</a>;</li><li><a href="https://www.bilibili.com/video/av13260183?from=search&amp;seid=2308745029556209710">2017 lecture video</a>;</li><li><a href="https://zhuanlan.zhihu.com/p/21930884">中文笔记</a></li><li>课程相关时间安排，课件，参考资料，作业详见homepage链接，以最新版为主；</li><li>官方的assignments和notes做得非常好，强烈推荐学习和反复观看；</li><li>时间原因，学习19版的时候恰好2020版春季课程将要开始，这一版换了team，里面加了不少新的research介绍，尤其是非监督，自监督之类的，因此会加入一下2020版的内容；</li><li>个人比较喜欢Justin Johnson的讲课风格和深度，因此配合他在UMICH (University of Michigan) 的 <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/">EECS498: Deep Learning for Computer Vision</a>课一起看，可以相互补充（和CS231N有很大重叠， 但也些不同）；</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/summary.JPG" alt="提前把最后一节课的总结放出来。整个课程将按照这个fundamentals-CNN-applications来讲"></p><h2 id="Lecture1-Introduction"><a href="#Lecture1-Introduction" class="headerlink" title="Lecture1. Introduction"></a>Lecture1. Introduction</h2><p>Related courses in Stanford:</p><ul><li><p>CS131: Computer Vision: Foundations and Applications</p></li><li><p>CS231a: Computer Vision, from 3D Reconstruction to Recognition</p></li><li><p>CS 224n: Natural Language Processing with Deep Learning</p></li><li><p>CS 230: Deep Learning</p></li><li><p><strong>CS231n: Convolutional Neural Networks for Visual Recognition</strong></p></li><li><p>Andrew Ng的CS229虽然不在列表中，但个人觉得也值得一看，毕竟也属于经典ML课程。</p></li></ul><p>A  Brief history of Computer Vision and CS 231n:</p><ul><li>从史前生物产生视觉开始，到后面的人类对于视觉的研究：相机，生物学研究，以及陆续地对于计算机视觉/机器视觉的系统研究，在社会各个领域结合产生的特定任务的建模设计等工作，这里再一次提到了David Marr的《vision》一书对整个计算机视觉领域的奠基于推动作用。</li><li>介绍卷积神经网络CNN对计算视觉的推动作用，简介了CNN的历史以及在各个视觉有关问题上的强大作用。</li><li>推荐课本教材：《Deep Learning》 by Goodfellow, Bengio, and Courville.</li></ul><a id="more"></a><h2 id="Lecture2-Image-Classification"><a href="#Lecture2-Image-Classification" class="headerlink" title="Lecture2. Image Classification"></a>Lecture2. Image Classification</h2><p>material: 主讲人<a href="http://cs.stanford.edu/people/jcjohns/">Justin Johnson</a>编写的<a href="http://cs231n.github.io/python-numpy-tutorial/">python numpy tutorial</a>，个人觉得是个很不错的教程。</p><p>官方的<a href="http://cs231n.github.io/classification/">notes1—image classification</a>和<a href="http://cs231n.github.io/linear-classify/">notes2-linear classification</a>是对本次课的一个总结，补充和拓展（其中第二个笔记大部分是下次课的内容，放在这里我猜可能是预习吧），可以说写得十分详细了，而且常温常新。</p><ul><li>首先阐述对于计算机而言，为什么分类图片很难（光照，视点，类似纹理背景等）；</li><li>介绍了一些传统方法对此的努力，但是这些方法效果不好，具有单一性，鲁棒性和generalization都很差；</li><li>引入data-drive approcah, 也就是收集数据，训练，预测三段式方法；</li><li>介绍Nearest Neighbours分类器，并阐释不同K值和distance metric选取的不同performance（L1和L2的选取与坐标轴旋转对数据引起的改变）；</li><li>介绍交叉验证的作用，以及说明K-NN的维度诅咒（处理大数据吃力）和 predict速度慢的缺陷（需要一个个和训练集图片计算distance）；</li><li>引入线性分类器，权重矩阵<strong>W</strong>，偏置矩阵<strong>b</strong>，并在第二个官网的notes中详细地介绍了SVM和Softmax两种loss衡量方式；</li></ul><p><a href="https://www.jianshu.com/p/43318a3dc715">关于交叉熵，相对熵以及为什么选择交叉熵作为损失函数</a>。</p><p>官网推荐的一篇阅读review <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">A Few Useful Things to Konw About Machine Learnig</a>，写于2012年，但是仍不过时，可以一读。文中写到的12个”folk wisdom”:</p><ul><li>Learning = representation+evaluation+optimization;</li><li>It’s generalization that counts;</li><li>Data alone is not alone;</li><li>Overfitting has many faces;</li><li>Intuition falis in high dimensions;</li><li>Theoretical guarantees are not what they seem;</li><li>Feature engineering is the key;</li><li>More data beats a cleverer algorithm;</li><li>Learn many models, not just one;</li><li>Simplicity does not imply accuracy;</li><li>Representable does not imply learnable;</li><li>Gneralization does not imply causation;</li></ul><h2 id="Lecture3-Loss-Functions-and-Optimization"><a href="#Lecture3-Loss-Functions-and-Optimization" class="headerlink" title="Lecture3. Loss Functions and Optimization"></a>Lecture3. Loss Functions and Optimization</h2><p>官方笔记：<a href="http://cs231n.github.io/linear-classify/">linear classification</a>, <a href="http://cs231n.github.io/optimization-1/">optimization</a>有更为详细的课程内容解释和总结;</p><p>其中linear classification笔记中有个交互式<a href="http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/">demo</a>，主要用来体会SVM和Softmax来训练线性分类器的过程，包括调节正则项，学习率等超参数；</p><ul><li>上节课引入linear classification，现在需要loss function定量分类器对分类结果与training data的GT之间的差异，需要一些optimization方式去有效寻找模型参数可以最小化loss；</li><li>介绍两种loss衡量metric: Multiclass SVM和Softmax，并认为两种方式在训练分类器时差异不会很大，SVM是为了拉大正确的score与非正确score之间的差值到设定的margin，所以分的越差惩罚越重，分的越好基本上不再去多费精力，而Softmax则不断将正确的probability拉的更高，错误的拉的更低；</li><li><p>由于满足loss function的权重矩阵<strong>W</strong>有无数个（针对linear classification），之间可成正比例关系，因此需要引入正则项让模型自己适应数据，得到generalization比较高的模型参数；</p></li><li><p>针对优化参数方法，通过随机搜索过渡到梯度下降gradient descent，在官方笔记中更加仔细地介绍了针对实际工程应用的梯度下降法（数值梯度，分析梯度等），满足计算性能之间的trade-off</p></li><li><p>最后针对图像特征，介绍了color histogram, HoG (Histogram of Oriented Gradients), Bag of Words等方法。将feature映射到更高维的空间，使分类问题变得容易，这也是CNN的一大作用，从而引入下一节主题；</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/3_1.JPG" alt="特征映射"></p><p>在这里插一下sigmoid, softmax和cross entropy之间的联系，我们一般说的分类是单标签多分类，也就是说一张图片只会有一个标签，标签与标签之间是互斥的，而softmax计算的类别概率是类别互斥的，因为和为1，而针对多标签分类，也就是说一张图片可能有多个标签存在，标签之间不是非此即彼的，类别不互斥，此时用sigmoid激活函数，因为sigmoid是互相独立的计算，只判断此神经元是否属于该位置对应的类别，所以在计算多分类交叉熵时是每个神经元按照二分类交叉熵算然后求和平均。参考<a href="https://www.zhihu.com/question/341500352/answer/795497527">blog1</a>, <a href="https://blog.csdn.net/tsyccnh/article/details/79163834">blog2</a>.</p><p>梯度下降算法理解：</p><p>假设一个模型拟合表达式为：$h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$，其中$\theta_{i}, x_{i}$一般都是矩阵，对于一个输入，我们希望计算的值与真实函数$y$尽可能得接近，直接照着真实函数分布去“依葫芦画瓢”比较困难，所以来最小化他们之间得残差来搜索拟合函数的参数，假设我们用平方损失函数：$J(\theta)=\frac{1}{2} \sum \limits _{i=1}^{m}(h_{\theta}(x)-y)^{2}$，现在我们对每一个超参数$\theta_{j}$求偏导，得到关于它得梯度（函数上升最快得方向），由于我们是想找最小值，所以要反方向走（实际可以在损失函数前加个符号，转为找最大值）：</p><script type="math/tex; mode=display">\begin{array}{c}\frac{\partial}{\partial \theta_{j}} J(\theta)=\frac{\partial}{\partial \theta_{j}} \frac{1}{2}(h _{\theta}(x)-y)^{2}=(h _{\theta}(x)-y) \cdot \frac{\partial}{\partial \theta_{j}}(h _{\theta}(x)-y) \\=(h _{\theta}(x)-y) \cdot \frac{\partial}{\partial \theta_{j}}\left(\sum_{i=0}^{n} \theta_{i} x_{i}-y\right)=(h _{\theta}(x)-y) x_{j}\end{array}</script><p>根据梯度，加上步长learning rate, 对参数$\theta_{j}$进行更新：$\theta_{j}:=\theta_{j}-\alpha(h _{\theta}(x)-y) x_{j}$</p><p>通过不断地迭代，使函数往最小值逼近。在深度学习实际应用时，训练的数据样本可能很大，一次迭代要计算所有的样本会非常耗时，所以利用SGD方法，抽取一些数据作为batch，认为可以近似代表整体样本的分布，每次换不同的样本去学习，也可以得到近似的解（但这也只是工程近似）。深度学习中的模型拟合函数是非常庞大而又复杂的，结合BP加梯度下降可以使函数收敛到局部最小值。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/3_2.JPG" alt="模型优化过程"></p><h2 id="Lecture4-Introduction-to-Neural-Networks"><a href="#Lecture4-Introduction-to-Neural-Networks" class="headerlink" title="Lecture4. Introduction to Neural Networks"></a>Lecture4. Introduction to Neural Networks</h2><p>这一节官方指定阅读的笔记为：<a href="http://cs231n.github.io/optimization-2/">optimization-2-bp</a>, <a href="http://cs231n.stanford.edu/handouts/linear-backprop.pdf">linear-classifier-bp</a>; 此外还给了一些推荐的阅读材料，以供选择，其中包括一份<a href="http://cs231n.stanford.edu/handouts/derivatives.pdf">向量，矩阵，BP的导数推导示意笔记</a>，Yan leCun在1998年写的名为“Efficient BackProp”的<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">论文</a>，两篇解释BP的博客（<a href="http://colah.github.io/posts/2015-08-Backprop/">1</a>, <a href="http://neuralnetworksanddeeplearning.com/chap2.html">2</a>）和MIT <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/">Artificial Intelligence公开课</a>中有关BP讲解的课程视频，此外还有一篇ML中automatic differentiation的<a href="https://arxiv.org/abs/1502.05767">survey</a>. 个人感觉如果对微积分和线性代数比较熟悉的话，看一下官方指定阅读的两篇笔记就差不多了，主要是为了深入理解BP，有时间的话也强烈推荐那篇推荐阅读的博客2，是<a href="http://michaelnielsen.org/">Micahel Nielsen</a>写的《nndl》教材中的一节，在入门DL的时候我就是看的这本书，十分经典耐读。BP的使用大多是对矩阵或者说tensor张量进行的，如果对矩阵求导不熟悉，或者想系统的学习巩固下，可以移步<a href="https://zhuanlan.zhihu.com/p/24709748">矩阵求导术-上</a>和<a href="https://zhuanlan.zhihu.com/p/24863977">矩阵求导术-下</a>，上-篇写的是标量对矩阵的求导方法，下-篇写的是矩阵对矩阵的求导方法，都是通过标量上全微分表达式来进行推广和计算的：</p><p>“<strong>导数与微分的联系是计算的枢纽</strong>，标量对矩阵的导数与微分的联系是$d f=\operatorname{tr}\left(\nabla_{X}^{T} f d X\right)$，先对$f$求微分，再使用迹技巧可求得导数，特别地，标量对向量的导数与微分的联系是$d f=\nabla_{x}^{T} f d \boldsymbol{x}$；矩阵对矩阵的导数与微分的联系是$\operatorname{vec}(d F)=\frac{\partial F^{T}}{\partial X} \operatorname{vec}(d X)$，先对$F$求微分，再使用向量化的技巧可求得导数，特别地，向量对向量的导数与微分的联系是$d \boldsymbol{f}=\frac{\partial \boldsymbol{f}^{T}}{\partial \boldsymbol{x}} d \boldsymbol{x}$。”（其中，$x,\boldsymbol {x}, X$分表代表标量，向量和矩阵）</p><p>我想对矩阵的求导可能是本节课的一个小重点以及后面进行DL理论学习的基石。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/4_1.JPG" alt="2020版的第四讲对BP做了更为细致的讲解"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/4_2.JPG" alt="实际计算不会存储jacobian矩阵，而且隐式计算"></p><ul><li><p>该课延续上一讲提出的linear classifier,介绍了神经元的概念，历史和应用，引入fully-connected network；</p></li><li><p>指出如果层数加多，再加上激活函数，损失函数等，网络所代表的拟合函数表达式将非常庞大而复杂，采用梯度下降的方式训练时，推出分析表达式非常不现实和实际（如果想要零时改变一个函数就要重新推算）；</p></li><li><p>由于我们不关心函数梯度具体表达式，我们只是想通过计算梯度值来更新参数，所以采用computation graph的方式来逆推local gradient，forward + backpropagation形成一个完美体系（upstream gradient, local gradient）；</p></li></ul><p>在此课之前我推导过全连接和卷积神经网络的BP，就是当成链式法则”chain rules“来对待，而这次课给我了一个数学公式之外一个更加形象化理解，也即是”computation graph“和”门电路“：</p><blockquote><p>Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.</p></blockquote><p>而网络通过一系列”add gate”, “mul gate”, “max gate”等”gate”一步步垒成复杂的函数，而且部分组织可以组合成一个特殊的”gate”，也可以被分解成其他简易的”gate”，方便我们进行梯度计算（注意variable分流forward之后bp要加起来+=）。通过简单的example可以了解到”gate” input和output之间的梯度分配关系，非常形象生动。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/4_3.JPG" alt></p><h2 id="Lecture-5-Convolutional-Neural-Networks"><a href="#Lecture-5-Convolutional-Neural-Networks" class="headerlink" title="Lecture 5. Convolutional Neural Networks"></a>Lecture 5. Convolutional Neural Networks</h2><p>该节课的<a href="http://cs231n.github.io/convolutional-networks/">官方笔记</a>对卷积神经网络CNN来龙去脉和计算都介绍得很详细，同时也对每个层（Conv layer, pooling layer, fc layer）都做了全面的回顾和介绍。</p><p>另有一篇<a href="https://www.matongxue.com/madocs/32.html">博客</a>解释卷积神经网络中”卷积“一词和信号与系统中得卷积操作的联系，知乎也有个理解卷积的<a href="https://www.zhihu.com/question/22298352">回答</a>，看完之后可以加深对卷积核的理解。</p><ul><li>首先介绍感知机，神经元和CNN的历史；</li><li>简介一个卷积神经网络的结构以及直觉上层级学习的内容和特征；</li><li>大部分时间讲解卷积的计算过程；</li></ul><p><a href="https://github.com/vdumoulin/conv_arithmetic">Convolution arithmetic</a></p><h2 id="Assignment-1"><a href="#Assignment-1" class="headerlink" title="Assignment 1"></a>Assignment 1</h2><p><a href="http://cs231n.github.io/assignments2019/assignment1/">assignment page</a>;</p><p>reference：</p><ul><li><p><a href="https://github.com/Wangxb06/CS231n">github-code</a>;</p></li><li><p>参考培训机构深度之眼的cs231n-camp的文档</p></li></ul><p>具体过程和结果参见这个<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments">repository</a></p><p>注意：一开始运行<code>knn.ipynb</code>文件时可能显示<code>scipy.misc.imread</code>缺失，这是因为新版的<code>scipy</code>包去掉了<code>imread</code>这个module，解决方法是可以在<code>data_utils.py</code>中的<code>from scipy.misc import imread</code>改为<code>from imageio import imread</code>。</p><h2 id="Lecture-6-Deep-Learning-Hardware-and-Software"><a href="#Lecture-6-Deep-Learning-Hardware-and-Software" class="headerlink" title="Lecture 6. Deep Learning Hardware and Software"></a>Lecture 6. Deep Learning Hardware and Software</h2><p>讲义中包含的官方制作的两大深度学习框架（Pytorch, TensorFlow）的tutorial，我把它们直接下载下来放在<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments">GitHub</a>便于观看。</p><p>2017版中该课在train neural network之后，19年改为之前，由于两年间框架格局也在发生改变，所以以最新版<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture06.pdf">slides</a>为主（17版主推TensorFlow，也花了一大半时间介绍TF，19版偏向于PyTorch）。</p><p>结合前面的tutorial一起看，可以当作非常好的入门学习材料。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/dynamic_static_graph.JPG" alt="动态计算图和静态计算图"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/advice_for_frameworks.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/6_1.JPG" alt="利用ONNX制作静态pytorch模型"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/6_2.JPG" alt="利用TorchScript制作pytorch静态模型"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/6_3.JPG" alt="分布式利用pytorch训练数据，以及使用第三方库Horovod进行分布式训练：https://github.com/horovod/horovod/blob/master/docs/pytorch.rst"></p><h2 id="Lecture-7—8-Training-Neural-Networks-I-amp-II"><a href="#Lecture-7—8-Training-Neural-Networks-I-amp-II" class="headerlink" title="Lecture 7—8. Training Neural Networks I &amp; II"></a>Lecture 7—8. Training Neural Networks I &amp; II</h2><p>这两节课硬货非常非常的多，很多东西都值得深究。课程讲的不是很清楚，因为内容多，所以需要线下仔细看看笔记。笔记主要分为三个部分：</p><p>1.<a href="http://cs231n.github.io/neural-networks-1/">neural fc network model</a>: 介绍来自脑启发的神经元neuron，常用的激活函数（sigmoid, tanh, ReLU, Leaky ReLU, Maxout, ELU, SELU），全连接神经网络的一些结构问题，比如是否可以拟合任意连续函数，多少层足够，多大足够，以及表征能力和前向计算等。</p><p>Sigmoid在训练时除了会有梯度消失的情况外还有输出不是0均值（zero-centered）的情况：这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。以 $f=sigmoid(Wx+b)$为例， 假设输入均为正数（或负数），那么对$W$的导数（sigmoid的导数总是正的）总是正数（或负数），这样在反向传播过程中要么都往正方向更新，要么都往负方向更新，导致有一种捆绑效果，使得收敛缓慢。</p><blockquote><p>“<em>What neuron type should I use?</em>” Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout. </p><p>larger networks will always work better than smaller networks, but their higher model capacity must be appropriately addressed with stronger regularization (such as higher weight decay), or they might overfit. We will see more forms of regularization (especially dropout) in later sections.</p></blockquote><hr><p>2020.6更新：</p><p>最近Yolo v4使用了<strong>Mish</strong>激活函数，2020年新版CS231N也加入了Swish激活函数，后续可以实验看是否高于ReLU，但是可能要注意权重初始化的问题。</p><p>Swish: $f(x) = x * sigmoid(\beta x)$</p><p>Mish: $f(x)=x*tanh(ln(1+e^{x}))$</p><p>以及更多激活函数对比见<a href="https://zhuanlan.zhihu.com/p/139696588">此</a>。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/7_5.JPG" alt="激活函数选择建议"></p><p>2.<a href="http://cs231n.github.io/neural-networks-2/">one time setup</a>: 介绍数据预处理（先减去mean image，或者是mean RGB vector中心化，然后除以std进行normalization；进行PCA降维数据，进行decorrelated或者利用whitening得到各向同性的Gaussian blob；一般利用CNN训练时不需要进行太复杂的数据预处理），<strong>权重初始化</strong>（非常重要和经验化的一点，不要以零对权重进行初始化，而是随机高斯分布采样初始化，“Xavier” initialization, 并且进行<code>w = np.random.randn(n) * sqrt(1/n)</code>对方差进行校准，为的是让输出的方差与输入的相同，后来何恺明的研究中表明，采用ReLU激活函数，初始化为<code>w = np.random.randn(n) * sqrt(2.0/n)</code>见论文”<a href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>“，在conv中n是kernel _size^2 * input _channels​），batch normalization（直接作为一层插入FC/CNN与激活函数层之间，把batch数据进行标准高斯分布化，当然后面还有一个超参的逆向高斯化，主要是为了让网络自己去判断这哪种对自己适合是一种启发式处理），正则化（L1，L2正则化，一般L2更好些；Max norm constraints设置权重边界；Dropout进行组合训练或者是认为进行数据增强）和损失函数（classification, regression (最好不要硬来，因为没有规律，试着往分类转化), structured prediction）等问题。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/7_6.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/8_1.JPG" alt="网络权重初始化问题"></p><p>权重初始化中还有一个transfer learning技巧，利用ImageNet的预训练模型，再根据自己的数据进行层的选择性finetune. 不过目前的研究来看，在object detection领域，自训练似乎比预训练更好（数据够多，卡够多的情况下，数据少还是老老实实用预训练吧），参照恺明大神的<a href="https://arxiv.org/abs/1811.08883">Rethinking ImageNet Pre-training</a>。</p><p>Dropout: Forces the network to have a redundant representation; Prevents co-adaptation of features</p><p><a href="https://blog.csdn.net/stdcoutzyx/article/details/49022443">对Dropout起防过拟合作用的解释—组合派和噪声派</a>；<a href="https://blog.csdn.net/fu6543210/article/details/84450890">对Inverted Dropout中神经元进行Dropout之后为何需要对余下工作的神经元进行rescale的一个解释—乘再除确保下一层输出期望不变</a></p><p>（除以p是为了保证训练和测试时输入给下一层的期望是一样的，那些失活的神经元损失的值会通过其他神经元补回来，这样就等于加强了某些神经元的鲁棒性，如果不除的话等于是随机挑选神经元训练，这样可能达不到预期的增强效果。）</p><p>Data Augmentation: translation, rotation, stretching（拉伸）, shearing（修剪）, lens distortions（镜头畸变）, color jitter （颜色抖动）等等。</p><p>Batch Normalization（类似于”白化“，实现数据独立同分布，但是BN并没有实现独立同分布，仅仅是压缩再变换）: 在前向传播时，分两种情况进行讨论：如果是在train过程，就使用当前batch的数据统计均值和标准差，并按照第二章所述公式对Wx+b进行归一化，之后再乘上gamma，加上beta得到Batch Normalization层的输出；如果在进行test过程，则使用记录下的均值和标准差，还有之前训练好的gamma和beta计算得到结果（作业中记录的值也是采用了指数权重滑动平均的方法）。</p><p>此外，CNN使用时叫spatial batchnorm，主要是考虑到卷积神经网络的参数共享特性。（<a href="https://www.zhihu.com/question/269658514">知乎回答</a>)</p><p>之前大家对BN的理解就是为了解决ICS (internal covariate shift)，让每一层输入的分布不再变化莫测（网络需要不停调整参数适应数据分布变化），同时也要加上线性变换层保留一些模型需要的原始信息，也让输出的数据值落在激活函数的敏感区域，也可作为正则化的手段（进一步解读见：<a href="https://zhuanlan.zhihu.com/p/55852062">1</a>, <a href="https://www.cnblogs.com/skyfsm/p/8453498.html">2</a>, <a href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/">3</a>）。</p><p>但是MIT 发表在NeurIPS 2018上的研究”<strong>How Does Batch Normalization Help Optimization</strong>“发现二者并无关系。研究者证明 BatchNorm 以一种基础的方式影响着网络的训练：它使相关优化问题的解空间更平滑了。这确保梯度更具预测性（见机器之心的<a href="https://www.jiqizhixin.com/articles/2018-11-13-8">翻译</a>）。</p><p>另外，最近，2020 DeepMind刚出的论文”<strong>Batch Normalization Biases Deep Residual Networks Towards Shallow Paths</strong>“研究了BN为何可以提升深度残差网络的训练深度：”<strong>在初始化阶段，批归一化使用与网络深度的平方根成比例的归一化因子来缩小与跳跃连接相关的残差分支的大小</strong>。这可以确保在训练初期，深度归一化残差网络计算的函数由具备表现良好的梯度的浅路径（shallow path）主导“（见机器之心的<a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650783208&amp;idx=5&amp;sn=5ac1cbc22336a9e949e42a49310e5106&amp;chksm=871a7996b06df080454fe765caa7440b015329d4d67d3c70d5a2c807f928f8ec4151ac1ec075&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1585028895021&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=da21d41c58faea5bd0311609648073cf421257a6d02c4441b3868a2b447633270e7de10abb2aa824a08b5f711c6c34e277a648a52dd60752d3155ec6d6270b2dc5745360dae6f2aea673abbca5e3864c&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10&amp;version=62080079&amp;lang=zh_CN&amp;exportkey=AyJIB1yGzg%2B5xqUfbzluzjc%3D&amp;pass_ticket=IUc1Fn13evk5P8L18SlGS8pMmpxl8gKIhcPljl38AnLtjlv3XajT2eK9gnLXe6OU">报导</a>）。<strong>另外还有一个问题是BN到底应该放在激活函数之前还是之后</strong>？放前面的原因是可能会让激活函数反应在饱和区域，放在后面是因为原意就是为了让隐藏层更好的学习，接收比较规范化的输入，目前来看，似乎大多数论文都是放前面，实际中可能要根据具体问题具体实验，比如小网络放前面，大网络放后面？<a href="https://www.zhihu.com/question/283715823">知乎回答</a></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/7_1.JPG" alt="BatchNorm，CNN中常用"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/7_2.JPG" alt="LayerNorm，RNN中常用（因为RNN的输入样本不一定都是同样长度的序列）"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/7_3.JPG" alt="InstanceNorm，后面的style transfer会用到"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/7_4.JPG" alt="GroupNorm受HOG特征启发，有些特征是相互联系的，针对目标检测这样batch_size很小的情况做了优化，结合LN和IN"></p><p>出了上述几种变体外，还有个结合版本的Switchable Normalization，给IN, BN, LN分配权重，让网络自己去学习哪种归一化方式适合（<a href="https://blog.csdn.net/liuxiao214/article/details/81037416">总结1</a>，<a href="https://zhuanlan.zhihu.com/p/33173246">总结2</a>，在NLP中也UCB也通过观测BN的异常情况提出了最新的PN，见<a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247486486&amp;idx=1&amp;sn=3901b342dab93dae889a744887cce454&amp;chksm=970c24c0a07badd676bcaf24833265eddede3fefc468c38031cf54c19f3d384233b1baf41375&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1587291237878&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=bdd3db2c3f0f72f7f2db9c82510b3febc816e0a31a5eec50b93b6b3bf0119676f36421d454c5468967fe63a0f4e502b7215e6264ee0cf4d8a2c1fb594d7b81f30102c4b35c6f73a07e6e38a5676d59c4&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6209005f&amp;lang=zh_CN&amp;exportkey=A4N3AlFSdyWYx%2FUgnxpPAk8%3D&amp;pass_ticket=gmEgMHc7Z0mPJZbQjqSRRs0scY2%2F8o3EWf%2FiXl5R0OEoJMA1VU0QqZve4JWTLBSa">此</a>）。</p><p>在防止模型过拟合方面，正则化方法除了对大权重进行惩罚外，还有引入“random noise”的思路，具体有以下几种（如下图所示）。2019年 CS231N 跟进潮流，新加了Cutout和Mixup方法，其中MixUp方法本人在custom dataset上用过，对小数量数据确实有用。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/8_2.JPG" alt="对模型进行正则化的一些方法"></p><blockquote><ul><li>The recommended preprocessing is to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature</li><li>Initialize the weights by drawing them from a gaussian distribution with standard deviation of sqrt(2/n), where n is the number of inputs to the neuron. E.g. in numpy: <code>w = np.random.randn(n) * sqrt(2.0/n)</code>.</li><li>Use L2 regularization and dropout (the inverted version)</li><li>Use batch normalization</li><li>We discussed different tasks you might want to perform in practice, and the most common loss functions for each task</li></ul></blockquote><p>3.<a href="http://cs231n.github.io/neural-networks-3/">training dynamics &amp; evaluation</a>: 梯度检查，一些网络学习的注意事项，参数更新，超参搜索，模型ensemble等。这里的重点在于optimizer优化器，从一开始的普通SGD到带<a href="http://zh.gluon.ai/chapter_optimization/momentum.html">动量的SGD</a>，然后是一些自适应学习率的优化器，比如目前最好用的Adam等。</p><p>Momentum updateing: <a href="https://blog.csdn.net/dawningblue/article/details/89487418">关于“动量”更新参数的动力学解释—“动量”应当是“阻尼系数”，点的质量和运动时间看成“1”，然后推导关于位置的更新</a>；</p><p>Nesterov Momentum: <a href="https://blog.csdn.net/dawningblue/article/details/89487480">利用趋势点希望可以跳出“坑”的困境</a>；</p><p>Second order optimization: quasi-Newton methods, <a href="https://blog.csdn.net/Im_Chenxi/article/details/80546742">数学公式</a>，<a href="https://zh.wikipedia.org/wiki/黑塞矩陣">Hessian矩阵</a>，二阶方法是利用函数的曲率，也就是导数的导数来进行参数更新，速度会更快，但是Hessian阵难以计算，带数学推导的知乎推荐论文<a href="https://arxiv.org/abs/1401.7020">A Stochastic Quasi-Newton Method for Large-Scale Optimization</a>. 2020.2.20，Google brain放出一篇二阶优化算法训练Transformer的论文<a href="https://arxiv.org/abs/2002.09018">Second Order Optimization Made Practical</a>. 关于非线性优化方法的一些<a href="https://www.cnblogs.com/maybe2030/p/4751804.html">区别</a>：一阶法（梯度下降，最速下降），二阶（牛顿法，拟牛顿法），共轭梯度等。</p><blockquote><p><strong>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）</strong></p><p><strong>根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</strong></p></blockquote><p><a href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a>优化算法: 其实就是momentum + RMSProp，减缓mini-batch训练中的抖动，同时自适应更新学习率，对超参的选择比较鲁棒。相关解释：<a href="https://blog.csdn.net/liuyuemaicha/article/details/52497512">矩估计</a>，<a href="https://zhuanlan.zhihu.com/p/32335746">指数加权移动平均与初始时刻阶段偏差修正</a>，<a href="https://moodle2.cs.huji.ac.il/nu15/pluginfile.php/316969/mod_resource/content/1/adam_pres.pdf">a review ppt</a>.</p><blockquote><p>The two recommended updates to use are either SGD+Nesterov Momentum or Adam.（有时候带动量的SGD法可能比Adam训练出的效果更好，但是调参的难度可能大点）</p></blockquote><p>优化算法总结推荐阅读Juliuszh的知乎专栏：<a href="https://zhuanlan.zhihu.com/p/32230623">1</a>，<a href="https://zhuanlan.zhihu.com/p/32262540">2</a>，<a href="https://zhuanlan.zhihu.com/p/32338983">3</a></p><p>另有一篇大牛Sebastian Ruder写的<a href="https://ruder.io/optimizing-gradient-descent/index.html">An overview of gradient descent optimization algorithms</a>，最近一次更新在2020.3.20，加入了新的优化器，博客中也提供了arXiv版本。</p><p>超参Hyperparameters主要包括网络架构，学习率以及其deacy 机制，参数更新机制，和正则化regularization选择及其强度。面对这些超参，在实验时可以通过以下步骤来进行：首先检查模型初始化是的loss，一般权重对每一类都是零认知的，所以会给出平均猜测；然后拿出小部分样本进行实验，进行初始化，给出一些学习率然后对其训练，从中挑出靠谱的；接着再组合学习率和deacy参数，看看哪些不错；然后对上面最好的进行长时间训练，一言以蔽之就是先在数据中的小范围中进行预演。此外也要注意learning rate中的deacy参数选择和加入时间，否则可能使学习不充分。训练刚开始可以采取<a href="https://blog.csdn.net/sinat_36618660/article/details/99650804">warm up</a>或者grad warm up处理，这个主要是针对大数据，大网络中权重的初始平稳问题，详见<a href="https://www.zhihu.com/question/338066667">知乎回答</a>。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/8_4.JPG" alt="学习率下降处理"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/8_5.JPG" alt="warmup"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/8_3.JPG" alt="超参选择方法"></p><blockquote><p>To train a Neural Network:</p><ul><li>Gradient check your implementation with a small batch of data and be aware of the pitfalls.</li><li>As a sanity check, make sure your initial loss is reasonable, and that you can achieve 100% training accuracy on a very small portion of the data</li><li>During training, monitor the loss, the training/validation accuracy, and if you’re feeling fancier, the magnitude of updates in relation to parameter values (it should be ~1e-3), and when dealing with ConvNets, the first-layer weights.</li><li>The two recommended updates to use are either SGD+Nesterov Momentum or Adam.</li><li>Decay your learning rate over the period of the training. For example, halve the learning rate after a fixed number of epochs, or whenever the validation accuracy tops off.</li><li>Search for good hyperparameters with random search (not grid search). Stage your search from coarse (wide hyperparameter ranges, training only for 1-5 epochs), to fine (narrower rangers, training for many more epochs)</li><li>Form model ensembles for extra performance</li></ul></blockquote><h2 id="Lecture-9-CNN-Architecture"><a href="#Lecture-9-CNN-Architecture" class="headerlink" title="Lecture 9. CNN Architecture"></a>Lecture 9. CNN Architecture</h2><p>这一节主要讲CNN的几个经典结构（AlexNet, VGG, GoogLeNet, ResNet），课外阅读材料相关网络对应的论文。</p><p>推荐阅读Justin ho对CNN architecture的介绍文章，链接在<a href="https://zhuanlan.zhihu.com/p/28749411">此</a>。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_1.JPG" alt="2019较2017加了最新的NAS技术，自动搜索最优结构网络"></p><p><strong>AlexNet</strong>: 开启CNN风潮的奠基之作，规划了CNN网络该有的部件（data augmentation, filter, maxpool, normalization, non-linearity activation function, fc, dropout, gpu加速等）。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_2.JPG" alt="AlexNet结构（5Conv+3FC），初版由于GPU显存小所以一半一半丢到两块GPU训练，最后合并"></p><p><strong>VGG</strong>: 引入小卷积核，一方面小卷积核堆叠拥有大卷积核同样的感受野，另一方面还可以减小参数，同时加深网络，提高拟合精度。后续的网络基本吸取了VGG网络中小卷积核（3x3, p=1, s=1）提取特征，mxpool (s=2) downsample 特征图，同时在缩小后的特征图上加倍filter数量保持时间复杂度。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_3.JPG" alt="现在以16和19层为主，且只有3x3卷积核，和s=2的maxpool缩小特征图尺寸"></p><p><strong>GoogLeNet</strong>: 受到<a href="https://arxiv.org/pdf/1312.4400.pdf">Network in Network</a>很大启发（一个是跨通道融合的1x1卷积核带来的MLP功能，提高局部特征的抽象表达能力，另一个是Global Average Pooling (GAP)，单独利用特征图分类），同时提升网络的深度和宽度。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_4.jpg" alt="GoogLeNet--22层，三个分类器，FC前面利用AP减少参数"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_5.JPG" alt="采用inception module进行堆叠，并增加网络宽度，同时采用1x1卷积核降维特征图层数，减少参数并增加非线性"></p><blockquote><p>为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉。</p></blockquote><p>之后Google对inception进行改进到v4版本(详见<a href="https://my.oschina.net/u/876354/blog/1637819">blog</a>)，v2, v3主要是改小卷积核，v4是加入了Resnet的残差连接思想，然后将basic block或者bottleneck中的conv组合换成inception module。</p><p>GoogLeNet团队发表的有关GoogLeNet的论文：《Going deeper with convolutions》、《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》、《Rethinking the Inception Architecture for Computer Vision》、《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》</p><p><strong>ResNet</strong>：加深网络可以获得更好的性能，但是采用常规方法堆叠卷积加深优化困难，实际效果不好。</p><p>恒等映射，深网络中有不少是对浅网络的缝补，深层网络中，如果保留那些浅层网络的性能，同时又考虑采用恒等映射，那么效果应该不会比单独用浅层网络更差。但是直接学习恒等映射不如学习输出与恒等映射的残差有效（redidual模块会明显减小模块中参数的值从而让网络中的参数对反向传导的损失值有更敏感的响应能力，虽然根本上没有解决回传的损失小的问题，但是却让参数减小，相对而言增加了回传损失的效果，也产生了一定的正则化作用），因此引入跳级连接，ResNet诞生（参考<a href="https://blog.csdn.net/weixin_43624538/article/details/85049699?from=timeline&amp;isappinstalled=0">PRIS-SCMonkey</a>，<a href="https://zhuanlan.zhihu.com/p/54289848">Pascal</a>）。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_6.JPG" alt="basic block和bottleneck结构应对不同深度的网络，bottleneck结构应该受到GoogLeNet启发进行降维减少参数"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_7.JPG" alt="不同深度网络结构参数配置"></p><p>后续的研究者一直在思考为什么ResNet会如此简洁高效，它到底在解决网络学习中的什么问题？1.degradation问题，梯度有效性被跳级连接解决（何恺明在论文中指出梯度消失爆炸问题已经被BN等normalization手段解决得很好了，跳级连接是为了深层的网络参数对梯度更加敏感，保持前面梯度的相关性，因为前向传播的时候将原始的信息带出一部分。当然ResNet也可以在一定程度上缓解梯度弥散的问题，但是这不是最主要的，详见<a href="https://www.zhihu.com/question/64494691">这个知乎回答</a>），且必须是1系数。多个浅层子网络代表的函数的组合函数，并不是真正意义上的深层特征网络。</p><blockquote><p><strong>神经网络的退化才是难以训练深层网络根本原因所在，而不是梯度消散。</strong>虽然梯度范数大，但是如果网络的可用自由度对这些范数的贡献非常不均衡，<strong>也就是每个层中只有少量的隐藏单元对不同的输入改变它们的激活值，而大部分隐藏单元对不同的输入都是相同的反应，此时整个权重矩阵的秩不高。并且随着网络层数的增加，连乘后使得整个秩变的更低。</strong>这也是我们常说的网络退化问题，虽然是一个很高维的矩阵，但是大部分维度却没有信息，表达能力没有看起来那么强大。<strong>残差连接正是强制打破了网络的对称性。</strong>(来自Orhan A E, Pitkow X. Skip connections eliminate singularities[J]. arXiv preprint arXiv:1701.09175, 2017.)</p><p>来自知乎专栏：<a href="https://zhuanlan.zhihu.com/p/42833949">https://zhuanlan.zhihu.com/p/42833949</a></p></blockquote><p>2.ResNet中恒等变换的意义目前有多种解释，我目前倾向的<a href="https://www.zhihu.com/question/293243905">解释</a>是这种残差结构是为了动态自适应拟合训练数据集网络深度的一种方法，它除了可以平稳甚至加速训练外（跳级连接让不同分辨率的feature组合，梯度的相关性得到了保持，后续的DenseNet则用了更多的不同分辨率的特征组合），就是可以根据你的问题去慢慢补充浅层网络解决的最主要的拟合之外的小边边角角，让最后的函数更加平滑。网络输出的残差$F(x)$部分可能在某个block接近0，可能某个部分比较大，所以是一种自适应复杂度。</p><blockquote><p>ResNet是完成一个微分方程的差分形式，dx(t)/dt=H(t)=&gt;x(t+1)=x(t)+H(t). 所以ResNet就是用微分方程来描述一个动态系统，寻找合适的H(t)来构造系统。采用这个结构的好处是H(t)具有某种自适应特性（相比于普通的CNN），对网络的深度不会过于敏感（自适应差分方程的步长），同时简化网络的结构，让网络结构更加均匀化，从几何角度看就是制造了一个更为平直的流形并在其上进行优化，导致系统的收敛性更好。</p><p>不同的网络结构实际上构造了不同的解空间的流形，并且部分的定义了其上的度量，而不同的度量引出不同的曲率分布，这对系统的收敛性能影响重大。所以不同网络结构的差异表现在：（1）网络结构是否和问题匹配，是否是一个包含了可能解的足够紧致的空间；（2）由网络结构所部分定义的度量是否导致一个比较平直的均匀的流形（度量还被所采用的代价函数所部分定义）。从这两个角度就可以定性和半定量的分析不同网络结构的特性和关系。</p><hr><p>残差模块并不是就是恒等啊，我觉得可以理解为:当网络需要这个模块是恒等时，它比较容易变成恒等。而传统的conv模块是很难通过学习变成恒等的，因为大家学过信号与系统都知道，恒等的话filter的冲激响应要为一个冲激函数，而神经网络本质是学概率分布 局部一层不太容易变成恒等，而resnet加入了这种模块给了神经网络学习恒等映射的能力。</p><p>所以我个人理解resnet除了减弱梯度消失外，我还理解为这是一种自适应深度，也就是网络可以自己调节层数的深浅，不需要太深时，中间恒等映射就多，需要时恒等映射就少。当然了，实际的神经网络并不会有这么强的局部特性，它的训练结果基本是一个整体，并不一定会出现我说的某些block就是恒等的情况</p><p>来自知乎回答：<a href="https://www.zhihu.com/question/293243905">https://www.zhihu.com/question/293243905</a></p></blockquote><p>在论文<a href="https://arxiv.org/abs/1603.09382">Deep Networks with Stochastic Depth</a>中，作者设置概率p让residual block随机失活（均匀概率与线性概率，线性概率更有效些，符合ResNet的设计思想），只保留shortcut，结果精度也没有怎么下降，反而提高了训练速度，这也说明Identity Mapping的正确性。</p><p>对ResNet的改进以及后续其他网络：</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_8.jpg" alt="改进downsample部分，减小信息流失"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_9.JPG" alt="把激活函数和BN放在residual模块中，称其为full pre-activation，其效果最好。论文来自ResNet原班人马，Identity Mappings in Deep Residual Networks，在该论文中也分析讨论了有关ResNet的其他设置问题"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_10.JPG" alt="GoogLeNet引入ResNet结构，Inception-ResNet-v1"></p><p><a href="https://arxiv.org/pdf/1605.07146.pdf">Wide Residual Networks</a>认为ResNet中有些block去掉也没有影响，所以residual是最主要的影响因素，作者尝试加倍block中的卷积核数量，而不是增加深度，结果发现效果更好，而且由于加深宽度可以由并行化计算解决，所以计算也更加快速。另外，作者也使用了卷积层上的dropout，结果也可以得到一些提升（论文指出何恺明他们是在identity part使用dropout效果不好，所以他们用在这，我想可能是因为拓宽了宽度，特征图增多，可能带来冗余信息，存在过拟合信息）。我没有仔细看论文，也没看源代码，不清楚作者是不是随机给特征图某个元素置0来进行的，实际上给卷积层做dropout的操作并不常见，一般都是BN就足够了，有部分研究是针对卷积层上或者特征图上的droput的，而且认为随机置某元素为0并不能起到作用，因为特征图一般是局部关联的，所以提出了区域置0，某个特征图置0等操作，见此知乎<a href="https://www.zhihu.com/question/52426832/answer/926104467">回答</a>。</p><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf">Aggregated Residual Transformations for Deep Neural Networks</a>: ResNeXt，来自ResNet的作者，想法和上面的差不读，在一个block里采用多组重复卷积核操作，拓宽网络。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_11.JPG" alt="ResNet和ResNeXt结构差异"></p><p><a href="https://arxiv.org/pdf/1709.01507.pdf">Squeeze-and-Excitation Networks</a> : SENet是在网络的特征通道之间进行建模，显示地通过FC去得到通道之间的依赖关系，然后利用最后的sigmoid激活函数给每个通道附一个新的权重，依照这个重要程度去提升有用的特征抑制用处不大的特征 (feature recalibration)。相关介绍见作者胡杰在机器之心的<a href="https://www.cnblogs.com/bonelee/p/9030092.html">解释</a>。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_12.JPG" alt="在inception module和residual module中加入SE Block结构"></p><p><a href="https://arxiv.org/abs/1605.07648">FractalNet: Ultra-Deep Neural Networks without Residuals</a>: 分形（从多个层次、多个角度、多个组成成分来共同解决一个问题）网络结构，类似于ResNet，但是认为深层网络的关键是浅层到深层的有效过度，residual representation不是必须的。分阶段组合不同分辨率特征，达到了类似于教师-学生机制、深度监督的效果。由于比较复杂，所以没有ResNet用的广泛，但是思想很有深度，其中最突出的contribution就是drop path。在下图的示意中，从输入到预测有很多条路可以走通，drop path机制就是随机失活某些路径，假如其中一条承担的作用是比较大，那么一旦被失活，其他网络就得承担全部的责任，加强其学习力度，所以一方面防止过拟合，另一方面让整个网络变得非常鲁棒，不会出现退化问题，参考此<a href="https://blog.csdn.net/wspba/article/details/73251731">博客</a>。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_13.JPG" alt="Fractal architecture"></p><p><a href="https://arxiv.org/abs/1608.06993">Densely Connected Convolutional Networks</a>: CVPR2017 Best paper，写的也非常简洁通透。其dense block的设计是ResNet的“完全版”，不仅提高了特征的利用率，减小了参数，也减轻了梯度弥散，每一dense block单独产生的特征图层数也相对较少，也便于训练，具体解读见此<a href="https://blog.csdn.net/u014380165/article/details/75142664">博客</a>。但是一开始的DenseNet虽然参数少，但是由于复杂的的skip connection和concatenate操作，中间量的存在会占用很多显存，作者后续论文<a href="https://arxiv.org/abs/1707.06990">Memory-Efficient Implementation of DenseNets</a>减缓了这个问题，现在PyTorch等框架也应该自动解决了这个问题，在一定程度上。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_14.JPG" alt="Dense Block结构，前面每一层都会和后面每一层沟通"></p><p>MobileNet, ShuffleNet等轻量级计算网络，通过引入depthwise separable convolution, pointwise group convolution, channel shuffle (顺序重分配每组的channel到新的组，减少每个组单独流动信息的局限性)等操作改变传统卷积计算方式，来加速计算。详见知乎白裳的<a href="https://zhuanlan.zhihu.com/p/35405071">博客随笔</a>和AI之路的<a href="https://blog.csdn.net/u014380165/article/details/75137111">详解</a>。</p><p>Meta-learning，NAS自动搜索合适架构，开山之作是Google的<a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search with Reinforcement Learning </a>，通过RNN确定架构中每个元素的参数，然后利用reinforcement learning奖励精度高的，惩罚精度差的，以此来学习。NAS领域交叉知识多，暂时无法深入理解。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/9_15.JPG" alt="google最新的EfficientNet"></p><h2 id="Assignment-2"><a href="#Assignment-2" class="headerlink" title="Assignment 2"></a>Assignment 2</h2><p>宝藏作业，值得认真做。</p><p><a href="http://cs231n.github.io/assignments2019/assignment2/">assignment page</a>;</p><p>reference：</p><ul><li><p><a href="https://github.com/Wangxb06/CS231n">github-code</a>;</p></li><li><p>参考培训机构深度之眼的cs231n-camp的文档</p></li></ul><p>具体过程和结果参见这个<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments">repository</a></p><p>在做CNN那一节时（ubuntu环境，如果是Windows环境可能需要安装python for c++等库），如果在fast_layer.py环节出现<code>CS231n A2: Global name &#39;col2im_6d_cython&#39; is not defined解决</code>，请关掉jupyter notebook，然后重新编译就可以了。</p><h2 id="Lecture-10-Recurrent-Neural-Networks"><a href="#Lecture-10-Recurrent-Neural-Networks" class="headerlink" title="Lecture 10. Recurrent Neural Networks"></a>Lecture 10. Recurrent Neural Networks</h2><p>这一节课主要讲RNN的网络结构（recursive neural network是树结构，recurrent neural network是chain结构），设计文本生成，image caption, visual question之类的网路，后面通过training问题提及了下LSTM。课程的目的在于希望NLP和CV两者结合，相辅相成，解决更为智能化的问题。</p><p>参考阅读材料方面主要是花书《deep learning》中的<a href="http://www.deeplearningbook.org/contents/rnn.html">第十章</a>: Sequence Modeling: Recurrent and Recursive Nets. 其内容不难理解，也都是介绍性文字，大部分细节都是列出论文便于感兴趣读者去进一步阅读。（个人觉得看好Justin Johnson的<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture10.pdf">课件</a>就差不多了，主要内容都是RNN结构和LSTM）</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/10_1.JPG" alt="一个典型RNN结构，其中对每个类型变量的权重都是相同的，会重复使用"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/10_4.JPG" alt="多输出一个特征图的权重注意力分配信息"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/10_5.JPG" alt="VQA机制"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/10_6.JPG" alt="根据用户言语指导进行视觉导航"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/10_2.JPG" alt="原生RNN梯度回传会出现弥散或者爆炸现象，难以训练"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/10_3.JPG" alt="long-short-term-memory (LSTM)修正梯度回传问题"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/10_7.JPG" alt="LSTM处理方式示意"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/10_8.JPG" alt="LSTM可以很好缓解梯度问题，但是不能完全避免"></p><p>这里有一篇讲解LSTM的<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">blog</a>（嫌麻烦的可以直接看<a href="https://www.jianshu.com/p/9dc9f41f0b29">中文翻译版本</a>）。</p><p>有关Justin Johnson课上提到的RNN的一些fun application可以在karpathy的这篇<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">blog</a>找到。</p><p>目前，有关机器翻译，文本生成等NLP问题自谷歌的”Attention is all you need“之后，基本上都直接采用self-attention机制了，但这并不代表RNN没有用，已被淘汰了，其在处理依赖时间序列问题还是有很多用武之地的。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/10_9.JPG" alt="自Facebook的DETR检测模型开了transformers和CNN特征结合的先河之后，预测后续会有更多的想法出现"></p><h2 id="Lecture-11-Gnerative-Models"><a href="#Lecture-11-Gnerative-Models" class="headerlink" title="Lecture 11. Gnerative Models"></a>Lecture 11. Gnerative Models</h2><p>这节课没有官方推荐阅读材料，只有<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture11.pdf">slides</a>首先介绍监督学习和分监督学习的一些区别（有无外部label带来的监督信号，前者着重利用数据做任务，后者着重学习数据内部的结构和分布等）。</p><p>2020新版增加了CS236的VAE的<a href="https://deepgenerativemodels.github.io/notes/vae/">notes</a>和Ian Goodfellow在NIPS2016上的<a href="https://arxiv.org/pdf/1701.00160.pdf">tutorial</a>.</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_1.JPG" alt="supervised v.s. unsupervised learning"></p><p>后面主要讲无监督学习中的一个核心问题density estimation，介绍跟此问题有关的三类generative models：</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_2.JPG" alt="PixelRCNN/CNN, VAE, GAN"></p><p>目前在生成式模型方面还未进行研究，所以暂时也没有啥思考，也就单纯把这节课当成一次introduction。课程中有关这三类模型的概念的公式也不搬过来了，直接看课件就好。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_3.JPG" alt="三类生成式模型比较"></p><p>后续会好好研究一下GAN，那时会再专门写笔记讨论体会心得，先放几个不错的resources链接：</p><ul><li>review-<a href="https://arxiv.org/abs/2001.06937">A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications</a></li><li>paper list-<a href="https://github.com/hindupuravinash/the-gan-zoo">the-gan-zoo</a></li><li>how-to-train-gan-<a href="https://github.com/soumith/ganhacks">ganhacks</a></li><li>知乎文章-<a href="https://zhuanlan.zhihu.com/p/25071913">令人拍案叫绝的Wasserstein GAN</a></li><li>researcher-<a href="https://people.csail.mit.edu/junyanz/">Jun-Yan-Zhu</a></li><li>Stanford-cs236-<a href="https://deepgenerativemodels.github.io/">deep generative models</a></li><li>UCB-cs294-<a href="https://sites.google.com/view/berkeley-cs294-158-sp19/home">deep unsupervised learning</a></li></ul><hr><p>2020.6更新：</p><p>普通的pixelRNN/CNNs是显示估计数据分布，即：$p_{\theta}(x)= \prod\limits _{i=1}^{n} p_{\theta}\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)$，虽然效果不错，但是计算很慢，VAE是利用一个中间的隐变量来进行，即：$p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x \mid z) d z$</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_4.JPG" alt="利用中间变量进行降维，提取主要特征"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_5.JPG" alt="训练好的VAE还可以讲decoder去掉进行监督任务训练"></p><p>但是上述的VAE只能重建输入图片，如果想要重建满足输入数据分布下的另一个图片该怎么做？</p><p>我们可以选择$p(z)$为一个已知的比较简单的先验分布，比如高斯分布，$p(x|z)$是神经网络解码出的分布，但是前面的积分会使得整个过程intractable. 考虑后验概率，即$p_{\theta}(z \mid x)=p_{\theta}(x \mid z) p_{\theta}(z) / p_{\theta}(x)$，进一步地有：</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_6.JPG" alt="丢掉最后一个KL散度，最大化data likelihood"></p><p>现在我们先让encoder去让后验概率去近似我们设定的先验概率$p(z)$，比如说是标准正态分布，然后让decoder从拟合的后验概率中采样去重建图片。训练时重建输入图片，用以反向传播，测试时可以直接丢掉encoder，利用decoder随机从先验概率分布中采一个数然后decoder出新的图片。非常漂亮的思想！</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_7.JPG" alt="VAE的学习过程，两步近似"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_8.JPG" alt="测试时可以生成新的图片"></p><p>此外，隐变量$z$也是多维的，每个维度可以带便不同的图像特征，这样的话可以控制我们想要输出的图片内容。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_9.JPG" alt="不同维度的z编码了图片的不同因素"></p><p>以上的两种方法都在试图显示建立数据的分布，而GAN试图绕过复杂的数据分布建模，利用博弈的思想隐式建立数据分布，从而直接生成新的图片样本。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_10.JPG" alt="GAN将最大最小化改为双最大化，原因是在对生成器进行参数更新的时候，原始是希望最小化判别器对生成器的判断正确的能力，这样的话根据数学公式，导数-1/(1-x)前期变化慢，后期变化快，不利用学习，后来将其改成最大化判别器对生成器判断错误的能力，这样计算出的导数1/x，前期变化快，后期变化慢，符合一般的网络学习规律"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/11_11.JPG" alt="训练CNN结构GAN的建议"></p><h2 id="Lecture-12-Detection-and-Segmentation"><a href="#Lecture-12-Detection-and-Segmentation" class="headerlink" title="Lecture 12. Detection and Segmentation"></a>Lecture 12. Detection and Segmentation</h2><p>这节课介绍了计算机视觉中比较重要的四个任务：classification, semantic segmentation, object detection, instance segmentation, <a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture12.pdf">slides</a>做的很不错。</p><p>此外，18版的课程里有个两个相关的discussion section：<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds06.pdf">Practical Object Detection and Segmentation</a>，列出了经典的检测和分割结构，并给出了相应的代码实现链接和一些其他resources；<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds08.pdf">Video Understanding</a>.</p><p>1.semantic segmentation主要通过FCN的结构介绍了downsampling—upsampling这种经典结构，重点引出一般的upsampling的方法，比如unpooling (一种是最近邻补值，一种是记下pooling的索引，然后其他补零)和转置卷积（利用权重矩阵乘以每个元素，然后得到大的特征图区域，overlap的地方累加&lt;累加只是一种处理手段，这方面或许可以有更好的办法恢复信息&gt;）</p><p>转置卷积transpose convolution： upsampling的一种 ，unpooling, max unpooling, 为什么叫转置卷积更好点，Justin Johnson在课上也给了数学解释。（<a href="https://www.zhihu.com/question/43609045/answer/132235276">知乎问题</a>）</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/12_1.JPG" alt="转置卷积"></p><p>空洞卷积Dilated Convolution：源自于语义分割领域，目前也大多用于语义分割领域。初衷是为了解决downsample丢失的信息会影响pixel-wise的分类。“dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息”（<a href="https://www.zhihu.com/question/54149221">知乎问题</a>）</p><p>2.object detection和instance detection是交叉介绍的，因为后者是前者的集成，也顺带提及了human pose estimation，keypoint。这一部分的介绍相比2017版多了一些细节（尤其是RoI pooling&lt;浮点数两次转整型处理，丢失proposal原始像素信息&gt;和RoI Align&lt;保留浮点数，利用双线性插值得到每个max pooling小区域的四个点，对小目标更好&gt;）和scene graph, 3D检测，3D shape prediction等部分（3D Machine Learning应该是目前工业界比较看重的一个领域，在无人驾驶，增强现实等领域可以发挥作用）。其中object detection部分着重介绍了R-CNN系列，更详细的解释可以阅读另一篇<a href="http://densecollections.top/2019/11/30/RCNN-series-in-object-detection/">博客</a>。课程中也推荐一篇综述类型的paper: <a href="https://arxiv.org/abs/1611.10012">Speed/accuracy trade-offs for modern convolutional object detectors</a>便于进一步了解（在我的另一篇R-CNN检测专题<a href="http://densecollections.top/2020/01/10/RCNN-series-in-object-detection-续/">博客</a>会有所提及）。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/12_2.JPG" alt="ROI Align利用双线性插值提高ROI映射精度"></p><p>可变形卷积Deformable convolutional networks：受Andrew Zisserman的STN (Spatial Transformer Network) 启发，针对物体的形状自发的选择卷积的区域，而不是按照约定的矩形计算规则，更适合实际。在<a href="https://www.zhihu.com/question/57493889">知乎回答</a>中，有人说可变性卷积是对空洞卷积的一种generalization，增大了感受野 。</p><h2 id="Lecture-13-Visualizing-and-Understanding"><a href="#Lecture-13-Visualizing-and-Understanding" class="headerlink" title="Lecture 13. Visualizing and Understanding"></a>Lecture 13. Visualizing and Understanding</h2><p>1.试图理解卷机神经网络是怎么做决策的，这些layers到底学了什么东西。因此引入了特征图可视化，看看网络对图像的那些区域或者blob甚至是pixel反应大些（感受野，遮挡，<strong>回传对pixel的梯度，gradient ascent</strong>等），激活的是哪些地方（这里面的针对像素的梯度上升方法与学习权重等参数的方法是对立统一的，固定神经元参数，给定zero 图像，加一些正则化方法，然后对图像进行优化，看看什么样的图像可以获得更高的类别分数），然后借此产生对抗样本adversarial example试图欺骗网络（17年的最后一课就是请Ian Goodfellow讲这个）。</p><p>2.利用可视化思想通过选定特定的layer对输入图像做gradient，然后更新图像，放大特征，产生<a href="https://github.com/google/deepdream">DeepDream</a>效果；</p><p>3.Neural Texture Synthesis, 利用<em>Gram matrix</em>对小区域图像进行延展；</p><p>4.Neural Style Transfer, 对输入图像进行特定风格的渲染。这项技术现在已经被商业化了，有不少手机APP和其他应用都有此功能！Justin Johnson也有个torch的<a href="https://github.com/jcjohnson/neural-style">实现</a>。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/13_1.JPG" alt="Neural Style Transfer model"></p><p>但是得到一张转换的图片很慢，因为有很多的forward和backword，因此J.J做了个<a href="https://github.com/jcjohnson/fast-neural-style">Fast Style Transfer</a>, 训练另外一个网络来实现style transfer, 其中IN (instance norm)就是为了快速风格转换发明的。课程的<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture13.pdf">slides</a>里也提供了很多相关的paper，对细节感兴趣的可以看看。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/13_2.JPG" alt></p><p>有关神经网络可视化，个人觉得是个很有意义也和好玩的一个领域，它在一定程度上解释了模型为什么work，也可以结合一些idea做出可消费的funny application出来。<del>Assignment3也有相关的作业，到时候应该可以深入学习理解下。</del>从assignment3来看，为了让输入图像的风格靠近目标图像 (style loss)，采用gram矩阵来编码模型各个层产生的通道之间的相关性，然后尽量让两幅图像的gram矩阵（以指定维度方向上的各个向量之间做内积）相近。gram矩阵代表图像特征中哪些特征是同时出现的，哪些是此消彼长，不相干的，而特征呢就相当于图像的某个特性，比如在修图的时候，我们会调调对比度，加些高光，调调饱和度等，为了得到我们想要的风格的图像，其中有些修图选项可以一起做，有些往大的值调，有些就得往小的调，gram矩阵也是类似的操作，缩小目标gram矩阵和输入gram矩阵之间的差异就等于是给图像加滤镜。此外，gram矩阵的对角线上还包含了每个特征图的原始信息，也就是说不仅有哪些特征（哪些修图选项），也有各个特征之间的联系（修图选项的调整值），因此可以大致代表图像的风格。（相关<a href="https://www.zhihu.com/question/49805962?sort=created">知乎问题</a>)</p><p>相关blog: Hungryof的“谈谈图像的style transfer”系列—-<a href="https://blog.csdn.net/Hungryof/article/details/53981959">1</a>, <a href="https://blog.csdn.net/Hungryof/article/details/71512406">2</a>, <a href="https://blog.csdn.net/Hungryof/article/details/80310527">3</a></p><h2 id="Discussion—Learning-on-videos"><a href="#Discussion—Learning-on-videos" class="headerlink" title="Discussion—Learning on videos"></a>Discussion—Learning on videos</h2><p>视频理解和3D machine learning 似乎成为现在CV中的两个新的热点。2020年的课程中也没有把视频理解放进前面的CV applications中，估计是考虑到视频理解中用的方法也是从那些分类，检测等基础任务中迁移过来的。从现在工业界短视频作为火热的媒介来看，视频理解的研究预计会出现比较大的需求和增长。</p><p><a href="http://cs231n.stanford.edu/slides/2020/section_8_video.pdf">Discussion Section—-learning on videos</a></p><p>其中提到目前主流的三种模型：</p><ul><li><p>CNN + RNN: video understanding as sequence modeling  </p></li><li><p>3D Convolution: embed temporal dimension to CNN </p></li><li><p>Two-stream: explicit model of motion (video = apperance + motion)</p></li></ul><h2 id="Lecture-14-Deep-Reinforcement-Learning"><a href="#Lecture-14-Deep-Reinforcement-Learning" class="headerlink" title="Lecture 14. Deep Reinforcement Learning"></a>Lecture 14. Deep Reinforcement Learning</h2><p>  对于强化学习的印象只停留在一个框架的系统层面上：输入是状态（state），动作（action）和奖励（reward），输出时方案或者策略（policy），大意就是通过奖惩机制，在某个环境中为对象制定在初始状态实现目标的一系列策略。</p><p>强化学习的热度似乎没有神经网络，GAN等技术大，而且据我身边研究的朋友说，强化学习的学习难度有点高，具体没有细问，可能是模型，critic制作和训练的难度不低。一位bloger在他对强化学习简介的<a href="https://www.cnblogs.com/geniferology/p/what_is_reinforcement_learning.html">blog</a>中说到：</p><blockquote><p>强化学习的领导研究者<a href="http://www.incompleteideas.net/">Richard S. Sutton</a>认为，只有这种学习法才考虑到 <em>自主个体</em>、<em>环境</em>、<em>奖励</em> 等因素，所以它是人工智能中最 top-level 的 architecture，而其他人工智能的子系统，例如 logic 或 pattern recognition，都应该在它的控制之下，我觉得颇合理。</p></blockquote><p>个人认为，目前的强化学习方式确实是一个可以融合许多机器学习技术和模型的一个理论框架，视觉中的注意力机制可以用到，NAS中也可以用到，随着时间推移，可能会有更多的结合和应用会产生，甚至引导未来很多产业的落地。</p><p>一些相关resources:</p><ul><li><p><a href="https://www.zhihu.com/question/49230922/answer/115011594">知乎—强化学习（reinforcement learning)有什么好的开源项目、网站、文章推荐一下</a></p></li><li><p><a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course">Deep_reinforcement_learning_Course</a></p></li><li><p><a href="https://pathmind.com/wiki/deep-reinforcement-learning">A Beginner’s Guide to Deep Reinforcement Learning</a></p></li><li><p><a href="http://www.incompleteideas.net/book/the-book.html">textbook—Reinforcement Learning: An Introduction</a></p></li></ul><p>Besides，这<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture14.pdf">课件</a>做得也挺好的，虽然Serena Yeung小姐姐讲的课让我一知半解，云里雾里的 :(</p><h2 id="Assignment-3"><a href="#Assignment-3" class="headerlink" title="Assignment 3"></a>Assignment 3</h2><p><a href="https://cs231n.github.io/assignments2019/assignment3/">assignment page</a>;</p><p>reference：</p><ul><li><p><a href="https://github.com/FortiLeiZhang/cs231n/tree/master/code/cs231n/assignment3">github-code</a>;</p></li><li><p>参考培训机构深度之眼的cs231n-camp的文档</p></li></ul><p>具体过程和结果参见这个<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments">repository</a></p><p>COCO_Caption数据集下载速度很能很慢，将链接放至迅雷可能快点，我这边是1～2M/s。</p><p>做<code>RNN_Caption.ipynb</code>时出现类似assignment1的无法导出<code>imread,imresize</code>问题，原因就是<code>scipy</code>包现在不支持这个函数了，直接将<code>scipy</code>从1.4退回到1.2.0即可。</p><p>在执行<code>Look at the data</code>部分时出现<code>[WinError 10054] 远程主机强迫关闭了一个现有的连接</code>错误，按照<a href="http://188.131.244.232/article/10">此博客</a>的解释，可能是链接存在一些无法访问的情况，将“http”改成”https”即可，<code>url = url.replace(http://&quot;, &#39;https://&#39;)</code></p><p>改完之后可以顺利访问，但是出现<code>进程被占用</code>情况，解决方法同上面博客，修改<code>image_utils.py</code>中的<code>image_from_url</code>函数：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def image_from_url(url):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Read an image from a URL. Returns a numpy array with the pixel data.</span><br><span class="line">    We write the image to a temporary file then read it back. Kinda gross.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    try:</span><br><span class="line">        f &#x3D; urllib.request.urlopen(url)</span><br><span class="line">        #_, fname &#x3D; tempfile.mkstemp()</span><br><span class="line">        # 为了解决“另一个程序正在使用此文件，进程无法访问的错误&quot;</span><br><span class="line">        fd, fname &#x3D; tempfile.mkstemp(dir&#x3D;&quot;G:\CS231N&quot;)</span><br><span class="line">        with open(fname, &#39;wb&#39;) as ff:</span><br><span class="line">            ff.write(f.read())</span><br><span class="line">        img &#x3D; imread(fname)</span><br><span class="line">        # 在删除临时文件前关闭该文件，防止被占用</span><br><span class="line">        os.close(fd)</span><br><span class="line">        os.remove(fname)</span><br><span class="line">        return img</span><br><span class="line">    except urllib.error.URLError as e:</span><br><span class="line">        print(&#39;URL Error: &#39;, e.reason, url)</span><br><span class="line">    except urllib.error.HTTPError as e:</span><br><span class="line">        print(&#39;HTTP Error: &#39;, e.code, url)</span><br></pre></td></tr></table></figure><p>在<code>NetworkVisualization-Pytorch.ipynb</code>的<code>load some ImageNet images</code>中，运行报错<code>Object arrays cannot be loaded when allow_pickle=False</code>，通过<a href="https://blog.csdn.net/weixin_42096901/article/details/89855804">这篇博客</a>发现是自己<code>numpy</code>版本过高问题，将其回退到1.16.1或1.16.2即可解决</p><p>在使用pytorch时，要注意对<code>requires_grad=True</code>的Tensor和求梯度阶段需要用到的Tensor都不能用inplace operation，也就是不能直接对这些张量进行数值操作，最好加上.detach()曲线救国，见<a href="https://zhuanlan.zhihu.com/p/38475183">此</a>。</p><p>在<code>Generative_Adversarial_Networks_PyTorch.ipynb</code>中下载MNIST可能很慢，可以直接找到数据网址自己下载好数据集，然后放到代码自动创建的存放数据的文件夹中的<code>raw</code>中，再次运行就可以不用在线下载，而是直接读取在process了。还有一种办法是改变源代码的中的urls到本地，详情可见<a href="https://blog.csdn.net/york1996/article/details/81780065">here</a>.</p><h2 id="2017-Invited-Talk-Efficient-Methods-and-Hardware-for-Deep-Learning-Song-Han"><a href="#2017-Invited-Talk-Efficient-Methods-and-Hardware-for-Deep-Learning-Song-Han" class="headerlink" title="2017 Invited Talk-Efficient Methods and Hardware for Deep Learning, Song Han"></a>2017 Invited Talk-Efficient Methods and Hardware for Deep Learning, Song Han</h2><p>CS231N每年都会邀请不同的专家来做一些专题介绍，由于2017年以后都没release视频了，所以就直接看17年的talk了，正好17年的这两个topic都比较不错，在应用上都很重要，因此也很值得学习下。</p><ul><li><p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture15.pdf">课件</a></p></li><li><p><a href="https://songhan.mit.edu/">Song Han</a></p></li><li><p>17年的旷视也有一节课专门讲神经网络压缩，<a href="https://www.bilibili.com/video/BV1E7411t7ay?p=9">链接</a></p></li></ul><p>韩松在该领域算是比较有名的一位，他在此节课从算法讲到硬件（分inference和training两部分），非常系统。虽然大多是overview，但是包含的内容很多，后续研究压缩领域也会从他主页中列举的资料来入门和学习。</p><p>韩松教授2017年的phd thesis: <a href="[https://stacks.stanford.edu/file/druid:qf934gh3708/EFFICIENT%20METHODS%20AND%20HARDWARE%20FOR%20DEEP%20LEARNING-augmented.pdf](https://stacks.stanford.edu/file/druid:qf934gh3708/EFFICIENT METHODS AND HARDWARE FOR DEEP LEARNING-augmented.pdf">EFFICIENT METHODS AND HARDWARE FOR DEEP LEARNING</a>)</p><p><a href="https://songhan.github.io/DSD/">DSD (Dense-Sparse-Dense Training) Model Zoo</a></p><h2 id="2017-Invited-Talk-Adversarial-Examples-and-Adversarial-Training-Ian-Goodfellow"><a href="#2017-Invited-Talk-Adversarial-Examples-and-Adversarial-Training-Ian-Goodfellow" class="headerlink" title="2017 Invited Talk-Adversarial Examples and Adversarial Training, Ian Goodfellow"></a>2017 Invited Talk-Adversarial Examples and Adversarial Training, Ian Goodfellow</h2><p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture16.pdf">课件</a></p><p>Ian Goodfellow的语速很慢，然而我整节课却没怎么听懂他在讲什么。</p><ul><li><p>知乎文章<a href="https://zhuanlan.zhihu.com/p/42667844">对抗样本Adversarial Examples</a></p></li><li><p>知乎问题<a href="https://www.zhihu.com/question/49129585">如何看待机器视觉的“对抗样本”问题，其原理是什么？</a></p></li></ul><h2 id="Lecture-17-Human-Centered-AI"><a href="#Lecture-17-Human-Centered-AI" class="headerlink" title="Lecture 17. Human-Centered AI"></a>Lecture 17. Human-Centered AI</h2><p>李飞飞最后的总结性课程，描绘AI对社会和人类的影响和蓝图，提出HAI的概念。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/17_1.JPG" alt="HAI"></p><h2 id="Lecture-18-Scene-Graphs"><a href="#Lecture-18-Scene-Graphs" class="headerlink" title="Lecture 18. Scene Graphs"></a>Lecture 18. Scene Graphs</h2><p>2020版新增了Scene Graphs and Graph Convolutions，<a href="http://cs231n.stanford.edu/slides/2020/lecture_18.pdf">slides</a>.</p><p>这一部分应该是让计算机去更好的理解图片。分类，检测都是认识图片的一种手段，不是目的。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/18_1.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/18_2.JPG" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/18_3.JPG" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.stanford.edu/&quot;&gt;homepage&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/av13260183?from=search&amp;amp;seid=2308745029556209710&quot;&gt;2017 lecture video&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21930884&quot;&gt;中文笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;课程相关时间安排，课件，参考资料，作业详见homepage链接，以最新版为主；&lt;/li&gt;
&lt;li&gt;官方的assignments和notes做得非常好，强烈推荐学习和反复观看；&lt;/li&gt;
&lt;li&gt;时间原因，学习19版的时候恰好2020版春季课程将要开始，这一版换了team，里面加了不少新的research介绍，尤其是非监督，自监督之类的，因此会加入一下2020版的内容；&lt;/li&gt;
&lt;li&gt;个人比较喜欢Justin Johnson的讲课风格和深度，因此配合他在UMICH (University of Michigan) 的 &lt;a href=&quot;https://web.eecs.umich.edu/~justincj/teaching/eecs498/&quot;&gt;EECS498: Deep Learning for Computer Vision&lt;/a&gt;课一起看，可以相互补充（和CS231N有很大重叠， 但也些不同）；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/notesofCS231N/summary.JPG&quot; alt=&quot;提前把最后一节课的总结放出来。整个课程将按照这个fundamentals-CNN-applications来讲&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Lecture1-Introduction&quot;&gt;&lt;a href=&quot;#Lecture1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;Lecture1. Introduction&quot;&gt;&lt;/a&gt;Lecture1. Introduction&lt;/h2&gt;&lt;p&gt;Related courses in Stanford:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CS131: Computer Vision: Foundations and Applications&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CS231a: Computer Vision, from 3D Reconstruction to Recognition&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CS 224n: Natural Language Processing with Deep Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CS 230: Deep Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Andrew Ng的CS229虽然不在列表中，但个人觉得也值得一看，毕竟也属于经典ML课程。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A  Brief history of Computer Vision and CS 231n:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从史前生物产生视觉开始，到后面的人类对于视觉的研究：相机，生物学研究，以及陆续地对于计算机视觉/机器视觉的系统研究，在社会各个领域结合产生的特定任务的建模设计等工作，这里再一次提到了David Marr的《vision》一书对整个计算机视觉领域的奠基于推动作用。&lt;/li&gt;
&lt;li&gt;介绍卷积神经网络CNN对计算视觉的推动作用，简介了CNN的历史以及在各个视觉有关问题上的强大作用。&lt;/li&gt;
&lt;li&gt;推荐课本教材：《Deep Learning》 by Goodfellow, Bengio, and Courville.&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="课程记录" scheme="http://densecollections.top/categories/%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="public course" scheme="http://densecollections.top/tags/public-course/"/>
    
    <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
    <category term="Standford" scheme="http://densecollections.top/tags/Standford/"/>
    
  </entry>
  
  <entry>
    <title>旷视2017年深度学习实践课程</title>
    <link href="http://densecollections.top/posts/megviidlcourse/"/>
    <id>http://densecollections.top/posts/megviidlcourse/</id>
    <published>2020-02-25T03:57:44.000Z</published>
    <updated>2021-01-02T12:57:44.770Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><ul><li><a href="https://www.bilibili.com/video/av88056282/">视频</a>;</li><li>课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取;</li><li>这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好。</li></ul><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul><li>computer vision 在AI中的地位属于感知（perception）智能（还包括speech），另外一块是认知（cognitive），包括NLP和AGI（通用人工智能）；</li><li>人使用眼睛和大脑认识世界，电脑使用图像传感器和算力来视觉感知周围环境；</li><li>大脑皮层的出现，灵活，结构化，计算处理；</li><li>CV和AI的关系：其中非常重要的一个task/不同的研究工作和成果/作为关键应用；</li><li>现阶段的CV任务：classification (image)/ detection (region) /segmentation (pixel，实例，语义，全景)/ sequence (video，spatial+temporal)；</li><li>David Marr 的《vision》一书，这在visual SLAM中也十分重要，视觉知识的表示，part representation (拆成块，用各种模型表示，举例关键点检测);</li><li>part representation存在局限，有些不可分，引发了神经网络第二次复兴，Yann Lecun 的卷积神经网络应用于手写字体识别和人脸检测。由于当时难以复现，且懂的人不多，加上小规模数据和SVM等模型流行，神经网络出现衰落；</li><li>learning-based representation/ feature-based representation，特征工程+分类器（handcraft features engineering+SVM/Random Forest），浅层学习pipeline a short sequence；</li><li>端到端学习，所有参数联合优化 ，a long or very long sequence实现高维非线性映射；</li><li>受感知机启发的多层感知机（multilayer perceptron，MLP），利用backpropagation (BP) 梯度训练逼近（局部最优解）任意非线性函数；</li><li>90年代的神经网络成果：CNN/ autocoder/ boltzmann machine/ belief nets/ RNN；</li><li>复兴：data+computing+industry competition+a few breakthrough；</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/a_brief_history.JPG" alt="深度学习沉浮历史"></p><ul><li><p>Resnet的思想：由浅到深学习，保持梯度数值较大，防止梯度消失 ；</p></li><li><p>从以前的手工设计feature为重点到现在设计网络结构（2012-2017为止）为重点，不同的结构所需算力不同，现在轻量级网络是一个热点；</p></li><li>卷积核的方式：1x1，3x3，<a href="https://blog.csdn.net/tintinetmilou/article/details/81607721">depthwise 3x3</a>等，网络layer连接的方式；</li></ul><h2 id="2-Math-in-DL-and-ML-Basics"><a href="#2-Math-in-DL-and-ML-Basics" class="headerlink" title="2. Math in DL and ML Basics"></a>2. Math in DL and ML Basics</h2><ul><li><p>深度学习的内涵：deep learning) representation learning) machine learning) AI;</p></li><li><p>Linear Algebra：</p><ul><li><p>向量，矩阵，集合，群，封闭性，矩阵乘法是为了表示一种变换关系，向量映射到另一个向量；</p><ul><li>方阵，正交矩阵，特征值，特征向量，实对称矩阵，二次型，正定矩阵，半正定矩阵，奇异值分解；</li></ul></li></ul></li><li><p>Probability：</p><ul><li><p>随机事件，随机变量，概率密度函数，联合分布，边缘分布，条件分布，独立变量；</p></li><li><p>贝叶斯法则，先验分布，后验分布，期望，方差，协方差矩阵（半正定）；</p></li><li><p>常见分布：二值分布，二项分布，多值/多相分布（图像分类问题），正态分布（高斯分布）；</p></li><li><p>信息熵（分类中的交叉熵损失函数，发生概率越大的事情信息越不值钱），交叉熵和KL-divergence，生成式模型中的wassertein distance；</p></li></ul></li><li><p>Optimize：</p><ul><li>minimization（最小化）— 梯度下降gradient descent（步长的选取很关键），stochastic gradient descent;</li></ul></li><li><p>机器学习基本知识（machine learning basics）— 定义，假设，模型，评估，supervised &amp; unsupervised learning (learning $p(y | x) \quad or \quad p(x,y)$，判别式模型，生成式模型&lt;目前都用判别式模型&gt;, learning $p(x)$，auto encoder，GAN)，“no free lunch theorem”（all learning algorithms are equal, but some algorithm are more equal than others），overfitting &amp; underfitting，model capacity vs. generalization error，regularization (正则项，数据增强，parameter reduce and tying);</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/supervised&amp;unsupervisedlearning.JPG" alt></p><h2 id="3-Neural-Networks-Basics-amp-Architecture-Design"><a href="#3-Neural-Networks-Basics-amp-Architecture-Design" class="headerlink" title="3. Neural Networks Basics &amp; Architecture Design"></a>3. Neural Networks Basics &amp; Architecture Design</h2><ul><li><p>Fundamental task in CV: classification, object detection, semantic segmentation, instance segmentation, keypoint detection, human pose estimation, VQA…</p></li><li><p>计算机识别图像的难点：图像内容的复杂性和多样性，比如姿势，光照，模糊等；</p></li><li><p>特征是计算机认识图像的一个灯塔，且应当使用非线性特征抽取器；</p></li><li><p>线性组合特征（kernel learning，boosting），缺点是需要大量的templates，对特征的利用性差；</p></li><li><p>特征层级组合，重复利用特征，更为高效 —-&gt; concepts reuse in DL，网络层级的特征也是由低到高，但是这样高度非线性的函数难以优化（目前采用收敛到局部最优值）；</p></li><li><p>key ideas of DL: nolinear system, learn it from data, feature hierarchies, end-to-end learning；</p></li><li><p>激活函数，神经元，全连接网络，训练决定网络参数（前向，反向，更新）；</p></li><li><p>针对图像的认识从locally-connected net到convolutional net的设计，参数共享；</p></li><li><p>卷积层的卷积操作，pooling layer等；</p></li><li>网络结构设计：网络拓扑结构，layer function，超参，优化算法等经验性的东西，手动/autoML；</li><li>简介AlexNet（包含LRN，加速收敛），VGG（发掘3x3小卷积核的显著作用，但并不代表最高效的做法），GoogleNet，ResNet（拟合残差而不是直接拟合原函数），Xception，ResNeXt (借鉴Xception在resnet基础改进)，ShuffleNet，DesneNet，SqueezeNet；</li><li>structure design: deeper and wider, ease of optimization, multi-path design, resdiual path, sparse connection；</li><li>简介部分layer design：SPP，batch normalization，parametric rectifiers，bilinear CNNs（做细粒度分类）；</li><li>针对特定任务的结构设计：Deepface (人脸识别)，Global Convolutional Networks (语义分割)，Hourglass Networks (沙漏结构，大的感受野，用于pose estimation或者关键点)；</li></ul><h2 id="4-Introduction-to-Computation-Technologies-in-Deep-Learning"><a href="#4-Introduction-to-Computation-Technologies-in-Deep-Learning" class="headerlink" title="4. Introduction to Computation Technologies in Deep Learning"></a>4. Introduction to Computation Technologies in Deep Learning</h2><p>该节课偏底层，听的不是很懂，权当了解。</p><ul><li><p>symbolic computation：</p><ul><li><p>深度学习框架overview—program, compilation, runtime mangement, kernels, hardware；</p></li><li><p>computing graph, graph structure—variable, operator, edge；</p></li><li><p>静态图和动态图； </p></li><li><p>执行和优化；</p></li></ul></li><li><p>dense numerical computation:</p><ul><li><p>CPU computation (机器码，流水线，超流水线，超标量，乱序执行/cache hierarchy/…)；</p></li><li><p>other computation devices (NVIDIA GPU&lt;单指令，多线程架构&gt;，Google TPU，Huawei NPU in Kirin 970，Mobile CPU+GPU+DSP)；</p></li><li><p>computation &amp; memory gap；</p></li></ul></li><li><p>distributed computation:</p><ul><li><p>system (communication，Remote Direct Memory Access);</p></li><li><p>optimization algorithm (synchronous SGD，asynchronous SGD)；</p></li><li><p>communication algorithm (MPI Primitives，An AllReduce Algorithm)；</p></li></ul></li></ul><h2 id="5-Neural-Network-Approximation-low-rank-sparsity-and-quantization"><a href="#5-Neural-Network-Approximation-low-rank-sparsity-and-quantization" class="headerlink" title="5. Neural Network Approximation(low rank, sparsity, and quantization)"></a>5. Neural Network Approximation(low rank, sparsity, and quantization)</h2><p>该节课着重神经网络压缩，for faster training，faster inference， smaller capacity;</p><p>convolution as matrix product，利用近似权重矩阵达到网络压缩的目的;</p><p>Low Rank (本质是对矩阵进行一系列分解变换近似操作，减小计算量和存储量):</p><ul><li><p>对权重矩阵进行奇异值分解，singular value decomposition；</p></li><li><p>SVD+Kronecker Product ——&gt; KSPD；</p></li><li><p>矩阵分解：C-HW-K====》C-HW-R-(1X1)-K，然后通过reshape进行重新分解，目前horizontal-vertical decomposition最好；</p></li><li><p>shared group convolution is a kronrcker layer；</p></li><li><p>CP-decomposition与depthwise；</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/CNN_decomposition.JPG" alt></p><p>Sparse Approximation:</p><ul><li><p>权重分布有点类似高斯分布 ，0附近很多，微调网络，weight pruning: 韩松博士的deepcompression，让为0的权重逐渐增多（掩模矩阵使权重为0），不让0附近的权重在训练时抖动，FC层效果压缩明显；</p></li><li><p>网络加速计算—稀疏矩阵计算，channel purning，sparse communication for distributed gradient descent；</p></li></ul><p>Quantization：</p><ul><li><p>用什么精度算；</p></li><li><p>参数的量化，激活的量化，梯度的量化；</p></li><li><p>二值化，binary network；</p></li><li><p>大容量模型利用小bit训练时掉点不明显，小容量模型视情况而定；</p></li></ul><p>主讲人<a href="http://zsc.github.io/">周舒畅</a>推荐的几篇文章，其中XNOR-Net为课程阅读要求材料：</p><blockquote><p>Bit Neural Network<br>● Matthieu Courbariaux et al. BinaryConnect: Training Deep Neural Networks with binary<br>weights during propagations. <a href="http://arxiv.org/abs/1511.00363">http://arxiv.org/abs/1511.00363</a>.<br>● Itay Hubara et al. Binarized Neural Networks <a href="https://arxiv.org/abs/1602.02505v3">https://arxiv.org/abs/1602.02505v3</a>.<br>● Matthieu Courbariaux et al. Binarized Neural Networks: Training Neural Networks with<br>Weights and Activations Constrained to +1 or -1. <a href="http://arxiv.org/pdf/1602.02830v3.pdf">http://arxiv.org/pdf/1602.02830v3.pdf</a>.<br>● Rastegari et al. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural<br>Networks <a href="http://arxiv.org/pdf/1603.05279v1.pdf">http://arxiv.org/pdf/1603.05279v1.pdf</a>.<br>● Zhou et al. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with<br>Low Bitwidth Gradients <a href="https://arxiv.org/abs/1606.06160">https://arxiv.org/abs/1606.06160</a>.<br>● Hubara et al. Quantized Neural Networks: Training Neural Networks with Low Precision<br>Weights and Activations <a href="https://arxiv.org/abs/1609.07061">https://arxiv.org/abs/1609.07061</a>.</p></blockquote><h2 id="6-Modern-Object-Detection"><a href="#6-Modern-Object-Detection" class="headerlink" title="6. Modern Object Detection"></a>6. Modern Object Detection</h2><p><a href="https://zhuanlan.zhihu.com/p/62372897">anchor-free与anchor-based的交替轮回</a>;</p><p>Representation:</p><ul><li><p>Bounding-box: face detection, human detection, vehicle detection, text detection, general object detection;</p></li><li><p>Point: semantic segmentation (下一节课);</p></li><li><p>Keypoint: face landmark, human keypoint;</p></li></ul><p>Evaluation Criteria：</p><ul><li>precision (预测为真的里面真正是真的比例), recall (所有是真的里面预测为真的比例), Average Prcision (AP), mean AP (mAP)，IoU, mmAP(coco);</li></ul><p>Perform a detection:</p><ul><li><p>之前是手工特征+图像金字塔+滑动窗口+分类器（robust real-time detection; IJCV 2001）；</p></li><li><p>通过Fully Convolutional Network进行计算共享;</p></li></ul><p>Deep Learning for Object Detetcion:</p><ul><li><p>proposal and refine;</p></li><li><p>one stage:</p><ul><li><p>example: Densebox, YOLO, SSD, Retina Net…</p></li><li><p>keyword: anchor, divide and conquer, loss sampling;</p></li></ul></li><li><p>two stage:</p><ul><li>example: Faster R-CNN, RFCN, FPN, Mask R-CNN;</li></ul></li><li>keyword: speed, performance;</li></ul><p>One Stage:</p><p><strong>Densebox:</strong></p><ul><li><p>流程：图—&gt;图像金字塔—&gt;卷积神经网络—&gt;upsampling—&gt;卷积神经网络—&gt;（4+1）通道—&gt;预测+threshold+NMS；</p></li><li><p>输入：$m \times n \times 3$，输出：$m/4 \times n/4 \times 5$；</p></li><li><p>输出的feature map每个像素对应一个带分数的边框：</p></li></ul><p>$t_{i}=\left\{s_{i}, d x^{t}=x_{i}-x_{t}, d y^{t}=y_{i}-y_{t}, d x^{b}=x_{i}-x_{b}, d y^{b}=y_{i}-y_{b},\right\}$</p><p>其中t和b分别代表左上角和右下角坐标；</p><ul><li>问题：回归的L2损失函数选的不好（不同程度scale的object学习程度不同），GT assignment也存在问题，object比较拥挤的情况下，多个物体可能缩小在最后特征图上的一个点上，FP比较多，回归变量选取问题，误差较大；</li></ul><p>UnitBox：</p><ul><li>把L2 loss换成IoU loss = $-\ln IoU$;</li></ul><p>YOLO：</p><ul><li>$7 \times 7$的grid，加了fc层可以覆盖到一些更全局的context，但是受限于固定输入尺寸，运行速度虽快但是拥挤场景检测不是很work；</li></ul><p><strong>SSD</strong>：</p><ul><li><p>引入不同scale和aspect ratio的anchor；</p></li><li><p>回归GT与anchor的offset；</p></li><li>不同layer检测不同尺寸的物体，小物体浅层出，大物体深层出（但是并没有直接证据证明此法可靠）；</li><li>loss sampling和OHEM；</li><li><a href="https://blog.csdn.net/qq_30815237/article/details/90292639">blog</a>；</li></ul><p>DSSD:</p><ul><li><p>SSD利用浅层检测小目标，但是浅层语义信息少；</p></li><li><p>利用upsampling和融合加强语义信息；</p></li></ul><p>RON:</p><ul><li><p>reverse connect (similar to FPN)；</p></li><li><p>loss sampling: objectness prior (先做二分类在再细分)；</p></li></ul><p>RetainaNet:</p><ul><li><p>引入Focal loss；</p></li><li><p>FPN结构；</p></li></ul><p>One Stage Detector: Summary</p><ul><li><p>Anchor：</p><ul><li><p>No anchor: YOLO, densebox/unitbox/east；</p></li><li><p>Anchor: YOLOv2, SSD, DSSD, RON, RetinaNet；</p></li></ul></li><li><p>Divide and conquer：</p><ul><li>SSD, DSSD, RON, RetinaNet；</li></ul></li><li>loss sample：<ul><li>all sample: densebox；</li><li>OHEM: SSD；</li><li>focal loss: RetinaNet；</li></ul></li></ul><p>Two Stage:</p><p>RCNN:</p><ul><li>selective search+分类proposal；</li></ul><p>Fast RCNN:</p><ul><li>selective search对应到特征图，通过RoI pooling去分类；</li></ul><p>Faster RCNN:</p><ul><li>用预设的anchor去找proposal；</li></ul><p>RFCN，Deformable Convolutional Networks，FPN，Mask RCNN…</p><p>Two Stages Detector-Summary:</p><ul><li>Speed：<ul><li>RCNN -&gt; Fast RCNN -&gt; Faster RCNN -&gt; RFCN；</li></ul></li><li>Performance：<ul><li>Divide and conquer：<ul><li>FPN；</li></ul></li><li>Deformable Pool/ROIAlign；</li><li>Deformable Conv；</li><li>Multi-task learning；</li></ul></li></ul><p>Open Problem in Detection：</p><ul><li>FP；</li><li>NMS (detection in crowd)；</li><li>GT assignment issue；</li><li>Detection in video：<ul><li>detect &amp; track in a network；</li></ul></li></ul><p>Human Keypoint Task:</p><ul><li><p>Single Person Skeleton：</p><ul><li><p>CPM；</p></li><li><p>Hourglass；</p></li></ul></li><li><p>Multiple-Person Skeleton：</p><ul><li><p>top down: </p><ul><li><p>detect-&gt;single person skeleton；</p></li><li><p>Depends on the detector：</p><ul><li><p>Fail in the crowd case；</p></li><li><p>Fail with partial observation；</p></li><li>can detect the small-scale human；</li></ul></li><li><p>More computation；</p></li><li><p>Better localization when the input-size of single person skeleton is large；</p></li></ul></li><li><p>bottom up: </p><ul><li>Deep/Deeper cut, OpenPose, Associative Embedding；</li><li>Fast computational speed；</li><li>good at localizing the human with partial observation；</li><li>Hard to assemble human；</li></ul></li></ul></li></ul><h2 id="7-Scene-Text-Detection-and-Recognition"><a href="#7-Scene-Text-Detection-and-Recognition" class="headerlink" title="7. Scene Text Detection and Recognition"></a>7. Scene Text Detection and Recognition</h2><p>Background:</p><ul><li><p>文字的重要性：文明标志，携带高层语义信息，作为visual recognition的线索；</p></li><li><p>problem: scene text detection+scene text recognition；</p></li><li><p>challenge: 比OCR更复杂，比如背景，颜色，字体，方向，文字混杂等；</p></li><li><p>application: card recognition，图片定位，产品搜索，自动驾驶，工业自动化等；</p></li></ul><p>conventional methods：</p><ul><li><p>detection before deep learning: MSER (maximally stable extremal regions)，SWT (stroke width transform)，Multi-Oriented；</p></li><li><p>recognition: Top-down and bottom-up cues（滑窗+统计特性），Tree-structured Model (DPM+CRF)，Label embedding (另辟蹊)；</p></li><li><p>统一检测和识别：Lexicon Driven；</p></li></ul><p>Deep learning methods：</p><p>包含传统辅助方法的：</p><ul><li><p>end-to-end-recognition: PhotoOCR，Deep Features，Reading Text；</p></li><li><p>detection: MSER Trees；</p></li></ul><p>不包含传统辅助方法的：</p><ul><li>detection: Holistic (当作语义分割来做)，<a href="https://github.com/argman/EAST">EAST</a> (旷视CVPR2017，多任务学习)，Deep Direct Regression (与EAST相似)，SegLink (多尺度特征图)，Synthetic Data (在图片上产生文字)；</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/EAST.JPG" alt="EAST框架"></p><ul><li><p>recognition：$R^2AM$ (递归循环神经网络+soft-attention)，Visual Attention；</p></li><li><p>end-to-end recognition：<a href="https://github.com/MichalBusta/DeepTextSpotter">Deep TextSpotter</a>；</p></li><li><p>summary: ideas from object detection and segmentation，end-to-end，use synthetic data；</p></li></ul><p>datasets and competitions：</p><ul><li>dataset: ICDAR 2103, MARA-TD500, ICDAR 2015, IIIT 5K-Word, COCO-Text, MLT, Total-Text；</li></ul><p>conclusion:</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/7_conclusion.JPG" alt></p><p>challenges:</p><ul><li>Diversity of text: language, font, scale, orientation, arrangement, etc；</li><li>Complexity of background: virtually indistinguishable elements (signs, fences, bricks and grasses, etc.)；</li><li><p>Interferences: noise, blur, distortion, low resolution, nonuniform illumination, partial occlusion, etc；</p><p>Trends:</p></li><li><p>Stronger models (accuracy, efficiency, <strong>interpretability</strong>)；</p></li><li>Data synthesis；</li><li>Muiti-oriented text；</li><li>Curved text；</li><li>Muiti-language text；</li></ul><p>References:</p><ul><li><p>Survey:</p><ul><li>Ye et al.. Text Detection and Recognition in Imagery: A Survey. TPAMI, 2015.</li><li>Zhu et al.. Scene Text Detection and Recognition: Recent Advances and Future Trends. FCS, 2015.</li></ul></li><li><p>Conventional Methods:</p><ul><li>Epshtein et al.. Detecting Text in Natural Scenes with Stroke Width Transform. CVPR, 2010.</li><li>Neumann et al.. A method for text localization and recognition in real-world images. ACCV, 2010.</li><li>Yao et al.. Detecting Texts of Arbitrary Orientations in Natural Images. CVPR, 2012.</li><li>Wang et al.. End-to-End Scene Text Recognition. ICCV, 2011.</li><li>Mishra et al.. Scene Text Recognition using Higher Order Language Priors. BMVC, 2012.</li><li>Busta et al.. FASText: Efficient Unconstrained Scene Text Detector. ICCV 2015.</li></ul></li><li><p>Deep Learning Methods:</p><ul><li>Bissacco et al.. PhotoOCR: Reading Text in Uncontrolled Conditions. ICCV, 2013.</li><li>Jaderberg et al.. Deep Features for Text Spotting. ECCV, 2014.</li><li>Gupta et al.. Synthetic Data for Text Localisation in Natural Images. CVPR, 2016.</li><li>Zhou et al.. EAST: An Efficient and Accurate Scene Text Detector. CVPR, 2017.</li><li>Busta et al.. Deep TextSpotter: An End-To-End Trainable Scene Text Localization and Recognition Framework. ICCV, 2017.</li><li>Ghosh et al.. Visual attention models for scene text recognition. 2017. arXiv:1706.01487.</li><li>Cheng et al.. Focusing Attention: Towards Accurate Text Recognition in Natural Images. ICCV, 2017.</li></ul></li></ul><p>Useful Resources:</p><ul><li>Laboratories and Papers<br><a href="https://github.com/chongyangtao/Awesome-Scene-Text-Recognition">https://github.com/chongyangtao/Awesome-Scene-Text-Recognition</a></li><li>Datasets and Codes<br><a href="https://github.com/seungwooYoo/Curated-scene-text-recognition-analysis">https://github.com/seungwooYoo/Curated-scene-text-recognition-analysis</a></li><li>Projects and Products<br><a href="https://github.com/wanghaisheng/awesome-ocr">https://github.com/wanghaisheng/awesome-ocr</a></li></ul><h2 id="8-Image-Segmentation"><a href="#8-Image-Segmentation" class="headerlink" title="8. Image Segmentation"></a>8. Image Segmentation</h2><p>semantic segmentaion, instace segmentation, scene parsing,  human parsing, stuff segmentation, UlrtraSound segmentation, selfie segmentation…</p><p>评价指标：</p><script type="math/tex; mode=display">\begin{array}{l}{\operatorname{Accuracy}(\mathbf{y}, \hat{\mathbf{y}})=\sum_{i=0}^{n} \frac{I\left[y_{i}=\hat{y}_{i}\right]}{n}} \\{\operatorname{mean} I O U(\mathbf{y}, \hat{\mathbf{y}})=\frac{\sum_{c}^{m} \sum_{i}^{n} I\left[y_{i}=c, \hat{y}_{i}=c\right]}{\sum_{c}^{m} \sum_{i}^{n} I\left[y_{i}=c \text { or } \hat{y}=c\right]}}\end{array}</script><p>Semantic Segmantation:</p><ul><li><p>FCN: 第一篇语义分割工作；</p></li><li><p>Learning Deconvolution Network for Semantic Segmentation，引入unpool和反卷积deconvolution；</p></li><li><p>DeepLab，引入空洞卷积<a href="https://www.jianshu.com/p/f743bd9041b3">dilated-convolution</a>和DenseCRF；</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/CRF.JPG" alt></p><ul><li><p>CRF AS RNN;</p></li><li><p>Deeplab Attention;</p></li><li><p>PSPNet;</p></li><li><p>GCN (Global Convolutional Network，主讲人的工作，想要框住任意尺度的物体);</p></li><li><p>Deeplab V3;</p></li><li><p><strong>Deformable Convolution</strong>;</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/deformable_convolution.JPG" alt="deformable deconvolution"></p><p>Instance Segmentation:</p><p>Top-down pipeline (目前主流，依赖detection框架)：</p><ul><li><p>先detection再segmentation</p></li><li><p>FCIS (框得不准，但是分割依然准)；</p></li><li><p>Mask RCNN;</p></li></ul><p>Bottom-up pipeline (效果差，难实现，思考空间大):</p><ul><li><p>不出框分割;</p></li><li><p>Semantic instance segmentation via metric learning;</p></li></ul><hr><p>介绍旷视的框架：</p><ul><li>batch size in training:</li></ul><p>​       detection得batch size往往比分类小很多，主要是训练尺寸不同，另外可能一张图片会有很多proposal…</p><p>​       小batch size 会导致：unstable gradient，inaccurate BN statistics， extremely imbalanced data, very    </p><p>​       long training period…</p><ul><li><p>Multi-device BatchNorm;</p></li><li><p>Sublinear Memory;</p></li><li><p>Large Learning Rate;</p></li><li><p>打COCO instance segmentation比赛的一些tricks: precise RoI pooling, context extractor, mask generator；</p></li><li><p>keypoint比赛tricks；</p></li></ul><h2 id="9-Recurrent-Neural-Network"><a href="#9-Recurrent-Neural-Network" class="headerlink" title="9. Recurrent Neural Network"></a>9. Recurrent Neural Network</h2><p>RNN Bascis:</p><ul><li><p><a href="https://plato.stanford.edu/entries/turing-machine/">Turning Machine</a>, RNN is Turing Complete, Sequence Modeling；</p></li><li><p>RNN Diagram,$(h_{i}, y_{i}) = F(h_{i-1},x_{i},W)$ ；</p></li><li><p>根据input/output分类：many-to-many,  many-to-one, one-to-many, many-to-one+one-to-many;</p></li><li><p>many-to-many example: language model (predict next word by given previous words, tell story, write books in LaTex…);</p></li><li><p>many-to-one example: Sentiment analysis…</p></li><li><p>many-to-one+one_to_many exapmle: Neural Machine Translation (encoder+decoder)…</p></li><li><p>训练RNN，梯度爆炸和梯度消失: singular value &gt; 1 =&gt; explodes, singular value &lt; 1 =&gt; vanishes… LSTM (Long short-term memory) come to the resuce;</p></li><li><p>why LSTM works (input gate, forget gate, output gate, temp variable, memory cell); </p></li><li><p>GRU (similar to LSTM, let information flow without a separate memory cell);</p></li><li><p>Search for better RNN architecture;</p></li></ul><p>Simple RNN Extentsions:</p><ul><li><p>Bidirectional RNN (BDRNN)，预测未来；</p></li><li><p>2D-RNN: Pixel-RNN, each pixel depends on its top and left neighbor  (补图，segmentation);</p></li><li><p>Deep RNN (stack more of them, harder to train); </p></li></ul><p>RNN with Attention:</p><ul><li><p>attention: differentiate entities by its importance, spatial attention is related to location;  temporal attention is related to causality;</p></li><li><p>attention over input sequence: Neural Machine Translation (NMT);</p></li><li><p>Image Attention: Image Captioning (input image—&gt; Convolutional feature extraction—&gt;RNN with attention over the image—&gt;Word by word generation);</p></li></ul><p>RNN with External Memory:</p><ul><li>copy a sequence: Neural Turning Machines (NTM);</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/NTM.JPG" alt></p><p>More Applications:</p><ul><li><p>RNN without a sequence input: read house numbers from left to right, generate images of digits by learning to sequentially add color to canvas;</p></li><li><p>generalizing recurrence (a computation unit with shared parameter occurs at multiple places in the computation graph);</p></li><li><p>apply when there’s tree structure in data;</p></li><li><p>bottom-up aggregation of information;</p></li><li><p>speech recognition;</p></li><li><p>generating sequence;</p></li><li><p>question answering;</p></li><li><p>visual question answering;</p></li><li><p>combinatorial problems;</p></li><li><p>learning to excute;</p></li><li><p>compress image;</p></li><li><p>model architecture search;</p></li><li><p>meta-learning;</p></li></ul><p>…</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/RNN_pros_cons.JPG" alt></p><p>RNN’s RIval:</p><ul><li><p>WaveNet: causal dilated convolution,  Oord, Aaron van den, et al. “Wavenet: A generative model for raw audio.” arXiv preprint arXiv:1609.03499 (2016).</p></li><li><p>Attention is All You Need (Transformer) ; </p></li></ul><h2 id="10-Introduction-to-Generative-Models-and-GANs"><a href="#10-Introduction-to-Generative-Models-and-GANs" class="headerlink" title="10. Introduction to Generative Models (and GANs)"></a>10. Introduction to Generative Models (and GANs)</h2><p>Basics:</p><ul><li><p>Generative Models: Learning the distributions;</p></li><li><p>Discriminative: learn the likelihood;</p></li><li><p>Generative: performs Density Estimation (learns the distribution) to allow sampling;</p></li><li><p>回归建模的话会取平均值，回归的是最可能情况的平均值，显得不真实，a driscrminative model just smoothes all possibilities, ambiguity and “blur” effect;</p></li><li><p>application of generative models: image generation from sketch, interactive editing, image to image translation;</p></li></ul><p>How to train generative models:</p><ul><li>给出一系列样本点，模型生成符合预期分布的输出；</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/taxonomy_of_generative_models.JPG" alt="从左往右方法逐渐work"></p><ul><li><p>exact model: NVP (non-volume preserving), real NVP: invertible no-linear transforms, 理论要求过于严格（Restriction on the source domain: must be of the <strong>same</strong> as the target.），效果不好（人脸稍微好点，因为其structure比较规矩）；</p></li><li><p>Variational Auto-Encoder (VAE): encoder 做density estimation的过程， decoder做sampling的过程。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/VAE.JPG" alt></p><ul><li><p>Generative Adversarial Networks (GAN): 生成器和判别器相互学习进步，交替训练；</p></li><li><p>DCGAN: example of feature manipulation （人脸加眼镜，变性别之类的的操作）；</p></li><li><p>conditional, cross-domain generation (genenative adversarial text to image synthesis);</p></li><li><p>GAN training problems: unstable losses（训练时应该G和D应该处于动态平衡）, mini-batch fluctuation （每个batch之间生成的图像不同），model collapse (lack of diversity in generated results);</p></li><li><p>improve GAN training: label smoothing, <strong>Wasserstein GAN (WGAN)</strong> (stabilized taining curve, non-vanishing gradient), loss sensitive GAN (LS-GAN)… <a href="https://github.com/hindupuravinash/the-gan-zoo">The GAN Zoo</a>;</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/WGAN.JPG" alt></p><p>举一些有名的GAN例子：</p><ul><li><p>zhu junyan—-Cycle GAN :correspondence from unpaired data;</p></li><li><p>DiscoGAN: cross-domain relation;</p></li><li><p>GeneGAN: shorter pathway improves training  (cross breeds and reproductions, 生成笑容)，object transfiguration (变发型)，interpolation in object subspace (改变发型方向)；</p></li></ul><p>Math behind Generative Models:</p><ul><li><p>formulation: sampling vs. density estimation;</p></li><li><p>RBM  (现在已经不怎么使用)；</p></li><li><p>from density to sample: 给定概率密度方程，无法有效采样；</p></li><li><p>from sample to density: 给定black-box sampler，是否可以估计概率密度（频率）；</p></li></ul><p>​        Given samples, some properties of the distribution can be learned, while others cannot.</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/game_G_D.JPG" alt></p><ul><li><p>the future of GANs: guaranteed stabilization (new distance), broader application (apply adversarial loss in xx/ different type of data);</p></li><li><p>GAN tutorial from Ian Goodfellow: <a href="https://arxiv.org/abs/1701.00160">https://arxiv.org/abs/1701.00160</a>;</p></li></ul><h2 id="11-Persom-Re-Identification"><a href="#11-Persom-Re-Identification" class="headerlink" title="11. Persom Re-Identification"></a>11. Persom Re-Identification</h2><p>ReID: from face to person;</p><ul><li><p>face recognition (verification, size: $32 \times 32$, horizontal: -30~30, vertical: -20~20, little occlusion);</p></li><li><p>person Re-Identification (trcaking in cameras, searching person in videos, clustering person in photos, challenges: inaccurate detection, misalignment, illumination difference, occlusion…);</p></li><li><p>common in FR &amp; ReID: deep metric learning, mutual learning, re-ranking;</p></li><li><p>special in ReID: feature alignment, ReID with pose estimation, ReID with human attributes;</p></li></ul><p>from classification to metric learning: </p><ul><li><p>classification network只能辨别那些“见过的”物体，没见过的物体就要重训练，对于人脸识别部署来说，不现实。为了克服这点，加入metric learning，拿pre-train过的classification网络在metric learning中finetune (similar feature);</p></li><li><p>有些工作是fusing intermediate feature maps, 但是计算量和存储都加大，拖慢了速度，不实用；</p></li></ul><p>Metric Learning: </p><ul><li><p>Learn a function that measures how similar two objects are. Compared to classification which works in a closed-word, metric learning deals with an open-world.</p></li><li><p>contrastive loss: $L_{\text {pairwise}}=\delta\left(I_{A}, I_{B}\right) \cdot\left|f_{A}-f_{B}\right|_{2}+\left(1-\delta\left(I_{A}, I_{B}\right)\right)\left(\alpha-\left|f_{A}-f_{B}\right|_{2}\right)_{+}$ （最后一项有focus困难样本的作用，$\delta$ is Kronecker Delta，$\alpha$ is the margin for different identities），让有相同identity的图像距离变小，反之变大，$\alpha$被用来略掉那些“naive”的negative pairs；</p></li><li><p>triplet loss: $L_{t r p}=\frac{1}{N}  \sum \limits ^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha\right)_{+}$ (The distance of A and A’ should be smaller than that of A and B. $\alpha$ is the margin between negative and positive pairs. Without  $\alpha$,  all distance converge to zero.);</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/closs_Tloss.JPG" alt></p><ul><li><p>improved triplet loss: $ L_{i m t r p}=\frac{1}{N} \sum^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha\right)_{+} +\frac{1}{N} \sum^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\beta\right)_{+} $ ($\beta$ penalizes distance between features of $A$ and $A^{\prime}$), only consider image pairs with the same identity;</p></li><li><p>quadruplet loss: $\begin{aligned} L_{q u a d} &amp;=\frac{1}{N} \sum^{N}(\overbrace{\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha}^{\text {relative distance}}) \\ &amp;+\frac{1}{N} \sum^{N}(\overbrace{\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{C}-f_{B}\right|_{2}+\beta}^{\text {absolute distance }}) \end{aligned}$, 结合了triplet loss和pairwise loss，任何有着相同identity的image之间的distance都要比不同不同image之间的distance小；</p></li><li>triplet loss较contrastive loss提升明显，后面的quadruplet loss较triplet提升不多，而带来了计算量和搜索空间的提升，因此常用triplet loss;</li></ul><p>Hard Sample Mining:</p><ul><li><p>triplet hard loss: $ L_{\text {trihard}}=\frac{1}{N} \sum_{A \in \text {batch}}(\overbrace{\max _{A^{\prime}}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}\right)}^{\text {hard positive pair }} -\overbrace{\min \left(\left|f_{A}-f_{B}\right|_{2}\right)}^{\text {hard negative pair }}+\alpha) $, 找出矩阵中相同identity images中最不像的（the largest distance in the diagonal block）和不同identity images中最像的（The smallest distance in other places）;</p></li><li><p>soft  triplet hard loss: 不用一个个找出来，而是利用softmax自动去分配大权重给harder samples;</p></li><li><p>margin sample mining: $L_{e m l}=(\overbrace{\max _{A, A^{\prime}}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}\right)}^{\text {hardest positive pair }}-\overbrace{\min _{C, B}\left(\left|f_{C}-f_{B}\right|_{2}\right)}^{\text {hardest negative pair }}+\alpha)_{+}$;</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/conclu_dmetriclea.JPG" alt></p><p>Mutual Learning:</p><ul><li><p>knowledge distill: 知识蒸馏，学生网络学习老师网络的输出；</p></li><li><p>mutual learning: 几个学生网络自己相互学习，利用KL散度算各个网络output pro之间的接近程度；</p></li><li><p>metric mutual learning: $ L_{M}=\frac{1}{N^{2}} \sum_{i}^{N} \sum_{j}^{N} \left(\left[Z G\left(M_{i j}^{\theta_{1}}\right)-M_{i j}^{\theta_{2}}\right]^{2}+\left[M_{i j}^{\theta_{1}}-Z G\left(M_{i j}^{\theta_{2}}\right)\right]^{2}\right) $, ZG代表zero gradient，不计算梯度，不进行反向传播，学习distance matrix;</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/framework_MML.JPG" alt></p><ul><li>re-ranking: 对initial ranking list进行再ranking，使其smooth，on Supervised Smoothed Manifold/ by K-reciprocal Encoding;</li></ul><p>Person Re-Identification:</p><ul><li><p>difficulties: inaccurate detection, misalignment, illumination difference, occlusion, non-rigid body deformation, similar apperance…</p></li><li><p>evaluation criteria: CMC (Cumulative Math Characteristic)<rank-1, rank-5, rank-10>, mAP (based on rank);</rank-1,></p></li><li><p>datasets: Marke1501, CUHK03, DukeMTMC-reid, MARS;</p></li></ul><p>Feature Alignment:</p><p>motivations:</p><ul><li>Person is highly structured;</li><li>Local similarity plays a key role to decide the identity;</li></ul><p>methods:</p><ul><li>Local Features from local regions<ul><li>Traditional Methods (colors, texture…);</li><li>Deep Learning Methods;</li></ul></li><li>Local Feature Alignment<ul><li>Fusion by LSTM (RNN cannot fuse local features properly);</li><li>Alignment in PL-Net (Part Loss Network, unsupervised);</li><li>Alignment in AlignedReID (Face++出品，性能超越人类，global feature+7个local feature，代表人的7个部分，横向pool，只拿对应的边，使用动态规划);</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/AlignedReID.JPG" alt></p><p>ReID with Extra Information:</p><p>ReID with Pose Estimation:</p><ul><li>Providing explicit guidance for alignment;</li><li>Global-Local Alignment Descriptor (GLAD);<ul><li>Vertical alignment by pose estimation;</li></ul></li><li>SpindleNet;<ul><li>Fusing local features from regions proposed by pose estimation;</li></ul></li></ul><p>ReID with Human Attributes:</p><ul><li>Attributes is critical in discriminating different persons;</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/11_conclusion.JPG" alt></p><h2 id="12-Shape-from-X-3D-reconstruction-传统和DL"><a href="#12-Shape-from-X-3D-reconstruction-传统和DL" class="headerlink" title="12. Shape from X (3D reconstruction: 传统和DL)"></a>12. Shape from X (3D reconstruction: 传统和DL)</h2><ul><li><p>Structure from Motion (SfM): the most easy-to-understand approach, triangulation gets depth;</p></li><li><p>triangulation: the epipolar constraint对极约束，单目；</p></li><li><p>stereo, rectification (更正), disparity (视差，depth): correspondence, 不能远距离测量；</p></li><li><p>3D point cloud: paper-building Rome in one day, 多视角图片SfM重建，3D geometry；</p></li><li><p>surface reconstruction: integration of oriented point;</p></li><li><p>SfM scanning: SLAM based, positioning, 华为手机发布会实现静止小熊猫玩偶重建；</p></li><li><p>depth sensing: active sensors, structured light, ToF (Time of Flight);</p></li><li><p>short baseline stereo: phase detection autofous;</p></li><li><p>shape from shading: shading as cue of 3D shape (the Lambertian law);</p></li><li><p>photometric stereo;</p></li><li><p>shape from texture, depth from focus, depth from defocus, shape from shadows, shape from sepcularities, object priors paper-<a href="https://arxiv.org/abs/1612.00603">A point set generation network for 3D object reconstruction from single image</a>;</p></li></ul><p>3D reconstruction from single image:</p><ul><li><p>the ShapeNet dataset;</p></li><li><p>depth map;</p></li><li><p>volumetric occupancy;</p></li><li><p>XML file;</p></li><li><p>ponit-based represenation;</p></li></ul><p>A neural method to stereo matching:</p><ul><li><p>Flownet &amp;  Dispnet (using raw left and right images as input, output disparity map);</p></li><li><p>stereo matching cost convolutional neural network—Yan lecun;</p></li><li><p>MRF (马尔可夫随机场) stereo methods;</p></li><li><p>global local stereo neural network;</p></li><li><p>PatchMatch Communication Layer;</p></li></ul><h2 id="13-Visual-Object-Tracking"><a href="#13-Visual-Object-Tracking" class="headerlink" title="13. Visual Object Tracking"></a>13. Visual Object Tracking</h2><p>Motion estimation/ Optical flow:</p><ul><li><p>motion field: the projection of the 3D motion onto a 2D image;</p></li><li><p>optical flwow: the pattern of apparent motion in images, $I(x, y, t)=I(x+d x, y+d y, t+d t)$, 在adjacent frames中像素的运动；</p></li><li><p>motion field与optical flow不是完全相等；</p></li><li><p>KLT feature tracker (找点，计算光流，更新点)，比较成熟，available in OpenCV；</p></li><li><p>optical flow with CNN: FlowNet / <a href="https://github.com/lmb-freiburg/flownet2">FlowNet 2.0</a>, lack of training data (Flying Chairs / ChairsSDHom, Flying Things 3D);</p></li><li><p>optical flow长距离跟踪和复杂场景跟踪容易失效，不建议采用；</p></li></ul><p>Single object tracking:</p><ul><li><p>model free:  nothing but a single training example is provided by the bounding box in the first frame;</p></li><li><p>short term and subject to causality;</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/single_object_tracking.JPG" alt></p><ul><li><p><a href="https://github.com/foolwood/benchmark_results">paper list</a>;</p></li><li><p>correlation fiter: 模板匹配，similar to convolution;</p></li><li><p>MOSSE (Minimum Output Sum of Squared Error) Filter;</p></li><li><p>KCF (Kernelized Correlation Filter);</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/KCF.JPG" alt></p><ul><li><p>from KCF to Discriminative CF Trackers: Martin Danelljan, 从Deep SRDCF开始利用CNN feature; </p></li><li><p><a href="https://github.com/martin-danelljan/Continuous-ConvOp">Continous-Convolution Operator Tracker</a>: very slow (~1fps) and easy to overfitting;</p></li><li><p>Efficient Convolution Operators: based (factorized convolution operator + Guassian mixture model) on C-COT, ~15fps on GPU;</p></li><li><p><a href="https://github.com/HyeonseobNam/MDNet">Multi-Domain Convolutional Neural Network Tracker</a>: online tracking, bounding box regression, ~1fps;</p></li><li><p><a href="http://davheld.github.io/GOTURN/GOTURN.html">GOTURN</a>: ~100fps;</p></li><li><p><a href="https://github.com/bertinetto/siamese-fc">SiameseFC</a>: ~60fps, a deep FCN is trained to address a more general similarity learning problem in an initial offline phase;</p></li><li><p>Benchmark: VOT (accuracy, robustness, EAO-expect average overlap), OTB(one pass evaluation, spatial robustness evaluation);</p></li></ul><p>Multiple object tracking:</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/multi_object_tracking.JPG" alt></p><ul><li><p>tracking by detection: assocation based on location (IoU, L1/L2 distance), motion (modeling the movement of objects, Kalman filter), apperance (feature) ans so on;</p></li><li><p>association: </p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/association.JPG" alt></p><ul><li><p>association as optimization: local method (Hungarian algorithm), global methods (clustering, network flow, minimum cost multi-cut problem), do optimization in a window (trade off speed against acc);</p></li><li><p>Benchmark: MOT, KITTI, ImageNet VID;</p></li><li><p>evaluation metrics: Multiple object tracking accuracy (MOTA);</p></li></ul><p>Other:</p><ul><li><p>fast moving object (FMO): an object that moves over a distance exceeding its size within the<br>exposure time;</p></li><li><p>multiple camera tracking;</p></li><li><p>tracking with multiple cues: with multiple detectors, with key points, with semantic segmentation, with RGBD camera;</p></li><li><p>multiple object tracking with NN: </p><ul><li>Milan, Anton, et al. “Online Multi-Target Tracking Using<br>Recurrent Neural Networks“. AAAI. 2017.</li><li>Son, Jeany, et al. “Multi-Object Tracking with Quadruplet<br>Convolutional Neural Networks.” CVPR. 2017.</li></ul></li></ul><h2 id="14-Neural-Network-in-Computer-Graphics"><a href="#14-Neural-Network-in-Computer-Graphics" class="headerlink" title="14. Neural Network in Computer Graphics"></a>14. Neural Network in Computer Graphics</h2><p>计算机视觉是将图像信息转换成抽象的语义信息等，而计算机图形学是将抽象的语义信息转换成图像信息。</p><ul><li><p>Graphics: rendering, 3D modeling, visual media retouching （图像修整）；</p></li><li><p>Neural Network for graphics: faster, better, more robust;</p></li><li><p>NN rendering: </p><ul><li><p>Monte Carlo ray tracing （光线追踪，寻找光源），paper-[SIGGRAPH17] Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings ( utilize CNN to predict de-noising kernels, thus enhance ray tracing rendering result);</p></li><li><p>volume rendering;</p></li><li>NN shading (real-time rendering), paper-Deep shading: Convolutional Neural Networks for Screen-space shading (2016);</li><li>goal is to accelerate, all training data can be gathered virtually;</li></ul></li><li><p>NN 3D modeling:</p><ul><li>shape understanding: <ul><li>3D ShapeNets: A Deep Representation for Volumetric Shapes (2015).</li><li>VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition (2015).</li><li>DeepPano: Deep Panoramic Representation for 3-D Shape Recognition (2015).</li><li>FusionNet: 3D Object Classification Using Multiple Data Representations (2016).</li><li>OctNet: Learning Deep 3D Representations at High Resolutions (2017).</li><li>O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis (2017).</li><li>Orientation-boosted voxel nets for 3D object recognition (2017).</li><li>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (2017).</li></ul></li><li>shape synthesis: 3D-conv, also use GAN</li><li>from 2D to 3D, data becomes harder to handle, design of mesh representation, high-resolution 3D problem;</li></ul></li><li><p>NN visual retouching:</p><ul><li>tone mapping:  paper-Deep Bilateral Learning for Real-Time Image Enhancement (2017), it can handle high-resolution images relatively fast;</li><li>automatic enhancement: paper- Exposure: A white-box Photo Post-processing Framework (2017);</li></ul></li></ul><p>Example-NN 3D Face:</p><ul><li><p>given a face RGB/RGBD still/sequence, reconstruct for each frame (intrinsic image or inverse rendering):</p><ul><li>Inner/outer camera matrix;</li><li>Face 3D pose;</li><li>Face shape;</li><li>Face expression;</li><li>Face albedo;</li><li>lighting;</li></ul></li><li><p>3D face priors: shape &amp; albedo, paper-A 3D Morphable Model learnt from 10,000 faces (2016);</p></li><li><p>3D face priors: expression: paper- FaceWarehouse: a 3D Facial Expression Database for Visual Computing (2012);</p></li><li><p>optimization: based 3D face fitting;</p></li><li><p>Coarse Net, Fine Net;</p></li><li><p>3D Face-without prior: paper-DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild(2017);</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/megviidlcourse/3D_face.JPG" alt></p><ul><li><p>render for CV:</p><ul><li>Synthesizing Training Data for Object Detection in Indoor Scenes, (2017);</li><li>Playing for Data: Ground Truth from Computer Games (2016)</li><li>Learning from Synthetic Humans (2017);</li></ul></li><li><p>demo: Face2Face, Real-Time high-fidelity facial performance capture, DenseReg;</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/av88056282/&quot;&gt;视频&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取;&lt;/li&gt;
&lt;li&gt;这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="课程记录" scheme="http://densecollections.top/categories/%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
    <category term="Megvii" scheme="http://densecollections.top/tags/Megvii/"/>
    
  </entry>
  
  <entry>
    <title>2019-2020:漫长的告别</title>
    <link href="http://densecollections.top/posts/Summaryofthisyear-1/"/>
    <id>http://densecollections.top/posts/Summaryofthisyear-1/</id>
    <published>2020-01-10T02:52:06.000Z</published>
    <updated>2021-01-02T11:09:18.871Z</updated>
    
    <content type="html"><![CDATA[<p>搭博客的时间大概只有一年，当初是因为受到<a href="http://pluskid.org/">pluskid网站</a>和博客的影响，觉得做一个有趣的人，学习自己想了解的，放淡自己的心态是是每天很重要的事，因此想记录下来，有些规划，也好作为充实自己的见证，搁以前每年都来一个总结这件事我是不怎么做的。求学生涯没有结束，每年都是反复地上课，做项目，似乎都是浑浑噩噩度过，科研中的磕碰和一些不甘心也会时不时消磨自己的意志，让自己在怀疑，焦虑，麻木，强迫的交织中蹒跚着。我之前也知道很多励志结论，他们说的都很对，但每个人的生活总是悲喜交加的，而大多数情况下都是在平平无奇中暗生悲戚，羁绊越多的人似乎悲戚越浓。</p><p>2019年的寒假里，我翻看了很多<a href="http://freemind.pluskid.org/">pluskid的博客</a>内容，其中以年终总结最多。一开始我是被博主优秀的履历所吸引，后来在那些博客里我读到了对于生活细节的热爱，以及对于艺术，真理和其他人类活动的浓郁兴趣，真诚，质朴同时又很能温暖人，所以算是“始于履历，陷于才华”吧！（厚脸皮地说，有一种惺惺相惜之感）。罗曼罗兰在《米开朗琪罗传》中有句很出名的话：<em>“Il n’ya qu’un héroïsme au monde : c’est de voir le monde tel qu’il est et de l’aimer.（世界上只有一种真正的英雄主义，那就是认识生活的真相后依然热爱它）”</em>，我开始渐渐明白其中的血肉故事，目前虽然谈不上热爱，但可以说是慢慢从走出到走入，慢慢进入状态。</p><p>接下来，我想还是先从自己的学校科研生活讲起，然后再去讲讲自己看过的书，去过的地方，听过的音乐，拍过的照片，看过的电影等，一步步勾勒出自己的故事，就像是在索拉里斯星上模拟出的记忆花园，感想估计无法给出多少，倒是想能抓住几分情绪便好。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%94%B5%E5%BD%B1%E5%A4%A7%E5%B8%88%E5%89%A7%E7%85%A7.jpg" alt="电影&#39;大师(the Master)&#39;剧照"></p><a id="more"></a><h2 id="研究生科研工作"><a href="#研究生科研工作" class="headerlink" title="研究生科研工作"></a>研究生科研工作</h2><p>2019年开始慢慢接受18年底国外读博计划失败的经历，审视自己这一个大时间段的失意，认清自己的能力和所需。前半年一半时间在慢慢恢复神志，试图理清自己后面的研究方向，还是想坚持SLAM+DL的路线。从头开始看高翔在深蓝学院的《视觉 SLAM 14讲》，看书，推公式，理解代码，再总结下来写到博客上，当然也时不时关注DL+CV相关论文的最新进展。大概是在3月底的时候，老板受人所托，给了我个在校外一家初创AI医疗研究院实习的机会，当时正好毫无头绪， 像个无头苍蝇到处乱撞，便答应下来。</p><p>为了能够顺利拿到实习机会（小型团队，气氛很好。mentor是海归，前百度员工，有梦想的工作狂），开始正式去看深度学习的理论，同时也参加了一个小型的“蔚蓝杯”人工智能 竞赛（赛题是检测图片中的商品，给出商品数量），熟悉一下数据分析，pytorch掉包和调参。趁着研一上课空余时间，突击学习理论，撕撕代码，看完了<a href="http://michaelnielsen.org/">Michael Nielsen</a>(这个人好像是个物理学家，写了一本关于量子通信，量子计算机方面的书籍，最近又搞了个网站专门介绍量子通信)的<a href="http://www.liuxiao.org/2016/10/dnn-《神经网络与深度学习》中文版及代码下载/">nndl</a>，<a href="http://www.weixiushen.com/">魏秀参</a>的《解析卷积神经网络》和一部分<a href="https://github.com/scutan90/DeepLearning-500-questions">深度学习500问</a>，本来计划接着看<a href="https://xpqiu.github.io/">邱锡鹏</a>教授的新书《神经网络与深度学习》，不过时间没赶得上就去面试了，个人感觉这些都是不错的参考资料。</p><p>实习面试的时候理论问题问得比较浅，可能对方知道我是个DL新手，只有训练CNN的一些防止过拟合技巧，BP中的链式法则等等；编程题就比较惨了，题目大概是在一个包含”x”和“o”的矩阵（”o”是少量元素）中找出起始“o”和终止”o”之间的最短距离（只能顺着“o”走）。很久没撕过代码， 加上第一次工作面试有点紧张，连暴力搜索都没能写出来，场面一度十分尴尬。后来我觉得可能看在老板的面子上，还是让我过了。。。实习期是6月底到9月底，期间做的一些<a href="https://densecollections.top/posts/worksummaryofintern/">工作</a>和<a href="https://densecollections.top/posts/thoughtsofintern/">感想</a>我都放在了之前的博客里，认识的师兄，了解到的新知识都打开了我的思路，也培养了我一些工作习惯，锻炼了自己的代码能力，总之还是十分感谢这段实习经历的。</p><p>回来之后就在准备研究生毕业开题的事情，同时也赶着实习工作总结出的论文。当然要不是因为组里小老师给的项目，也不会三个月实习期结束就回来。年初的时候想着拿深度视觉里程计开题，后来实习的时候一度打算一边实习，一边拿实习的AI医疗项目做毕设，最后，还是综合考虑答应了做小老师的项目，以此作为毕设。毕设的大致内容其实就是检测无人机，但是由于要求检测距离远，所以可能常规的Faster R-CNN等框架的检测效果可能不太好，要引入GAN和超分辨率等手段。目前最大的坑处应该是数据集的收集了，既要拍摄RGB，也要拍摄红外；既要考虑不同出现场景和环境，还要引入多形态的无人机，鸟，风筝类别，此外还得保证将近1km的拍摄距离。估计数据拍完了，毕设时间也差不多到了(&gt;_&lt;|||)。</p><p>2019年的学习与科研不算太顺利，自己也经常没进入状态，算是那种没别人努力，却还时常抱怨，焦虑的人。希望2020年能加把劲，不管是工作还是继续申请读博，都得把自己整硬实起来才行啊（@_@）！</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/work-harder.jpg" alt="Photo by Jordan Whitfield on Unsplash"></p><h2 id="探访之境"><a href="#探访之境" class="headerlink" title="探访之境"></a>探访之境</h2><p>今年南京之外去过的其他城市不多，大多是借着出差机会逛逛，不像本科时期年年都会找几个地方专程去看看。出走对于我来说已经成为生活中必不可少的一部分了，以为我是个很容易“习惯”的人，日常生活中的每件事都可以成为惯例，从不会厌倦，直到自己觉得到了需要进行调整或者改变的时刻。人在一个熟悉的封闭的空间里待久了，容易怀疑自己和这个世界，容易陷入空想而无所为的状态，这种感觉最能吞噬人的精神和智力。可能是我自己心浮气躁的原因，有时候会觉得内心烦闷，精气神不足，以前就任其发展，以无所事事应对，等待其消失。现在，随着这种情况出现次数的增多，我开始体会到这可能是科技异化人性和人类自身特质共同作用导致的，为了应对，我强制给自己找合适的，能辅助静下心来的事情做：“要么学习，要么运动“。其中”学习“的方式有很多种，不仅仅是科技理论知识，还可以是培养兴趣爱好，发展艺术视野和思维，出走旅游也是其中一种形式。</p><p>我学校处于靠市中心的位置，离很多景区都比较近，偶尔朋友来金陵的时候便会一起去玩玩，比如夫子庙，中山陵，老门东等地方。自己平时有点累或者烦躁的时候，通常会选择傍晚围中山陵绿道或者体育公园骑一圈，或者偶尔跟朋友师兄们相约爬紫金山，逛灵谷寺看萤火虫等。一年四季，这些地方都有很多”隐秘之地“，树草虫鸣，风拂月泻，令人舒心。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%A4%AB%E5%AD%90%E5%BA%99%E5%A4%9C%E6%99%AF1.jpg" alt="夏天夫子庙的傍晚"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%A7%A6%E6%B7%AE%E6%B2%B3.jpg" alt="秦淮河夜景"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%A4%AB%E5%AD%90%E5%BA%99%E4%B8%AD%E7%9A%84%E5%85%AC%E5%9B%AD.jpg" alt="夫子庙中的一个公园，具体名字我记不得了。"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%A4%AB%E5%AD%90%E5%BA%99%E9%B2%A4%E9%B1%BC.jpg" alt="夫子庙中一个公园里的一大群人工饲养鲤鱼"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%85%AC%E5%9B%AD%E4%B8%AD%E7%9A%84%E9%B9%85.jpg" alt="公园中的一只鹅，对游客已经免疫了"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%B8%AD%E5%B1%B1%E9%99%B5%E7%BB%BF%E9%81%93.jpg" alt="傍晚骑行的中山陵绿道"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%BD%93%E8%82%B2%E5%85%AC%E5%9B%AD.jpg" alt="傍晚的南京体育公园，挨着中山陵。颜色有种克莱因蓝的感觉"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%B4%AB%E9%87%91%E5%B1%B1%E5%A4%A7%E8%B7%AF.jpg" alt="和朋友一起爬紫金山路上"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%B4%AB%E9%87%91%E5%B1%B1%E5%A4%B4%E9%99%80%E5%B2%AD%E4%BF%AF%E7%9E%B0.jpg" alt="站在紫金山山群头陀岭俯瞰南京城，有些霾，拍得不是很清楚"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%B8%AD%E5%B1%B1%E9%99%B5%E5%85%AC%E5%9B%AD%E4%B8%80%E9%9A%85.jpg" alt="去灵谷寺路上的一汪水池"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%9C%8B%E8%90%A4%E7%81%AB%E8%99%AB%E6%A0%A1%E5%A4%96%E7%9A%84%E9%A9%AC%E8%B7%AF.jpg" alt="灵谷寺看完萤火虫回来的路上。第一次去拍萤火虫，相机不太好，光圈不够没拍出来萤火虫，囧x_x"></p><p>本科的时候有一些喜欢骑行的小伙伴，经常会约出去骑车到处逛，自从毕业读了研以后，朋友都各自纷飞，自己的圈子也越来越小，大家也都在实验室忙着各自的事。我偶尔一个人出去，也少了好几分乐趣。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%95%99%E7%A0%94%E5%AE%A4%E5%A4%96%E7%9A%84%E5%A4%A9%E7%A9%BA.jpg" alt="实验室外的天空"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%90%86%E6%83%B3%E9%AA%91%E8%A1%8C1.jpg" alt="日本一个小哥推特发的骑行照片，很是理想中的样子了"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%90%86%E6%83%B3%E9%AA%91%E8%A1%8C2.jpg" alt="这样的骑行环境国内似乎很少"></p><p>南京不像上海深圳，文化艺术多样性方面还是有所欠缺的，好歹有个南京博物院会时不时联合办些展览，可以饱饱眼福。2019年除了专程去看了固定的艺术馆的书画展，就是三次合作的特展：世界巨匠-意大利文艺复兴三杰；从毕加索到基弗-路德维希的艺术课；仰之弥高-二十世纪中国画大家展。前两次展品不多，毕竟大作是不太可能借过来的，最近一次的书画展规模比较大，看得也比较过瘾，将近百幅的画作看了两三个小时。</p><p><strong><a href="https://mp.weixin.qq.com/s/xrxMLSUhycpbtMBk53Q2yg">世界巨匠-意大利文艺复兴三杰</a></strong></p><p>展览内容是文艺复兴三杰达芬奇，米开朗琪罗，拉斐尔和他们继承人的一些作品，一开始我还妄想会有“蒙娜丽莎”，实际上三个人的真迹不多，其中有一些还只是复制品。不过，能看到这样的一些合作已经很满足了(&gt;～&lt;)。</p><p>拍的作品不多，有几幅是自己心水的。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%B8%96%E7%95%8C%E5%B7%A8%E5%8C%A03.jpg" alt="展览门票"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%B8%96%E7%95%8C%E5%B7%A8%E5%8C%A06.jpg" alt="米开朗琪罗的“哀悼基督”，展览中最喜欢的作品之一，可惜只是复制品"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%B8%96%E7%95%8C%E5%B7%A8%E5%8C%A01.jpg" alt="展览入门处矗立的雕塑，米开朗琪罗的“倚靠十字架的基督”。然鹅展览中没有“大卫”和“拉奥孔”的身影 Q_Q"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%B8%96%E7%95%8C%E5%B7%A8%E5%8C%A04.jpg" alt="米开朗琪罗画像"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%B8%96%E7%95%8C%E5%B7%A8%E5%8C%A05.jpg" alt="达芬奇的“女孩头部像”，自己比较喜欢的一幅作品，还专程买了相似的纪念品"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%B8%96%E7%95%8C%E5%B7%A8%E5%8C%A02.jpg" alt="达芬奇的“美丽公主”"></p><p><strong><a href="https://mp.weixin.qq.com/s?__biz=MzA5Mzk1NzQxNQ==&amp;mid=2656021627&amp;idx=1&amp;sn=54a8be2efaa5acd0c40f8da4850b43ee&amp;chksm=8beef033bc997925f6f4e35c4c417c9972b2bafd903674a3b2b26070eb1c53bfa1440b3d6960&amp;mpshare=1&amp;scene=1&amp;srcid=0128YyCtgogxHWDWaC7K1g3a&amp;sharer_sharetime=1580211377613&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=008658d21f0cfb81aea856e3b585c2ee570e286de35cc8bd4d066218d574c041f1493ecdcc46b9cd65bfba07cee59c36a74b56a57911c6c0e9b4292c04f2c18bb7c2103218fe57a9e9d788fa2cbec43f&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10&amp;version=6208006f&amp;lang=zh_CN&amp;exportkey=A%2FAhzHlLl0jDCA0j%2FJwrG58%3D&amp;pass_ticket=2hEidokwNVuWD7Y6FL6DoxqyMg6FNzZEiYvkngh8pdxJI2%2Bneh%2F9h8tqW9J5PVnV">从毕加索到基弗一一路德维希的艺术课</a></strong></p><p>路德维希夫妇是非常有名的艺术收藏家，曾收集了很多毕加索的名画。这次展览也展出了毕加索的几幅真迹，比如“带鸟的步兵”等，不过鉴于规模，大部分还是我这个圈外人士不曾了解过的艺术家。画作风格上涵盖了立体主义，表现主义，抽象派，野兽派和波普艺术等，神秘，荒诞，又虚无。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%B7%AF%E5%BE%B7%E7%BB%B4%E5%B8%8C4.jpg" alt="贝尔纳德·舒尔茨，“伟大的母亲”。此次展览中最喜欢的一幅抽象画，我来回看了很久，但是没看出“母亲”的含义"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%B7%AF%E5%BE%B7%E7%BB%B4%E5%B8%8C5.jpg" alt="安塞尔姆·基弗，“阿拉里克的坟墓”"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%B7%AF%E5%BE%B7%E7%BB%B4%E5%B8%8C7.jpg" alt="娜塔丽亚·内斯特洛娃，“鸟”"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%B7%AF%E5%BE%B7%E7%BB%B4%E5%B8%8C1.jpg" alt="名字不记得了，画作内容好像是描绘新疆某个地方的烧烤摊的场景"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%B7%AF%E5%BE%B7%E7%BB%B4%E5%B8%8C2.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%B7%AF%E5%BE%B7%E7%BB%B4%E5%B8%8C3.jpg" alt="这幅画挺有意思的，人类拥有小范围太空探索的能力，与造物神之间的对峙和连接"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%B7%AF%E5%BE%B7%E7%BB%B4%E5%B8%8C6.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%B7%AF%E5%BE%B7%E7%BB%B4%E5%B8%8C8.jpg" alt></p><p>画作和诗歌一样，都得参照下创作背景，虽然有些艺术家可能是天赋异禀，像小松美羽，画作直接在脑海呈现，一气呵成。一千个读者，有一千个哈姆雷特，画作也是这样，那些吸引我的画，都有一种能攫住人的思绪得魅力，触发观赏者的千万感受。很抱歉我自己读书和思考比较少，鉴赏水平有限，没能做出比较完整的评价来。(+_+)</p><p><strong><a href="https://mp.weixin.qq.com/s?__biz=MzA5Mzk1NzQxNQ==&amp;mid=2656022782&amp;idx=1&amp;sn=e98882a7deac21af575b8298d24490e3&amp;chksm=8beeecb6bc9965a01af713ba2fd6a3618a1faaa24735cdf478f15dbf9bc548cd0cb2690f3f9b&amp;mpshare=1&amp;scene=1&amp;srcid=0128xHFfNOS2JdO2U7K6n5hW&amp;sharer_sharetime=1580211431161&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=1f3bd73ca2cb19588251434376e9c167a19004fc86747f5e66073c286ed4bf8a62151c98501fd2d8d6e7bb1bec11003fad8faf0ac6ae80516b1fbe2235c1e9f02f2d8dbe4128d1bd26a95a185e174974&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10&amp;version=6208006f&amp;lang=zh_CN&amp;exportkey=AzstWxsx9pxr1OrJifPhDaI%3D&amp;pass_ticket=2hEidokwNVuWD7Y6FL6DoxqyMg6FNzZEiYvkngh8pdxJI2%2Bneh%2F9h8tqW9J5PVnV">仰之弥高-二十世纪中国画大家展</a></strong></p><p>20世纪中国画作继往开来，有种特别的魅力。南京博物院这一次的联合展览规模据说是国内之最，由于展品较多，分为上下半场，上半场在2019末尾，下半场在2020开年，我只看了上半场，就已经觉得展品数量多且质量高，2个多小时下来后意犹未尽，又去买了配套的书珍藏。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%BB%B0%E4%B9%8B%E5%BC%A5%E9%AB%981.png" alt="展览的八大家"></p><p>国画当中，我最喜爱也是唯一爱的是山水画，尤其以写意为主，创造了与西方讲究逼真复刻思想截然不同的心境体验。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%BB%B0%E4%B9%8B%E5%BC%A5%E9%AB%982.jpg" alt="展览封面是张大千的江岸图"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E4%BB%B0%E4%B9%8B%E5%BC%A5%E9%AB%983.jpg" alt="展览门票"></p><p>齐白石挑了几张图，从左到右分别是山溪群虾图，秋荷图，松寿图，寻旧图。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E9%BD%90%E7%99%BD%E7%9F%B3.jpg" alt></p><p>展出的黄宾虹作品中，狮子林望松谷园，练江南岸图，栖霞山居图，天目奇峰图，湖滨山居图，江村图，黄山记游图，九子山，练滨草堂图个人都非常喜欢，可惜手机里只翻出其中三张。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E9%BB%84%E5%AE%BE%E8%99%B9.jpg" alt></p><p>徐悲鸿：双马图|侧目图|飞鹰图，愚公移山图，鹰图，山鬼图，九方皋相马图。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%BE%90%E6%82%B2%E9%B8%BF.jpg" alt></p><p>张大千的画作想必很多人都喜欢，毕竟颇有古人风骨。下面挑出的几张是江岸图，泼墨荷花图，荷花图，仿杨昇笔意设色山水图，夏山高隐图。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%BC%A0%E5%A4%A7%E5%8D%83.jpg" alt></p><p>潘天寿：青山白云图，雄视图，雨后千山铁铸成，记写雁荡山花。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%BD%98%E5%A4%A9%E5%AF%BF.jpg" alt></p><p>林风眠，中西集合的一位画家：夜枭图，劈山救母图，早春暮色图，风景图，灰鹭图，鱼鹰小舟图。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%9E%97%E9%A3%8E%E7%9C%A0.jpg" alt></p><p>傅抱石：万竿烟雨图，待细把江山图画，兰亭修褉图，龙蟠虎踞今胜昔。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%82%85%E6%8A%B1%E7%9F%B3.jpg" alt></p><p>李可染：苏州虎丘图，万山红遍 层林尽染，崇山茂林源远流长图，无尽江山入画图，树百重泉图。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%9D%8E%E5%8F%AF%E6%9F%93.jpg" alt></p><p>近日受到2019-ncov病毒影响，学校也不准提前到校，下半场的展览不知还能不能赶得上了。</p><p>19年除南京城外，只到访过西安和成都。西安是“故地重游”，当时本科毕业时去了甘肃，正好顺路就去西安找朋友玩了趟。当时正值夏季，大西北天气炎热，白天毒辣的阳光炙烤得人很难受，不过晚上舒服，没有南京那么闷热，有些凉爽。西安的夜景确实很不错，市民们的夜生活也丰富多彩。大雁塔，大唐芙蓉园周边的柔和灯光映衬着来回的身着汉服的姑娘，恍如隔世。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%A5%BF%E5%AE%89%E5%A4%9C%E6%99%AF1.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%A5%BF%E5%AE%89%E5%A4%9C%E6%99%AF2.jpg" alt></p><p>第一次玩那会没吃上臊子面，算是一大憾事，19年年底开会的时候正好借着机会找了一家“乡党臊子面”体验了一把一顿“几十碗”的快感。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%8A%E5%AD%90%E9%9D%A2.jpg" alt="疯狂吃各种口味的臊子面 &gt;_&lt;"></p><p>吃撑了肚子，跟朋友一起到了附近的城墙走了一圈，夜景还是像以前来那样令人惊喜。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%A5%BF%E5%AE%89%E5%9F%8E%E5%A2%991.jpg" alt="城墙的夜景"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%A5%BF%E5%AE%89%E5%9F%8E%E5%A2%993.jpg" alt="站在小桥上拍，护城河？"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%A5%BF%E5%AE%89%E5%9F%8E%E5%A2%992.jpg" alt="走着走着发现一条走廊里挂着古风似的红灯笼，觉得非常喜欢，就拍下来了，但是我的GR II好像没对上焦"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%A5%BF%E5%AE%89%E5%9F%8E%E5%A2%994.jpg" alt="河边上的小人雕塑"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%A5%BF%E5%AE%89%E5%9F%8E%E5%A2%995.jpg" alt="商店旁一只会变色的鹿"></p><p>两次的西安之行，没有很长时间，达不到能够感受一个城市风格的力度。粗略的印象是面很好吃，夜景很好看，人民生活丰富也比较健谈，可惜大西北灰尘太多，空气质量差了点，嗓子实在吃不消。</p><p>大学本科四年来，成都是我一直想去但没能去成的地方，要么是没钱，要么是没时间。自西安开会结束回来后，正好收到要赶去四川成都陪师兄们做oral的消息，甚是激动。4天的学术会议，只花了几个小时参加作报告就溜了，剩下的时间都和师兄，朋友们在成都吃喝玩乐闲逛了。刚下飞机的那天就跑去谭鸭血吃了火锅，和热情的成都朋友们喝得酩酊大醉，半夜回旅馆路上还看到街上人来人往，分外悠闲。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%B0%AD%E9%B8%AD%E8%A1%80.jpg" alt="谭鸭血火锅店。不得不说成都的火锅确实很好吃，佐料也非常可口"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%88%90%E9%83%BD1.jpg" alt="晚上喝晕了，第二天起来已经是中午了，参会前就近找了一家小吃店，点了老妈蹄花，重庆小面和夫妻肺片"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%88%90%E9%83%BD2.jpg" alt="汇报结束去文殊院闲逛，吃了正宗的龙抄手，晚上被朋友拉去川菜馆"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%88%90%E9%83%BD3.jpg" alt="吃罢晚饭到春熙路溜达"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%88%90%E9%83%BD4.jpg" alt="听说成都人民的一大爱好是喝下午茶"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%88%90%E9%83%BD5.jpg" alt="到朋友的学校川师大参观"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%88%90%E9%83%BD6.jpg" alt="宽窄巷子"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%88%90%E9%83%BD7.jpg" alt="人民广场。大爷大妈聚集在此开展各种文艺活动，喝茶闲聊，掏耳朵打牌，应有尽有。我们待在一处啃着兔头"></p><p>尽管只有将近4天的短暂游玩，但成都却给我留下了与众不同的印象，特色美食，热情的小伙伴，悠然的生活方式，以及温润的气候让我一度不想归宁。临行的出租车司机大叔对我们说“你们年轻人不要随便来四川工作生活”，估计也是怕缓慢慵懒的生活节奏让我们丧失奋斗的意志了吧。</p><h2 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h2><p>19年的阅读量比18年稍多了些，主要是在L同学的影响下，翻了很多画册或者手绘书。这些书看起来快，又不需要花时间思考，正好适合我现在的碎片阅读时间。本来今年打算着手学习一下画画，不管是什么绘画分支，得画起来才行，硬盘里也收藏了很多很多绘画教材和一些作品集，然而到现在都几乎没怎么翻过。</p><p>19年让我感触最大的两本书是两部小说，一本是科幻小说，《索拉里斯星》，塔可夫斯基也曾将其拍成电影《飞向太空》；另一本是写实结构主义小说，《城市与狗》，拉美文学作品，写作方式不怎么规则，但是阅读体验很好。我在豆瓣也建了个<a href="https://www.douban.com/photos/album/1686128240/">相册</a>存放自己有关看过的书的一些手拍照片，算是思考总结，也算是一种鞭策自己多阅读的手段吧。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E9%98%85%E8%AF%BB.jpg" alt="一大半的阅读量是绘画有关的书籍"></p><ul><li><p><a href="https://book.douban.com/subject/3618253/">和空姐同居的日子</a>是初中是很喜欢的一部广播小说，作者三十也居住在南京。内容虽然有些yy的成分，但是总体还算是清新自然，单纯美好。19年年初那会觉得心情不好，就重听了一遍，个人觉得第一部（季）质量最好，第二部可能是因为大家不喜欢第一部的结局而写出来的。现在喜马拉雅还有第一部的广播<a href="https://www.ximalaya.com/youshengshu/3087022/">资源</a>。</p></li><li><p><a href="https://book.douban.com/subject/26651221/">线性代数的几何意义</a>是19年年初看MIT 线性代数公开课阅读的课外材料。学过线代后看一遍会有不小的收获，也算是解释数学意义的一个体现吧，尤其是在实际工程上。</p></li><li><p><a href="https://book.douban.com/subject/5908654/">给青年人的信</a>是在自己的kindle上当作睡前读物看完的。大一时候听老师讲里尔克时心潮澎湃，专程去图书馆借了里尔克的诗集，囫囵吞枣地看完了，现在几乎对他的诗歌也没什么印象了，除了经典的几首。实际上，这本书作为枕边读物，常拿出来翻翻还是很有裨益的。里尔克写信的文字十分真诚，字里行间透露出对生活的热爱，对细节，真善美的观察和思考，不仅是对诗歌创作者，即使是对忧郁迷茫中人，也有不小的指导和振奋作用。</p></li><li><a href="https://book.douban.com/subject/25898192/">索拉里斯星</a>是波兰科幻作家斯塔尼斯瓦夫·莱姆的代表作，也是我目前最喜欢的科幻小说之一。它不同于一般的大格局的科幻小说，关心宇宙和人类的命运，或者讽刺人性等等，而是着眼于人的记忆和内心之中的隐秘之痛，和特德·姜的《你一生的故事》有些异曲同工之妙。这篇小说的故事内容很简单：人类在一个遥远的星系里发现了一个特别的天体，称之为索拉里斯，人类派出了空间站去探测研究这颗特别的天体，地球上还出现相关的研究学科。这颗天体本身就是一个生命体，表面几乎都被”海洋“覆盖，那些奇怪的液体可以随便被天体操作，模拟出各种形态，甚至是有自己意识的人类实体。男主到达空间站后发现几位同事都被索拉里斯星折磨得濒临崩溃，自己也随着索拉里斯星的实验慢慢揭开其中的秘密。实际上，索拉里斯星的这些实验都是处于自己对宇宙好奇的探索，空间站的人类在其面前没有任何秘密可言，包括记忆。记忆和亡妻之痛（或者说是每个人内心独有的隐秘之痛）是最近很着迷的点，总觉得它们代表着人类特有的印记，以此为内容的电影总能让我感触良多，弥漫着无法抗拒的悲痛感，像是“海边的曼彻斯特”，“登月第一人”等都可以说这种类型电影的代表。莱姆的索拉里斯星恰好也是有着这两个点，但是又不全是，书中包含着对人类道德，思想和精神世界的审视，虽然也有那种宇宙的孤独感，但是却又不全是那种冰冷的感觉。索拉里斯星上的大海，不像地球上孕育着生命，却表现出一种宇宙的隐士般的好奇，无欲，与人类触碰却又收回了手。书中有关男主的心理描写和索拉里斯星的描写都十分出色，在一定程度上缓解了阅读的枯燥。</li><li><a href="https://book.douban.com/subject/26827211/">小北野武</a>是导演北野武的童年生活记录，同时也亲自手绘了插图。北野武的真诚，诙谐，充满童趣的特点在这本书中表露无疑，以孩子天真无邪口吻说出的故事，表面让人发笑，背后实则充满辛酸之感，尤其读到最后“父亲的遗物”那里，看到北野武老爸叫菊次郎，突然觉得有点抑郁~。书里的插画我也上传到<a href="https://www.douban.com/photos/album/1686213156/?m_start=0">豆瓣相册</a>了。</li><li><a href="https://book.douban.com/subject/26863095/">素描的艺术：席勒</a>这本书是我在图书馆找书时无意发现的，看到书名时翻了一下，非常惊喜，国内有关埃·贡席勒作品介绍的书籍似乎不多，这本作品集编排得很用心，就借了出来。此外，前几年有部关于他的电影<a href="https://movie.douban.com/subject/26421474/">埃贡·席勒：死神和少女</a>，感兴趣可以看看。埃贡·席勒是个很有艺术天赋的艺术家，但是画作不走寻常路，人物通常瘦骨嶙峋，不符合正常规律的细长，大多都是反映内在世界得敏感，孤独，暧昧，神经质等，作品风格好像叫立体解构主义或表现主义？他也有很多自画像，评论家认为其有着明显的自恋和自我剖析倾向。作为作品集，自然文字内容会少很多，主要是不做二次加工展示给读者，让其自己去感受自己所看到的内容。书中大部分作品传到了我的<a href="https://www.douban.com/photos/album/1686395459/">豆瓣相册</a>上。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%9F%83%E8%B4%A1%E5%B8%AD%E5%8B%921.jpg" alt="埃贡·席勒作品"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%9F%83%E8%B4%A1%E5%B8%AD%E5%8B%922.jpg" alt="埃贡·席勒自画像"></p><ul><li><p><a href="https://book.douban.com/subject/30257644/">念书还是工作，这是一个问题</a>是法国一位女博士根据自己的经历画出的漫画故事。虽然是文学博士，但是很真实了，特别是在无穷的资料和方向中不断摸索，在碰壁，消沉，以及自我安慰中挣扎，有时还不得不面对社会和家庭的一些压力……在理工科界也有一本phd博士写的书，叫”<a href="http://pgbovine.net/PhD-memoir/pguo-PhD-grind.pdf">the phd grind</a>”。读不读博这个问题，感觉好难回答，那些读过了的人和没迈进去步子的人，感觉精神上应该有很大不同吧～或许那些有心得人会在读博期间不断地审视自己吧。</p></li><li><p><a href="https://book.douban.com/subject/30358084/">线条：斯坦伯格的世界</a>是漫画家斯坦伯格的一部作品集，这也是我偶然在图书馆发现的一本书。斯坦伯格的线条漫画作品简洁机智，有的讽刺意味很强，有的妙趣横生。他也曾经为杂志”New Yorker“作过不少次封面插画，在国外知名度很高。原书收集的作品比较多，我从书里挑了一些，放在了<a href="https://www.douban.com/photos/album/1686466375/">豆瓣相册</a>里。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%B4%A2%E5%B0%94%E6%96%AF%E5%9D%A6%E4%BC%AF%E6%A0%BC.jpg" alt="斯坦伯格作品"></p><ul><li><a href="https://book.douban.com/subject/27154489/">蓝色小药丸</a>是一部讲述HIV携带者相爱故事的漫画，作者画风凌厉，内容却细腻感人。难以想象，这样的事会如何发生在自己身上。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%93%9D%E8%89%B2%E5%B0%8F%E8%8D%AF%E4%B8%B8.jpg" alt="蓝色小药丸部分内容"></p><ul><li><a href="https://book.douban.com/subject/26597979/">梁山伯与祝英台</a>是一部似皮影戏般的绘本，当初从多抓鱼凑单买的，薄薄的一本，可以拿来收藏。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%A2%81%E5%B1%B1%E4%BC%AF%E4%B8%8E%E7%A5%9D%E8%8B%B1%E5%8F%B0.PNG" alt="梁山伯与祝英台插图"></p><ul><li><a href="https://book.douban.com/subject/25982252/">蝙蝠侠：致命玩笑</a>。在凤凰叔“Joker”资源未出来之前解渴用的。</li><li><a href="https://book.douban.com/subject/26930483/">英国插画师</a>里面介绍了10位不同风格的英伦插画师，有卡通的，也有成人暗黑的。当中有位Aubrey Beardsley(奥伯利·比亚兹莱)曾是奥斯卡·王尔德的情人，曾为其作品莎乐美绘制插图，带有日本浮世绘风格，算是这10位插画师当中风格最与众不同的一位，可惜英年早逝，20几岁就陨落了。此外，奥伯利在英国<a href="https://www.bl.uk/collection-items/the-yellow-book">the yellow book</a>期刊中的插画作品也非常经典，我之前还专门google找到了几期数字化的pdf，现在不知道链接存哪而去了/_\。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%A5%A5%E4%BC%AF%E5%88%A91.jpg" alt="书中有关奥伯利的介绍"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%A5%A5%E4%BC%AF%E5%88%A92.jpg" alt="奥伯利作品"></p><p>除了奥伯利之外，Arthur Rackham(亚瑟·拉克姆)的童话插画作品也十分梦幻，甚至带有些诡异。他为《爱丽丝梦游仙境》和《安徒生童话》绘制的插画神秘阴暗，带有一丝哥特风，风格十分独特。后来我在网上搜索，希望能翻到他的数字化作品，可惜童话插画作品太多，不如奥伯利的那么好找，只好放弃。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%8B%89%E5%85%8B%E5%A7%86.jpg" alt="拉克姆及其童话插画作品"></p><p>这本书的人物介绍和作品选取都很棒，但我还是没看过瘾，要是编辑能放多点图就好了。书中提到的10位插画师中我个人比较喜欢Kate Greenaway(凯特·格林纳威 )，Heath Robinson(希思·罗宾逊)，Edward Lear(爱德华·李尔)， Aubrey Beardsley(奥伯利·比亚兹莱)，John Millais(约翰·米莱斯)，Arthur Rackham(亚瑟·拉克姆)和Walter Crane(沃尔特·克莱恩)这7位，我也挑选了他们的一些作品放在<a href="https://www.douban.com/photos/album/1686128240/?m_start=36">豆瓣相册</a>。</p><ul><li><a href="https://book.douban.com/subject/26904293/">素描的艺术：毕加索</a>这本书与席勒那本同属一个系列。由于内容限制在素描，所以很多作品都是平时不常见的。这本书展示了毕加索自己平常生活中的速写，寥寥几笔，形神兼备，让我感觉比那些挂在博物馆里的大作更有温度和情感。美中不足的是，作品排列似乎没有按照一定的顺序，比较杂乱，也缺乏一定的说明。部分作品上传到了<a href="https://www.douban.com/photos/album/1686128240/?m_start=18">豆瓣相册</a>。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E6%AF%95%E5%8A%A0%E7%B4%A2.jpg" alt="毕加索素描作品"></p><ul><li><a href="https://book.douban.com/subject/30304849/">拍电影时我在想的事</a>是日本导演是枝裕和的随笔集，交织了电影拍摄历程，人生感悟。是枝裕和导演和村上春树的随笔风格很像，真诚又细腻，谦逊又不乏思考，没任何架子，感觉与这样的人聊天不仅很舒服，而且还会获益良多。不过说来惭愧，是枝裕和的影视作品看得不多，因此这本书也就是快速翻阅般地看完了，对他的作品有些了解之后再去看这本书或许阅读体验会更好。</li><li><a href="https://book.douban.com/subject/30228730/">观山海</a>是杉泽根据《山海经》所画的异兽部分。杉泽是微博上很火的手绘古风神话鬼怪艺术家，托L同学强烈推荐，我才开始慢慢了解到此类画风的作品和艺术家。总的来说，第一次看很新鲜，色彩很美，梦幻又诡谲。但是看多了有些审美疲劳，尤其是人物的脸部造型，觉得没什么变化，可能是一种想象和代表吧。同样，我也拍了些放在<a href="https://www.douban.com/photos/album/1688487996/">豆瓣相册</a>上。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%A7%82%E5%B1%B1%E6%B5%B7.jpg" alt="随意放几张"></p><ul><li><a href="https://book.douban.com/subject/26727997/">Neural Network and Deep Learning</a>和魏秀参的<a href="https://book.douban.com/subject/30381203/">卷积神经网络</a>是当初准备实习面试补的书，写得都还可以，不过魏秀参博士那本不太适合刚入门卷积神经网络的小白看。</li><li><a href="https://book.douban.com/subject/10750155/">数学之美</a>应当属于理工科内，尤其是是计算机专业圈内久负盛名的一本书，主要是以NLP自然语言处理，搜索引擎算法等讲述一些具体数学算法在工程中的实际应用。没有研究过相关领域，作为科普读物看看。</li><li><a href="https://book.douban.com/subject/26695174/">灯塔</a>是一本讲述孤独忧伤的“畸形儿”困在灯塔几十年，渴望自由和情感的漫画故事，故事很短很有想象力，像电影分镜一般，推荐阅读。其实19年也有一部电影也叫<a href="https://movie.douban.com/subject/30143336/">灯塔</a>（罗伯特·帕丁森和威廉·达福主演，导演之前的<a href="https://movie.douban.com/subject/26276364/?from=subject-page">女巫</a>我也很喜欢），不过故事截然不同，要暗黑惊悚得多，目前还没来得及看，在此不再多说啦。</li><li><a href="https://book.douban.com/subject/26780984/">城市与狗</a>算是今年读到的比较惊喜的一部小说。因为APP MONO中的一个海报特地从图书馆借来，阅读之前也并不知道是结构主义小说，刚开始的时候没有准备，确实被混乱的时间线叙述，不同的人物视角搞得有点蒙，直到第一部分一半，整体故事脉络就慢慢缕清了。后面的阅读体验极佳，略萨的对话和心理描写别具一格，画面感很强，读者自己很容易就代入进去，成为旁观者或主角，很难想象20出头的略萨就已经有如此高的写作技巧。“我曾有过二十岁，我也不同意任何人说那是最美好的年华。”这句话算是全书最无奈的最忧伤的精髓了。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E5%9F%8E%E5%B8%82%E4%B8%8E%E7%8B%97.jpg" alt></p><h2 id="音乐，电影，公开课以及其他"><a href="#音乐，电影，公开课以及其他" class="headerlink" title="音乐，电影，公开课以及其他"></a>音乐，电影，公开课以及其他</h2><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E9%9F%B3%E4%B9%90%E4%B8%93%E8%BE%91.png" alt>19年听的除了一般的流行歌曲之外，主要是古典和爵士。听古典是由于一方面可以培养和提升个人品味，另一方面是可以放松舒缓心情，帮助大脑思考；听爵士呢主要是之前受村上春树和伍迪·艾伦的影响，自己也很喜欢爵士乐营造出的那种氛围，那种音乐里的情感和故事性，很能激发人的想象力。</p><p>目前两种类型的音乐领域我都处于泛听阶段，当然也有一些比较喜好的作曲家和演奏家等，像德彪西，萨蒂，肖斯塔科维奇，鲁宾斯坦，查里贝克，路易斯·阿姆斯特朗，Karen Souza等。接下来还会多去探索，等到了一个适合的饱和点，然后再去慢慢整理自己在这方面的喜好，同时我也在看耶鲁大学<a href="https://www.bilibili.com/video/av16809187?from=search&amp;seid=10378298280102696019">聆听音乐</a>这门公开课，当作一个参考的鉴赏指导，希望明年总结时也能给其他想入门的人一些听的建议。</p><p>观影上，19年看的总量不多，主要是当时的爆米花商业片加上一些偶尔看到的冷门片。以前都是专门找自己喜欢的口碑片来看，现在基本都是想找些没什么情感压抑的爽片，可能是19年比较劳累了吧:-}。电视剧方面倒是看了好几部韩剧，比如《春夜》，《我的鬼神大人》等，另外还有一部木村拓哉的《东京大饭店》，看完之后学做菜的热情猛地高涨起来，木村拓哉的”带货能力“是真的强:-D。美剧里由于《权力的游戏》最终季和《黑镜》第五季都让我很失望，后面就没再看其他的了。</p><p>我从自己的观影记录中挑出了10佳，其中昆汀的《好莱坞往事》是最喜欢的，而且还非常喜欢前面很长时间的絮絮叨叨，后面的”曼森杀人案“高潮也是一贯的昆汀式B级风格；马丁·斯科塞斯的《爱尔兰人》也还可以，更像是一曲挽歌，满怀忧伤唏嘘之感；凤凰叔的《小丑》中规中矩，凤凰叔的表演加上剧本上的点到为止成就了这部高分片；最惊艳的当属于基努·李维斯的《John Wick 3》了，续集不失前作的水准，反而让故事中的杀手世界变得越来越丰满…</p><ul><li><a href="https://movie.douban.com/subject/27087724/">好莱坞往事</a></li><li><a href="https://movie.douban.com/subject/26909790/">疾速备战</a></li><li><a href="https://movie.douban.com/subject/27119724/">小丑</a></li><li><a href="https://movie.douban.com/subject/33415943/">我失去了身体</a>   比较意识流的一部法国动画片</li><li><a href="https://movie.douban.com/subject/27138615/">自卫的艺术</a>   一部诡异黑色片，如果不是卷西，效果可能会大打折扣</li><li><a href="https://movie.douban.com/subject/30165034/">昨日奇迹</a>    纪念致敬披头士乐队</li><li><a href="https://movie.douban.com/subject/6538866/">极速车王</a>    改编自福特与法拉利的勒芒大赛竞争，没有落入俗套的热血，也顺带讽刺了系统和机构以及对其的反抗</li><li><a href="https://movie.douban.com/subject/4185834/">丧失乐园2</a>   时隔10年的续作，一如既往的脑洞大开</li><li><a href="https://movie.douban.com/subject/27089612/">纽约的一个雨天</a>  伍迪·艾伦的近作，甜茶饰演的男主彷佛就是老爷子本尊</li><li><a href="https://movie.douban.com/subject/6981153/">爱尔兰人</a></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%94%B5%E5%BD%B1.jpg" alt></p><p>戏剧方面，除了一如既往地搜寻NT Live的资源，一边也慢慢探索其他类型的舞台剧，比如音乐剧，芭蕾舞剧等，不过19年还没完整看过其中的任何一部。对于舞台剧，要想有好的观感体验，获得共情能力，是必须要去现场的，可惜资源和机会有限，能偶尔盼到一两部电子拷贝也是极好的。</p><p>听公开课是19年下半年开展的一个计划，每天都拿出30min~60min的时间花在一些有意义的课程上，不管是艺术类，理工科类，不管是通识概括还是深入讲解都可能拿来学习。但是自己的自律性还不够，没能每天坚持，尤其是年末那段时间基本属于自我放纵阶段了，20年里这个计划要得严格执行点才行。</p><ul><li><a href="https://www.bilibili.com/video/av68107287?from=search&amp;seid=15101072632370554317">梁老师的爱情课</a>  这个是一席 万象组织的一个演讲，内容比较充实，聊的也很透彻，需要观众在课后反复思考。里面的很多价值观我是持赞同态度的，然而爱情是人类中最复杂的问题之一，感性的，理性的手段都不一定能很好处理其中的纠葛联系。这个课更多地是让我看到了爱情中一些行为背后的人性因素，权当一种拓展知识面的材料了。</li><li><a href="https://www.bilibili.com/video/av18102433">戏剧入门-张先</a>  这个是在B站上发现的为数不多的国内戏剧入门公开课之一，是中央戏剧学院开设的。课程内容个人觉得不是很充足，学习体验不是很好，推荐后面Crash Course中的剧院公开课系列。</li><li><a href="https://www.bilibili.com/video/av21376839">Crash Course-Computer Science</a>  看的第一门Crash Course公开课，后面准备把整个系列中自己喜欢的系列都刷一遍。这门课我看很多微信公众号都推送了，内容上确实做得很用心，字幕组<a href="https://github.com/1c7/crash-course-computer-science-chinese">翻译</a>的也很棒。</li><li><a href="https://www.bilibili.com/video/av13762839">Crash Course-Philosophy</a>  课程基本上是以提出哲学问题形式展开的，看完之后可能需要看一些书籍辅助理解（这里有一个推荐的<a href="https://www.zhihu.com/question/19588342/answer/786026336">知乎回答</a>)，然后再去找自己感兴趣的哲学家的专著阅读。</li><li><a href="https://www.bilibili.com/video/av19463816?from=search&amp;seid=1384032167897391415">Crash Course-Theatre</a>  这个课看了一大半了，字幕组还没翻译完，内容属于通识类，老师的讲说风格有种”学术幽默“，挺有趣，只不过东西太多，记不住，有点伤脑筋@_@。</li></ul><p>我非常频繁的找有关戏剧的东西看是因为它这种融合了人性的考察、无处不在的想象力、轮回式的角色扮演的艺术形式让我深深着迷，我越深入其中，越发现它其实探讨的是人的命运问题，通过把虚构的/非虚构的人的过去、现在、和未来展现在观众面前，让我们发现生活的真相。</p><p>什么是戏剧：</p><blockquote><p> Percy Bysshe Shelley  The Cenci, ACT1. SCENE1</p><p>The highest moral purpose aimed at in the highest species of the drama, is the teaching the human heart, through its sympathies and antipathies, the knowledge of itself.</p></blockquote><p>亚里士多德关于悲剧的描述： </p><blockquote><p> An imitation of an action that is serious, complete, and of a certain magnitude; in language embellished with each kind of artistic ornament, the several kinds being found in separate parts of the play; … in the form of action, not of narrative; through pity and fear effecting the proper purgation.</p></blockquote><p>这些公开课在看的时候我是没有记笔记的，后来发现几乎全都忘光了，而且看的时候难免会分心走神，因此还是推荐做一些总结，加强记忆和理解，否则看公开课就成了一种形式了，起不到扩展知识面的作用。</p><h2 id="摄影捕捉笔记"><a href="#摄影捕捉笔记" class="headerlink" title="摄影捕捉笔记"></a>摄影捕捉笔记</h2><p>19年的摄影陷入了瓶颈，同时拍摄热情也大大减少了。在写这一版块之前，我翻了翻手机相册，19年的作品少得可怜，更何况在这些片子之中基本上没有满意的作品。专门出去街拍的情况几乎没有，只是有时候碰巧出去会携带下GR II，因此相机SD卡里面的照片就更糟糕了。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%9B%B8%E6%9C%BA.jpg" alt="对于器材的热忱远比拍摄本身要多得多。19年在B站看的相机测评视频占据了总量的一大半，虽然我极力抑制，但那种狂热很快又会卷土重来"></p><p>为了寻求突破，我开始陆续看一些大师的摄影作品，而且尽量按照他们出版的摄影集的顺序看，毕竟目前还买不起纸质摄影出版物。从森山大道，到荒木经惟，再到深濑昌久，从薇薇安，罗伯特·卡帕，到罗伯特·弗兰克，再到亚历克斯·韦伯，平卡索夫，Stephen Gill，只要是有照片看出了某种意义，就会专门花时间找来看看该摄影师的作品集。下半年的时候，渐渐在B站上发现了一些“宝藏UP主 ”，他们有的将自己的作品剪成视频，有的讲解自己的拍摄心得和后期技巧，有的介绍摄影大师和他们的作品集，还有的是有了很高摄影鉴赏水平和摄影理念，直接在视频中讲解属于自己的摄影笔记，这些对我都有或多或少的帮助，非常感谢他们的分享。</p><p>回到我自己的摄影作品上来，拍景的明显多于拍人的，实际上，我个人一开始是倾向于拍人的，尤其是人景交织的作品对我最具有吸引力，人的独特性会让景更加生动化，富有情感。不过慢慢地，我不再执着于此，在开始拍之前，我不应该对将要拍到的内容做任何预期的规划，我觉得那样会破坏掉我拍摄时候的“捕捉力”，尤其对是任何未知的，不可预知的细节的捕捉，这些细节不局限于任何物和人，不局限于何种情绪，何种故事，恰是由于这些细节让观者解读到了属于他们每个人的独特的世界，成就了作品的魅力。</p><p>诚然，我私以为摄影理念应该会随着自己的拍摄历程的积累而不断地发生进化，这种进化可能是完善性的，也可能是转变性的，具体可能会取决于人的想象力。等到写20年总结的时候，或许自己又是另一番说法了。</p><p>我挑了几张自己的图片，基本上都是19年拍的，少数几张是19年之前的。自己的作品数量不多，质量也不高，希望20年能作为一个新的起点，多多去实践和思考。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%841.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%842.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%843.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%844.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%845.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%846.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%847.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%848.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%849.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%8410.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%8411.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%8412.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E8%87%AA%E6%91%8413.jpg" alt></p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>其实我在写总结之前，一直在想该取什么题目，由于第一次这样写年终总结，很多事情拿捏不准，有点小紧张。“漫长的告别”这个名字来源于美国侦探小说家雷蒙德·钱德勒的同名小说，书里面有句经典的话是“To say goodbye is to die a little.”，不过我取这个名字没有这么重的情怀，也没有这么哀伤，我只是觉得19年让我明白了很多事情，真正地认识到”这个充满偏见的，残酷的真实的世界“和渺小的自己，所以我想以此为人生的一个分割线，整理一下，告别20多年的过去，然后继续下一段路。</p><p>现在2020年已经过去一个月了，我希望这年可以：</p><ul><li><p>把自己未来3，4年的路明确下来，不管是读博还是工作，都希望能追求到一个满意的结果；</p></li><li><p>现场看一次音乐会和戏剧或者舞台剧；</p></li><li><p>好好写一篇论文；</p></li><li><p>多多看书，看不同类型的书，开卷有益；</p></li><li><p>坚持摄影，日常摄影；</p></li><li><p>多陪陪、关心身边爱我的人；</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;搭博客的时间大概只有一年，当初是因为受到&lt;a href=&quot;http://pluskid.org/&quot;&gt;pluskid网站&lt;/a&gt;和博客的影响，觉得做一个有趣的人，学习自己想了解的，放淡自己的心态是是每天很重要的事，因此想记录下来，有些规划，也好作为充实自己的见证，搁以前每年都来一个总结这件事我是不怎么做的。求学生涯没有结束，每年都是反复地上课，做项目，似乎都是浑浑噩噩度过，科研中的磕碰和一些不甘心也会时不时消磨自己的意志，让自己在怀疑，焦虑，麻木，强迫的交织中蹒跚着。我之前也知道很多励志结论，他们说的都很对，但每个人的生活总是悲喜交加的，而大多数情况下都是在平平无奇中暗生悲戚，羁绊越多的人似乎悲戚越浓。&lt;/p&gt;
&lt;p&gt;2019年的寒假里，我翻看了很多&lt;a href=&quot;http://freemind.pluskid.org/&quot;&gt;pluskid的博客&lt;/a&gt;内容，其中以年终总结最多。一开始我是被博主优秀的履历所吸引，后来在那些博客里我读到了对于生活细节的热爱，以及对于艺术，真理和其他人类活动的浓郁兴趣，真诚，质朴同时又很能温暖人，所以算是“始于履历，陷于才华”吧！（厚脸皮地说，有一种惺惺相惜之感）。罗曼罗兰在《米开朗琪罗传》中有句很出名的话：&lt;em&gt;“Il n’ya qu’un héroïsme au monde : c’est de voir le monde tel qu’il est et de l’aimer.（世界上只有一种真正的英雄主义，那就是认识生活的真相后依然热爱它）”&lt;/em&gt;，我开始渐渐明白其中的血肉故事，目前虽然谈不上热爱，但可以说是慢慢从走出到走入，慢慢进入状态。&lt;/p&gt;
&lt;p&gt;接下来，我想还是先从自己的学校科研生活讲起，然后再去讲讲自己看过的书，去过的地方，听过的音乐，拍过的照片，看过的电影等，一步步勾勒出自己的故事，就像是在索拉里斯星上模拟出的记忆花园，感想估计无法给出多少，倒是想能抓住几分情绪便好。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/Summaryofthisyear-1/%E7%94%B5%E5%BD%B1%E5%A4%A7%E5%B8%88%E5%89%A7%E7%85%A7.jpg&quot; alt=&quot;电影&amp;#39;大师(the Master)&amp;#39;剧照&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="纪录与总结" scheme="http://densecollections.top/categories/%E7%BA%AA%E5%BD%95%E4%B8%8E%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="learning" scheme="http://densecollections.top/tags/learning/"/>
    
    <category term="movies" scheme="http://densecollections.top/tags/movies/"/>
    
    <category term="reflection" scheme="http://densecollections.top/tags/reflection/"/>
    
    <category term="summary" scheme="http://densecollections.top/tags/summary/"/>
    
    <category term="memory" scheme="http://densecollections.top/tags/memory/"/>
    
  </entry>
  
  <entry>
    <title>RCNN-series-in-object-detection(续)</title>
    <link href="http://densecollections.top/posts/RCNNseries-2/"/>
    <id>http://densecollections.top/posts/RCNNseries-2/</id>
    <published>2020-01-10T02:51:19.000Z</published>
    <updated>2021-01-02T11:49:46.217Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>自Faster R-CNN后，基于深度学习的目标检测框架大致形成，且精度也较为不错。在这之后，围绕着对图像数据更深层次理解，以及根据现有结构进行改进成为了一个主流点。</p><p><a href="https://arxiv.org/abs/1611.10012">Speed/accuracy trade-offs for modern convolutional object detectors</a></p><p><a href="https://arxiv.org/abs/1905.05055">object detection in 20 years: a survey</a></p><p> <a href="https://arxiv.org/abs/1912.05190">IoU-uniform R-CNN: Breaking Through the Limitations of RPN</a></p><p><a href="https://arxiv.org/pdf/1909.02466v2.pdf">https://arxiv.org/pdf/1909.02466v2.pdf</a></p><h2 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a><a href="https://arxiv.org/abs/1605.06409">R-FCN</a></h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>内容有参考：</p><ul><li><a href="https://arleyzhang.github.io/articles/7e6bc4a/">blog1</a></li></ul><a id="more"></a><h3 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h3><h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a><a href="https://arxiv.org/abs/1612.03144">FPN</a></h2><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>Feature Pyramid Networks(FPN)考虑到了卷积神经网络中各个尺度特征图的作用，认为像Fast R-CNN和Faster R-CNN这样的目标检测网络只用了一个特征图去来做RoI 和搜寻proposal，不一定能很好地处理全部尺寸的物体，虽然这种方式是为了speed-acc之间的trade off。作者认为，网络越深层产出的特征图往往语义信息越高，但是位置信息比较模糊，对小目标检测来说不太好；浅层的特征图提取出来的特征都是比较低级的边缘，纹理信息，但是分辨率好，位置信息得到了保留，因此将这些特征图结合起来，充分利用到高层语义信息，同时也不丢掉位置信息，应当能很大程度上提高检测的精度和鲁棒性。</p><p>博客内容有参考：</p><ul><li><a href="https://vision.cornell.edu/se3/wp-content/uploads/2017/07/fpn-poster.pdf">FPN在CVPR的poster</a></li><li><a href="https://zhuanlan.zhihu.com/p/34144226">blog1</a>, <a href="https://www.jiqizhixin.com/articles/2017-07-25-2">blog2</a>, <a href="https://zhuanlan.zhihu.com/p/61536443">blog3</a></li></ul><h3 id="Content-1"><a href="#Content-1" class="headerlink" title="Content"></a>Content</h3><p>为了让各个尺寸的物体都能很好的检测到，以往的工作提出了图像金字塔，利用不同大小的图像尺寸进行滑窗，到了深度学习时代，直接通过神经网络输出的高层特征图，进行对应特征上的检测分类，之后考虑到尺度问题，开始挑选网络中产出的几个level的feature  map，分别进行检测，然后合并筛选给出最后的结果。FPN认为，既然检测既需要高层的特征便于分类和统筹全局观念，又需要特征图具有一定的分辨率去定位物体的图像位置，那么应该想个办法将这两个重要信息结合起来。但是在网络的前向传播中，这两者是矛盾的，低层的特征图特征抽象度不够，高层的特征图物体分辨率过低。我想，作者应该是受到当时resnet等跳级连接和FCN，U-Net等语义分割模型的启发，通过下采样提取高级特征，上采样恢复尺度，同时侧级连接补充位置信息，然后在每个上采样的特征图上进行检测来覆盖到各个物体（这一点借鉴了SSD）。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-2/various_pyramid_ways_in_cv.PNG" alt="目标检测中的金字塔模型"></p><p>如果没有每个特征图的预测，乍看就是FCN的经典结构。不过FPN的侧重点是为了结合高级语义特征和位置信息，因此加了一些额外的卷积操作，让网络在梯度下降中去focus这一点。下采样过程属于正常的网络操作，上采样时每个特征图进行2倍放大（<strong>最近邻插值</strong>，非转置卷积），当然这个2倍是根据你的下采样倍数来的，一般都是2倍，然后侧向对应的不是直接加过来（FCN），也不是叠操作（U-Net），而且先用个$1 \times 1$卷积处理下采样的特征图，然后加在一起，最后再做个$3 \times 3$的卷积。$1 \times 1$的卷积是为了减少通道数（上采样的通道数是固定的），否则不能相加，同时我想可能也是去提取一下位置信息，$3 \times 3$的卷积是为了处理一下加在一起后的特征图的混叠效应，提取出两者的有用信息。</p><p>值得一提的是，作者在论文中也说了，按照解决问题的思想，这样的金字塔形式应当是最简单的，没有加入任何其他的复杂技巧，实验效果证明效果也足够好，简单又有效。</p><blockquote><p>Simplicity is central to our design and we have found that our model is robust to many design choices. We have experimented with more sophisticated blocks (e.g., using multilayer residual blocks [16] as the connections) and observed marginally better results. Designing better connection modules is not the focus of this paper, so we opt for the simple design described above.</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-2/FPN_architecture.PNG" alt="FPN的结构示意"></p><p>实验方面，FPN主要针对RPN和Fast R-CNN进行，通过ablation study论证了FPN结构的有效性，同时结构之间的各个部件都是必要的。其中 RPN 和 Fast RCNN 分别关注的是召回率和正检率，在这里对比的指标分别为 Average Recall(AR) 和 Average Precision(AP)，分别对比了不同尺度物体检测情况。</p><p>以resnet为例，不管第一次feature map的缩小（$7 \times 7$的卷积），用后面的$\left\{ C_{2}, C_{3}, C_{4}, C_{5} \right\}$。对RPN来说，只用了$C_{4}$去生成预选框，然后利用pyramid of anchor去搜区域，由于FPN已经有了多尺度的作用，因此每个上采样的特征图中，会根据其特点设置一个固定size的anchor area，anchor aspect ratio还是保持[0.5, 1, 2]不变。为了照应到RPN中的512大小的anchor，FPN实验时多加了一个特征图（在$P_{5}$上下采样2倍），对应的区域大小和特征图level分别是：$\left\{ 32^{2}, 64^{2}, 128^{2}, 256^{2}, 512^{2} \right\} \longleftrightarrow \left\{ P_{2}, P_{3}, P_{4}, P_{5}, P_{6} \right\}$，用$P$代表上采样的特征图，便于区分下采样的$C$。从设置里可以看出，大特征图里找小物体，小特征图里找大物体。</p><p>实验Fast R-CNN时，固定FPN+RPN提取的proposal结果，在其中也加入FPN分别在$\left\{ P_{2}, P_{3}, P_{4}, P_{5} \right\}$中找物体做RoI Pooling，一样的，对于大尺度的RoI就用小的特征图，小尺度的RoI就用大的特征图。为了安排每个RoI Pooling尺度对应的特征图，作者给出如下公式：</p><script type="math/tex; mode=display">k=\left\lfloor k_{0}+\log _{2}(\sqrt{w h} / 224)\right\rfloor</script><p>其中，224是ImageNet的标准输入尺寸，$k_{0} = 5$是基准值，代表最后一个特征图，$w, h$分别代表RoI区域（RPN+FPN给的原图的proposal）的宽和高。假设RoI大小是$112 \times 112$，那么$k=5-1=4$，就在$P_{4}$特征图上找，做RoI Pooling。一般来说proposal大小不固定，所以应该取整处理。</p><p>因为resnet的Conv5也作为特征金字塔的一部分，而原先的Fast R-CNN和Faster R-CNN在RoI Pooling后面才接上Conv5继续提取特征，所以论文简单的加了两个1024维的fc层在分类器和回归器之前，代替一下原先Conv5的工作。</p><p>最后在加入FPN的Faster  R-CNN中进行参数共享，检测精度也得到了一定的提升。具体实验结果直接看图，不再赘述。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-2/FPN_FasterRcnn.png" alt="图来自：https://github.com/unsky/FPN"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-2/FPN_experiments.PNG" alt="部分实验结果"></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-2/FPN_QA.PNG" alt="FPN在CVPR现场QA"></p><p>FPN的贡献思想在于向上采样，融合了特征信息和位置信息，而且简洁有效。在这之后，何恺明率先利用FPN实现了Mask R-CNN，一统检测和实例分割，斩获马尔奖。现在FPN也被广泛使用，成为检测的必备组件（R-FCN由于自身设计缘故，无法加入FPN）。</p><p>但是，FPN设计中的上采样和侧向连接，其实主要是给小目标检测提供了帮助，因为主要是引入位置信息，然后放大特征图（实验结果也说明小目标检测精度提升多）。对于大目标来说，顶层特征图的高级语义固然重要，位置信息肯定还是没有底层特征图的多的，因此可以对一开始网络产出的浅层特征图跳级连接到顶层特征图，类似下面的结构（<a href="https://arxiv.org/abs/1803.01534">PANet</a>）：</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-2/FPN_for_bigobject.jpg" alt="FPN针对大物体检测改进"></p><h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h2><p><a href="https://zhuanlan.zhihu.com/p/133317452">https://zhuanlan.zhihu.com/p/133317452</a></p><h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><p><a href="https://arxiv.org/pdf/1912.04488.pdf">SOLO: Segmenting Objects by Locations</a></p><p><a href="http://deeplearning.csail.mit.edu/instance_ross.pdf">http://deeplearning.csail.mit.edu/instance_ross.pdf</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;自Faster R-CNN后，基于深度学习的目标检测框架大致形成，且精度也较为不错。在这之后，围绕着对图像数据更深层次理解，以及根据现有结构进行改进成为了一个主流点。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.10012&quot;&gt;Speed/accuracy trade-offs for modern convolutional object detectors&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.05055&quot;&gt;object detection in 20 years: a survey&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/1912.05190&quot;&gt;IoU-uniform R-CNN: Breaking Through the Limitations of RPN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1909.02466v2.pdf&quot;&gt;https://arxiv.org/pdf/1909.02466v2.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;R-FCN&quot;&gt;&lt;a href=&quot;#R-FCN&quot; class=&quot;headerlink&quot; title=&quot;R-FCN&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://arxiv.org/abs/1605.06409&quot;&gt;R-FCN&lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;内容有参考：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arleyzhang.github.io/articles/7e6bc4a/&quot;&gt;blog1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
    <category term="object detection" scheme="http://densecollections.top/tags/object-detection/"/>
    
    <category term="semantic segmentation" scheme="http://densecollections.top/tags/semantic-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>尝试远程控制Ubuntu服务器</title>
    <link href="http://densecollections.top/posts/controlLinuxserver/"/>
    <id>http://densecollections.top/posts/controlLinuxserver/</id>
    <published>2020-01-05T11:45:06.000Z</published>
    <updated>2021-01-02T12:24:04.090Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>暑假实习用公司的显卡的经历很舒服，所以寒假回家之前，想在教研室的工作站上搭个ssh，方便回家也可以用显卡跑网络。但是自己对这方面知识不是很了解，尤其是端口映射之类的操作，所以导致外网登陆服务器的时候折腾了一点时间，最后勉强利用ngork进行了内网穿透。。。使用过程中发现，ngork似乎有些延迟，加上scp传文件问题（我是win利用git bash登Ubuntu服务器）一直没解决，还是放弃了ssh转战了teamviewer（还是有点香的。。）</p><a id="more"></a><h2 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h2><p>我安装的时候没关心密匙之类的问题，也没去修改ssh 的config文件，直接在Ubuntu 18.04上安装openssh-server就好了。</p><p>在Ubuntu终端上输入：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get openssh-server</span><br><span class="line"># 启动ssh</span><br><span class="line">sudo service ssh start</span><br><span class="line"># 重启</span><br><span class="line">sudo service ssh restart</span><br><span class="line"># 查看有没有启动</span><br><span class="line">ps -aux | grep &#39;&#39;ssh&#39;&#39;</span><br><span class="line"># 关闭ssh</span><br><span class="line">sudo &#x2F;etc&#x2F;init.d&#x2F;ssh stop  </span><br></pre></td></tr></table></figure><p>然后我在windows上打开git bash，输入：</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/controlLinuxserver/log_command.PNG" alt="登陆命令"></p><p><code>-p</code>代表端口，默认是22，如果需要修改可以去<code>/etc/ssh/sshd_config</code>修改（改过记得source一下生效），参考<a href="https://blog.mythsman.com/post/5d2d65fca2005d74040ef873/">blog1</a>和<a href="https://www.jianshu.com/p/7028e5fecf2b">blog2</a>。端口后面跟的是<code>uername@ip_of_server</code>，密码输入服务器用户名密码，如果和服务器处于一个局域网，你的windows是可以无缝登陆的。服务器的ip地址可以通过<code>ifconfig</code>查询。</p><p>但是只能在一个局域网登陆没什么用处。。我们需要随时随地登陆服务器，这时候就需要科普一些公网，内网，端口映射等知识，看得我也有点眼花…后来我就找了个比较简单的内网穿透方法，也没有自己买服务器降低延迟啥的，基本上按照<a href="https://zhuanlan.zhihu.com/p/60962957">blog3</a>的教程，去<a href="https://ngrok.com/">ngork</a>下载软件，然后解压，在输入命令<code>./ngork tcp 22</code>，这个<code>tcp 22</code>默认端口22。在<code>Forwarding</code>一行会出现地址（<code>0.tcp.ngrok.io</code> ）和端口号，同样地在git bash输入即可。</p><h2 id="Teamviewer"><a href="#Teamviewer" class="headerlink" title="Teamviewer"></a>Teamviewer</h2><p>没自己买服务器做内网穿透的结果就是卡！卡！延迟让我有点受不了，而且scp一直搞不到win上来，也懒得查是不是自己命令哪里错了。当然除了上面的ngork外还可以搞个VPN，让你的电脑连上服务器所在的网，然后就可以按照最开始的方式登陆和传输了，暑假实习的公司就是这么干的，不过他们有专人维护这块，稳定性以及文件传输速度都很快。</p><p>最后…..</p><p>我还是转向了teamviewer，设置了固定密码。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/controlLinuxserver/teamviewer.PNG" alt="这延迟还能接受，传输速度也不赖，还要啥自行车..."></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;暑假实习用公司的显卡的经历很舒服，所以寒假回家之前，想在教研室的工作站上搭个ssh，方便回家也可以用显卡跑网络。但是自己对这方面知识不是很了解，尤其是端口映射之类的操作，所以导致外网登陆服务器的时候折腾了一点时间，最后勉强利用ngork进行了内网穿透。。。使用过程中发现，ngork似乎有些延迟，加上scp传文件问题（我是win利用git bash登Ubuntu服务器）一直没解决，还是放弃了ssh转战了teamviewer（还是有点香的。。）&lt;/p&gt;</summary>
    
    
    
    <category term="技术支持" scheme="http://densecollections.top/categories/%E6%8A%80%E6%9C%AF%E6%94%AF%E6%8C%81/"/>
    
    
    <category term="Ubuntu" scheme="http://densecollections.top/tags/Ubuntu/"/>
    
    <category term="ssh" scheme="http://densecollections.top/tags/ssh/"/>
    
    <category term="git" scheme="http://densecollections.top/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>RCNN series in object detection</title>
    <link href="http://densecollections.top/posts/RCNNseries-1/"/>
    <id>http://densecollections.top/posts/RCNNseries-1/</id>
    <published>2019-11-30T07:30:42.000Z</published>
    <updated>2021-01-02T11:47:11.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>Faster  R-CNN是目标检测领域中”two-stage”的代表性方法，其精度高，适应性强，兼具学术和工程价值。整个框架由于吸取了很多先前工作的经验，因此比较庞大，而且细节很多，因此需要认真研读下相关paper和Faster R-CNN的python代码。</p><p>在此之前，先贴上一位博主做的“<a href="https://nikasa1889.github.io/2017/05/02/The-Modern-History-of-Object-Recognition-—-Infographic-1/">The Modern History of Object Recognition — Infographic</a>”，其中也包括了“one-stage”的方法，不过2017年以后的没再更新了。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/HistoryOfObjectRecognition.png" alt="modern history of object recognition"></p><a id="more"></a><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><a href="https://arxiv.org/pdf/1311.2524.pdf">R-CNN</a></h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>内容有参考<a href="https://zhuanlan.zhihu.com/p/23006190?refer=xiaoleimlnote">blog1</a>和<a href="https://blog.csdn.net/WoPawn/article/details/52133338">blog2</a>。</p><p>R-CNN(Rich feature hierarchies for accurate object detection and semantic segmentation)是将深度学习应用于目标检测的开山之作，以前传统的目标检测算法使用滑动窗口法依次判断所有的可能区域，在该文章中，采用selective search方法先预先提取一系列可能是物体的候选区域（foreground），之后将这些候选区域（proposals）整合成固定的大小（$227 \times 227$， 采用AlexNet）送到预训练的CNN模型上提取特征然后进行fine-tuning迁移学习，然后通过fc6和fc7层，得到了针对目标检测任务的特征向量，然后再过SVM二分类器得到物体的每个类别分数（至于为什么用了两层fc，不直接用三层fc得到softmax分数，Ross他也在论文中作了说明和实验，主要原因就是proposal的筛选更加严格点，mAP更高点），得到2K个左右proposal的所属类别之后再做非极大抑制(NMS)，去掉那些多余的框，留下局部最优的建议框，然后根据这些剩下的框再去训练20个类别（针对pascal VOC数据）的regression器，最后终于得到图像中物体的种类和矩形框坐标信息。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/RCNN-overview.PNG" alt="R-CNN architecture overview"></p><p>下面对一些重点内容进行分析，同时由于是object detection based CNN开篇，所以也顺便加上评价指标等细节。</p><h3 id="评价指标IoU和AP"><a href="#评价指标IoU和AP" class="headerlink" title="评价指标IoU和AP"></a>评价指标IoU和AP</h3><p><strong>IoU(insertion of union).</strong>中文名为交并比，主要用来衡量框与框的重合程度，计算公式为$IoU=\frac{area(A \cap B)}{area(A \cup B)}$</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/IOU.png" alt="IoU"></p><p><strong>AP(Average Precision)与mAP(mean Average Precision).</strong>这是目标检测中常用的评价指标，AP代表每一类的检测精度，mAP代表所有类的整体检测精度，也就是所有类AP的平均值。AP一般是二分类问题的P-R曲线（precision-recall curve）与x轴围成的面积。</p><p>accuracy（准确率）=$(TP+TN)/(TP+FP+TN+FN)$，预测对的样本/所有样本；</p><p>precision（精准率）=$TP/(TP+FP)$，预测对的正样本/预测出的正样本；</p><p>recall（召回率）=$TP/(TP+FN)$，预测对的正样本/真正的正样本；</p><p><a href="https://blog.csdn.net/u013249853/article/details/96132766">如何绘制PR曲线?</a>首先将样本按照置信度从大到小排列，然后设置一个从高到低的阈值，大于该阈值的才能认定为正样本，否则为负样本，在该阈值下就能得到一组（P,R）值，阈值设的越细，点对就越多，这样连接成线就得到了P-R曲线。</p><p><a href="https://arleyzhang.github.io/articles/c521a01c/">AP的计算</a>一般是通过插值或者估算的方式进行的，并不是直接积分。</p><p>针对Pascal VOC数据来说，2010前后有两种计算方式，现在主要是<a href="https://www.zhihu.com/question/53405779/answer/419532990">第二种</a>，计算方法如下（Pascal VOC给出的评价文本结果是img_name+置信度+x1_lefttop+y1_lefttop+x2_rightbottom+y2_rightbottom，通过坐标计算IoU，大于0.5的认为是TP，小于等于0.5或者检测到同一个GT的多余框认为是FP，没有检测到GT认为是FN）：</p><blockquote><p>假设样本中有M个正样本，且每个样本都有预测的类别置信度和预测的GT（IoU大于0.5），根据置信度顺序给出各处的P-R值，画出曲线，根据样本设定[0/M, 1/M, 2/M, 3/M, …, M/M]这几个recall点，找出每个点之后最大的precision值（如果曲线不能全部遍历完则只遍历到最后出现的recall值，没有的对应点，precision做0处理），以precision为高，相邻recall点之间的距离为宽，相乘相加近似得到曲线与X轴的面积，即为AP</p></blockquote><p>算出所有类的AP，再做平均就得到了mAP。</p><p>举个例子：</p><p>我现在用训练好的Faster R-CNN测试自己的数据集，总共有三类：drone，bird，kite，最后会在<code>results</code>文件夹下生成三类检测的.txt文件。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/faster%20rcnn%20eval%20results.jpg" alt="三类检测的结果文件"></p><p>比如在<code>comp4_det_test_bird.txt</code>里面，前面几行是：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">njust_drone2videotest_0119 0.002 536.2 359.2 564.2 378.1</span><br><span class="line">njust_drone2videotest_0119 0.000 563.2 404.0 580.2 413.3</span><br><span class="line">njust_drone2videotest_0119 0.000 522.2 338.8 539.9 346.3</span><br><span class="line">njust_drone2videotest_0053 0.983 663.7 407.7 671.3 416.6</span><br><span class="line">njust_drone2videotest_0053 0.939 259.6 358.7 267.8 367.3</span><br><span class="line">njust_drone2videotest_0053 0.077 667.9 410.4 675.7 420.0</span><br><span class="line">njust_drone2videotest_0053 0.005 265.2 359.2 272.8 368.0</span><br><span class="line">njust_drone2videotest_0053 0.003 591.5 400.0 619.1 419.3</span><br><span class="line">njust_drone2videotest_0053 0.002 657.2 406.7 667.6 416.6</span><br><span class="line">njust_drone2videotest_0053 0.002 261.8 362.8 269.0 372.3</span><br><span class="line">njust_drone2videotest_0053 0.000 259.6 353.4 271.0 361.2</span><br><span class="line">njust_drone2videotest_0053 0.000 280.1 399.3 292.0 407.7</span><br><span class="line">njust_drone2videotest_0053 0.000 561.4 439.9 576.8 446.5</span><br><span class="line">njust_drone2videotest_0053 0.000 666.5 415.0 672.5 424.4</span><br><span class="line">njust_drone2videotest_0053 0.000 662.8 402.8 673.9 411.6</span><br><span class="line">njust_drone2videotest_0053 0.000 647.6 444.0 659.6 461.4</span><br><span class="line">njust_drone2videotest_0053 0.000 269.3 390.4 278.0 400.9</span><br><span class="line">njust_drone2videotest_0278 0.001 686.4 397.4 711.0 415.7</span><br><span class="line">njust_drone2videotest_0280 0.001 678.2 399.2 703.1 416.0</span><br><span class="line">njust_drone2videotest_0280 0.000 710.2 437.4 725.8 446.0</span><br><span class="line">njust_drone2videotest_0280 0.000 654.5 382.8 673.3 391.2</span><br><span class="line">njust_drone2videotest_0280 0.000 704.2 454.3 719.0 463.2</span><br></pre></td></tr></table></figure><p>之后需要对上述文本就行处理，主要是筛选检测框，比如设置置信度阈值，使用NMS等，然后根据推测的坐标和真值坐标计算IOU，大于0.5的设置框的GT为1，否则为0。值得注意的是，即使做过筛选后，最后得到的检测框还是可能误检（FP)，多检（FP，一个物体框了多个），漏检（FN），大致流程如下：</p><blockquote><p>首先对所有的结果按置信度排序，从高到低，然后根据坐标判断检测是否成功。排好序的坐标每一组都得和真实标注的GT进行比较计算IoU,大于0.5的认为检测成功，那么赋给他的标签就是1（TP)，否则是0(FP)，同时，每当一个GT被成功检测了，那么就会被标记，后续如果发现有其他的检测与这个GT重复，那么就把IoU最大的当作TP，其他多检的当作FP，遍历完成之后，如果存在没有匹配到的标注GT，也就是漏检，认定为FN。这样一整个流程下来就会得到TP,FP和FN。</p></blockquote><p>借用上头<a href="https://www.zhihu.com/question/53405779/answer/419532990">知乎链接</a>的答案，具体讲下怎么计算AP：</p><p>在筛选，NMS，比较等步骤之后，假设对于bird这一类，有如下检测结果(IoU&gt;0.5时GT=1，按照置信度排好了顺序)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BB  | confidence | GT</span><br><span class="line">----------------------</span><br><span class="line">BB1 |  0.9       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB2 |  0.9       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB1 |  0.8       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB3 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB4 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB5 |  0.7       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB6 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB7 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB8 |  0.7       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB9 |  0.7       | 1</span><br><span class="line">----------------------</span><br></pre></td></tr></table></figure><p>其中BB代表bounding box，其中两个BB1代表一个物体被框了两次，则应属于FP，因此在计算的时候要注意，虽然给出的GT是1，此外还有两个bird没有被检测出来，那么属于FN，整体上实际是正样本的有5+2=7个。现在从上到下以此按置信度大小计算（recall,precison）点对。（$recall \in [0/7, 1/7, 2/7, 3/7, 4/7, 5/7,  6/7, 7/7]) $</p><blockquote><p>1.首行以下认为预测值全为0，则TP=1(BB1)，BB2,BB5,BB8,BB9实际是1，但是预测为0，所以FN=4+2=6(两个漏检的)，FP=0，所以recall=1/(1+6)=0.14,precision=1/(1+0)=1.00;<br>2.第二行一下认为预测值全为0，则TP=2(BB1,BB2),BB5,BB8,BB9实际是1，但是预测0，所以FN=3+2=5,FP=0，所以recall=2/(2+5)=0.29,precision=2/(2+0)=1.00;<br>3.第三行一下认为预测值全为0，则TP=2(BB1,BB2,BB1为多余的，算FP)，BB5,BB8,BB9实际是1，但是预测是0，所以FN=3+2=5,FP=1,所以recall=2/(2+5)=0.29,precision=2/(2+1)=0.67;<br>4.第四行以下认为预测值全为0，则TP=2(BB1,BB2),BB5,BB8,BB9实际是1，但是预测是0，所以FN=3+2=5,BB3实际为0，预测为1，BB1多检一个，所以FP=2，则recall=2/(2+5)=0.29,precision=2/(2+2)=0.50;<br>5.第五行以下认为预测值全为0，则TP=2,FN=5,FP=3,所以recall=2/(2+5)=0.29,precision=2/(2+3)=0.40;<br>6.第六行以下认为预测值全为0，则TP=3,FN=4,FP=3,所以recall=3/(3+4)=0.43,precision=3/(3+3)=0.50;<br>7.第七行以下认为预测值全为0，则TP=3,FN=4,FP=4,所以recall=3/(3+4)=0.43,precision=3/(3+4)=0.43;<br>8.第八行以下认为预测值全为0，则TP=3,FN=4,FP=5,所以recall=3/(3+4)=0.43,precision=3/(3+5)=0.38;<br>9.第九行以下认为预测值全为0，则TP=4,FN=3,FP=5,所以recall=4/(4+3)=0.57,precision=4/(4+5)=0.44;<br>10.第十行以下认为预测值全为0，则TP=5,FN=2,FP=5,所以recall=5/(5+2)=0.71,precision=5/(5+5)=0.50</p></blockquote><p>接着根据每个不同的recall值去找对应的<strong>最大的precision值</strong>，即：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">recall&gt;&#x3D;0.00,precision_max&#x3D;1.00</span><br><span class="line">recall&gt;&#x3D;0.14,precision_max&#x3D;1.00;</span><br><span class="line">recall&gt;&#x3D;0.29,precision_max&#x3D;1.00;</span><br><span class="line">recall&gt;&#x3D;0.43,precision_max&#x3D;0.50;</span><br><span class="line">recall&gt;&#x3D;0.57,precision_max&#x3D;0.50;</span><br><span class="line">recall&gt;&#x3D;0.71,precision_max&#x3D;0.50;</span><br><span class="line">recall&gt;&#x3D;1.00,precision_max&#x3D;0.00;</span><br></pre></td></tr></table></figure><p>则</p><script type="math/tex; mode=display">AP=(0.14-0) \times 1 + (0.29-0.14) \times 1 + (0.43-0.29) \times 0.5 + \\(0.57-0.43) \times 0.5 + (0.71-0.57) \times 0.5 + (1-0.71) \times 0 = 0.50</script><p>实际上，这就是在P-R曲线上找出一些特定的recall点，然后利用多个矩形的面积和来近似代替曲线与X轴围成的面积。</p><p>如果是VOC2010之前，recall值的选取是固定的，即$recall \in [0, 0.1, 0.2, …, 1]$，对应的最大precision为1，1，1，0.5，0.5，0.5，0.5，0.5，0，0，0，此时AP的计算公式是11个precision的和的平均值，为0.5。</p><p>在COCO数据集中，IoU要求在[0,5, 0.95]区间每隔0.05取一次作为正样本判断阈值，然后计算出10个类似Pascal VOC的mAP，然后再做平均，作为最后的AP。COCO并不将AP和mAP做区分，COCO中的AP@0.5等同于Pascal中的mAP。</p><p>AP是衡量检测器性能的一个综合指标，但是对于你的数据集，可能并不是最适合的，因为有的数据场景认为误检几个影响不大，主要是都能检测出来，那么这时候recall值就大点好；有的数据场景呢，认为漏检几个没问题，但是不能检测错了，那么precision大点好。而AP是对recall和precision做了一个整体的评估，是检测器对数据普遍场景下的检测性能的打分。后续的一些研究工作发现了此评价指标在一些场景下水土不服，以及没有考虑具体物体检测的置信度得分情况而导致泛化能力失衡的情况，并对此做了一些改进，有兴趣的可以阅读下南京旷视研究院的两篇博文做个大概的了解。（<a href="https://zhuanlan.zhihu.com/p/55575423">blog1</a>, <a href="https://zhuanlan.zhihu.com/p/56899189">blog2</a>）</p><h3 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h3><p><strong>Region proposals—Selective Search(<a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf">paper</a>, <a href="https://blog.csdn.net/mao_kun/article/details/50576003">blog</a>,<a href="https://www.cnblogs.com/zyly/p/9259392.html">code</a>).</strong>最开始的阶段，算法要通过该方法提取出每张图像上存在物体的可能区域，即建议框，每张图片大概会提取2k左右。Selective Search算法属于经典计算机视觉范畴，主要是利用不同的尺度（颜色，纹理，大小等），采用图像分割，层次算法等，先分割成小区域，然后通过颜色直方图相近（颜色相似），梯度直方图相近（纹理相似）等规则合并，得到最终的proposals，由于这个方法严重限制了整体框架的速度，而且后面也被Faster R-CNN中的RPN取代，因此我也没有兴趣深究，感兴趣的读者可以通过原论文和代码进一步了解。</p><p><strong>CNN feature extraction.</strong>利用selective search得到的预选框由于大小不一，不方便过卷积层后reshape成统一维度的矩阵，因此需要进行resize等预处理。变形操作方面主要就是拉伸和填充的组合。</p><p>如下图所示，(A)是原图，(B)，(C)是各向同性变形，(D)是各项异性变形。(B)考虑了proposal周围的纹理内容（context），利用其扩充到$227 \times 227$，如果遇到了图像边界就利用proposal的像素均值填充；(C)不考虑proposal周围的像素信息，直接用其像素均值填充到$227 \times 227$；(D)是直接对proposal resize到$227 \times 227$，不过在此之前先进行padding处理，padding的尺寸分别为0和16，像素值为proposal的像素均值。论文给出的结论是padding=16加各向异性缩放的效果最好。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/different%20object%20proposal%20transformations.PNG" alt="different object proposal transformations"></p><p>网络层面，R-CNN选用了Alexnet和VGG这两个网络，由于VGG采用了更小的卷积核和小的stride，因此最后的检测mAP也更高。在训练方面，R-CNN利用的是在ILSVC数据集上的预训练分类模型，等于是进行了较好的初始化，有了一个比较通用的特征提取器，然后在Pascal VOC这样的小数据集上进行fine-tuning，这样的迁移学习也算当时的一个contribution，但是现在来看已经属于比较正常的操作了。以Alexnet为例，proposal经过其中的5个卷积层之后，得到的是$6 \times 6 \times 256$的特征图矩阵，再经过fc6和fc7之后得到了4096个神经元，最后再经过一个有21个神经元的fc，即分类数，最终得到了适合该数据样本的分类器（和GT的IOU大于0.5的那些proposals都会标为正样本，否则为负样本）,但是并不能准确预测位置。</p><p><strong>SVM classification.</strong>一般来说，根据图像分类的操作，都是将最后的全连接层改成需要分类个数的神经元，因此针对Pascal VOC来说，可以将最后的fc进行softmax，但是R-CNN却在fc7后面加了个SVM分类器，是什么原因导致这样做精度更高呢？根据论文的说法，Ross在微调CNN的时候，和GT的IOU大于0.5的那些proposals都会标为正样本，否则为负样本，这样的话对bndbox的限制非常宽松，也就是说监督信息不是严格有效的，因为CNN对小样本容易过拟合，因此这样的操作只是为了微调网络，尤其是最后的fc6和fc7，使其能够得到针对Pascal VOC数据的分类特征。采用SVM是因为SVM适用于少样本训练，而且由于是最后的分类，所以IOU的阈值设定也比较严格，目的是为了提供尽量正确的监督信息（GT为正样本，IOU小于0.3的负样本，这个0.3也是调参试出来的）。SVM分类器（<a href="https://blog.csdn.net/luoshixian099/article/details/51073885">可以与sigmoid函数结合，进行概率输出，在scikit-learn package中的SVM函数可以直接输出概率</a>，<a href="https://blog.csdn.net/v_JULY_v/article/details/7624837">理解SVM</a>）其实就是$4096 \times N$的权值矩阵，最后得到了每类的置信值，这样就得到了类别结果。此外还需注意的是，由于GT只有一个，而IOU小于0.3的proposal可能会有很多，这样就导致了训练SVM分类器的时候正负样本不均衡，论文中也提到，SVM的负样本是经过hard negative mining筛选的（负样本的处理是object detection中的一个技术细节和难点），具体怎么做的，我没有继续深究，我准备在Ross的下一篇文章“OHEM”进行梳理。</p><p><strong>NMS非极大值抑制。</strong>SVM分类器输出的是2k个proposal的类别置信矩阵，后面需要对每一类做NMS处理，去除掉无用的proposals，留下最接近的proposals（一个物体可能被selective search提取出了很多类似的proposals）。具体步骤如下：</p><p>假设是2000个proposals，然后是21类（加一个background），那么一张图片最后过SVM得到了一个$2000 \times 21$的矩阵，每一列代表这2k个proposals的每个类别的置信度，那么：</p><blockquote><p>1.对此矩阵按列按从大到小的顺序排列；<br>2.对每列，先选取该列最大的那么proposal，然后与该列后面每个得分对应的proposals计算IoU,如果大于设定阈值，则剔除该proposal，否则认定是这张图片存在该类别的多个物体；<br>3.对这一列剩下的次大的proposal进行2的操作，并不断重复，直到该列遍历完；<br>4.对21列（所有类）进行步骤2，3的操作</p></blockquote><p>经过NMS之后，就可以得到位置比较准确且类别置信度较高的一些proposals。</p><p><strong>Bounding box regression.</strong>分类完成并且NMS之后需要对proposals的位置进行精修，因为selective search得到的proposal并不是精确的目标检测器，因此还需要对物体的位置做进一步的修正。</p><p>a).如何设计回归？首先是挑选与GT比较接近的proposal（框）进行回归，如果差的太远，是没办法进行学习的。其次也不是直接回归矩形框的四个坐标，而是学习一种框的变换，即从检测框到GT的平移和缩放，这样的话会使网络学习比较稳定，直接回归无规律的坐标可能导致网络不稳定。如下图所示，P代表送入网络的region proposal，G是标注的GT，$\hat{G}$是学习过后的regression模型预测出的更接近GT的bounding box。其中$P=(P_{x},P_{y},P_{w},P_{h}), G=(G_{x},G_{y},G_{w},G_{h}), \hat{G}=(\hat{G_{x}},\hat{G_{y}},\hat{G_{w}},\hat{G_{h}})$，下标$x,y$代表矩形框中心坐标，$w,h$代表矩形框的宽和高。$d_{x}(P),d_{y}(P)$为待学习的平移变换，$d_{w}(P),d_{h}(P)$为待学习的缩放变换，即：</p><script type="math/tex; mode=display">\begin{array}{l}{\hat{G}_{x}=P_{w} d_{x}(P)+P_{x}} \\ {\hat{G}_{y}=P_{h} d_{y}(P)+P_{y}} \\ {\hat{G}_{w}=P_{w} \exp \left(d_{w}(P)\right)} \\ {\hat{G}_{h}=P_{h} \exp \left(d_{h}(P)\right)}\end{array}</script><p>(这里缩放变换用了exp可能还是因为网络回归小的值容易些，变化大的值不稳定）P实际上代表着proposal的信息，因为是对proposal进行修正，因此在回归学习时候，都可以看作是Alexnet最后一个pool层的线性函数，目的是学习对最后的feature map使其可以变换到真值附近的变换组合，即：</p><script type="math/tex; mode=display">d_{*}(P), *=x, y, w, h</script><script type="math/tex; mode=display">d_{\star}(P)=\mathbf{w}_{\star}^{\mathrm{T}} \boldsymbol{\phi}_{5}(P)</script><script type="math/tex; mode=display">Loss = \underset{\hat{\mathbf{w}}_{\star}}{\operatorname{argmin}} \sum_{i}^{N}\left(t_{\star}^{i}-\hat{\mathbf{w}}_{\star}^{\mathrm{T}} \boldsymbol{\phi}_{5}\left(P^{i}\right)\right)^{2}+\lambda\left\|\hat{\mathbf{w}}_{\star}\right\|^{2}</script><p>$\mathbf{w}_{\star}^{\mathrm{T}}$就是要间接梯度下降学习的参数，$\boldsymbol{\phi}_{5}(P)$代表最后pool层出来的结果，$\lambda$是正则项系数，防止过拟合，在论文中Ross提到这个参数很关键，否则效果不好，论文中设的值是1000。$t_{\star}$是要去学习的准确的变换，即从P到G的变换：</p><script type="math/tex; mode=display">\begin{aligned} t_{x} &=\left(G_{x}-P_{x}\right) / P_{w} \\ t_{y} &=\left(G_{y}-P_{y}\right) / P_{h} \\ t_{w} &=\log \left(G_{w} / P_{w}\right) \\ t_{h} &=\log \left(G_{h} / P_{h}\right) \end{aligned}</script><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/bbox%20regression.png" alt="bounding box regerssion--learning the transformation"></p><p>b).怎么训练？为了回归器有效训练，每类样本只采取与GT之间IoU最大的且大于0.6的region proposal，输入的是P和G的坐标信息，以及Alexnet的$pool_{5}$层特征，然后根据loss函数对每一类单独训练回归器。</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>1.虽然整体繁琐，占用内存大，速度也慢，但是技术框架和细节为后面的改进工作奠定了整体基础，具有开创意义；</p><p>2.论文写得非常详实，没有难懂的地方和句子，通读以后对其工作和贡献了解得很清楚；</p><p>3.考虑问题很全面，实验的各个因素都考虑到了，而且做了很多对比试验（Ablation Study），让人信服，有理有据。Ross的写作技巧和框架对自己写论文有很大的借鉴意义。</p><p>4.Ross论文中的推荐的<a href="http://dhoiem.web.engr.illinois.edu/publications/eccv2012_detanalysis_derek.pdf">object detection errors analysis </a>，进一步了解目标检测的错误分析。</p><h2 id="SPPNet"><a href="#SPPNet" class="headerlink" title="SPPNet"></a><a href="https://arxiv.org/pdf/1406.4729.pdf">SPPNet</a></h2><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>内容有参考<a href="https://zhuanlan.zhihu.com/p/24774302">blog1</a>和<a href="https://zhuanlan.zhihu.com/p/24780433">blog2</a>。</p></li><li><p>何恺明在ICCV2015上作的tutorial，<a href="http://kaiminghe.com/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf">Convolutional Feature Maps —Elements of efficient (and accurate) CNN-based object detection</a>，讲解了SPPNet的一些内容，同时对比了R-CNN, Fast R-CNN和Faster R-CNN。</p></li></ul><p>SPPNet是在R-CNN的基础上进一步提高目标检测的速度和精度。何恺明等作者认为，R-CNN每次都要将Selective Search提取出的2k左右的region proposals进行crop和warp，然后分别过卷积这样的操作太费时间而且可能做了很多重复的计算，因此他们认为可不可以直接将full image过一次卷积网络，然后在feature map进行操作，毕竟图像的特征在卷积提取之后都是一样的，由此SPPNet应运而生。</p><p>SPPNet的好处是速度得到了大幅提升，而且简化了部分操作，但是随之而来的问题是：</p><ol><li><p>region proposal的大小不一，但是conv之后的fc层是固定的向量长度，怎么去适应？</p></li><li><p>解决这个固定尺度的问题后，那么怎么找region proposal对应的feature map区域？</p></li></ol><p>对于第一个问题，SPPNet提出了spatial pyramid pooling（SPP, 空间金字塔池化），通过多次pooling输出不同尺度（预设好尺寸）的特征图并进行concatenate叠操作（类似U-Net），得到了固定长度的特征向量，这也是该论文的主要contribution；对于第二个问题，SPPNet通过简化receptive field和对应中心坐标计算，来近似得到region proposal图像的top-left, right-bottom坐标对应在最后feature map上的坐标，从而确定区域。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/RCNN-SPP-1.PNG" alt="R-CNN和SPPNet的结构比较"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/RCNN-SPP-2.PNG" alt="R-CNN和SPPNet的不同region proposal处理方式比较"></p><h3 id="Content-1"><a href="#Content-1" class="headerlink" title="Content"></a>Content</h3><p><strong>SPP空间金字塔池化</strong></p><p>不同的regional proposal对应的feature map区域尺寸看成一个个不同尺寸的小feature map，然后在该map上做几次不同的maxpooling，得到尺度依次变小的特征图，不改变channel数，然后将这些不同尺度的金字塔特征图reshape成一维的向量，然后合并在一起形成固定维度的fc层。由于特征金字塔的尺度是预设的，所以不管region proposal或者image的尺寸如何，都不会影响最后的分类。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/SPP-object-detection.PNG" alt="基于R-CNN的SPP示意图"></p><p>此外，由于可以tolerate multi-scale，论文也采用了多尺度的图像去训练，并做了实验，结果也证明确实比单一尺度要好。</p><blockquote><p> We develop a simple multi-size training method. For a single network to accept variable input sizes, we approximate it by multiple networks that share all parameters, while each of these networks is trained using a fixed input size. In each epoch we train the network with a given input size, and switch to another input size for the next epoch. Experiments show that this multi-size training converges just as the traditional single-size training, and leads to better testing accuracy.</p><p>Note that the above single/multi-size solutions are for training only. At the testing stage, it is straightforward to apply SPP-net on images of any sizes</p><p>SPP is better than no-spp, and full-image representation is better than crop</p></blockquote><p><strong>region proposal 映射</strong></p><p>在进行SPP之前，网络需要知道最后卷出来的feature map中哪些部分和最初Selective Search得到的原始region proposal是对应的。论文对此方法的解释比较简略，只在附录最后做了提及，估计也是个工程性，实验性的处理。其大致思路前面也已经说过，就是找左上和右下的对应点，而这种对应的映射关系主要由网络的感受野（receptive field）决定。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/mapping-a-window-to-feature-maps.PNG" alt="论文附录中提到的映射思路"></p><p><em>感受野</em></p><blockquote><p><em>The</em> receptive field is defined as the region in the input space that a particular CNN’s feature is looking at (i.e. be affected by)</p></blockquote><p>简而言之，感受野就是当前你的特征图上的像素点对应的是前面特征图的哪些部分区域（这个点是从多少视野中抽象出来的），这个区域一般是矩形大小的。对应分类来说，一般网络越深、感受野越大越好（高层语义信息越准确），对于目标检测，如果感受野太大，那么小目标的信息可能丢失，因此需要针对具体的任务去分析调节。</p><p>对于卷积之后特征图大小计算，感受野概念和推导来源，可以参考下面的references，我在之后的内容中就直接给出公式和结论了。</p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/23185164?refer=xiaoleimlnote">卷积神经网络(CNN)简介</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/44106492">卷积神经网络的感受野</a></p></li><li><p><a href="https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807">A guide to receptive field arithmetic for Convolutional Neural Networks—需要梯子</a></p></li><li><p><a href="https://fomoro.com/research/article/receptive-field-calculator#3,1,1,VALID;2,2,1,VALID;3,1,1,VALID;2,2,1,VALID;3,1,1,VALID;3,1,1,VALID;2,2,1,VALID">感受野网页计算器</a></p></li><li><p><a href="https://arxiv.org/pdf/1603.07285.pdf">A guide to convolution arithmetic for deep learning</a></p></li></ul><p>假设特征图是正方形的，特征图尺寸是$F$，感受野尺寸是$RF$，卷积核kernel_size是$k$，步长stride是$s$，填充padding大小是$p$，感受野中心坐标为$C$，下标$i$代表从上到下的顺序标号，则：</p><p>经过一次卷积后特征图大小变为（向下取整，比如22.5取22）：</p><script type="math/tex; mode=display">F_{i+1} = \lfloor (F_{i}+2p_{i}-k_{i}) / s_{i} \rfloor + 1</script><p>当前特征图的像素点对应上一特征图的感受野尺寸是（根据上面的公式逆推，下标意义不完全准确）：</p><script type="math/tex; mode=display">RF_{i}=(RF_{i+1}-1) * s_{i} + k _{i}</script><p>当前特征图在上一个特征图的感受野大小就是卷积核大小，即$k$。感受野计算一般不加padding，因为感受野是指在原图上的感受野，与填充无关，虽然逆推公式时候padding会有影响，但是将其忽略然后近似计算。如果要计算最后特征图在原图上的感受野，依次递归即可。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">举个例子：</span><br><span class="line">原图输入</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">k1&#x3D;3, s1&#x3D;2;</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">特征图1</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">k2&#x3D;3, s2&#x3D;2;</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">特征图2</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">k3&#x3D;3, s3&#x3D;1;</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">特征图3</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line"></span><br><span class="line">特征图3在原图上的感受野尺寸是多大？</span><br><span class="line">特征图3在特征图2上的感受野就是卷积核大小，即3；</span><br><span class="line">特征图3在特征图1上的感受野尺寸：(3-1)*2+3&#x3D;7;</span><br><span class="line">特征图3在原图上的感受野尺寸：(7-1)*2+3&#x3D;15;</span><br></pre></td></tr></table></figure><p>特征图像素点对应上一个特征图的感受野中心坐标：</p><script type="math/tex; mode=display">C_{i}=s_{i} * C_{i+1} + ((k_{i}-1)/2-p_{i})</script><p>这时候要考虑padding，因为坐标有影响。尺寸加上坐标就可以定位region proposal在特征图与特征图之间的映射关系。</p><blockquote><p>NIPS 2016的论文<a href="https://arxiv.org/abs/1701.04128">Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</a>提出了有效感受野的概念，也就是说感受野内部的每个像素的作用和贡献不是相同的，有效感受野仅占理论感受野的一部分，一般中心较多，属高斯分布影响。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/receptive-field-solution.PNG" alt="How to compute the center of the receptive field"></p><p>上图是何恺明简化的计算中心点的公式，也就是计算region proposal左上和右下坐标点的方法。</p><p>设$p_{i} = \lfloor (k_{i} /2) \rfloor$，当$k_{i}$为奇数时，$p_{i}=(k_{i}-1)/2$，则$C_{i}=s_{i}\times C_{i+1}$；当$k_{i+1}$为偶数时，$p_{i}=k_{i}/2$，则$C_{i}=s_{i}\times C_{i+1}-0.5$，由于坐标取的都是整数，所以近似认为$C_{i}=s_{i} \times C_{i+1}$，也就是说感受野的中心坐标只跟步长以及后面的中心坐标有关，因此通过这种关系一步一步将原图的region proposal坐标映射到feature map上。另外，可能考虑到近似处理的原因，论文最后对映射到feature map上的坐标值做了进一步处理：</p><p>左上坐标值：$x^{‘}=\lfloor x/s \rfloor + 1, y^{‘} = \lfloor y/s \rfloor + 1$；</p><p>右下坐标值：$x^{‘}=\lfloor x/s \rfloor - 1, y^{‘} = \lfloor y/s \rfloor - 1$；</p><p>也就是说，把区域缩小一点（左上点下移，右下点上移），应该是何恺明考虑到最后feature map上的点计算感受野映射回去的时候扩大了区域，所以做了一个这么经验化的处理。</p><p><strong>最后总结一下SPP方法，通过原图region proposal的左上和右下坐标，分别以各自作为中心坐标扩展出感受野大小的区域，然后映射到feature map上，找到对应的像素点，这样就定位出了原图region proposal对应的特征，然后对该特征进行预设金字塔尺寸的池化，最后对不同的特征图reshape，“叠”在一起给fc层，进行分类。</strong></p><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">Fast R-CNN</a></h2><h3 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h3><p>内容有参考：</p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/24780395">blog1</a>, <a href="https://blog.csdn.net/WoPawn/article/details/52463853">blog2</a>, <a href="https://blog.deepsense.ai/region-of-interest-pooling-explained/">blog3-RoI pooling</a></p></li><li><p>Ross Girshick在ICCV 2015上对自己论文的讲解，在他的个人<a href="https://www.rossgirshick.info/">主页</a>可以找到slide</p></li></ul><p>Fast R-CNN吸收了SPPNet的优点，并针对R-CNN冗余的的多推理结构，以及带来的占用内存资源多，推理速度慢的问题进行了改进，采用shared computation和mutli-task  learning将整个流程（原始图像region proposal提取除外）合在了一个网络中，最终使得推理速度得到了显著提升，占用内存减小，训练也更加容易便捷。</p><p><strong>R-CNN的问题</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/slow-RCNN.PNG" alt="R-CNN结构示意"></p><ul><li>分阶段训练，多个步骤，首先是softmax分类网络，然后时linear  SVM分类器，最后是bounding-box regression，而且每个阶段的训练样本都得按照各自的规则重新选取；</li><li>训练时间很长（84h），region proposals 要一个个地进行resize，然后一个个送进网络，占用大量硬盘空间；</li><li>训练好的结构推理时间长（利用vgg16 backbone一张图片47s）；</li></ul><p><strong>SPPNet的问题</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/SPP-net.PNG" alt="SPPNet结构示意"></p><p>SPP进行了图像级别的计算共享，只需在最后的feature map上进行region-wise computation计算，加速了推理时间。但是由于SPPNet的其他部分完全照搬R-CNN，所以也存在训练繁琐，训练时间长，占用磁盘空间大的问题。此外，Ross认为，SPPNet引入了一个新的问题：在训练时，SPP layer以下的卷积层等layer的参数无法更新。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/sppnet-problem.PNG" alt="训练时SPPNet的spp layer以下的层参数无法更新"></p><p>其原因主要跟mini-batch sample样本的策略有关，Ross也在其ICCV 2015的oral中解释了。SPPNet在训练时，一个batch随机采样128个regional proposal，这样的话样本有很大概率是来自于不同的图片，特征图上的一个proposal对应到原图上的感受野通常很大，最坏的情况是感受野是整个图像，那么每个batch可能就要去计算很多个几乎是整幅图像的梯度，而且不同图像间不能共享卷积计算和内容，反向传播会很慢且很耗内存。</p><p><strong>Fast R-CNN的优缺点</strong></p><ul><li><p>单个网络训练，占用内存少；</p></li><li><p>推理速度快，精度更高；</p></li><li><p>region proposal依然耗时；</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/fast-RCNN-testing.PNG" alt="Fast R-CNN网络结构示意图"></p><h3 id="Content-2"><a href="#Content-2" class="headerlink" title="Content"></a>Content</h3><p>如上图，Fast R-CNN的推理流程如下：</p><p>输入任意size的图片和Selective Search产生的region  proposals；原图经过一系列卷积层得到最终的特征图，并根据原region proposal找到feature map上的对应特征区域；对特征区域进行RoI pooling，得到固定大小的特征图，并经过fc层得到固定大小的特征向量（可由SVD分解加速推理）；特征向量继续向后传递产生两个branch，一个用softmax估计region proposal的类别（N+1个神经元），另一个回归bounding-box的平移和伸缩变换系数（4个神经元）；最后利用NMS剔除多余的框。</p><p><strong>RoI pooling</strong></p><p>RoI pooling是特殊的SPP，因为只有一个尺度的pooling，目的是为了不同尺寸的region proposal产生固定size的输出，兼容后面的fc层。论文中提到使用多尺度pooling对精度会有较小的提升，但是会带来成倍的计算量，所以只用单尺度是一种trade-off。此外RoI pooling的网格尺寸是自适应的，利用原proposal大小和期望输出的大小进行计算。</p><p>由于RoI pooling采用max pooling，是可微分的，用于BP进行反向传播更新参数。但是由于一幅图像中不同proposal之间内容会有重叠，所以特征图上一个比较大的值可能出现在不同的pooling后的proposal上，因此论文提出直接将其相加。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/RoI-BP.PNG" alt="论文对RoI pooling 梯度计算的描述"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/differentiable-RoI.PNG" alt="梯度计算示意（前面提到的blog2也有相关内容的详细描述）"></p><p><strong>one-stage training—SGD &amp; multi-task loss</strong></p><p>如前所述，如果采取SPPNet mini-batch sample样本的方式，梯度计算会很慢，将会很难训练网络。Fast R-CNN采用了向同幅图片随机采多个region proposals（hierarchical sampling）来加快梯度计算和传递：先随机采两幅图片，然后从这两幅图像中分别再各自采取64（128/2）个proposals，这样的话感受野会有很大重叠，梯度计算就不会很慢，占用内存也不会太大。但是Ross认为这种方式可能会导致网络收敛很慢，因为proposals很多都是相关的，不过实际中并没有出现这个问题。</p><p>分类损失函数：$L_{c l s}(p, u)=-\log p_{u}$，依然是交叉熵，分类采用softmax，未继续采用svm，实验证明采用前者精度更高，原因可能是因为多任务学习以及RoI pooling的作用。</p><p>回归损失函数不再是平方损失函数，而换成了$smooth_{L1}$损失函数：</p><script type="math/tex; mode=display">L_{l o c}\left(t^{u}, v\right)=\sum_{i \in\{x, y, w, h\}} \operatorname{smooth}_{L_{1}}\left(t_{i}^{u}-v_{i}\right)</script><script type="math/tex; mode=display">\operatorname{smooth}_{L_{1}}(x)=\left\{\begin{array}{ll}{0.5 x^{2},} & {|x|<1} \\ {|x|-0.5,} & {\text { otherwise }}\end{array}\right.</script><p>回归分支输出预测的平移伸缩变换参数：$t^{u}=\left(t_{x}^{u}, t_{y}^{u}, t_{w}^{u}, t_{h}^{u}\right)$，根据预测结果得到一个预测框，计算出真实平移伸缩参数：$v=\left(v_{x}, v_{y}, v_{w}, v_{h}\right)$，然后计算误差不断逼近即可。</p><p>训练中分类正样本是与GT的IoU属于[0.5,1]之间的proposal，负样本是与所有类别GT的IoU的最大值落在[0.1, 0.5)之间的proposal（0.1来自困难样本挖掘，论文中一笔带过），两者按照1：3配比（没有这个约束来平衡正负样本，训练出的模型精度会下降）。</p><p>Ross也在论文中做了对比实验，验证多任务学习确实多两者都有提升，也进一步说明这种一个流程下来的结构是符合期望也有不错的效果。</p><p>此外Ross也研究了多尺度训练和单尺度训练的问题（曾作为SPPNet的一个小contribution），也就是输入的原始图像的size不同，实验证明，deep neural network善于学习尺度不变性，对目标的scale不敏感，虽然多尺度效果效果确实好一点点，但是没必要以时间和硬件资源的牺牲来换取。</p><p><strong>SVD分解或许对提高目标检测的实时性有潜在的帮助</strong></p><p>由于开始提取出的region proposal比较多，所以大部分时间都会消耗在全连接层上（一张图片只要过一整遍卷积），为了提高速度，Ross实验了SVD分解全连接层，结果证明时间可以得到不小的提升，同时只损失很小的精度。</p><p>假设全连接层输入为$x$，输出为$y$，权重矩阵为$W$，尺寸为$u \times v$，则：</p><script type="math/tex; mode=display">y = Wx</script><p>其计算复杂度为$u \times v$，现对$W$进行SVD分解，并用前$t$个特征值近似代替，则：</p><script type="math/tex; mode=display">W=U_{u \times t} \sum \nolimits_{t \times t}  V_{v \times t}^{T}</script><script type="math/tex; mode=display">y=W x=U \cdot\left(\sum \cdot V^{T}\right) \cdot x=U \cdot z</script><p>实际上上述操作将一个全连接层拆成两个小的全连接，$z$是中间的全连接层。经过SVD分解后的计算复杂度变为$u \times t + v \times t$，如果$t$比$min(u,v)$小不少的话，可以显著减少计算量。</p><p><strong>more proposals is harmful</strong></p><p>由于region proposal的提取直接影响着后面检测的准确度，为了更高的recall，就要尽可能让region proposal覆盖到所有图像中的object，所以生成越多可能越好，但是Ross在实验中发现并不是这样，逐步增多Selective Search生成的region proposal的数量，发现mAP先增大后减小，我猜想可能是因为太多的proposal导致了相似信息增多，使得网络训练出现过拟合。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/more-proposals-is-harmful.PNG" alt="多数量的proposal并不会增高检测精度"></p><h3 id="Discussion-1"><a href="#Discussion-1" class="headerlink" title="Discussion"></a>Discussion</h3><p>Ross官方吐槽：一体化proposal提取。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/further-work-fast-rcnn.PNG" alt="Fast R-CNN进一步可以改进的地方"></p><h2 id="Faster-R-NN"><a href="#Faster-R-NN" class="headerlink" title="Faster R-NN"></a><a href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-NN</a></h2><h3 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h3><p>内容有参考：</p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/31426458">blog1</a>, <a href="https://zhuanlan.zhihu.com/p/24916624">blog2</a>, <a href="https://blog.csdn.net/WoPawn/article/details/52223282">blog3</a>, <a href="https://arleyzhang.github.io/articles/21c44637/">blog4</a>, <a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/">blog5</a>, <a href="https://blog.csdn.net/XiangJiaoJun_/article/details/85008477">blog6</a>;—-时间短直接看blog1和blog5</p></li><li><p>Ross Girshick 在ICCV2015tutorial上的演讲<a href="http://mp7.watson.ibm.com/ICCV2015/slides/iccv15_tutorial_training_rbg.pdf">Training R-CNNs of Various Velocities: Slow, Fast, and Faster</a></p></li></ul><p>Faster  R-CNN的主要贡献在于提出anchor机制，利用RPN网络和Fast R-CNN一体化检测框架，实现一个完全end-to-end的目标检测网络。因此，阅读该论文需要解决两个主要问题，而且大部分内容应该着重解决第一个问题：</p><ul><li><p>RPN是如何工作的，具体设计流程和实现细节？</p></li><li><p>PRN加上Fast R-CNN怎么训练？</p></li></ul><h3 id="Content-3"><a href="#Content-3" class="headerlink" title="Content"></a>Content</h3><p><strong>Faster R-CNN是个非常复杂和精细的结构，包含了很多细节和参数，只通过读论文去完全了解是不现实的，想真正理解必须要去读下源码，如果能手撕出来的话就更好了。</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/FasterRCNN_architecture.png" alt="FasterRCNN_architecture，图来自blog5"></p><p>上图是Faster R-CNN的整体网络框架，图片在进入网络前要减去RGB三通道像素均值，并适当resize处理。处理好的图像矩阵送入一系列卷积层进行特征提取，过max pooling缩小特征图尺寸，在特定的卷积层后，特征图送入RPN网络，再过一个$3\times 3$的卷积层，然后分别送入两个$1 \times 1$的卷积层对生成的anchor进行前景/背景的二分类，并对其进行靠近GT的坐标回归修正；之后从中挑出topN的已经修正过坐标的anchor给RoI pooling从最后的特征图（输入PRN的特征图和RoI pooling的特征图是一样的）中抠出相应的feature region然后合成固定长度的向量送给最后的fc层去分类所有的类别，回归所有类别的坐标，进行二次修正。</p><p><strong>RPN</strong></p><p>RPN的全称是Region Proposal Network，是用以替代R-CNN和Fast R-CNN中的Selective Search方法，实现端对端的object预选框的提取工作。神经网络擅长于分类，同时我们结合R-CNN等前人的工作，要想通过网络去提取出一系列预选框该怎么做？以往经典视觉是通过滑动窗口或者图像金字塔的方法，但是类似的方法移植到网络上太费时间：可以用不同的卷积核代表滑动窗口，然后卷完再分类；用不同尺寸的特征图代表图像金字塔，然后再分类，这样的话就等于几乎复制了一遍R-CNN这样的架构，速度可能会更慢。因此，任少卿采取了pyramids-of-filter，pyramids-of-images之外的另一种替代的方法：pyramids-of-reference-boxes，也就是说，在RPN里面，我们自己预先根据一定的规则，生成许多的矩形框，覆盖住整个图像的几乎每个object的内容，然后再用网络去识别哪些是“好的”，哪些是“坏的”，然后让“好的”尽量好点，送给Fast R-CNN去进一步识别检测，“坏的”直接丢弃掉。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/pyramids_of_achors.PNG" alt="pyramids-of-images, pyramids-of-filters, pyramids-of-reference-boxes"></p><p>这些参考框称为anchor，具体生成方式如下：</p><p>参照Faster R-CNN的网络结构图，假设输入给RPN的特征图尺寸比原图缩小了16倍，特征图上的每一个像素点，或者说grid，可以看作是原图的$16 \times 16$的gird的缩小版，然后以每个原图上的这些$16 \times 16$的grid的中心为参考点，画出预设面积和宽高比的矩形框，这就作为一个初始的预选框。论文设置了三个anchor scale（代表面积）和三个宽高比，分别是[8, 16, 32]，[0.5, 1, 2]，其中anchor scale的base是16，也就是说预设的面积分别是：$16 \times 8 = 128, 16 \times 16 = 256, 32 \times 16 = 512$，宽高为：$area \times \sqrt{aspect ratio}, area \times \sqrt{1/aspectratio}$（这里说个题外话，在训练自己的数据集的时候，可能需要修改anchor scale和anchor aspect ratio，但是我自己平时实验发现，改anchor scale的作用稍微大点，比如检测小的目标的时候，改ratio效果就不怎么明显，我曾经统计了标注数据的长宽比例，然后根据此比例当作anchor aspect ratio训练，结果效果差不多，我猜想这可能就是二次回归修正的功劳，只要你覆盖到了，挑选到了就好说。）</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/anchors.png" alt="生成anchor示意图"></p><p>在第一次读论文的时候我以为是根据送进RPN的那个特征图的每个grid的感受野去得到原图的位置，然后画框，但是这样的话，每个grid在原图的位置就固定了，而每个grid代表的感受野不能像anchor那样去覆盖几乎所有的object，数量远不如anchor那么多，那么必然是会漏掉的。另外一点是跟PRN的网络结构有关。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/RPN_architecture.PNG" alt="RPN_architecture"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/RPN_Network.png" alt="详细的RPN_Network示意图"></p><p>看上面两个图，RPN不是用fc来分类和回归的，而是直接用全卷积来完成，有点类似FCN。一开始是对输入的特征图进行$3 \times 3$卷积，完成进一步特征提取，非线性化和降维之类的工作，然后再用两个$1 \times 1$卷积完成分类和回归，值得注意的是，里面的卷积操作不改变feature map的尺寸，也就是grid的数量不变，最后分类和回归卷积层输出的tensor的尺寸分别是$(9 \times2, h, w), (9 \times 4, h, w)$，通道数分别为18（前景\背景2类，每个grid有9个anchor）和64（每个grid有9个anchor，每个anchor回归4个坐标），其后的reshape是便于softmax和回归处理。</p><p>也就是说，RPN通过全卷积，强制让网络去学习每个grid生成的9个预设anchor的质量，并进行适当的修正。实际上，我个人还是比较疑惑的，我不理解为什么这种形式可以去学习特征图在原图对应的anchor哪些是promising的，因为anchor只在训练时分配GT时有用，网络学习时并没有把anchor作为输入的一部分，但是事实证明，RPN确实可以学到，毕竟Faster R-CNN的检测效果摆在那儿。我大概猜测，特征图的每个grid经过$3 \times 3$卷积后，包含了这9个anchor的一些信息，再分别经过$1 \times 1$卷积后又分别提取出了类别和坐标信息，网络隐式地学习到了anchor的存在以及他们的信息。</p><p>此外，论文中还提到anchor这种方式也具有平移不变性，就是你物体动了，我anchor也会跟着动，这对于图像分类问题来说是很好的一个特性，不管物体在哪个位置都要求较高的准确性。但是对于目标检测来说，还需要定位出物体在图片上的位置，因此平移不变性会抹掉这点。</p><p>RPN的训练和推理不是完全一样的。训练RPN时，预先产生的anchor数量很多，可能有上万个，这么多的框确实会包含到几乎所有的object信息，但是很多都是冗余的，也有很多是背景，因此不能全部拿去训练，否则不仅速度慢，效果可能还不好。</p><p>论文中训练的anchor总量只有256个（也就是说每次训练只有256个位置有GT，没有GT的就不去计算误差），但是每张图片生成的训练anchor的位置是不一样的，所以图片比较多，训练比较充足的情况下，基本上整个网络应该都是可以训练到的（但是我感觉有些anchor位置可能就是比较容易选中训练，比如原图中心部分）。</p><p>训练RPN时候需要先计算anchor的label和坐标变换系数：</p><ul><li><p>foreground认定：anchor box和GT box的IoU超过了设定阈值（论文是0.7），为了防止没有超过的，对于每一个GT box，anchor box跟它有最大IoU的也认为是前景框；</p></li><li><p>background认定：anchor box和GT box的IoU低于设定阈值（论文是0.3）；</p></li><li><p>落于两个阈值之间的认为是“don’t care”。此外，还有一个<code>TRAIN.RPN_FG_FRACTION</code>参数，一般是0.5，也就是每个batch size中采取的前景anchor只有$256 \times 0.5 =128$个，如果超过了，就随机挑128个。</p></li></ul><p>训练好的RPN需要给Fast R-CNN提供一些可靠的RoI，这时RPN进入推理阶段：</p><ul><li><p>将PRN输出的偏移量给原始的anchor，得到新的区域，同时带有置信度score；</p></li><li><p>对超出图像范围的框进行裁剪，保证框都在图像内部，这一步没有改变框的数量；</p></li><li><p>丢掉小于设定的最小尺寸(<code>TRAIN.RPN_MIN_SIZE = 8</code>)的anchor；</p></li><li><p>根据置信度，选择topK(<code>TRAIN.RPN_PRE_NMS_TOP_N = 12000</code>)个框，和设定的NMS(<code>TRAIN.RPN_NMS_THRESH = 0.7</code>)阈值，通过非极大抑制去除冗余的框；</p></li><li><p>从NMS之后的框中，选择得分为topN（<code>TRAIN.RPN_POST_NMS_TOP_N = 2000</code>）的框作为region proposal输入给Faster R-CNN；</p></li></ul><p>到此为止，RPN完成了类似Selective Search的工作。</p><p>类似于Fast R-CNN的操作，这些2000个region proposal并不会全部进行RoI pooling，而是从中挑出128个样本进行训练：</p><p>region proposal与任一GT box之间的IoU 大于阈值(<code>TRAIN.FG_THRESH=0.5</code>)认为是前景，在0.1(<code>TRAIN.BG_THRESH_LO</code>)和0.5(<code>TRAIN.BG_THRESH_HI</code>)之间认为是负样本；</p><p>从前景中挑出一定数量（batch size乘以比例）作为正样本，比例为<code>TRAIN.FG_FRACTION = 0.25</code>，如果正样本没有这么多，则全部选出来，其他的用负样本凑。</p><p>如果使用VGG16作为特征提取网络，那么送入RPN的特征图就是最后的特征图，然后利用RoI找到基于原图的128个region proposal对应在这个特征图上的特征向量，最后送到fc层进行分类和回归即可（回归坐标的时候有个bbox_inside_weights，只有前景的时候为1，背景为0，计算loss时候通过这个weight来抹掉背景）。</p><p>对于resnet101而言，网络相对而言已经很深了，上面提到的平移不变性此时在最后输出的特征图上基本已经消失了，包含的都是一些高层的语义信息，如果还是把最后的特征图给RPN，那么特征图再去卷积学习anchor的信息，然后提取，将会很难，最后的检测结果也不一定理想，所以此时送入RPN的特征图需要稍微往前些。一般我看的都是把Conv4出来的特征图送给RPN，然后RoI pooling提取出对应的特征区域后再过最后的Conv5，最后送入fc。</p><p>此外，RoI pooling也是影响Faster R-CNN精度的一个部分，后面出现了RoI Align（Mask R-CNN）, Crop pooling（<a href="https://arxiv.org/pdf/1506.02025.pdf">Spatial Transformer Networks</a>, <a href="https://zhuanlan.zhihu.com/p/37110107">blog</a>）, RoI warp（<a href="https://blog.csdn.net/Julialove102123/article/details/80567827">blog</a>）等取代，目的都是想尽可能地提高回归精度。这些细节我们后续再谈。</p><p>在利用Faster R-CNN进行预测的时候，跟训练Fast R-CNN部分差不多，但是region proposal的数量等参数会有所不同，比如PRN在NMS之前的nchor数量设置为6000，在NMS之后设置为300，然后Fast R-CNN直接对这300个进行推理，NMS之后输出预测框即可。</p><p><strong>如何训练Faster R-CNN?</strong></p><p>RPN网络的loss函数是二分类交叉熵和$soomth \quad  L_{1}$回归损失：</p><script type="math/tex; mode=display">\begin{aligned} L\left(\left\{p_{i}\right\},\left\{t_{i}\right\}\right)=& \frac{1}{N_{c l s}} \sum_{i} L_{c l s}\left(p_{i}, p_{i}^{*}\right) +\lambda \frac{1}{N_{r e g}} \sum_{i} p_{i}^{*} L_{r e g}\left(t_{i}, t_{i}^{*}\right) \end{aligned}​</script><p>$\frac{1}{N_{c l s}}, \frac{1}{N_{r e g}},  \lambda$分别是分类正负样本mini bacth size（256），anchor数量（大约2400）正则化系数和两个loss的权重系数，论文中说明前两个正则化系数可以不用，效果没差，$\lambda$也不是敏感系数，取值扩大100倍也不会太影响精度。</p><p>回归部分依然是学习平移和缩放变换系数，按照我前面的说法，RPN是强制是学习有anchor这么个东西，然后再去学习辨别anchor，修正其位置。论文中给出的公式也说明，RPN应当是先学到anchor的位置，再去学GT的位置。当然，给回归部分的GT是anchor和对应GT box之间的变换系数。</p><script type="math/tex; mode=display">t_{\mathrm{x}}=\left(x-x_{\mathrm{a}}\right) / w_{\mathrm{a}}, \quad t_{\mathrm{y}}=\left(y-y_{\mathrm{a}}\right) / h_{\mathrm{a}} \\t_{\mathrm{w}}=\log \left(w / w_{\mathrm{a}}\right), \quad t_{\mathrm{h}}=\log \left(h / h_{\mathrm{a}}\right) \\t_{\mathrm{x}}^{*}=\left(x^{*}-x_{\mathrm{a}}\right) / w_{\mathrm{a}}, \quad t_{\mathrm{y}}^{*}=\left(y^{*}-y_{\mathrm{a}}\right) / h_{\mathrm{a}} \\t_{\mathrm{w}}^{*}=\log \left(w^{*} / w_{\mathrm{a}}\right), \quad t_{\mathrm{h}}^{*}=\log \left(h^{*} / h_{\mathrm{a}}\right)</script><p>Fast R-CNN部分的loss在前一节已经说了，就不再赘述，所以Faster R-CNN一共有rpn_cls, rpn_box, rcnn_cls, rcnn_box这四个loss。</p><p>最后需要解决的是Faster R-CNN是如何联合训练的问题。RPN结合Fast R-CNN有点像GAN，但是两者之间不是对抗的关系，而是相互依存的，而且Fast R-CNN更依赖具有良好性能的RPN多一点。作者在论文中分别提到了3种方法，分别是：<em>Alternating training, Approximate joint training, Non-approximate training</em></p><p><em>Alternating training</em>是分别训练两个网络，比如用CNN特征提取网络VGG16去初始化RPN，训练让其收敛，再让RPN产生的proposal去训练Fast R-CNN，让其收敛…循环往复；<em>Approximate joint training, Non-approximate training</em>都是将其当成一体的，直接利用SGD训练，两种的区别在于前者丢弃RPN的bbox预测梯度，后者则不丢弃，这两点论文都未细说具体细节，比如丢弃RPN的回归梯度的话，anchor还需要向GT box修正吗？</p><p>根据其他博客的内容，官方源码给出的训练思路是这样的（类似第一种交替迭代训练方法），估计这也是目前常用的流程（可能要看完源码之后才清楚这件事）：</p><ul><li><p>利用在ImageNet上预训练好的CNN模型，训练RPN；</p></li><li><p>利用上一步训练好的RPN产生的proposals训练已被ImageNet预训练模型初始化的Fast R-CNN：</p></li><li><p>利用上一步的Fast R-CNN初始化RPN，不更新共享的特征提取网络，仅仅更新RPN独有的卷积层，重新训练RPN；</p></li><li><p>加入Fast R-CNN，形成一个整体，但是只训练Fast R-CNN特有的卷积层和fc层，共享卷积层参数冻结，proposals来自上一步训练好的RPN；</p></li></ul><p>上述过程类似迭代训练并且进行了两次，作者提到循环多次没有更多的提升。</p><h3 id="Discussion-2"><a href="#Discussion-2" class="headerlink" title="Discussion"></a>Discussion</h3><p>Faster R-CNN中最大的contribution可能就是anchor的使用了，解决了物体中尺度的多样性问题。此外，根据这篇<a href="https://zhuanlan.zhihu.com/p/73024408">博客</a>的内容，anchor的使用还顺带解决了<strong>gt box与gt box之间overlap过大导致gt box丢失问题</strong>。大意就是两个不同的物体标的框重合度很大，导致CNN特征图里面也分不清了，这样的话这两个物体可能一起存在特征图的一个grid里面，就会丢掉一个物体，但是anchor是在原图搜的，每个grid都有不同尺度的内容，因此可以一定程度缓解recall降低（漏检）的问题。当然“抠图”来进行二次回归的操作，也一定程度上解决了级联回归的feature alignment问题。</p><p>（最近的新文章: <a href="https://arxiv.org/pdf/1912.05190.pdf">IoU-uniform R-CNN: Breaking Through the Limitations of RPN</a>，还没来得及看，后面搞懂了再来开一篇说下，顺便也加上anchor  free）</p><p>基于pytorch的<a href="https://github.com/jwyang/faster-rcnn.pytorch/tree/pytorch-1.0">faster rcnn</a>中的<a href="https://github.com/jwyang/faster-rcnn.pytorch/blob/pytorch-1.0/lib/model/utils/config.py">config</a>文件，里面设置了几乎所有的参数：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Training options</span><br><span class="line">#</span><br><span class="line">__C.TRAIN &#x3D; edict()</span><br><span class="line"></span><br><span class="line"># Initial learning rate</span><br><span class="line">__C.TRAIN.LEARNING_RATE &#x3D; 0.001</span><br><span class="line"></span><br><span class="line"># Momentum</span><br><span class="line">__C.TRAIN.MOMENTUM &#x3D; 0.9</span><br><span class="line"></span><br><span class="line"># Weight decay, for regularization</span><br><span class="line">__C.TRAIN.WEIGHT_DECAY &#x3D; 0.0005</span><br><span class="line"></span><br><span class="line"># Factor for reducing the learning rate</span><br><span class="line">__C.TRAIN.GAMMA &#x3D; 0.1</span><br><span class="line"></span><br><span class="line"># Step size for reducing the learning rate, currently only support one step</span><br><span class="line">__C.TRAIN.STEPSIZE &#x3D; [30000]</span><br><span class="line"></span><br><span class="line"># Iteration intervals for showing the loss during training, on command line interface</span><br><span class="line">__C.TRAIN.DISPLAY &#x3D; 10</span><br><span class="line"></span><br><span class="line"># Whether to double the learning rate for bias</span><br><span class="line">__C.TRAIN.DOUBLE_BIAS &#x3D; True</span><br><span class="line"></span><br><span class="line"># Whether to initialize the weights with truncated normal distribution</span><br><span class="line">__C.TRAIN.TRUNCATED &#x3D; False</span><br><span class="line"></span><br><span class="line"># Whether to have weight decay on bias as well</span><br><span class="line">__C.TRAIN.BIAS_DECAY &#x3D; False</span><br><span class="line"></span><br><span class="line"># Whether to add ground truth boxes to the pool when sampling regions</span><br><span class="line">__C.TRAIN.USE_GT &#x3D; False</span><br><span class="line"></span><br><span class="line"># Whether to use aspect-ratio grouping of training images, introduced merely for saving</span><br><span class="line"># GPU memory</span><br><span class="line">__C.TRAIN.ASPECT_GROUPING &#x3D; False</span><br><span class="line"></span><br><span class="line"># The number of snapshots kept, older ones are deleted to save space</span><br><span class="line">__C.TRAIN.SNAPSHOT_KEPT &#x3D; 3</span><br><span class="line"></span><br><span class="line"># The time interval for saving tensorflow summaries</span><br><span class="line">__C.TRAIN.SUMMARY_INTERVAL &#x3D; 180</span><br><span class="line"></span><br><span class="line"># Scale to use during training (can list multiple scales)</span><br><span class="line"># The scale is the pixel size of an image&#39;s shortest side</span><br><span class="line">__C.TRAIN.SCALES &#x3D; (600,)</span><br><span class="line"></span><br><span class="line"># Max pixel size of the longest side of a scaled input image</span><br><span class="line">__C.TRAIN.MAX_SIZE &#x3D; 1000</span><br><span class="line"></span><br><span class="line"># Trim size for input images to create minibatch</span><br><span class="line">__C.TRAIN.TRIM_HEIGHT &#x3D; 600</span><br><span class="line">__C.TRAIN.TRIM_WIDTH &#x3D; 600</span><br><span class="line"></span><br><span class="line"># Images to use per minibatch</span><br><span class="line">__C.TRAIN.IMS_PER_BATCH &#x3D; 1</span><br><span class="line"></span><br><span class="line"># Minibatch size (number of regions of interest [ROIs])</span><br><span class="line">__C.TRAIN.BATCH_SIZE &#x3D; 128</span><br><span class="line"></span><br><span class="line"># Fraction of minibatch that is labeled foreground (i.e. class &gt; 0)</span><br><span class="line">__C.TRAIN.FG_FRACTION &#x3D; 0.25</span><br><span class="line"></span><br><span class="line"># Overlap threshold for a ROI to be considered foreground (if &gt;&#x3D; FG_THRESH)</span><br><span class="line">__C.TRAIN.FG_THRESH &#x3D; 0.5</span><br><span class="line"></span><br><span class="line"># Overlap threshold for a ROI to be considered background (class &#x3D; 0 if</span><br><span class="line"># overlap in [LO, HI))</span><br><span class="line">__C.TRAIN.BG_THRESH_HI &#x3D; 0.5</span><br><span class="line">__C.TRAIN.BG_THRESH_LO &#x3D; 0.1</span><br><span class="line"></span><br><span class="line"># Use horizontally-flipped images during training?</span><br><span class="line">__C.TRAIN.USE_FLIPPED &#x3D; True</span><br><span class="line"></span><br><span class="line"># Train bounding-box regressors</span><br><span class="line">__C.TRAIN.BBOX_REG &#x3D; True</span><br><span class="line"></span><br><span class="line"># Overlap required between a ROI and ground-truth box in order for that ROI to</span><br><span class="line"># be used as a bounding-box regression training example</span><br><span class="line">__C.TRAIN.BBOX_THRESH &#x3D; 0.5</span><br><span class="line"></span><br><span class="line"># Iterations between snapshots</span><br><span class="line">__C.TRAIN.SNAPSHOT_ITERS &#x3D; 5000</span><br><span class="line"></span><br><span class="line"># solver.prototxt specifies the snapshot path prefix, this adds an optional</span><br><span class="line"># infix to yield the path: &lt;prefix&gt;[_&lt;infix&gt;]_iters_XYZ.caffemodel</span><br><span class="line">__C.TRAIN.SNAPSHOT_PREFIX &#x3D; &#39;res101_faster_rcnn&#39;</span><br><span class="line"># __C.TRAIN.SNAPSHOT_INFIX &#x3D; &#39;&#39;</span><br><span class="line"></span><br><span class="line"># Use a prefetch thread in roi_data_layer.layer</span><br><span class="line"># So far I haven&#39;t found this useful; likely more engineering work is required</span><br><span class="line"># __C.TRAIN.USE_PREFETCH &#x3D; False</span><br><span class="line"></span><br><span class="line"># Normalize the targets (subtract empirical mean, divide by empirical stddev)</span><br><span class="line">__C.TRAIN.BBOX_NORMALIZE_TARGETS &#x3D; True</span><br><span class="line"># Deprecated (inside weights)</span><br><span class="line">__C.TRAIN.BBOX_INSIDE_WEIGHTS &#x3D; (1.0, 1.0, 1.0, 1.0)</span><br><span class="line"># Normalize the targets using &quot;precomputed&quot; (or made up) means and stdevs</span><br><span class="line"># (BBOX_NORMALIZE_TARGETS must also be True)</span><br><span class="line">__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED &#x3D; True</span><br><span class="line">__C.TRAIN.BBOX_NORMALIZE_MEANS &#x3D; (0.0, 0.0, 0.0, 0.0)</span><br><span class="line">__C.TRAIN.BBOX_NORMALIZE_STDS &#x3D; (0.1, 0.1, 0.2, 0.2)</span><br><span class="line"></span><br><span class="line"># Train using these proposals</span><br><span class="line">__C.TRAIN.PROPOSAL_METHOD &#x3D; &#39;gt&#39;</span><br><span class="line"></span><br><span class="line"># Make minibatches from images that have similar aspect ratios (i.e. both</span><br><span class="line"># tall and thin or both short and wide) in order to avoid wasting computation</span><br><span class="line"># on zero-padding.</span><br><span class="line"></span><br><span class="line"># Use RPN to detect objects</span><br><span class="line">__C.TRAIN.HAS_RPN &#x3D; True</span><br><span class="line"># IOU &gt;&#x3D; thresh: positive example</span><br><span class="line">__C.TRAIN.RPN_POSITIVE_OVERLAP &#x3D; 0.7</span><br><span class="line"># IOU &lt; thresh: negative example</span><br><span class="line">__C.TRAIN.RPN_NEGATIVE_OVERLAP &#x3D; 0.3</span><br><span class="line"># If an anchor statisfied by positive and negative conditions set to negative</span><br><span class="line">__C.TRAIN.RPN_CLOBBER_POSITIVES &#x3D; False</span><br><span class="line"># Max number of foreground examples</span><br><span class="line">__C.TRAIN.RPN_FG_FRACTION &#x3D; 0.5</span><br><span class="line"># Total number of examples</span><br><span class="line">__C.TRAIN.RPN_BATCHSIZE &#x3D; 256</span><br><span class="line"># NMS threshold used on RPN proposals</span><br><span class="line">__C.TRAIN.RPN_NMS_THRESH &#x3D; 0.7</span><br><span class="line"># Number of top scoring boxes to keep before apply NMS to RPN proposals</span><br><span class="line">__C.TRAIN.RPN_PRE_NMS_TOP_N &#x3D; 12000</span><br><span class="line"># Number of top scoring boxes to keep after applying NMS to RPN proposals</span><br><span class="line">__C.TRAIN.RPN_POST_NMS_TOP_N &#x3D; 2000</span><br><span class="line"># Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)</span><br><span class="line">__C.TRAIN.RPN_MIN_SIZE &#x3D; 8</span><br><span class="line"># Deprecated (outside weights)</span><br><span class="line">__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS &#x3D; (1.0, 1.0, 1.0, 1.0)</span><br><span class="line"># Give the positive RPN examples weight of p * 1 &#x2F; &#123;num positives&#125;</span><br><span class="line"># and give negatives a weight of (1 - p)</span><br><span class="line"># Set to -1.0 to use uniform example weighting</span><br><span class="line">__C.TRAIN.RPN_POSITIVE_WEIGHT &#x3D; -1.0</span><br><span class="line"># Whether to use all ground truth bounding boxes for training,</span><br><span class="line"># For COCO, setting USE_ALL_GT to False will exclude boxes that are flagged as &#39;&#39;iscrowd&#39;&#39;</span><br><span class="line">__C.TRAIN.USE_ALL_GT &#x3D; True</span><br><span class="line"></span><br><span class="line"># Whether to tune the batch normalization parameters during training</span><br><span class="line">__C.TRAIN.BN_TRAIN &#x3D; False</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Testing options</span><br><span class="line">#</span><br><span class="line">__C.TEST &#x3D; edict()</span><br><span class="line"></span><br><span class="line"># Scale to use during testing (can NOT list multiple scales)</span><br><span class="line"># The scale is the pixel size of an image&#39;s shortest side</span><br><span class="line">__C.TEST.SCALES &#x3D; (600,)</span><br><span class="line"></span><br><span class="line"># Max pixel size of the longest side of a scaled input image</span><br><span class="line">__C.TEST.MAX_SIZE &#x3D; 1000</span><br><span class="line"></span><br><span class="line"># Overlap threshold used for non-maximum suppression (suppress boxes with</span><br><span class="line"># IoU &gt;&#x3D; this threshold)</span><br><span class="line">__C.TEST.NMS &#x3D; 0.3</span><br><span class="line"></span><br><span class="line"># Experimental: treat the (K+1) units in the cls_score layer as linear</span><br><span class="line"># predictors (trained, eg, with one-vs-rest SVMs).</span><br><span class="line">__C.TEST.SVM &#x3D; False</span><br><span class="line"></span><br><span class="line"># Test using bounding-box regressors</span><br><span class="line">__C.TEST.BBOX_REG &#x3D; True</span><br><span class="line"></span><br><span class="line"># Propose boxes</span><br><span class="line">__C.TEST.HAS_RPN &#x3D; False</span><br><span class="line"></span><br><span class="line"># Test using these proposals</span><br><span class="line">__C.TEST.PROPOSAL_METHOD &#x3D; &#39;gt&#39;</span><br><span class="line"></span><br><span class="line">## NMS threshold used on RPN proposals</span><br><span class="line">__C.TEST.RPN_NMS_THRESH &#x3D; 0.7</span><br><span class="line">## Number of top scoring boxes to keep before apply NMS to RPN proposals</span><br><span class="line">__C.TEST.RPN_PRE_NMS_TOP_N &#x3D; 6000</span><br><span class="line"></span><br><span class="line">## Number of top scoring boxes to keep after applying NMS to RPN proposals</span><br><span class="line">__C.TEST.RPN_POST_NMS_TOP_N &#x3D; 300</span><br><span class="line"></span><br><span class="line"># Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)</span><br><span class="line">__C.TEST.RPN_MIN_SIZE &#x3D; 16</span><br><span class="line"></span><br><span class="line"># Testing mode, default to be &#39;nms&#39;, &#39;top&#39; is slower but better</span><br><span class="line"># See report for details</span><br><span class="line">__C.TEST.MODE &#x3D; &#39;nms&#39;</span><br><span class="line"></span><br><span class="line"># Only useful when TEST.MODE is &#39;top&#39;, specifies the number of top proposals to select</span><br><span class="line">__C.TEST.RPN_TOP_N &#x3D; 5000</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># ResNet options</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">__C.RESNET &#x3D; edict()</span><br><span class="line"></span><br><span class="line"># Option to set if max-pooling is appended after crop_and_resize.</span><br><span class="line"># if true, the region will be resized to a square of 2xPOOLING_SIZE,</span><br><span class="line"># then 2x2 max-pooling is applied; otherwise the region will be directly</span><br><span class="line"># resized to a square of POOLING_SIZE</span><br><span class="line">__C.RESNET.MAX_POOL &#x3D; False</span><br><span class="line"></span><br><span class="line"># Number of fixed blocks during training, by default the first of all 4 blocks is fixed</span><br><span class="line"># Range: 0 (none) to 3 (all)</span><br><span class="line">__C.RESNET.FIXED_BLOCKS &#x3D; 1</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># MobileNet options</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">__C.MOBILENET &#x3D; edict()</span><br><span class="line"></span><br><span class="line"># Whether to regularize the depth-wise filters during training</span><br><span class="line">__C.MOBILENET.REGU_DEPTH &#x3D; False</span><br><span class="line"></span><br><span class="line"># Number of fixed layers during training, by default the first of all 14 layers is fixed</span><br><span class="line"># Range: 0 (none) to 12 (all)</span><br><span class="line">__C.MOBILENET.FIXED_LAYERS &#x3D; 5</span><br><span class="line"></span><br><span class="line"># Weight decay for the mobilenet weights</span><br><span class="line">__C.MOBILENET.WEIGHT_DECAY &#x3D; 0.00004</span><br><span class="line"></span><br><span class="line"># Depth multiplier</span><br><span class="line">__C.MOBILENET.DEPTH_MULTIPLIER &#x3D; 1.</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># MISC</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># The mapping from image coordinates to feature map coordinates might cause</span><br><span class="line"># some boxes that are distinct in image space to become identical in feature</span><br><span class="line"># coordinates. If DEDUP_BOXES &gt; 0, then DEDUP_BOXES is used as the scale factor</span><br><span class="line"># for identifying duplicate boxes.</span><br><span class="line"># 1&#x2F;16 is correct for &#123;Alex,Caffe&#125;Net, VGG_CNN_M_1024, and VGG16</span><br><span class="line">__C.DEDUP_BOXES &#x3D; 1. &#x2F; 16.</span><br><span class="line"></span><br><span class="line"># Pixel mean values (BGR order) as a (1, 1, 3) array</span><br><span class="line"># We use the same pixel mean for all networks even though it&#39;s not exactly what</span><br><span class="line"># they were trained with</span><br><span class="line">__C.PIXEL_MEANS &#x3D; np.array([[[102.9801, 115.9465, 122.7717]]])</span><br><span class="line"></span><br><span class="line"># For reproducibility</span><br><span class="line">__C.RNG_SEED &#x3D; 3</span><br><span class="line"></span><br><span class="line"># A small number that&#39;s used many times</span><br><span class="line">__C.EPS &#x3D; 1e-14</span><br><span class="line"></span><br><span class="line"># Root directory of project</span><br><span class="line">__C.ROOT_DIR &#x3D; osp.abspath(osp.join(osp.dirname(__file__), &#39;..&#39;, &#39;..&#39;, &#39;..&#39;))</span><br><span class="line"></span><br><span class="line"># Data directory</span><br><span class="line">__C.DATA_DIR &#x3D; osp.abspath(osp.join(__C.ROOT_DIR, &#39;data&#39;))</span><br><span class="line"></span><br><span class="line"># Name (or path to) the matlab executable</span><br><span class="line">__C.MATLAB &#x3D; &#39;matlab&#39;</span><br><span class="line"></span><br><span class="line"># Place outputs under an experiments directory</span><br><span class="line">__C.EXP_DIR &#x3D; &#39;default&#39;</span><br><span class="line"></span><br><span class="line"># Use GPU implementation of non-maximum suppression</span><br><span class="line">__C.USE_GPU_NMS &#x3D; True</span><br><span class="line"></span><br><span class="line"># Default GPU device id</span><br><span class="line">__C.GPU_ID &#x3D; 0</span><br><span class="line"></span><br><span class="line">__C.POOLING_MODE &#x3D; &#39;crop&#39;</span><br><span class="line"></span><br><span class="line"># Size of the pooled region after RoI pooling</span><br><span class="line">__C.POOLING_SIZE &#x3D; 7</span><br><span class="line"></span><br><span class="line"># Maximal number of gt rois in an image during Training</span><br><span class="line">__C.MAX_NUM_GT_BOXES &#x3D; 20</span><br><span class="line"></span><br><span class="line"># Anchor scales for RPN</span><br><span class="line">__C.ANCHOR_SCALES &#x3D; [8,16,32]</span><br><span class="line"></span><br><span class="line"># Anchor ratios for RPN</span><br><span class="line">__C.ANCHOR_RATIOS &#x3D; [0.5,1,2]</span><br><span class="line"></span><br><span class="line"># Feature stride for RPN</span><br><span class="line">__C.FEAT_STRIDE &#x3D; [16, ]</span><br><span class="line"></span><br><span class="line">__C.CUDA &#x3D; False</span><br><span class="line"></span><br><span class="line">__C.CROP_RESIZE_WITH_MAX_POOL &#x3D; True</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="OHEM-Online-Hard-Example-Mining"><a href="#OHEM-Online-Hard-Example-Mining" class="headerlink" title="OHEM(Online Hard Example Mining)"></a><a href="https://arxiv.org/abs/1604.03540">OHEM(Online Hard Example Mining)</a></h2><h3 id="Introduction-4"><a href="#Introduction-4" class="headerlink" title="Introduction"></a>Introduction</h3><p>论文全称是”Training Region-based Object Detectors with Online Hard Example Mining”，<a href="https://github.com/abhi2610/ohem">code</a></p><p>内容有参考：<a href="https://blog.csdn.net/u012905422/article/details/52760669">blog1</a>, <a href="https://zhuanlan.zhihu.com/p/58162337">blog2</a></p><p>对样本进行挖掘，自动选出困难样本让网络学习，提高检测精度。</p><h3 id="Content-4"><a href="#Content-4" class="headerlink" title="Content"></a>Content</h3><p>传统机器学习方法，比如SVM，在进行分类训练时，会采取 ”hard negative mining“这样的bootstrapping策略对难以学习的样本进行选取。大致流程是先用正样本和随机采样的负样本组成初始训练数据集，训练一会之后（比如差不多收敛了），去掉容易分类的样本，然后加入一些现有模型不能很好判断的样本，重新进行训练，迭代几轮之后得到最后的分类模型。</p><p>Ross等人受到此方法的启发，想在基于region-conv-based这样的object detection方法上也进行困难样本的挖掘。</p><blockquote><p>Our motivation is the same as it has always been – detection datasets contain an overwhelming number of easy examples and a small number of hard examples</p></blockquote><p>原因有以下几点：</p><ul><li><p>基于深度学习的目标检测网络中，通常forground和background的region proposal数量很不均衡，负样本要比正样本多得多，这样会导致网络学习的focus被淹没；</p></li><li><p>每个样本对网络学习的贡献是不同的，学习过程中肯定出现容易和困难的样本，因此也需要网络自己去调节对他们的学习权重；</p></li><li><p>Fast R-CNN中对图像搜索出来的region proposal进行可正负样本1：3的配比，同时根据传统的hard negative mining得到了人工设置的IoU阈值0.1，去挑选负样本，虽然不是最优的方法，但是也是不能省去的部分，否则会掉点；</p></li></ul><p>因此，Ross等人专门在这篇paper中探讨样本选择问题，根据模型测试样本得到的误差大小来针对所有类别的样本自动进行挖掘（loss越大说明样本对该模型越难判断），故取名”online hard example mining“。但是不同于传统机器学习，此方法在神经网络上直接实施会存在以下问题:</p><ul><li><p>神经网络基于SGD随机梯度下降，在固定数据集上进行很多次迭代后才可以训练好的，如果先训练几个batch，然后froze参数，再去测试，然后挑出困难的样本，然后再去训练更新参数会大大拖慢模型产出的速度；</p></li><li><p>常规选择样本的方法是，首先将loss排序，然后选择loss比较大的一部分，其他的置0，这样就等于把其他样本认定为简单样本”丢掉了“，但是由于当时深度学习框架的限制，即使这些样本是0，但是依然占据空间，依然需要参与反向传播，并不能很好实现速加快和空间节省；</p></li></ul><p>考虑到这些现实问题，作者采取了折中的方法，由于计算loss和卷积层无关，卷积层只负责提取原始图像特征，loss计算只由RoI pooling+fc层决定，所以网络在卷积之后设置了两个RoI pooling+fc层，一个只读（读所有的proposal），用于计算loss，挑选样本（样本数量为B，类似Fast R-CNN的计算方式），不梯度下降，另一个负责正常的训练，样本来自于只读网络。两个之间权重参数是共享的，整体结构如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/OHEM_architecture.PNG" alt="Fast R-CNN+OHEM结构流程示意图"></p><p>考虑到RoI中可能存在一些具有重复区域的proposal，而这些样本可能会造成相同的loss，所以如果一个是困难样本，其他的IoU比较高的重复proposal也会被选进来，造成了样本的冗余，所以不仅需要排序loss，还需要对样本进行NMS处理。</p><blockquote><p>Given a list of RoIs and their losses, NMS works by iteratively selecting the RoI with the highest loss, and then removing all lower loss RoIs that have high overlap with the selected region. We use a relaxed IoU threshold of 0.7 to suppress only highly overlapping RoIs.</p></blockquote><p>作者根据上述思想在Pascal VOC和COCO数据集上对了一系列对比试验，包括对比hard negative mining(原Fast R-CNN设置的阈值0.1，用于选择background)，batch_size，B，以及加入了一些”bells and whistles”（多尺度和迭代式检测框回归），结果都证明OHEM的效果良好，而且也容易实现，占用空间也不会很大，在某些类别的检测上有很大的精度提升（可能每个类的学习程度也不同，可能也可以挖掘下，open problem）。</p><p>此外，作者还提到如果不去选择的话，把样本全部都倒进去，那些容易的样本的loss很低，这样的话对梯度就没什么贡献，网络应该可以自己去focus那些困难的样本。作者据此也设计了实验，对不同方法的loss做了可视化，最终发现，OHEM的效果确实最好，lower loss，higher mAP。</p><h3 id="Discussion-3"><a href="#Discussion-3" class="headerlink" title="Discussion"></a>Discussion</h3><p>OHEM是在two-stage的检测框架上提出和实现的，主要关注困难的样本，抛弃容易的样本。这种主动选择样本的方式，一方面可能间接平衡了训练的样本比例，另一方面提升了网络学习的针对性。在one -stage检测框架上提出的focal loss，也是为了处理相似的问题，通过权重分配，加大对分错样本的惩罚力度，让网络主动挖掘那些样本比例较少的类别。不过focal loss接受了所有样本，并没有完全抛弃容易的样本，而且也更加容易部署，因此两者是否可以结合，产出更有效的学习criterion，也值得去思考一下。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;Faster  R-CNN是目标检测领域中”two-stage”的代表性方法，其精度高，适应性强，兼具学术和工程价值。整个框架由于吸取了很多先前工作的经验，因此比较庞大，而且细节很多，因此需要认真研读下相关paper和Faster R-CNN的python代码。&lt;/p&gt;
&lt;p&gt;在此之前，先贴上一位博主做的“&lt;a href=&quot;https://nikasa1889.github.io/2017/05/02/The-Modern-History-of-Object-Recognition-—-Infographic-1/&quot;&gt;The Modern History of Object Recognition — Infographic&lt;/a&gt;”，其中也包括了“one-stage”的方法，不过2017年以后的没再更新了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/RCNNseries-1/HistoryOfObjectRecognition.png&quot; alt=&quot;modern history of object recognition&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
    <category term="object detection" scheme="http://densecollections.top/tags/object-detection/"/>
    
    <category term="semantic segmentation" scheme="http://densecollections.top/tags/semantic-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>FutureMapping2</title>
    <link href="http://densecollections.top/posts/FutureMappingofAJD-2/"/>
    <id>http://densecollections.top/posts/FutureMappingofAJD-2/</id>
    <published>2019-11-17T08:24:29.000Z</published>
    <updated>2021-01-02T11:30:57.054Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>继第一篇<a href="https://arxiv.org/abs/1803.11288">FutureMapping</a>之后，视觉SLAM领域内的奠基者Andrew Davison最近又将他的和别人讨论的有关未来空间AI对地图构建，机器人协同定位，以及动态问题等新想法撰写成了新的论文<a href="https://arxiv.org/abs/1910.14139">FutureMapping2</a>，置顶在了自己的推特上，表示欢迎大家交流自己的想法。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/FutureMappingofAJD-2/Andrew-twitter.PNG" alt="Andrew-twitter"></p><p>总的来说，这篇文章干货还算是很多的，主要是着眼于factor graph（因子图）和Gaussian Belief Propagation（GBP，高斯置信传播）对整体，动态机器人建图等问题的潜力和前景，不仅在数学上进行一些tutorial，还给出了三个python demos。由于我对视觉SLAM只是了解整体特点，其中各种计算细节和优化方法并没有认真看过和代码书写过，因此读完这篇充满amazing reflections的文章之后我只能把握到整体的idea和一些实现方法。对于GBP和factor graph 结合构建的数学模型，我将在后续花时间弄懂后再对该blog进行补充。</p><p><strong>非常建议对SLAM或者机器视觉领域感兴趣的同行阅读此论文，相信会帮助您开阔思路！</strong></p><a id="more"></a><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h3><blockquote><p>Abstract:  We argue the case for Gaussian Belief Propagation (GBP) as a strong algorithmic framework for the distributed, generic and incremental probabilistic estimation we need in Spatial AI as we aim at high performance smart robots and devices which operate within the constraints of real products. Processor hardware is changing rapidly, and GBP has the right character to take advantage of highly distributed processing and storage while estimating global quantities, as well as great flexibility. We present a detailed tutorial on GBP, relating to the standard factor graph formulation used in robotics and computer vision, and give several simulation examples with code which demonstrate its properties.  </p></blockquote><p>关键词：分布式，边缘计算，局部估计，GBP，因子图，概率模型</p><p><strong>（边缘计算，分布式的，局部计算和存储，整体计算和构建以一种‘graph’的方式展开）</strong></p><p>首先，背景是现在的spatial AI系统要试着处理异质的数据，通过不同的估计手段将其转换成一致性的表示方式，但是这受限于现在的处理器性能（实时，持续地处理带来计算负担，存储负担和转换负担）。Andrew认为目前有两种方式可以对此进行提升：</p><ul><li>One is to focus on scene representation, and to find new parameterisations of world models which allow high quality scene models to be built and maintained much more efficiently. 这是表征方式的问题，这也是学术界都在解决的问题，representation learning. 机器人，计算机如何利用自己的硬件特点来认识和表征周围的世界，这跟人认识世界不一定是一样的。</li><li>The other is to look towards the changing landscape in computing and sensing hardware. 另一个就是硬件设计，比如现在视觉SLAM中的“event camera”，利用事件来记录。这里Andrew也推荐了ETH苏黎世联邦理工大学 J. Martel的phD thesis，<a href="https://www.research-collection.ethz.ch/handle/20.500.11850/362900">传送门</a></li></ul><p>硬件问题上，Andrew并没有说太多，大都还是一些常见的设想，在后续的section中，Andrew主要是是针对第一点来说的。</p><blockquote><p>The purest representation of the knowledge in a <strong>Spatial AI problem is the factor graph itself, rather than probability distributions derived from it, which will always have to be stored with some approximation</strong>. What we are really seeking is an algorithm which implements Spatial AI in a distributed way on a computational resource like a graph processor, by storing the factor graph as the master representation and operating on it in place using local computation and message passing to implement estimation of variables as needed but taking account of global influence. </p></blockquote><p>(有关因子图的资料，推荐<a href="http://www.cs.cmu.edu/~kaess/pub/Dellaert17fnt.pdf">Factor Graphs for Robot Perception</a>， 关于高斯置信传播，Andrew是根据bishop的PRML一书来的，此外他在文中推荐的是比较老的参考文献<a href="http://web.cs.iastate.edu/~honavar/factorgraphs.pdf">Factor Graphs and the Sum-Product Algorithm</a>，我自己也在网上发现了一篇伯希来大学的phD论文<a href="https://arxiv.org/pdf/0811.2518.pdf"> Gaussian Belief Propagation: Theory and Application </a>，希望对后续理解有所帮助。）</p><p>Andrew认为可以通过因子图来进行局部估计，进行边缘计算，然后进行信息传递，通过这种方法来构建分布式和全局上的计算，从而达到认知和执行任务的目的。</p><p>—-to be continued</p><h3 id="Mathematics-models"><a href="#Mathematics-models" class="headerlink" title="Mathematics models"></a>Mathematics models</h3><h4 id="factor-graph"><a href="#factor-graph" class="headerlink" title="factor graph"></a>factor graph</h4><h4 id="GBP"><a href="#GBP" class="headerlink" title="GBP"></a>GBP</h4><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;继第一篇&lt;a href=&quot;https://arxiv.org/abs/1803.11288&quot;&gt;FutureMapping&lt;/a&gt;之后，视觉SLAM领域内的奠基者Andrew Davison最近又将他的和别人讨论的有关未来空间AI对地图构建，机器人协同定位，以及动态问题等新想法撰写成了新的论文&lt;a href=&quot;https://arxiv.org/abs/1910.14139&quot;&gt;FutureMapping2&lt;/a&gt;，置顶在了自己的推特上，表示欢迎大家交流自己的想法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/FutureMappingofAJD-2/Andrew-twitter.PNG&quot; alt=&quot;Andrew-twitter&quot;&gt;&lt;/p&gt;
&lt;p&gt;总的来说，这篇文章干货还算是很多的，主要是着眼于factor graph（因子图）和Gaussian Belief Propagation（GBP，高斯置信传播）对整体，动态机器人建图等问题的潜力和前景，不仅在数学上进行一些tutorial，还给出了三个python demos。由于我对视觉SLAM只是了解整体特点，其中各种计算细节和优化方法并没有认真看过和代码书写过，因此读完这篇充满amazing reflections的文章之后我只能把握到整体的idea和一些实现方法。对于GBP和factor graph 结合构建的数学模型，我将在后续花时间弄懂后再对该blog进行补充。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;非常建议对SLAM或者机器视觉领域感兴趣的同行阅读此论文，相信会帮助您开阔思路！&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="SLAM" scheme="http://densecollections.top/tags/SLAM/"/>
    
    <category term="mapping" scheme="http://densecollections.top/tags/mapping/"/>
    
    <category term="AI system" scheme="http://densecollections.top/tags/AI-system/"/>
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="hardware" scheme="http://densecollections.top/tags/hardware/"/>
    
  </entry>
  
  <entry>
    <title>实习痰涂片项目总结</title>
    <link href="http://densecollections.top/posts/worksummaryofintern/"/>
    <id>http://densecollections.top/posts/worksummaryofintern/</id>
    <published>2019-10-04T07:47:17.000Z</published>
    <updated>2021-01-02T13:03:09.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="classification"><a href="#classification" class="headerlink" title="classification"></a>classification</h2><p>在上篇博客提到，该任务就是将原始数据的每张图片（256x256）进行grid级别的label预测，思路很简单，就是最后卷出的feature map是4x4的，不要过average global pooling layer，直接拉成1x16的向量过sigmoid激活函数即可（label也要变成16个字符，有点类似OCR)。</p><h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p>原始数据给的标注是json格式的框标注，但是框不是杆菌的具体位置，而是代表这个grid里面存在杆菌：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;frames&quot;:&#123;&quot;0_grid.png&quot;:[&#123;&quot;x1&quot;:162.42542787286064,&quot;y1&quot;:25.34963325183374,&quot;x2&quot;:170.24938875305622,&quot;y2&quot;:34.11246943765281,&quot;width&quot;:256,&quot;height&quot;:256,&quot;box&quot;:&#123;&quot;x1&quot;:162.42542787286064,&quot;y1&quot;:25.34963325183374,&quot;x2&quot;:170.24938875305622,&quot;y2&quot;:34.11246943765281&#125;,&quot;UID&quot;:&quot;fd548124&quot;,&quot;id&quot;:0,&quot;type&quot;:&quot;Rectangle&quot;,&quot;tags&quot;:[&quot;TB01&quot;],&quot;name&quot;:1&#125;,&#123;&quot;x1&quot;:214.3765281173594,&quot;y1&quot;:24.410757946210268,&quot;x2&quot;:224.07823960880197,&quot;y2&quot;:34.11246943765281,&quot;width&quot;:256,&quot;height&quot;:256,&quot;box&quot;:&#123;&quot;x1&quot;:214.3765281173594,&quot;y1&quot;:24.410757946210268,&quot;x2&quot;:224.07823960880197,&quot;y2&quot;:34.11246943765281&#125;,&quot;UID&quot;:&quot;5318dd81&quot;,&quot;id&quot;:1,&quot;type&quot;:&quot;Rectangle&quot;,&quot;tags&quot;:[&quot;TB01&quot;],&quot;name&quot;:2&#125;,...&#125;</span><br></pre></td></tr></table></figure><p>部分标注内容如上，主要包含了对应的文件夹下有哪些图片，图片上有无杆菌，杆菌的位置在哪个格子（要自己判断），以及一张图片有杆菌的话共有几个（”name”）。</p><a id="more"></a><p>首先找出哪些是positive的图片，并且根据坐标位置写出标签:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def find_write_positive_imgs(src_json_path, src_imgs_path, dst_csv_path, dst_imgs_path):</span><br><span class="line">    </span><br><span class="line">    data_csv &#x3D; open(dst_csv_path, &#39;a+&#39;, newline&#x3D;&#39;&#39;)</span><br><span class="line">    csv_writer &#x3D; csv.writer(data_csv)</span><br><span class="line">    csv_writer.writerow([&quot;ImageName&quot;, &quot;LabelId&quot;])</span><br><span class="line">    </span><br><span class="line">    with open(src_json_path,&#39;r&#39;) as load_json:</span><br><span class="line">         load_dict &#x3D; json.load(load_json)</span><br><span class="line"></span><br><span class="line">         img_names &#x3D; load_dict[&#39;visitedFrames&#39;]</span><br><span class="line"></span><br><span class="line">         for img_name in img_names:</span><br><span class="line">         </span><br><span class="line">             #n_name represents the boxes quantities of the img &lt;&quot;name&quot; attribute in .json file&gt;</span><br><span class="line">             n_name&#x3D;len(load_dict[&#39;frames&#39;][img_name])</span><br><span class="line">             </span><br><span class="line">             if n_name &gt; 0:</span><br><span class="line">                src_img_path &#x3D; os.path.join(src_imgs_path, img_name)</span><br><span class="line">                img &#x3D; cv2.imread(src_img_path)</span><br><span class="line">                H &#x3D; img.shape[0]</span><br><span class="line">                W &#x3D; img.shape[1]</span><br><span class="line">                dst_img_path &#x3D; os.path.join(dst_imgs_path, img_name.replace(&#39;.png&#39;, &#39;_22.png&#39;))</span><br><span class="line">                cv2.imwrite(dst_img_path, img)</span><br><span class="line">                </span><br><span class="line">                labelid &#x3D; [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] </span><br><span class="line">                for i in range(0,n_name):</span><br><span class="line">                    x1 &#x3D; load_dict[&#39;frames&#39;][img_name][i][&#39;x1&#39;]</span><br><span class="line">                    y1 &#x3D; load_dict[&#39;frames&#39;][img_name][i][&#39;y1&#39;]</span><br><span class="line">                    area_h0 &#x3D; 0</span><br><span class="line">                    area_w0 &#x3D; 0</span><br><span class="line">                    for area_h1 in range(H&#x2F;&#x2F;4, H+1, H&#x2F;&#x2F;4):</span><br><span class="line">                        if y1 &gt; area_h0 and y1 &lt; area_h1:</span><br><span class="line">                           row_id &#x3D; (area_h1 * 4 &#x2F; H) - 1</span><br><span class="line">                           for area_w1 in range(W&#x2F;&#x2F;4, W+1, W&#x2F;&#x2F;4):</span><br><span class="line">                               if x1 &gt; area_w0 and x1 &lt; area_w1:</span><br><span class="line">                                  col_id &#x3D; (area_w1 * 4 &#x2F; W) - 1</span><br><span class="line">                                  id &#x3D; int(col_id + 4 * row_id)</span><br><span class="line">                                  labelid[id] &#x3D; 1</span><br><span class="line">                                  break</span><br><span class="line">                               else:</span><br><span class="line">                                    area_w0 &#x3D; area_w1</span><br><span class="line">                           break</span><br><span class="line">                        else:</span><br><span class="line">                            area_h0 &#x3D; area_h1</span><br><span class="line">                  </span><br><span class="line">                csv_writer.writerow([img_name.replace(&#39;.png&#39;, &#39;_22.png&#39;), </span><br><span class="line">                                     &#39;&#39;.join(str(k) for k in labelid)])</span><br></pre></td></tr></table></figure><p>此外，由于最后找出的positive图片很少（好像只有320张），我又对其进行了数据扩增，先是原始旋转一圈，然后right-left翻转后又旋转了一圈，因此总共扩增到了8倍大小。之后进行一下train-val-test set的划分，一般生成随机数就可以按自己的意愿划分，也有专门的库，具体划分代码就不上了。</p><p>另外数据增强方面也考虑过rgb转hsv或者ycrcb的，但是我试了一个样例之后效果不是很好，毕竟这样做的目的就是为了将主要的前景和特征显示出来，奈何我的数据太差了些，不好操作，于是作罢。</p><p>准备好数据之后，要对数据进行抽取，我用的是pytorch，直接继承Dataset类就好：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class SSDataset(Dataset):</span><br><span class="line">      </span><br><span class="line">      def __init__(self, imgs_path, csv_path, </span><br><span class="line">                    img_transform&#x3D;None, loader&#x3D;default_loader):</span><br><span class="line">          with open(csv_path, &#39;r&#39;) as f:</span><br><span class="line">               #这里一定要按字符串读取，否则前面的0会丢掉</span><br><span class="line">               #类似于OCR的labe读取</span><br><span class="line">               data_info &#x3D; pd.read_csv(f, dtype&#x3D;str) </span><br><span class="line">               #第一列是image name</span><br><span class="line">               self.img_list &#x3D; list(data_info.iloc[:,0])</span><br><span class="line">               #第二类是labelid</span><br><span class="line">               self.label_list &#x3D; list(data_info.iloc[:,1])</span><br><span class="line">          self.img_transform &#x3D; img_transform</span><br><span class="line">          #loader用PIL.Image.open()</span><br><span class="line">          #不要用cv2.imread()</span><br><span class="line">          #pytorch默认PIL格式</span><br><span class="line">          self.loader &#x3D; loader</span><br><span class="line">          self.imgs_path &#x3D; imgs_path</span><br><span class="line">      </span><br><span class="line">      def __getitem__(self, index):</span><br><span class="line">          img_path &#x3D; os.path.join(self.imgs_path, self.img_list[index])</span><br><span class="line">          img &#x3D; self.loader(img_path)</span><br><span class="line">          label &#x3D; self.label_list[index]</span><br><span class="line">          if self.img_transform is not None:</span><br><span class="line">             img &#x3D; self.img_transform(img)</span><br><span class="line">          return img, label</span><br><span class="line"></span><br><span class="line">      def __len__(self):</span><br><span class="line">         return len(self.label_list)</span><br></pre></td></tr></table></figure><p>但是定义的labelid是str，还需要转成tensor去计算loss:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def labelid_switch(labels_str):</span><br><span class="line">    b_s &#x3D; len(labels_str)</span><br><span class="line">    pad_label &#x3D; []</span><br><span class="line">    for i in range(0, b_s):</span><br><span class="line">        temp_label &#x3D; [0]* 16</span><br><span class="line">        temp_label[:16] &#x3D; labels_str[i]</span><br><span class="line">        temp_label &#x3D; list(map(int, temp_label))</span><br><span class="line">        pad_label.append(temp_label)</span><br><span class="line">    pad_label &#x3D; torch.Tensor(pad_label)</span><br><span class="line">    labels_float &#x3D; pad_label.view(b_s, 16)</span><br><span class="line">    return labels_float</span><br></pre></td></tr></table></figure><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><p>训练模型是主要用的是resnet和vgg，这部分代码可以直接参考torchvision，然后改改后面的layer就好了。</p><p>loss function上我试了binary cross entropy和focal loss（毕竟整体上positive grids还是少于negative grids的），此外我也试了下<a href="https://github.com/facebookresearch/mixup-cifar10">mixup</a>，就是随机把batch里面的图片两两混合，计算loss的时候按照混合的比例分别计算相加，这也是一种应对过拟合，降低模型复杂度的办法（还有一种类似的方法叫sample pairing，只混合图片，不管label，我也试了，不过实际好像没mixup顶用）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class BFocalLoss(nn.Module):</span><br><span class="line"> </span><br><span class="line">    def __init__(self, gamma&#x3D;1,alpha&#x3D;0.8):</span><br><span class="line">        super(BFocalLoss, self).__init__()</span><br><span class="line">        self.gamma &#x3D; gamma</span><br><span class="line">        self.alpha &#x3D; alpha</span><br><span class="line">    def forward(self, inputs, targets):</span><br><span class="line">        p &#x3D; inputs</span><br><span class="line">        loss &#x3D; -self.alpha*(1-p)**self.gamma*(targets*torch.log(p+1e-12))-\</span><br><span class="line">               (1-self.alpha)*p**self.gamma*((1-targets)*torch.log(1-p+1e-12))</span><br><span class="line">        loss &#x3D; torch.sum(loss)</span><br><span class="line">        return loss</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def mixup_data(in_img, in_label, alpha&#x3D;1.0):</span><br><span class="line">    #alpha in [0.1,0.4] in paper has better gain(for imagenet)</span><br><span class="line">    #for cifar-10 is 1.</span><br><span class="line">    if alpha &gt; 0:</span><br><span class="line">       lam &#x3D; np.random.beta(alpha, alpha)</span><br><span class="line">    else:</span><br><span class="line">       lam &#x3D; 1</span><br><span class="line">    </span><br><span class="line">    Batch_Size &#x3D; in_img.size()[0]</span><br><span class="line">    Index &#x3D; torch.randperm(Batch_Size)</span><br><span class="line">    mixed_x &#x3D; lam * in_img + (1 - lam) * in_img[Index, :]</span><br><span class="line">    y_a, y_b &#x3D; in_label, in_label[Index]</span><br><span class="line">    return mixed_x, y_a, y_b, lam</span><br><span class="line">    </span><br><span class="line">#计算loss  </span><br><span class="line">loss_mixup &#x3D;  lam * criterion(pred, labels_a) + \</span><br><span class="line">                   (1 - lam) * criterion(pred, labels_b)</span><br></pre></td></tr></table></figure><p>接下来的事就是调参，对比实验，开tensorboard看loss的趋势了。（这里有一个现象，前期有一部分时间loss难以下降，总是在一个范围内波动，我猜想可能是因为数据扩增的原因。）</p><h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>这部分就是加载模型，一张张图片测试，然后写出预测的csv即可，然后给出grid acc</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#部分代码如下：</span><br><span class="line">file_pre &#x3D; open(PRE_TEST_CSV, &#39;w&#39;, newline&#x3D;&#39;&#39;)</span><br><span class="line">pre_writer &#x3D; csv.writer(file_pre)</span><br><span class="line">pre_writer.writerow([&quot;ImageName&quot;, &quot;LabelId&quot;])</span><br><span class="line"></span><br><span class="line">with open(SRC_TEST_CSV, &#39;r&#39;) as f_test:</span><br><span class="line">     test_data &#x3D; pd.read_csv(f_test, dtype&#x3D;str)</span><br><span class="line">     img_name &#x3D; list(test_data.iloc[:,0])</span><br><span class="line">     labelid &#x3D; list(test_data.iloc[:,1])</span><br><span class="line">     test_data_len &#x3D; len(test_data.index)</span><br><span class="line"></span><br><span class="line">     num_right &#x3D; 0 </span><br><span class="line">     positive_num &#x3D; 0 </span><br><span class="line">     positive_num_right &#x3D; 0</span><br><span class="line">     for i in range(0,test_data_len):</span><br><span class="line">         img_path &#x3D; os.path.join(TEST_DATA_PATH, img_name[i])</span><br><span class="line">         img &#x3D; Image.open(img_path)</span><br><span class="line">         img_tensor &#x3D; transformations(img).float()</span><br><span class="line">         img_tensor &#x3D; img_tensor.unsqueeze_(0)</span><br><span class="line">         </span><br><span class="line">         temp_label &#x3D; [0]*16</span><br><span class="line">         temp_label[:16] &#x3D; labelid[i]</span><br><span class="line">         temp_label &#x3D; list(map(int, temp_label))</span><br><span class="line">         for temp in temp_label:</span><br><span class="line">             if temp &gt; 0:</span><br><span class="line">                positive_num +&#x3D; 1</span><br><span class="line">         label &#x3D; torch.FloatTensor(temp_label)</span><br><span class="line">         label &#x3D; label.view(1, 16)</span><br><span class="line">          </span><br><span class="line">         input &#x3D; Variable(img_tensor)</span><br><span class="line">         input &#x3D; input.to(device)</span><br><span class="line">         pred &#x3D; net(input).data.cpu() #在CPU中比较</span><br><span class="line">         output &#x3D; pred</span><br><span class="line">         pred_len &#x3D; pred.size()[1]</span><br><span class="line">         out &#x3D; []</span><br><span class="line">         for j in range(0, pred_len):</span><br><span class="line">             if pred[0][j] &lt; 0.5:</span><br><span class="line">                output[0][j] &#x3D; 0</span><br><span class="line">                out.append(0)</span><br><span class="line">                if output[0][j] &#x3D;&#x3D; label[0][j]:</span><br><span class="line">                   num_right +&#x3D; 1</span><br><span class="line">             else:</span><br><span class="line">                 output[0][j] &#x3D; 1</span><br><span class="line">                 out.append(1)</span><br><span class="line">                 if output[0][j] &#x3D;&#x3D; label[0][j]:</span><br><span class="line">                    num_right +&#x3D; 1</span><br><span class="line">                    positive_num_right +&#x3D; 1</span><br><span class="line">         pre_writer.writerow([img_name[i],&#39;&#39;.join(str(k) for k in out)]) </span><br><span class="line">    </span><br><span class="line">     print(&#39;test acc is: &#39;, num_right&#x2F;(test_data_len*16))</span><br><span class="line">     print(&#39;postivite acc is: &#39;, positive_num_right, &#39;&#x2F;&#39;, positive_num)</span><br></pre></td></tr></table></figure><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><p>实际上这个代码下来，调参还是挺费劲的，尤其是对我这种刚开始搞深度学习，经验还不够的新手来说，着实走了不少弯路。可是数据集实在太差，实在是想不出什么招。。所以硬撑了快两个月（实际上前大半个月我是直接分割grid成单独的图片，然后全部丢进去训练的。。这样搞不仅正负样本差距极大，而且切断了图片的连续性，效果奇差也在意料之中了，基本训不动，即使加了focal loss也没什么卵用）最后最高也才得到90%的acc。</p><h2 id="weakly-semantic-segmentation"><a href="#weakly-semantic-segmentation" class="headerlink" title="weakly semantic segmentation"></a>weakly semantic segmentation</h2><p>好歹8月下旬那会找到了一个公开的sputum smear的数据集，还带着框的标注：</p><ul><li>Makerere University, Uganda<ul><li><a href="http://air.ug/microscopy/">homepage</a></li><li><a href="http://proceedings.mlr.press/v56/Quinn16.pdf">paper</a></li><li><a href="https://github.com/jqug/microscopy-object-detection/blob/master/CNN%20training%20%26%20evaluation%20-%20tuberculosis.ipynb">code</a></li></ul></li></ul><p>跟CTO交流后，他觉得这数据集质量不错，干脆就提议做弱监督分割，毕竟object detection现在都做烂了，而且开源这数据集的小哥自己也把object detection的acc刷的不错了，所以没必要再调包重复同样的事情了。我当时其实没啥思路，但是觉得应该挺有意思的，于是就接了下来。</p><p>后来通过调研发现，原来在自然图像上早就有人做了weakly segmentation(又是我恺明哥那些人…)，而且效果还不错，唯一可惜的就是完整的代码基本没人开源，不过后来参考GitHub上的一些相关代码也慢慢搭建出了整个框架。</p><p>整个项目思路主要参考的是这两篇论文：戴季峰的<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf">BoxSup</a>和Max Planck Institute的<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf">Simple Does It</a>，主要的思路就是先设定几个从bounding box annotations生成segment proposals的方法（主要是opencv中GrabCut），然后利用此label去进行supervised training，最后过一下<a href="https://github.com/lucasb-eyer/pydensecrf">denseCRF</a>优化一下，让boundary更加丝滑。当然也可以试试递归训练，让performance不错的model去预测生成新的training set中的label，然后进行下一轮的训练。</p><p>因为代码比较庞杂，分块不好展示，完整的代码就直接放在我的<a href="https://github.com/Richardyu114/weakly-segmentation-with-bounding-box">github</a>上。</p><h3 id="pre-processing"><a href="#pre-processing" class="headerlink" title="pre-processing"></a>pre-processing</h3><p>原始的数据集中有1217张阳性图片，此外这些图片的标注还有47张莫名奇妙多了些20x20的框（可能是标的时候手抖了），因此要先一个个去掉。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/worksummaryofintern/1.jpg" alt="看到了吗，左下角和右上角都多了个小框"></p><p>之后，对这些图片进行大致masks的生成，我这里给了三种方法：</p><ul><li><em>Box_segments</em>: 把整个box里面的像素都认为是杆菌（要把box的坐标都转成int，得对上像素）</li><li><em>Sbox_segments</em>:取box里面的80%的矩形框，认为该框里面的像素都是杆菌（同样，坐标都是int类型）</li><li><em>GrabCut_segments</em>: 利用经典的计算机视觉方法<a href="https://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf">GrabCut</a>来得到杆菌的分割区域，但是该方法一般对图片的里面的单个的大物体比较友好，而杆菌又细又长，同时又包含着染色质，所以利用颜色分布的GrabCut分割出的杆菌要么会大点，要么就没有。大点的我不管，没有的我在这里就直接用<em>Box_segments</em>代替了。</li></ul><p>GrabCut部分代码如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def grabcut(img_name):</span><br><span class="line">        masks &#x3D; [] </span><br><span class="line">        # one image has many object that need to grabcut</span><br><span class="line">        for i, ann_info in enumerate(ANNS[img_name], start&#x3D;1):</span><br><span class="line">               img &#x3D; cv.imread((img_dir +img_name).rstrip()+&#39;.jpg&#39;)</span><br><span class="line">               grab_name &#x3D; ann_info[1]</span><br><span class="line">               xmin &#x3D; ann_info[3]</span><br><span class="line">               ymin &#x3D; ann_info[2]</span><br><span class="line">               xmax &#x3D; ann_info[5]</span><br><span class="line">               ymax &#x3D; ann_info[4]</span><br><span class="line">               &quot;&quot;&quot;get int box coor&quot;&quot;&quot;</span><br><span class="line">               img_w &#x3D; img.shape[1]</span><br><span class="line">               img_h &#x3D; img.shape[0]</span><br><span class="line">               xmin, ymin, xmax, ymax &#x3D; get_int_coor(xmin, ymin, xmax, ymax, img_w, img_h)           </span><br><span class="line">               box_w &#x3D; xmax - xmin</span><br><span class="line">               box_h &#x3D; ymax - ymin</span><br><span class="line">               # cv.grabcut&#39;s para</span><br><span class="line">               mask &#x3D; np.zeros(img.shape[:2], np.uint8)</span><br><span class="line">               # rect is the tuple</span><br><span class="line">               rect &#x3D; (xmin, ymin, box_w, box_h)</span><br><span class="line">               bgdModel &#x3D; np.zeros((1, 65), np.float64)</span><br><span class="line">               fgdModel &#x3D; np.zeros((1, 65), np.float64)</span><br><span class="line">               #for small bbox:</span><br><span class="line">               if box_w * box_h &lt; MINI_AREA:</span><br><span class="line">                   img_mask &#x3D; mask[ymin:ymax, xmin:xmax] &#x3D; 1</span><br><span class="line">                # for big box that area &#x3D;&#x3D; img.area(one object bbox is just the whole image)</span><br><span class="line">               elif box_w * box_h &#x3D;&#x3D; img.shape[1] * img.shape[0]:</span><br><span class="line">                      rect &#x3D; [RECT_SHRINK, RECT_SHRINK, box_w - RECT_SHRINK * 2, box_h - RECT_SHRINK * 2]</span><br><span class="line">                      cv.grabCut(img, mask, rect, bgdModel,fgdModel, ITER_NUM, cv.GC_INIT_WITH_RECT)</span><br><span class="line">                      # astype(&#39;uint8&#39;) keep the image pixel in range[0,255]</span><br><span class="line">                      img_mask &#x3D;  np.where((mask &#x3D;&#x3D; 0) | (mask &#x3D;&#x3D; 2), 0, 1).astype(&#39;uint8&#39;)</span><br><span class="line">                # for normal bbox:</span><br><span class="line">               else:</span><br><span class="line">                       cv.grabCut(img, mask, rect, bgdModel,fgdModel, ITER_NUM, cv.GC_INIT_WITH_RECT)</span><br><span class="line">                       img_mask &#x3D; np.where((mask &#x3D;&#x3D; 0) | (mask &#x3D;&#x3D; 2), 0, 1).astype(&#39;uint8&#39;)</span><br><span class="line">                       # if the grabcut output is just background(it happens in my dataset)</span><br><span class="line">                       if np.sum(img_mask) &#x3D;&#x3D; 0:</span><br><span class="line">                           img_mask &#x3D; np.where((mask &#x3D;&#x3D; 0), 0, 1).astype(&#39;uint8&#39;)</span><br><span class="line">                        # couting IOU</span><br><span class="line">                        # if the grabcut output too small region, it need reset to bbox mask</span><br><span class="line">                       box_mask &#x3D; np.zeros((img.shape[0], img.shape[1]))</span><br><span class="line">                       box_mask[ymin:ymax, xmin:xmax] &#x3D; 1</span><br><span class="line">                       sum_area &#x3D; box_mask + img_mask</span><br><span class="line">                       intersection &#x3D; np.where((sum_area&#x3D;&#x3D;2), 1, 0).astype(&#39;uint8&#39;)</span><br><span class="line">                       union &#x3D; np.where((sum_area&#x3D;&#x3D;0), 0, 1).astype(&#39;uint8&#39;)</span><br><span class="line">                       IOU &#x3D; np.sum(intersection) &#x2F; np.sum(union)</span><br><span class="line">                       if IOU &lt;&#x3D; IOU_THRESHOLD:</span><br><span class="line">                           img_mask &#x3D; box_mask</span><br><span class="line">                # for draw mask on the image later           </span><br><span class="line">               img &#x3D; cv.cvtColor(img, cv.COLOR_BGR2RGB) </span><br><span class="line">               masks.append([img_mask, grab_name, rect])</span><br><span class="line">        </span><br><span class="line">        num_object &#x3D; i</span><br><span class="line">        &quot;&quot;&quot;for multi-objects intersection and fix the label &quot;&quot;&quot;</span><br><span class="line">        masks.sort(key&#x3D;lambda mask: np.sum(mask[0]), reverse&#x3D;True)</span><br><span class="line">        for j in range(num_object):</span><br><span class="line">              for k in range(j+1, num_object):</span><br><span class="line">                      masks[j][0] &#x3D; masks[j][0] - masks[k][0]</span><br><span class="line">              masks[j][0] &#x3D; np.where((masks[j][0]&#x3D;&#x3D;1), 1, 0).astype(&#39;uint8&#39;)</span><br><span class="line">              &quot;&quot;&quot;get class name  id&quot;&quot;&quot;</span><br><span class="line">              grab_name &#x3D; masks[j][1]</span><br><span class="line">              class_id &#x3D; grab_name.split(&#39;_&#39;)[-1]</span><br><span class="line">              class_id &#x3D; int(class_id.split(&#39;.&#39;)[0])</span><br><span class="line"></span><br><span class="line">              #set the numpy value to class_id</span><br><span class="line">              masks[j][0] &#x3D; np.where((masks[j][0]&#x3D;&#x3D;1), class_id, 0).astype(&#39;uint8&#39;)</span><br><span class="line">              # save grabcut_inst(one object in a image)</span><br><span class="line">              scipy.misc.toimage(masks[j][0], cmin&#x3D;0, cmax&#x3D;255, pal&#x3D;tbvoc_info.colors_map,</span><br><span class="line">                                                      mode&#x3D;&#39;P&#39; ).save((grabcut_dir).rstrip()+masks[j][1])</span><br><span class="line">        </span><br><span class="line">        &quot;&quot;&quot;merge masks&quot;&quot;&quot;</span><br><span class="line">        # built array(img.shape size)</span><br><span class="line">        mask_ &#x3D; np.zeros(img.shape[:2])</span><br><span class="line">        for mask in masks:</span><br><span class="line">                mask_ &#x3D; mask_ + mask[0]</span><br><span class="line">        # save segmetation_label(every object in a image)</span><br><span class="line">        scipy.misc.toimage(mask_, cmin&#x3D;0, cmax&#x3D;255, pal&#x3D;tbvoc_info.colors_map,</span><br><span class="line">                                                mode&#x3D;&#39;P&#39;).save((segmentation_label_dir+img_name).rstrip()+&#39;.png&#39;)</span><br></pre></td></tr></table></figure><p>这里面我是用scipy来保存masks的，我用的版本是0.19.0，超过这个版本的scipy就没有toimage()这个函数了，据说PIL有可以替代的函数，但是我看两个的功效好像不一样，就没去折腾了。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/worksummaryofintern/2.jpg" alt="原图，带框标注"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/worksummaryofintern/3.png" alt="GrabCut生成的segmentation label"></p><p>读取数据部分进行了resize处理，原图尺寸是1632x1224，1224不能被32整除，五次下采样和上采样的时候会出现feature map维度不匹配的错误，因此resize成了1632x1216。这里要注意，原图是利用双线性插值进行resize的，masks图是利用最近邻进行resize的（实际上我是生成好masks后训练时才意识到这个问题，实际上可以在最开始就把dataset的数据resize好，这样masks的误差可能就小点），PIL和cv2里面都有类似的函数。</p><p>数据读取部分代码：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class TBDataset(Dataset):</span><br><span class="line">      def __init__(self, txt_dir, width, height,  transform&#x3D;None):</span><br><span class="line">          self.img_names &#x3D; []</span><br><span class="line">          with open(txt_dir, &#39;r&#39;) as f_txt:</span><br><span class="line">               for img_name in f_txt:</span><br><span class="line">                   self.img_names.append(img_name)</span><br><span class="line">          </span><br><span class="line">          self.transform &#x3D; transform</span><br><span class="line">          self.txt_dir &#x3D; txt_dir</span><br><span class="line">          self.width &#x3D; width</span><br><span class="line">          self.height &#x3D; height</span><br><span class="line">                   </span><br><span class="line">      def __getitem__(self, index):</span><br><span class="line">          img_name &#x3D; self.img_names[index]</span><br><span class="line">          img &#x3D; Image.open(os.path.join(img_dir, img_name).rstrip()+&#39;.jpg&#39;)</span><br><span class="line">          # the resize function like bilinear</span><br><span class="line">          img &#x3D; img.resize((self.width, self.height), Image.LANCZOS)</span><br><span class="line">          img &#x3D; np.array(img)</span><br><span class="line">          label &#x3D; Image.open(os.path.join(label_dir, img_name).rstrip()+&#39;.png&#39;)</span><br><span class="line">          # for consider class_id is not consecutive and just fixed by user</span><br><span class="line">          label &#x3D; label.resize((self.width, self.height), Image.NEAREST)</span><br><span class="line">          label &#x3D; np.array(label)</span><br><span class="line">          if self.transform is not None:</span><br><span class="line">             img &#x3D; self.transform(img)</span><br><span class="line">          #img &#x3D; torch.FloatTensor(img)</span><br><span class="line">          label &#x3D; torch.FloatTensor(label)</span><br><span class="line">          return img, label</span><br><span class="line">                             </span><br><span class="line">      def __len__(self):</span><br><span class="line">          return len(self.img_names)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="train-1"><a href="#train-1" class="headerlink" title="train"></a>train</h3><p>训练部分模型用的是FCN和UNet，因为考虑到只有二分类，后面也可以考虑deeplab，UNet++等等。FCN用的是VGG-16 backbone，下采样5次，UNet下采样4次，都是按照论文来的，没做什么改动。模型最后输出的是一个1632x1216的feature map，然后直接过sigmoid激活函数，再和1632x1216的mask图片（读进来的是一个二维0-1矩阵，代表每个像素点的label）进行loss计算，然后BP，更新参数学习。loss也用了交叉熵和focal loss.</p><h3 id="post-processing"><a href="#post-processing" class="headerlink" title="post-processing"></a>post-processing</h3><p>对模型预测出的结果再过一遍denseCRF，优化分割的同时也会去掉一些false-positive</p><p>部分代码如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def run_densecrf(img_dir, img_name, masks_pro):</span><br><span class="line">        height &#x3D; masks_pro.shape[0]</span><br><span class="line">        width &#x3D; masks_pro.shape[1]</span><br><span class="line"></span><br><span class="line">        # must use cv2.imread()</span><br><span class="line">        # if use PIL.Image.open(), the algorithm will break</span><br><span class="line">        #TODO --need to fix the image problem</span><br><span class="line">        img &#x3D; cv.imread(os.path.join(img_dir, img_name).rstrip()+&#39;.jpg&#39;)</span><br><span class="line">        img &#x3D; cv.resize(img, (1632,1216), interpolation &#x3D; cv.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line">        # expand to [1,H,W]</span><br><span class="line">        masks_pro &#x3D; np.expand_dims(masks_pro, 0)</span><br><span class="line">        # masks_pro &#x3D; masks_pro[:, :, np.newaxis]</span><br><span class="line">        # append to array---shape(2,H,W)</span><br><span class="line">        # one depth represents the class 0, the other represents the class 1</span><br><span class="line">        masks_pro &#x3D; np.append(1-masks_pro, masks_pro, axis&#x3D;0)</span><br><span class="line">        #[Classes, H, W]</span><br><span class="line">        # U needs to be flat</span><br><span class="line">        U &#x3D; masks_pro.reshape(2, -1)</span><br><span class="line">        # deepcopy and the order is C-order(from rows to colums)</span><br><span class="line">        U &#x3D; U.copy(order&#x3D;&#39;C&#39;)</span><br><span class="line">        # for binary classification, the value after sigmoid may be very small</span><br><span class="line">        U &#x3D; np.where((U &lt; 1e-12), 1e-12, U)</span><br><span class="line">        d &#x3D; dcrf.DenseCRF2D(width, height, 2)</span><br><span class="line"></span><br><span class="line">        # make sure the array be c-order which will faster the processing speed</span><br><span class="line">        # reference: https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;59767914</span><br><span class="line">        U &#x3D; np.ascontiguousarray(U)</span><br><span class="line">        img &#x3D; np.ascontiguousarray(img)</span><br><span class="line"></span><br><span class="line">        d.setUnaryEnergy(-np.log(U))</span><br><span class="line">        d.addPairwiseGaussian(sxy&#x3D;3, compat&#x3D;3)</span><br><span class="line">        d.addPairwiseBilateral(sxy&#x3D;80, srgb&#x3D;13, rgbim&#x3D;img, compat&#x3D;10)</span><br><span class="line">        Q &#x3D; d.inference(5)</span><br><span class="line">        # compare each value between two rows by colum</span><br><span class="line">        # and inference each pixel belongs to which class(0 or 1)</span><br><span class="line">        map &#x3D; np.argmax(Q, axis&#x3D;0).reshape((height, width))</span><br><span class="line">        proba &#x3D; np.array(map)</span><br><span class="line"></span><br><span class="line">        return proba</span><br></pre></td></tr></table></figure><p>这里主要用到了二元势pairwise potential，比较每个像素和其他像素的关系，具体原理可以去看看原代码和论文。</p><p>此外，我还顺手进行了下迭代训练。实际上，对于我这个数据集，基本上用GrabCut生成label训练一遍效果就不错了，不过为了看下更新label再训练一轮会不会得到更好的结果，在固定的epoch结束后将训练好得模型设为eval模式，然后预测train set的数据，然后再返回train模式继续训练。需要注意的是，更新label的时候，可能会有漏诊和误诊，我就直接将预测的mask和<em>Box_segments</em>得到的mask相加，只取为2的部分，这样就去掉了假阳性，然后漏诊的部分再用box补回来。</p><p>从实验结果来看，一般我这个是更新3次label（每10个epoch更新一次）就差不多了，再多也没什么提升。总体上来说，这个操作可以提高单张图片同时存在多个杆菌的分割效果，但是提升力度也没什么太令人满意的地方。可能是我的更新姿势不对？</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def update_label(predict_model, device):</span><br><span class="line">       </span><br><span class="line">       &quot;&quot;&quot;load train_pairs.txt info for check the missed diagnosis objects&quot;&quot;&quot;</span><br><span class="line">       #ann_info:[image name, image name_num_ class_id.png, bbox_ymin,</span><br><span class="line">       #                    bbox_xmin,bbox_ymax, bbox_xmax, class_name]</span><br><span class="line">       print(&#39;start to update...&#39;)</span><br><span class="line">       ANNS &#x3D; &#123;&#125;</span><br><span class="line">       with open(dataset_pairs_dir, &#39;r&#39;) as da_p_txt:</span><br><span class="line">                 for ann_info in da_p_txt:</span><br><span class="line">                        # split the string line, get the list</span><br><span class="line">                        ann_info &#x3D; ann_info.rstrip().split(&#39;###&#39;)</span><br><span class="line">                        if ann_info[0].rstrip()  not in ANNS:</span><br><span class="line">                            ANNS[ann_info[0].rstrip()] &#x3D; []</span><br><span class="line">                        ANNS[ann_info[0].rstrip()].append(ann_info)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       predict_model.eval()</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">       # define the same image transformations</span><br><span class="line">       transformations &#x3D; transforms.Compose([</span><br><span class="line">                                             transforms.ToTensor(),</span><br><span class="line">                                             transforms.Normalize(mean&#x3D;[0.485, 0.456, 0.406], </span><br><span class="line">                                             std&#x3D;[0.229, 0.224, 0.225])</span><br><span class="line">                                             ])</span><br><span class="line"></span><br><span class="line">       update_num &#x3D; 0</span><br><span class="line">       print(&#39;updating progress:&#39;)</span><br><span class="line">       with open(dataset_txt_dir, &#39;r&#39;) as da_txt:</span><br><span class="line">                 # don&#39;t use the code line below</span><br><span class="line">                 # or it will close the file and the whole programm end here (I guess)</span><br><span class="line">                 # I debug here for two hours......</span><br><span class="line">                 #lines &#x3D; len(da_txt.readlines())</span><br><span class="line">                 for update_name in da_txt:</span><br><span class="line">                         update_num +&#x3D; 1</span><br><span class="line">                         # in RGB [W, H, depth]</span><br><span class="line">                         img &#x3D; Image.open(os.path.join(img_dir, update_name).rstrip()+&#39;.jpg&#39;)</span><br><span class="line">                         img_w &#x3D; img.size[0]</span><br><span class="line">                         img_h &#x3D; img.size[1]</span><br><span class="line">                         img &#x3D; img.resize((1632, 1216), Image.LANCZOS)</span><br><span class="line">                         input_ &#x3D; transformations(img).float()</span><br><span class="line">                         # add batch_size dimension</span><br><span class="line">                         #[3, H, W]--&gt;[1, 3, H, W]</span><br><span class="line">                         input_ &#x3D; input_.unsqueeze_(0)</span><br><span class="line">                         input_ &#x3D; input_.to(device)</span><br><span class="line">                         pred &#x3D; predict_model(input_).view([1216, 1632]).data.cpu()</span><br><span class="line">                         #pred.shape[H,W]</span><br><span class="line">                         pred &#x3D; np.array(pred)</span><br><span class="line">                         &quot;&quot;&quot;crf smooth prediction&quot;&quot;&quot;</span><br><span class="line">                         crf_pred &#x3D; run_densecrf(img_dir, update_name,  pred)</span><br><span class="line"></span><br><span class="line">                         &quot;&quot;&quot;start to update&quot;&quot;&quot;</span><br><span class="line">                         last_label &#x3D; Image.open(os.path.join(label_dir, update_name).rstrip()+&#39;.png&#39;)</span><br><span class="line">                         last_label &#x3D; last_label.resize((1632, 1216), Image.NEAREST)</span><br><span class="line">                         last_label &#x3D; np.array(last_label)</span><br><span class="line"></span><br><span class="line">                         # predicted label without false-positive segments</span><br><span class="line">                         updated_label &#x3D; crf_pred + last_label</span><br><span class="line">                         updated_label &#x3D; np.where((updated_label&#x3D;&#x3D;2), 1, 0).astype(&#39;uint8&#39;)</span><br><span class="line">                         # predicted label with missed diagnosis </span><br><span class="line">                         # we just use the box segments as missed diagnosis for now</span><br><span class="line">                         info4check &#x3D; ANNS[update_name.rstrip()]</span><br><span class="line">                         masks_missed &#x3D; np.zeros((1216, 1632), np.uint8)</span><br><span class="line">                         for box4check in info4check:</span><br><span class="line">                                xmin &#x3D; box4check[3]</span><br><span class="line">                                ymin &#x3D; box4check[2]</span><br><span class="line">                                xmax &#x3D; box4check[5]</span><br><span class="line">                                ymax &#x3D; box4check[4]</span><br><span class="line">                                xmin, ymin, xmax, ymax &#x3D; get_int_coor(xmin, ymin, </span><br><span class="line">                                                                       xmax, ymax, img_w, img_h)</span><br><span class="line">                                xmin &#x3D; int(xmin * 1632 &#x2F; img_w)</span><br><span class="line">                                xmax &#x3D; int(xmax * 1632 &#x2F; img_w)</span><br><span class="line">                                ymin &#x3D; int(ymin * 1216 &#x2F; img_h)</span><br><span class="line">                                ymax &#x3D; int(ymax * 1216 &#x2F; img_h)</span><br><span class="line">                                if np.sum(updated_label[ymin:ymax, xmin:xmax]) &#x3D;&#x3D; 0:</span><br><span class="line">                                    masks_missed[ymin:ymax, xmin:xmax] &#x3D; 1</span><br><span class="line"></span><br><span class="line">                         updated_label &#x3D; updated_label + masks_missed</span><br><span class="line">                         scipy.misc.toimage(updated_label, cmin&#x3D;0, cmax&#x3D;255, pal&#x3D;colors_map, </span><br><span class="line">                                                            mode&#x3D;&#39;P&#39;).save(os.path.join(label_dir, </span><br><span class="line">                                                                           update_name).rstrip()+ &#39;.png&#39;)</span><br><span class="line">                         print(&#39;&#123;&#125; &#x2F; &#123;&#125;&#39;.format(update_num, len(ANNS)), end&#x3D;&#39;\r&#39;)</span><br></pre></td></tr></table></figure><h3 id="metric"><a href="#metric" class="headerlink" title="metric"></a>metric</h3><p>一般的segmentation论文都是用IoU来进行比较的，但是这个数据集没有segmentation groundtruth，所以我就自己定义了个检测的acc：预测的mask和框有交叉(np.sum(region of box)!=0)，就认为检测出了一个，然后算average acc，通过这个指标和test set上的预测结果来大致衡量哪些方法组合在一起不错。最后总结下来，还是GrabCut+FCN+FL($\alpha=0.75,\gamma=1$)更好些，不过我没加大UNet的深度和通道数，否则的话我猜想可能UNet会占上风。</p><p>篇幅有限，放几个还不错的预测结果：</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/worksummaryofintern/4.png" alt="GrabCut+FCN+FL"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/worksummaryofintern/5.png" alt="GrabCut+FCN+FL"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/worksummaryofintern/6.png" alt="GrabCut+UNet+FL，UNet的结果似乎要圆润一些"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/worksummaryofintern/7.png" alt="GrabCut+FCN+FL更新3次label，效果。。也就马马虎虎吧"></p><h3 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h3><p>总的来说，最后的弱监督分割还是收获挺多的，尤其是自己的工程能力得到了锻炼，代码组织和书写也得到了一定地提升，最后相关成果也写成论文投了ISBI会议，如果能中的话，还是很舒服的^-^</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;classification&quot;&gt;&lt;a href=&quot;#classification&quot; class=&quot;headerlink&quot; title=&quot;classification&quot;&gt;&lt;/a&gt;classification&lt;/h2&gt;&lt;p&gt;在上篇博客提到，该任务就是将原始数据的每张图片（256x256）进行grid级别的label预测，思路很简单，就是最后卷出的feature map是4x4的，不要过average global pooling layer，直接拉成1x16的向量过sigmoid激活函数即可（label也要变成16个字符，有点类似OCR)。&lt;/p&gt;
&lt;h3 id=&quot;dataset&quot;&gt;&lt;a href=&quot;#dataset&quot; class=&quot;headerlink&quot; title=&quot;dataset&quot;&gt;&lt;/a&gt;dataset&lt;/h3&gt;&lt;p&gt;原始数据给的标注是json格式的框标注，但是框不是杆菌的具体位置，而是代表这个grid里面存在杆菌：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;frames&amp;quot;:&amp;#123;&amp;quot;0_grid.png&amp;quot;:[&amp;#123;&amp;quot;x1&amp;quot;:162.42542787286064,&amp;quot;y1&amp;quot;:25.34963325183374,&amp;quot;x2&amp;quot;:170.24938875305622,&amp;quot;y2&amp;quot;:34.11246943765281,&amp;quot;width&amp;quot;:256,&amp;quot;height&amp;quot;:256,&amp;quot;box&amp;quot;:&amp;#123;&amp;quot;x1&amp;quot;:162.42542787286064,&amp;quot;y1&amp;quot;:25.34963325183374,&amp;quot;x2&amp;quot;:170.24938875305622,&amp;quot;y2&amp;quot;:34.11246943765281&amp;#125;,&amp;quot;UID&amp;quot;:&amp;quot;fd548124&amp;quot;,&amp;quot;id&amp;quot;:0,&amp;quot;type&amp;quot;:&amp;quot;Rectangle&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;TB01&amp;quot;],&amp;quot;name&amp;quot;:1&amp;#125;,&amp;#123;&amp;quot;x1&amp;quot;:214.3765281173594,&amp;quot;y1&amp;quot;:24.410757946210268,&amp;quot;x2&amp;quot;:224.07823960880197,&amp;quot;y2&amp;quot;:34.11246943765281,&amp;quot;width&amp;quot;:256,&amp;quot;height&amp;quot;:256,&amp;quot;box&amp;quot;:&amp;#123;&amp;quot;x1&amp;quot;:214.3765281173594,&amp;quot;y1&amp;quot;:24.410757946210268,&amp;quot;x2&amp;quot;:224.07823960880197,&amp;quot;y2&amp;quot;:34.11246943765281&amp;#125;,&amp;quot;UID&amp;quot;:&amp;quot;5318dd81&amp;quot;,&amp;quot;id&amp;quot;:1,&amp;quot;type&amp;quot;:&amp;quot;Rectangle&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;TB01&amp;quot;],&amp;quot;name&amp;quot;:2&amp;#125;,...&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;部分标注内容如上，主要包含了对应的文件夹下有哪些图片，图片上有无杆菌，杆菌的位置在哪个格子（要自己判断），以及一张图片有杆菌的话共有几个（”name”）。&lt;/p&gt;</summary>
    
    
    
    <category term="工作总结" scheme="http://densecollections.top/categories/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
    <category term="CNN" scheme="http://densecollections.top/tags/CNN/"/>
    
    <category term="medical image analysis" scheme="http://densecollections.top/tags/medical-image-analysis/"/>
    
  </entry>
  
  <entry>
    <title>实习见闻及其他</title>
    <link href="http://densecollections.top/posts/thoughtsofintern/"/>
    <id>http://densecollections.top/posts/thoughtsofintern/</id>
    <published>2019-10-03T07:13:24.000Z</published>
    <updated>2021-01-02T13:00:42.835Z</updated>
    
    <content type="html"><![CDATA[<p>早有耳闻近年的机器学习和计算机视觉岗位不好找，很多研究生都会借着自己城市和学校的优势，去争取一些公司的实习。我是兜兜转转好几年后才开始正式去学机器学习，也算是半路出家，而且有种49年入国军的赶脚。平日里看着教研室的师兄们即使做着非机器学习算法也在求职的道路上一波三折，心里不免对自己的前途充满着担忧和焦虑。好在导师关系网络庞大，在今年5月丢给了我一个去AI医疗影像公司实习的机会。能够得到锻炼，同时也可以见识见识工业界的研究节奏和方式，这样的运气，我自然不会让其白白溜走。这家公司南京分部的CTO是一位加州海归博士，热爱技术又充满激情，在勉强通过他的面试之后，我得到了三个月的实习机会。</p><a id="more"></a><p>公司的效率和推进速度还是非常快的，刚开始确实不适应，毕竟放羊久了，长时间地坐在电脑前看论文，垒代码，调参数还是有点乏人的。不过我是分配了一个单独的项目，所以三个月来就相当于单干，虽然没有外部的压力，但是没能参与核心项目，与公司大佬们一起讨论，还是心存遗憾（不过自己水平确实不高，也很正常，咱心里还得有点数才行…）。实习的任务主要是对他们提供的痰涂片（sputum smear）数据集进行分类，但是不是一般意义上的单张图片分类。这里我先简单科普下，痰涂片是为了辅助诊断结核病而采集的样本，医生根据采集的病人的痰液里面的结核杆菌（tuberculosis bacillus）数量来判断阳性或阴性，这种手段主要在发展中国家，贫困地区等地方常见（有条件的直接照X光了）。原始数据集的每张图片都是从显微镜采的，而且分成了4x4的grid，我要做的就是分类图片每个grid的label，有点类似one-stage的目标检测。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/thoughtsofintern/1.png" alt="痰涂片数据示例"></p><p>然而，数据又脏又少。。。。折腾了两个月实在没弄出满意的结果（好像最后最好才到0.9的acc）。没办法，最后Google找了好几天偶然发现了一个公开的数据集，还带着bounding box的标注！！！于是后面和CTO商讨，干脆上了weakly semantic segmentation（最后10几天的工作量比前两个月的还要多的多。。果然deadline是第一生产力）。好歹最后结果还可以。详细的讲解可以看我下一篇博客。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/thoughtsofintern/2.png" alt="weakly segmentation example"></p><p>下面我想重点谈谈自己的收获与感想：</p><ul><li><strong>代码和算法</strong>。只有真正经历了才明白代码能力是多么的重要。从面试一直到解决实际工程问题，或者是自己平时研究，搭网站，作图，实现自己的一个想法…等等等等，都可以发挥巨大的的作用。虽然说做机器学习数学相对重要点，可是进入职场以后，算法和代码能力才是硬实力。对于我这种菜比来说，多练，多参考，多总结是唯一有效的办法</li><li><strong>工业界和学术界</strong>。工业界基本上都是以项目和数据驱动的（可能国内大的学教研室跟其也没什么不同。。），基本上不关心如何创造新方法，新理论，只关心产品如何更好地生成和落地。每天的迭代节奏都很快，各个领域的知识都会涉及（初创公司可能更多）。一般的项目组都是利用现成的SOTA去修改然后适配自己的数据，所以就要求职员得预先懂得很多东西，或者知道哪些方法可以用上，要是从头开始调研的话就太慢了。此外，工业界还有着模块化的管理，比如利用trello这样的软件和早会来安排具体每个人一段时间的任务，交流进展情况，及时解决blocked问题等等。对于涉及到算法的公司，代码的管理，维护，更新是个big problem，从代码托管平台或者服务器的选择，coding的规则制定，review code的指定和审阅标准，要审几轮之后才能merge等都是一些比较琐碎却很实际的问题，是需要慢慢完善的过程。现在想想，那么多学术大牛后面选择投身工业界不是没有道理的，对他们来说研究可以照常进行，资源也更加丰富，而且更有那种creating的青春感，反而学术界里面无拘无束的环境，可能会慢慢消磨他们的激情。处于象牙塔里是很可能被温水煮青蛙的。</li><li><strong>保持学习和交流的习惯</strong>。即使是工作以后，也要保持一种好奇心，要促使自己不断地去进步。学习不仅是让你更加游刃有余于自己的工作，也是为了打开自己的视野，丰富自己的生活。以技术学习为例，公司的研究组一般都会有paper reading这样的活动，每周读一篇典型的论文，轮流安排组员上去讲解，一方面打开工作思路，一方面也是锻炼人的说话和展示能力（费曼学习法）。不要对别人的工作满不在乎，漠不关心，经常主动和别人交流，既交了朋友，也可以扩展自己的领域认知。</li><li><strong>把握工作节奏，保持沟通</strong>。如今996大行其道，加上媒体大肆宣扬，搞得程序猿们人心惶惶。实际上如果是一个项目要到期了，某段时间可能要加点班一起赶出来的话，是可以接受的，但是如果每天强制加班，搞得连自己生活都没了，那我肯定是果断放弃了，除非给我年薪百万。。。因此，在找工作前，首先得平衡好自己的要求，认清一些现状，身体健康和生活质量是放在第一位的，其他的都是为了这两者服务的。有困难，有难题一定要及时反映或者在其恰当的时机吐露出来，不要动不动就轻视自己的生命（可能还没到这种地步），当然这是最差的情况。平时和同级沟通，和上级沟通（反映意见，提出工作思路，建议等）都是必不可少的职场状态。</li></ul><p>总的来说，第一次实习还是挺圆满的，虽然没学到什么大本事，但是起码这段经历让我收获了很多，也更加坚定了自己的选择，对未来的路也看得明晰了些。不得不说，有些事情还是要自己经历过后才能懂得，这些没法感同身受的经验，是怎么也无法理解的。趁着年轻，还是应该多把自己推出舒适区，多去争取一些历练和出走的机会，以此开阔自己的眼界，认识社会和世界。</p><hr><p>好像写到现在也没怎么提及到实习公司的情况。。。想着最后了，还是祝贺下组里的大佬们帮公司拿到了吴恩达<a href="https://stanfordmlgroup.github.io/competitions/chexpert/">chestXpert 数据集比赛</a>的冠军，而且霸榜了几个月！着实佩服。我也从他们身上学到了不少新知识和新姿势。</p><p>自己厚着脸皮也和他们一起公费嗨皮庆祝了下。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/thoughtsofintern/3.jpg" alt="去紫峰大厦吃自助"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/thoughtsofintern/4.jpg" alt="大龙虾，可是我吃不出啥道道来。很僵硬"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/thoughtsofintern/5.jpg" alt="从紫峰大厦观景台俯瞰玄武湖"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/thoughtsofintern/6.jpg" alt="夜景"></p><p>好了，今年的实习告一段落了，明年希望能自己争取到大厂的实习。(‘ o_o ‘)</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;早有耳闻近年的机器学习和计算机视觉岗位不好找，很多研究生都会借着自己城市和学校的优势，去争取一些公司的实习。我是兜兜转转好几年后才开始正式去学机器学习，也算是半路出家，而且有种49年入国军的赶脚。平日里看着教研室的师兄们即使做着非机器学习算法也在求职的道路上一波三折，心里不免对自己的前途充满着担忧和焦虑。好在导师关系网络庞大，在今年5月丢给了我一个去AI医疗影像公司实习的机会。能够得到锻炼，同时也可以见识见识工业界的研究节奏和方式，这样的运气，我自然不会让其白白溜走。这家公司南京分部的CTO是一位加州海归博士，热爱技术又充满激情，在勉强通过他的面试之后，我得到了三个月的实习机会。&lt;/p&gt;</summary>
    
    
    
    <category term="随笔杂谈" scheme="http://densecollections.top/categories/%E9%9A%8F%E7%AC%94%E6%9D%82%E8%B0%88/"/>
    
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="reflections" scheme="http://densecollections.top/tags/reflections/"/>
    
    <category term="job" scheme="http://densecollections.top/tags/job/"/>
    
    <category term="coding" scheme="http://densecollections.top/tags/coding/"/>
    
  </entry>
  
  <entry>
    <title>往者不谏，来者可追</title>
    <link href="http://densecollections.top/posts/personalthoughts-1/"/>
    <id>http://densecollections.top/posts/personalthoughts-1/</id>
    <published>2019-05-11T07:07:13.000Z</published>
    <updated>2021-01-02T13:06:57.916Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>要想得到某样东西，最可靠的办法先让自己配得上它</strong></p></blockquote><p><img src="https://raw.githubusercontent.com/Richardyu114/minds-thoughts-and-resources-about-research-/master/images/4.jpg" class="full-image"></p><p>前些日子无意间看到了一位A. J. Davison教授的新博士生在知乎上写的一篇<a href="https://www.zhihu.com/question/298181420/answer/619558578">回答</a>，他在里面谈到了自己科研之路曲折受挫的经历，尤其是屡屡面对着与自己期望不符合的情况，让我感触很深，想起了自己去年准备国外博士申请失败的那段苦痛记忆。可能对于我来讲，自己对读博的认识和准备都还欠缺得很，既没有拿得出手的本领，技能和honor，也没有对未来研究方向的清楚认识，与另一位机器人大牛<a href="https://zhuanlan.zhihu.com/p/41313724">YY硕</a>读博的初衷和认知相比，实在是汗颜。</p><blockquote><p>所谓人的成长，其实是“不断发现个人独特的经历原来都只是人类普遍经验的一部分”的过程</p><p>​                                                                                                                            —- 多丽丝 莱辛</p></blockquote><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/personalthoughts-1/3.jpg" alt></p><p>自己步入大学也快接近五年了，在这五年来的科研学习生涯上，没有出现一件让我满意的工作，甚至可以说连接触“像样的工作”的机会都没有。回想本科的生活，似乎陷入了一种想要寻求突破发展，却又每每哀叹碰壁于现实，最终困于浑浑噩噩的循环中。虽然高考成绩把我带到了这个所谓的“培优学院”中来，但是实际上鸡肋的培养模式和自己能力不足以及信息缺失等问题，让自己走了很多弯路。从大二上学期进入学院导师的实验室开始，就开始了漫长的专业方向调研和探索学习之旅：先是MEMS器件，再是小型固定翼无人机飞行控制系统，然后转到了飞控数据采集和处理硬件平台，后来又顺着跳到了四旋翼避障，然而结果最终都因为各种原因和变故而终止。现在磕磕碰碰转到了SLAM，从此开始接触计算机视觉，又加上人工智能大热，算是勉强把自己的研究定位在visual SLAM, Robotics和Machine Learning上，即使现在没有任何老师和同专业的师兄师姐指导，但是我心底里知道，这是自己好不容易觉得自己感兴趣的可以坚持做下去的方向，心里也十分珍惜，不愿轻易放弃，纵使单枪匹马很难在这个领域做出东西来。</p><p>心里开始真正坚定了出国留学的想法是在保研后期的时候，那时通过自己的偶然探寻和师兄的推荐，先后联系了两位国外的教授，虽然他们对我的态度都很好，也愿意接收我，但是最终都因为CSC奖学金的问题而以失败告终（甚至连尝试申请CSC的机会都没有）。在这期间，我曾自信满满地以为自己一定能与过去失败的生活说再见，可以在新的教研室，友好的导师的帮助和指导下开启自己目前最感兴趣的课题，然后三四年读完博士，之后再去争取更好的平台继续研究。现在看来，我想得太过于理想化，现实是残酷的，会时不时让人痛苦，“而这种痛苦往往来自于人对于自己无能的愤怒”。在得知CSC奖学金不再对我申请的学校提供名额的那一刻起，就基本上宣判我大半年的准备都是白费的，毕竟自己和意向导师的实验室都缺钱，所以CSC几乎是唯一的funding。当时的我陷入了极大的消极之中，就像看到了希望，看到了黑暗生活中的一丝光亮，想要伸手去触碰，去争取，却又被黑暗中的种种阻碍和枷锁困住，动弹不得。</p><p>后来我安慰自己，人是无法预测自己的命运的，我们可以做的就是不断地让自己变得更好，to be kind。其实我后来想想，这似乎也是天意，让我好好地去审视自己，看清楚自己身上到底出了什么问题，而不是就这么简单地给予我幸运的关照，让我顺顺利利地就能跑到国外读博，逃避现实。因为不论是从科研能力上，心理素质上，理性认知上，以及生活技能上我都还欠火候，还需要不断地修炼，毕竟选择科研这条路不是一句简单的承诺，不仅仅是付出就可以安然走下去的。</p><p>让我欣慰的是，自己地沉沦与消极并没有持续很久，几天之后就慢慢放下了，可能自己心底里也并不是对那些学校和生活特别期待和憧憬，可能自己心底里也还是有个标准的。我一边开始继续自己的研究生工作学习和课题探寻，一边反思自己的生活和精神状态：那些让自己觉得失败的经历到底是如何发生的，为什么我早就认识到了却没有去很好地补救？</p><p>我想，一方面是信息的缺失，身边没有一些志同道合的大佬级别的人之外，另一方面是自己持续不断的，反反复复的自怨自艾，想得太多而又做的太少，眼高手低。前者是没办法选择的，毕竟环境和出生不同，这是客观的因素，我能做的就是学会善用信息时代的网络技术，持续地阅读优质的内容，挖掘有效的有价值的信息，并且主动地结交那些志同道合的前辈；后者是主观因素，是内因，才是自己最应该去改变的地方。因为只有不断地去做，去尝试，机会和惊喜才会眷顾你。努力不一定有收获，但是起码会给你一些选择和机会，同时让人在这个过程中成长。</p><blockquote><p>Work cures everything.</p><p>​                                ——马蒂斯</p></blockquote><p>因此，我开始承认自己菜，而且承认自己是真的菜，那些优秀的人不仅在学习和科研上有很高的建树，在其他方面，诸如绘画，音乐，运动，哲学，文学等都有很高的领悟和施展技巧。我开始慢慢认识到自己的不足和平凡，开始努力地在各个方面去提升自己，当然也是自己一贯就感兴趣的地方，同时也想打开自己的眼界，愿意并持续地去尝试新事物，涉猎不同的领域，发展多种爱好。一言以蔽之，就是活在当下，持续工作，持续输入输出，以此提升自己，慢慢地体验生活，了解生活，享受生活。毕竟这才是人活于世的一项基本目的。</p><p>写到这，我想起了英国演员本尼迪克特在节目letters live上<a href="https://www.bilibili.com/video/av6465704/?spm_id_from=333.788.videocard.2">朗读的一封书信</a>，这是美国先驱艺术家<a href="https://zh.wikipedia.org/wiki/索爾·勒維特">索尔·勒维特</a>写给他的好友伊娃·黑塞一封信，背景是1965 年，黑塞经历了一段自我怀疑的时期，她的创作遇到瓶颈，她迷茫、沮丧，不知道该怎么办，不知道未来在哪里，她向勒维特倾诉自己遇到的“心灵困境”。几周后，勒维特用以下这件作品回复了她：一封精妙、宝贵的建议信，让她不要再彷徨，而是”stop it and just Do “.</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/personalthoughts-1/sol.jpg" alt="Sol LeWitt"></p><p>信的内容如下：</p><blockquote><p>Dear Eva,</p><p>It will be almost a month since you wrote to me and you have possibly forgotten your state of mind(I doubt it though).</p><p>You seem the same as always, and being you, hate every minute of it.</p><p>Don’t!</p><p>Learn to say Fuck You to the world every once in a while. You have every right to.</p><p><strong>Just stop thinking, worrying, looking over your shoulder wondering, doubting, fearing, hurting, hoping for some easy way out, struggling, grasping, confusing, itching, scratching, mumbling, bumbling, grumbling, humbling, stumbling, rumbling, rambling, numbling, gambling, tumbling, scumbling, scrambling, hitching, hatching, bitching, moaning, groaning, honing, boning, horse-shitting, hair-splitting, nit-picking, piss-trickling, nose-sticking, ass-gouging, eyeball-poking, finger pointing, alleyway-sneaking, long waiting, small stepping, evil-eyeing, back scratching, searching, perching, besmirching, grinding, grinding away at yourself. Stop it and just DO.</strong></p><p>From your description, and from what I know of your previous work and your ability; the work you are doing sounds very good. Drawing-clean-clear but crazy like machines, larger and bolder…real nonsense. That sounds fine, wonderful-real nonsense. Do more, more nonsensical, more crazy, more machines, more breasts, penises, cunts, whatever-make them abound with nonsense. Try and tickle something inside you, your weird humor.</p><p>You belong in the most secret part of you.</p><p><strong>Don’t worry about cool, make your own uncool.Make your own, your own world. If you fear, make it work for you-draw &amp; paint your fear and anxiety. And stop worrying about big, deep things such as to decide on a purpose and way of life, a consistent approach to even some impossible end or even an imagined end.</strong></p><p><strong>You must practice being stupid, dumb, unthinking, empty. Then you will be able to DO.</strong></p><p>I have much confidence in you and even though you are tormenting yourself, the work you do is very good. Try to do some BAD work. The worst you can think of and see what happens, but mainly relax and let everything go to hell.</p><p><strong>You are not responsible for the world-you are only responsible for your work-so DO it. And don’t think that your work has to conform to any preconceived form, idea or flavor. It can be anything you want it to be. But if life would be easier for you if you stopped working-then stop. Don’t punish yourself.</strong></p><p>However, I think it is so deeply engrained in you that it would be better for you to DO. It seems I do understand your attitude somewhat, anyway, because I go through a similar process every now and again myself. I have an Agonizing Reappraisal of my work and change everything as much as possible-and hate everything I’ve done, and try to do something entirely different and better. Maybe that kind of process is necessary to me, pushing me on and on. The feeling that I that I can do better than that shit I just did. Maybe you need your agony to accomplish what you do. And maybe it goads you on to do better.</p><p>But it is very painful I know.</p><p><strong>It would be better if you had the confidence just to do the stuff and not even think about it. Can’t you leave the world and ART alone and also quit fondling your ego. I know that you(or anyone) can only work so much and the rest of the time you are left with your thoughts. But when you work of before your work you have to empty you mind and concentrate on what you are doing. After you do something it is done and that’s that. After a while you can see some are better than others but also you can see what direction you are going.</strong></p><p>I’m sure you know all that.</p><p>You also must know that you don’t have to justify your work-not even to yourself.</p><p>Well, you know I admire your work greatly and can’t understand why you are so bothered by it. But you can see the next ones and I can’t. You also must believe in your ability. I think you do.</p><p>So try the most outrageous things you can-shock yourself. You have at your power the ability to do anything.</p><p>I would like to see your work and will have to content to wait until Aug or Sept. I have seen photos of some of Tom’s new things at Lucy’s. They are impressive-especially the ones with the more rigorous form: the simpler ones. I guess he’ll send some more later on.</p><p>Let me know how the shows are going and that kind of stuff. My work had changed since you left and it is much better. I will be having a show May 4-29 at the Daniels Gallery 17 E 64th St(where Emmerich was), I wish you could be there.</p><p>Much love to you both.</p><p> Sol</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/personalthoughts-1/letter1.jpg" alt="letter1"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/personalthoughts-1/letter2.jpg" alt="letter2"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/personalthoughts-1/letter3.jpg" alt="letter3"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/personalthoughts-1/letter4.jpg" alt="letter4"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/personalthoughts-1/letter5.jpg" alt="letter5"></p><p>实际上，这段朗读视频和信的内容我在本科大二的时候就接触到了，可惜当时并没有对此有很深的认知，看来很多道理和改变还是经历过才会真正懂得和接受。现在重读这封信，突然觉得字字箴言。每个人或多或少都会遇到类似的情况，会消沉，会不自信，会怀疑人生，但是事实如此，且事出有因，我们唯一能做的就是调整自己的状态和方向，持续“做下去”，而不是想太多，束缚自己的手脚，正如Sol在信中说的 : </p><blockquote><p>Don’t worry about cool, make your own uncool.Make your own, your own world. If you fear, make it work for you-draw &amp; paint your fear and anxiety.</p><p>After you do something it is done and that’s that. After a while you can see some are better than others but also you can see what direction you are going.</p></blockquote><p>“现在，我接受自己满意的作品，也接受不满意的作品，接受积极状态，也接受不积极状态，这是正常的生活，但我不会停止去做，在过程中体验其意义和价值。” </p><p>自怨自艾，消沉停滞是没有意义的，起码对那时的自己没有帮助，不管是顺境还是逆境，都得持续做下去，因为只有不断地进行下去，最后的意义才会明了。从某种意义上说这似乎是一种”down-top”的思想，我不知道最后结果如何，或者说意义如何，但是我现在要做这件事。就像我身边很多人跟我说“你有没有觉得这一年你感觉天天都很忙，每天起床干活，晚上上床睡觉，日复一日，可是我回想却发现好像啥也没做成”，包括我也是这样。我还年轻，这个问题我没办法理解透彻，只能姑且认为这些都是一种积累，大部分的积累最后会形成质变，其他的一些则会在以后的某个阶段给予我帮助，提供给我一些机会。</p><p>与“down-top”相反的则是”top-down”，拥有这些能力的估计应该都是比较厉害的人吧，从一开始就知道自己想要做什么，整个人生的大阶段和小阶段都有详细的目标和追求，然后对达到目标的过程又有很清晰的认识，有远见，胸有成竹，因此按照条件需求进行学习提升，后面根据成果和目标之间的差距进行微调，接着进行下一轮的提升。这样的人应当是具有很强的自制力的和自律性的，通常应该属于精英阶层，各方面条件都会不错。然而我只是一个普通人，在认识到自己的局限性后，希望能通过各方面的学习，交流，思考和输出来达到一个让自己满意的境界。“不丢脸是不可能变强的”。</p><p>那么到底什么样的东西是值得做的，什么样的人生是值得过的呢？“just DO”之前，如何知道现在做的或者之后做的事是值得的呢？实际上，我认为我们可能不会知道，正是因为不知道，所以我们之前才可能会陷入怀疑，但是如果喜欢却不继续工作下去是件很可惜的事，就像Yan LeCun在上个世纪坚持做神经网络和你现在决定做神经网络的道理一样，你做是因为现在神经网络很火，工资高，而Yan LeCun那个时候做是因为他认为这是值得做的，是自己喜欢做的，觉得是可以引发革命的，所以即使受到怀疑，也还是坚持做了下去（举个例子，当然事实可能不一定是这样，另外SLAM领域的大牛Andrew. J. Davison也是花了十年的时间才让人们注意到他工作的价值）。所以做与思考（个体和团体）是一个交织的整体，不应该是看别人做什么就做什么。在不知道自己喜欢什么之前，可以先和别人交流，看看大概是什么样子，觉得还不错的情况下可以去试着深入了解下，至少在科研上是如此，我在走了很多弯路之后才发现视觉SLAM是个非常有挑战性和综合性的领域，充满着令人兴奋的点和创新式的解决办法，所以我才愿意花时间深入研究，虽然申请该专业的博士失败了，但并不代表着我对相关领域的学习研究就此结束，至少我明白了，科研是科研，它代表的是自己的一种兴趣和工作的结合，它不应该掺杂太多的功利性和急于求成的心思在里面，不应该老是渴望着被人认可，希望受到其他研究工作者的高评价，这样会扰乱自己的思绪，甚至阻碍新想法的出现，同样也会过得不自由。</p><blockquote><p>自由就是不再寻求认可。</p><p>”你是这样年轻，一切都在开始，亲爱的先生，我要尽我所能请求你，对于你心里一切得疑难要多多忍耐，要去爱这些“问题得本身”，像是爱一间锁闭了的房屋，或是一本用别种文字写成的书。现在你不要去追求那些你还不能得到的答案，因为你还不能在生活里体验到它们，一切都要亲自生活“       ——-里尔克，《给青年人的十封信》</p></blockquote><p>还是从科研的功利性出发，对理工科学生来说，不一定非得以自己的大学专业谋生，虽然这有点站着说话不腰疼的感觉，但是如果觉得工作受限，或者有自己想法的话，不妨放下这种执念，虽然知识是财富，但是知识不等于金钱，这跟自己以后能赚大钱是没有必然的联系的（但是理工类学生的工资好像确实高点，而且学历和工资基本正相关）。这让我想起毕导公众号的一篇文章，叫<a href="https://mp.weixin.qq.com/s/EPXQwRj5YY65IsG1EUx3xQ">理工科的硬核浪漫</a>，他在里面说我们在科研，学习的过程中，更多的是训练了我们的一种独特的理工科思维，虽然有时候用这种思维看待问题会让人觉得有点蠢萌和直男，但更多的是会让我们发现生活中的闪光的地方，让我们不自觉地对自己的生活认真起来。</p><blockquote><p>因此在这里我想给大家传递一种观点：理工到现在已经成为我的思维方式，也许你觉得理工科男博士这个群体，他们秃头，他们穿衣很差，他们收入很低，有很多的缺点。但事实上，当他们把这种思维用在生活当中之后，<strong>在我们同样平凡无趣的生活当中，理工男也许能看到更多闪光的地方</strong>，看到更多精彩好玩的地方，他还可以把这个分享给你。</p><p>之前有人问过，毕导你是清华大学的博士生，你从清华大学博士生从科研转向去做自媒体这件事，你觉得可惜不可惜？</p><p><strong>我觉得一点不可惜。你在从事这个专业之后，就一条路走到黑，一直从事这一个专业里面的事情，我反而觉得有点可惜。因为你把自己思维局限住了。所以到现在为止，我给自己一点要求是什么呢？我希望自己好好学习理工科知识，把自己的思维学到一个高度，达到一定的高度之后，怀揣着理工科的理想，去到我感兴趣的领域。</strong></p></blockquote><p>写到这里，感觉也快差不多了，抱歉写得比较杂乱，似乎都是在说一些空洞而无味的东西。主要是最近一年自己身上确实发生了一些起起伏伏的事情，积压了太久的情绪和想法在沉淀了一段时间之后被知乎上的那个回答给带了出来，因此就想写点什么了。不过好久没写东西了，突然感觉想完全表达自己的想法变得费劲起来，以后还得多多练习才行。估计一年之后回过头来重新看这篇文章可能会觉得有点傻吧。&gt;_&lt;|||</p><p> 最后总结一下：</p><ul><li>不管现在状况如何，先找到自己喜欢的，或者认为值得自己做的，坚持做下去，并且keep working</li><li>停止抱怨和自怨自艾，少说多做，大多数的不如意来自于自身的缺陷</li><li>多阅读，多思考，多交流，尽量少看一些没有营养的东西，多多培养不同的兴趣</li><li>学会“输出”，不能只是不停地看，否则很容易忘记，对那些有用的，有趣的，通过费曼学习法让其在脑子里变得更加深刻</li><li>坚持写作，保持创作的热忱</li><li>主动一点，遇到比较厉害的前辈多多请教，不怕丢脸，学习优秀的人的习惯和思维是一种又快又好的进步方式</li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;要想得到某样东西，最可靠的办法先让自己配得上它&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Richardyu114/minds-thoughts-and-resources-about-research-/master/images/4.jpg&quot; class=&quot;full-image&quot;&gt;&lt;/p&gt;
&lt;p&gt;前些日子无意间看到了一位A. J. Davison教授的新博士生在知乎上写的一篇&lt;a href=&quot;https://www.zhihu.com/question/298181420/answer/619558578&quot;&gt;回答&lt;/a&gt;，他在里面谈到了自己科研之路曲折受挫的经历，尤其是屡屡面对着与自己期望不符合的情况，让我感触很深，想起了自己去年准备国外博士申请失败的那段苦痛记忆。可能对于我来讲，自己对读博的认识和准备都还欠缺得很，既没有拿得出手的本领，技能和honor，也没有对未来研究方向的清楚认识，与另一位机器人大牛&lt;a href=&quot;https://zhuanlan.zhihu.com/p/41313724&quot;&gt;YY硕&lt;/a&gt;读博的初衷和认知相比，实在是汗颜。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所谓人的成长，其实是“不断发现个人独特的经历原来都只是人类普遍经验的一部分”的过程&lt;/p&gt;
&lt;p&gt;​                                                                                                                            —- 多丽丝 莱辛&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="随笔杂谈" scheme="http://densecollections.top/categories/%E9%9A%8F%E7%AC%94%E6%9D%82%E8%B0%88/"/>
    
    
    <category term="reflections" scheme="http://densecollections.top/tags/reflections/"/>
    
    <category term="thoughts" scheme="http://densecollections.top/tags/thoughts/"/>
    
    <category term="feelings" scheme="http://densecollections.top/tags/feelings/"/>
    
    <category term="emotions" scheme="http://densecollections.top/tags/emotions/"/>
    
  </entry>
  
  <entry>
    <title>FutureMapping by A.J.Davison</title>
    <link href="http://densecollections.top/posts/FutureMappingofAJD-1/"/>
    <id>http://densecollections.top/posts/FutureMappingofAJD-1/</id>
    <published>2019-04-16T08:24:31.000Z</published>
    <updated>2021-01-02T11:31:51.217Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><hr><p>IML的<a href="https://www.imperial.ac.uk/people/a.davison">A.J.Davison</a>教授是visual SLAM领域里的奠基人之一，近期抽空看了他在推特上置顶的有关SLAM未来的一些思考的论文：  <strong>FutureMapping: The Computational Structure of Spatial AI Systems</strong>。对于做SLAM的同学，感觉这篇论文还是值得一看的，这里有个博主将该论文翻成了<a href="https://blog.csdn.net/cicibabe/article/details/79846466">中文</a>。</p><p>实际上，通篇读下来我的感受是并没有发现Davison提出了比较吸引人眼球的见解，不过有不少亮点，也让我加深了对SLAM这个东西的理解。虽然他对未来visual SLAM的功能性估计也和综述<strong>Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age</strong>  里描述得差不多，只不过后者说的比较“大而空”，都是一些常见的想法，什么动态啦，语义融合啦，模仿生物视觉啦，地图表征与更新等等，但是在Davison的这篇论文里，他以一个机器人学者的角度出发，试图从硬件和软件这两个方面去思考，未来的需求下机器人应该如何完成视觉任务，硬件应该如何发展去支持算法有效的计算，以及整个系统该有怎样的结构，才能使得机器人更好地在不同的场景下，甚至是大场景中完成不同的任务。</p><p>作为一个入门SLAM不算太久的工科学生来说，虽然不少技术知识还未掌握，但是偶尔看看这样的文章和思考也不是未尝不可，至少能激发我思考什么样的东西是值得做的，哪一个技术是未来需求的？</p><p>不过有一个疑问是一直存在我心中的，大家都谈到了未来的visual SLAM会结合机器学习或者模仿生物视觉机制和大脑存储记忆的机制，这个我是赞同的，<strong>但是对于三维地图而言，是否是有必要去重建的</strong>，除了它们在AR上的一些应用？大家都说想要融合语义标签，物体分类与识别融合到三维图里面，有的人可能还想让地图进行实时更新，容纳动态物体（至少我曾经是这样想的），但是这样的做的目的和意义到底是什么？单纯说目的是为了让机器进一步理解环境是无法让我满足的，所以我也试图在寻找和思考这个问题的答案。遗憾的是，Davison在这篇文章里面也没有提到此类问题，不过也没有夸大三维地图的作用，而是强调图模型的作用，这一点我是赞同的。也就是说，类似人一样，我们利用视觉和计算完成任务时，”graph”这个东西是肯定发挥了很大的作用，但是却没必要“事无巨细”的记下来，我们大多是提取重要的特征，压缩下来，然后进行推断，从而得出各种预测和结论，而且事后也可以在脑海中回忆重建出场景的三维模型，此外，这些压缩信息也会随时间及时进行更迭，经常重访的则会记得牢固一些，调取起来也很快，那些不常去的可能就会进一步压缩或者删除了。</p><p>Davison在论文是这样描述地图表征的利用形式的：</p><blockquote><p>In real-time, the system must maintain and update a world model, with geometric and semantic information, and estimate its position within that model, from primarily or only measurements from its on-board sensors.</p><p>The system should provide a wide variety of taskuseful information about <strong>‘what’ is ‘where’ in the scene</strong>. Ideally, <strong>it will provide a full semantic level model of the identities, positions, shapes and motion of all of the objects and other entities in the surroundings</strong>.</p><p>The representation of the world model will <strong>be close to metric</strong>, at least locally, to enable rapid reasoning about arbitrary predictions and measurements of interest to an AI or IA system.</p><p>It will probably <strong>retain a maximum quality representation of geometry and semantics only in a focused manner</strong>; most obviously for the part of the scene currently observed and relevant for near-future interaction. <strong>The rest of the model will be stored at a hierarchy of residual quality levels, which can be rapidly upgraded when revisited.</strong></p><p>The system will be generally alert, in the sense that every incoming piece of visual data is checked against a forward predictive scene model: for tracking, and for detecting changes in the environment and independent motion. The system will be able to <strong>respond to changes</strong> in its environment.</p></blockquote><p>所以我的意思是，在进行视觉任务时，重建三维地图应该不是必要的，至少在实际任务上目前可能起不到很大的作用，可能需要的是一种更加简洁凝练的图表征模式，这种模式更适合机器去认识环境，去进行编码，解码，计算，存储以及维护，而不是像人一样以为这样看到的是环境的理解方式，毕竟我们看到的是已经经过大脑处理的“人机交互结果”，并不是最核心的表征方式。但是大家为什么现在都比较热衷与做3D视觉，3D重建，我想可能是计算机视觉的一个难题吧，毕竟计算机图形学还是很有魅力的，毕竟未来的应用谁也说不准，只希望最后不要让这些技术让人类迷失在虚拟世界里。。。在这个方面，还得多一些<a href="https://www.technologyreview.com/s/613311/mapping-the-world-in-3d-will-let-us-paint-streets-with-augmented-reality/?utm_campaign=the_download.unpaid.engagement&amp;utm_source=hs_email&amp;utm_medium=email&amp;utm_content=71836962&amp;_hsenc=p2ANqtz-8xmRJCA-lRmkREUy6bMXIyqkH3NbAngJgynjDgBx2V-dGw-HLkRxMi3j1Z2izhsqPrpw6txfcuN5lVt9tU4FFzZ1ggiw&amp;_hsmi=71836962">产品层面的思考</a>。</p><p>以上只是我的一点不成熟的想法，还需要多去阅读思考和交流。</p><hr><a id="more"></a><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><p>这篇的文章的摘要如下：</p><blockquote><p>We discuss and predict the evolution of Simultaneous Localization and Mapping (SLAM) into a <strong>general geometric and semantic ‘Spatial AI’ perception capability for intelligent embodied devices</strong>. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or consumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We <strong>explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments.</strong></p></blockquote><p>可以看到Davison教授关注的是<strong>general geometric and semantic ‘Spatial AI’ perception capability for intelligent embodied devices</strong>。首先介绍了Spatial AI system的相关概念，<code>the goal of a Spatial AI system is not abstract scene understanding, but continuously to capture the right information, and to build the right representations, to enable real-time interpretation and action.</code> 然后分别从算法层面和硬件层面去探讨，什么样的元素应该具备，什么样的结构是合理的，什么样的计算方式和维护更新方式可能被采用，以及从被动的分析到主动的分析预测的思考（感觉这才是有点智能的味道），最后还批判现在的计算机视觉研究者都热衷于刷点，而不去思考架构本身的问题，这个吐槽很准了，毕竟听说今年CVPR2019刷点，刷速率的文章接受率都显著下降了，我们读者都疲劳了，更何况评委。当然教授最后吐槽的重点是为了思考Benchmark对visual SLAM的意义，因为visual SLAM是一个实践性很强的系统，是为了解决实际机器人问题而生的，因此在实际的实时实验中效果好才是真的好，一味地去比较各个指标没有太大的意义，而且也很难比较，指标又很多，比如最后文末列的：</p><blockquote><p>• Local pose accuracy in newly explored ares (visual odometry drift rate).<br>• Long term metric pose repeatability in well mapped areas.<br>• Tracking robustness percentage.<br>• Relocalisation robustness percentage.<br>• SLAM system latency.<br>• Dense distance prediction accuracy at every pixel.<br>• Object segmentation accuracy.<br>• Object classification accuracy.<br>• AR pixel registration accuracy.<br>• Scene change detection accuracy.<br>• Power usage.<br>• Data movement (bits×millimetres).</p></blockquote><p>文中的具体内容不再一一讲了，这里主要讲几个文章中让我感兴趣的点。</p><h3 id="ML-或者-DL能为Spatial-AI-system做什么"><a href="#ML-或者-DL能为Spatial-AI-system做什么" class="headerlink" title="ML 或者 DL能为Spatial AI system做什么"></a>ML 或者 DL能为Spatial AI system做什么</h3><p>传统机器学习算法和深度学习中的神经网络擅长做分类和回归，它们在对图像的特征学习上有着得天独厚的优势。近些年也有好多工作是利用CNN和RNN，以及unsupervised learning等深度学习的方法来进行位姿估计和深度估计，也取得了不错的效果。不过有些人认为深度学习在已经研究差不多的3D Geometry上并没有什么意义，况且数学模型我们都知道，没必要去及蹭热度利用深度学习来做，相反那些现有的算法无法处理的图像问题，比如鲁棒性好的特征点提取，光照的变化，纹理的单一，场景的识别以及运动模糊等可以尝试利用深度学习隐式地解决。此外，深度学习在object detection，semantic segmentation等都有很好的成果，可以进行应用。</p><blockquote><p>These are learning architectures which use the designer’s knowledge of the structure of the underlying estimation problem to increase what can be gained from training data, and can be seen as hybrids between pure black box learning and hand-built estimation.     …   Why should a neural network have to use some of its capacity to learn a well understood and modelled concept like 3D geometry? Instead it can focus its learning on the harder to capture factors such as correspondence of surfaces under varying lighting.</p><p>These insights support our first hypothesis that future Spatial AI systems will have recognisable and general map-ping capability which builds a close to metric 3D model. This ‘SLAM’ element may either be embedded within a specially architected neural network, or be more explicitly separated from machine learning in another module which stores and updates map properties using standard estimation methods (as in SemanticFusion for instance). In both cases, there should be an identifiable region of memory which contains a representation of space with some recognisable geometric structure.</p></blockquote><p>个人感觉机器学习以及现在，未来可能出现的一系列理论可能对Spatial AI system帮助最大地可能就是图像理解和环境表征方面了，另外长时间运行带来的认识融合和更新。以及压缩等，可能也会有帮助。有关这个方面的思考目前不是很深，因为我现在还没开始学习机器学习，所以对技术了解不深，但是我感觉这东西是个“万精油”，在SLAM上的应用很大程度上可能归功于设计者怎么用，用在哪里，也就是说怎么设计网络，然后通过什么去学习什么功能，而不是紧紧盯着传统的方法然后去想方设法实现它。</p><h3 id="硬件与云端"><a href="#硬件与云端" class="headerlink" title="硬件与云端"></a>硬件与云端</h3><p>硬件方面Davison教授主要探讨了装载该Spatial AI System的嵌入式硬件应该具有什么样的结构，而且还具有匹配视觉计算特点的算力，同时还得有一定的存储能力。他肯定了分割计算，并行计算，多核心，多线程，神经形态硬件架构的需求，也列出了一些正在研究的例子，具体的内容可以参见文章内容。</p><p>实际上，硬件对算法的促进具有着决定性的作用，从Yann Lecun在2019年的ISSCC上做的<a href="https://pan.baidu.com/s/1lXv0aDSEKXKYQVhJc5X6-A">报告</a>就可以看出</p><p>，没有良好的硬件支持，算法根本没办法进行实验验证，就很难进步。因此，硬件方面的迫切需求是现在整个智能行业的燃眉之急。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/FutureMappingofAJD-1/yanlecun_lesson1.jpeg" alt="yan lecun lesson1 about hardware"></p><p>另外，Davison教授也肯定了云端的重要性。因为云端相当于一个存储中心，可以存储环境表征这样的信息，而且可以同时对环境分布中的机器人进行通信和数据传输，这对机器人在大场景中，长时间执行任务起到“预热”等辅助性作用。</p><blockquote><p>Finally, when considering the evolution of the computing resources for Spatial AI, we should never forget that, cloud computing resources will continue to expand in capacity and reduce in cost. All future Spatial AI systems will likely be cloud-connected most of the time, and from their point of view the processing and memory resources of the cloud can be assumed to be close to infinite and free. What is not free is communication between an embedded device and the cloud, which can be expensive in power terms, particularly if high bandwidthdata such as video is transmitted. The other important consideration is the time delay, typically of significant fractions of a second, in sending data to the cloud for processing.</p><p>The long term potential for cloud-connected Spatial AI is clearly enormous. The vision of Richard Newcombe, Director of Research Science at Oculus, is that all of these devices should communicate and collaborate to build and maintain shared a ‘machine perception map’ of the whole world. The master map will be stored in the cloud, and individual devices will interact with parts of it as needed. A<br>shared map can be much more complete and detailed than that build by any individual device, due to both sensor coverage and the computing resources which can be put into it. A particularly interesting point is that the Spatial AI work which each individual device needs to do in this setup can in theory be much reduced. Having estimated its location within the global map, it would not need to densely map<br>or semantically label its surrounding if other devices had already done that job and their maps could simply be projected into its field of view. It would only need to be alert for changes, and in turn play its part in returning updates.</p></blockquote><h3 id="Graphs"><a href="#Graphs" class="headerlink" title="Graphs"></a>Graphs</h3><p>Davison在论文的第5节讲了很多有关”Graphs”的东西，我们知道，现在的visual SLAM框架都开始逐渐认同将图优化作为减小估计误差的手段要比滤波器估计的效果好得多，因为”graphs”本身就是视觉的一种表征方式，而且在约束上具有非线性性，能更好地模拟现实情况。</p><p>在SLAM方面，教授主要提出了geometry和local appearance两者是否可以联系起来的观点：</p><blockquote><p>We have not yet discovered a <strong>suitable feature representation which describes both local appearance and geometry in such a way that a relatively sparse feature set can provide a dense scene prediction.</strong> We believe that learned features arising from ongoing geometric deep learning research will provide the path towards this.</p><p>Some very promising recent work which we believe is heading in the right direction Bloesch et al.’s CodeSLAM. This method uses an image-conditioned autoencoder to discover an optimisable code with a small number of parameters which describes the dense depth map at a keyframe. <strong>In SLAM, camera poses and these depth codes can be jointly optimised to estimate dense scene shape which is represented by relatively few parameters.</strong> In this method, the scene geometryis stilllocked to keyframes, but we believe that the next step is to discover learned codes which can efficiently represent both appearance and 3D shape, and to make these the elements of a graph SLAM system.</p></blockquote><p>Davison教授另外一个观点是该实时系统中的”Computation Graph”，并且再次提出了”object-oriented SLAM”的概念。</p><blockquote><p> How can we get back to this ‘object-oriented SLAM’ capability in the much more general sense, where a wide range of object classes of varying style and details could be dealt with? As discussed before, <strong>SLAM maps of the future will probably be represented as multi-scale graphs of learned features which describe geometry, appearance and semantics</strong>. Some of these features will represent immediately recognised whole objects as in SLAM++. <strong>Others will represent generic semantic elements or geometric parts (planes, corners, legs, lids?</strong>) which are part of objects either already known or yet to be discovered. Others may approach surfels or other standard dense geometric elements in representing the geometry and appearance of pieces whose semantic identity is not yet known, or does not need to be known.</p><p><strong>Recognition, and unsupervised learning, will operate on these feature maps to cluster, label and segment them</strong>. The machine learning methods which do this job will themselves improve by selfsupervision during the SLAM process, taking advantage of dense SLAM’s properties as a “correspondence engine”.</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/FutureMappingofAJD-1/some_elements_of_computation_graph.PNG" alt="some elements of computation graph"></p><p>这个图基本上等于是把系统算法的框架给列出来了，可以看出，核心还是”定位“（camera state）和”建图“（world model）。只不过里面加入了深度学习来提高系统的性能。</p><blockquote><p><strong>Most computation relates to the world model</strong>, which is a persistent, continuously changing and <strong>improving data store</strong> where the system’s generative representation of the important elements of the scene is held; and the input camera data stream. Some of the main computational elements are:</p><p>• Empirical labelling of images to features (e.g. via a CNN).<br>• <strong>Rendering</strong>: getting a dense prediction from the world map to image space.<br>• Tracking: aligning a prediction with new image data, including finding outliers and detecting independent movement.<br>• <strong>Fusion</strong>: fusing updated geometry and labels back into the map.<br>• <strong>Map consolidation: fusing elements into objects, or imposing smoothing, regularisation</strong>.<br>• Relocalisation/loop closure detection: detecting self similarity in the map.<br>• Map consistency optimization, for instance after confirming a loop closure.<br>• <strong>Self-supervised learning of correspondence information from the running system.</strong></p></blockquote><p>这些都是当前比较主流的观点，而且里面涉及的知识体系比较庞大，因此大部分都是先针对一个来展开研究，不过我觉得要想对其进行突破，最大的，也是最有挑战性的问题应该就是世界模型表征问题了，对于机器来讲，这个应当是个非常简洁和高效的表征方式，同时也易于存储，调用，翻译和编码。</p><h3 id="地图的处理，表示，预测和更新"><a href="#地图的处理，表示，预测和更新" class="headerlink" title="地图的处理，表示，预测和更新"></a>地图的处理，表示，预测和更新</h3><p>其实这个部分前面已经提及了不少了，而Davison教授也单独在第6节讲了这个问题，对里面的几个关键问题进行了总结和思考：一个是硬件支持，一个是地图存储，一个是实时回环。</p><p>地图表征方面：</p><blockquote><p>There is a large degreeof choice possible in the representation of a 3D scene, but as explained in Section 5.1.2, we envision maps which <strong>consist of graphs of learned features, which are linked in multi-scale patterns relating to camera motion</strong>. These features must <strong>represent geometry as well as appearance,</strong> such that they can be used to render a dense predicted view of the scene from a novel viewpoint. It may be that they <strong>do not need to represent full photometric appearance</strong>, and that a somewhat abstracted view is sufficient as long as it captures geometric detail.</p></blockquote><p>地图存储与维持方面（更新）：</p><blockquote><p>Within the main processor, a major area will be devoted to storing this map, in a manner which is <strong>distributed around potentially a large number of individual cores which are strongly connected in a topology to mirror the map graph topology</strong>. In SLAM, of course the map is defined and <strong>grown dynamically</strong>, so the graph within the processor must either be able to change dynamically as well, or must be initially defined with a large unused capacity which is filled as SLAM progresses.</p><p>Importantly, a significant portion of the processing associated with large scale SLAM can be built directly into this graph. This is mainly the sort of ‘maintenance’ processing via which the map optimises and refines itself; including:</p><p>• <strong>Feature clustering; object segmentation and identification.</strong><br>• Loop closure detection.<br>• Loop closure optimization.<br>• <strong>Map regularisation (smoothing).</strong><br>• <strong>Unsupervised clustering to discover new semantic categories.</strong></p><p>With time, data and processing, a map which starts off as dense geometry and low level features can be refined towards an efficient object level map. Some of these operations will run with very high parallelism, as each part of the map is refined on its local core(s), while other operations such as loop closure detection and optimisation will require message passing around large parts of the graph. Still, importantly, they can take place in a manner which is internal to the map store itself.</p></blockquote><p>实时回环方面，教授提出了地图的存储与场景识别方面的一些难点，即“翻译”和“融合”之间协作的问题。因为相机的运动会对地图进行实时更新，该模块的重心在于维持，而不是比较数据，因此可能会对场景识别造成一定的影响。教授在这里提出了可以利用节点（node），海马体结构，以及小世界拓扑结构地图等来解决。我想可能是模仿人的记忆功能。</p><blockquote><p>Instead, a possible solution is to <strong>define special interface nodes which sit between the real-time loop block and the map store</strong>. These are nodes focused on <strong>communication</strong>, which are connected to the relevant components of real-time loop processing and then also to various sites in the map graph, and <strong>may have some analogue in the hippocampus of mammal brains</strong>. If the map store is <strong>organised such that it has a ‘small world’ topology</strong>, meaning that any part is joined to any other part by a small number of edge hops, then the interface nodes should be able to access (copy) any relevant map data in a small number of operations and serve them up to the real-time loop.</p><p><strong>Each node in the map store will also have to play some part in this communication procedure, where it will sometimes beused as part of the route for copying map databackwards and forwards.</strong></p></blockquote><h3 id="注意力机制，主动视觉"><a href="#注意力机制，主动视觉" class="headerlink" title="注意力机制，主动视觉"></a>注意力机制，主动视觉</h3><p>这里的主动视觉是指机器人主动移动相机去采集和任务有关的信息，是一种“top-down”的执行方式。</p><blockquote><p>The active vision paradigm advocates using sensing resources, such as the directions that a camera points towards or the processing applied to its feed, <strong>in a way which is controlled depending on the task at hand and prior knowledge available</strong>. <strong>This ‘top-down’ approach contrasts with ‘bottom-up’, blanket processing of all of the data received from a camera.</strong></p></blockquote><p>Davison提到人的视觉机制是“bottom-up”和”top-down“并存的，而且现在的”bottom-up“的图像处理机制也有很不错的发展，而且在处理很多问题上都很有效，因此两者的结合应当也是一种必然，毕竟”top-down“的执行是需要信息和预测来作为先决条件的。主动视觉在系统的实时性上会有很大的帮助，因为减少了信息和数据处理的冗余度，只分析我需要的数据，因此会大大减小对算力的需求。</p><blockquote><p><strong>It is important that when assessing the relative efficiency of bottom-up versus top-down vision, we take into account not just processing operations but also data transfer, and to what extent memory locality and graph-based computation can be achieved by each alternative.</strong> This will make certain possibilities in model-based prediction unattractive, such as searching large global maps or databases. The amazing advances in <strong>CNN-based vision</strong> means that we have to raise our sights when it comes to what we can expect from <strong>pure bottom-up image processing</strong>. But also, <strong>graph processing will surely permit new ways to store and retrieve model data efficiently,</strong> and will favour keeping and updating world models (such as graph SLAM maps) which have data locality.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Davison这篇论文提出的思考和观点还是比较符合现在的主流认知的，而且在技术上，教授也给出了一些比较具体的方案。不过这个目标比较长远，目前其中的小环节可能都还没处理好，而且还需要硬件铺路，因此想要彻底实现难度还是有点大的。总之，这样的系统我估计未来都是模块化的，分布式的，并且是多协作的，以任务为中心的，毕竟现在的AI还没有大的突破，因此想要实现像人类那样的视觉机制还比较困难，得需要很多个学科的大佬共同研究努力才行。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;hr&gt;
&lt;p&gt;IML的&lt;a href=&quot;https://www.imperial.ac.uk/people/a.davison&quot;&gt;A.J.Davison&lt;/a&gt;教授是visual SLAM领域里的奠基人之一，近期抽空看了他在推特上置顶的有关SLAM未来的一些思考的论文：  &lt;strong&gt;FutureMapping: The Computational Structure of Spatial AI Systems&lt;/strong&gt;。对于做SLAM的同学，感觉这篇论文还是值得一看的，这里有个博主将该论文翻成了&lt;a href=&quot;https://blog.csdn.net/cicibabe/article/details/79846466&quot;&gt;中文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;实际上，通篇读下来我的感受是并没有发现Davison提出了比较吸引人眼球的见解，不过有不少亮点，也让我加深了对SLAM这个东西的理解。虽然他对未来visual SLAM的功能性估计也和综述&lt;strong&gt;Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age&lt;/strong&gt;  里描述得差不多，只不过后者说的比较“大而空”，都是一些常见的想法，什么动态啦，语义融合啦，模仿生物视觉啦，地图表征与更新等等，但是在Davison的这篇论文里，他以一个机器人学者的角度出发，试图从硬件和软件这两个方面去思考，未来的需求下机器人应该如何完成视觉任务，硬件应该如何发展去支持算法有效的计算，以及整个系统该有怎样的结构，才能使得机器人更好地在不同的场景下，甚至是大场景中完成不同的任务。&lt;/p&gt;
&lt;p&gt;作为一个入门SLAM不算太久的工科学生来说，虽然不少技术知识还未掌握，但是偶尔看看这样的文章和思考也不是未尝不可，至少能激发我思考什么样的东西是值得做的，哪一个技术是未来需求的？&lt;/p&gt;
&lt;p&gt;不过有一个疑问是一直存在我心中的，大家都谈到了未来的visual SLAM会结合机器学习或者模仿生物视觉机制和大脑存储记忆的机制，这个我是赞同的，&lt;strong&gt;但是对于三维地图而言，是否是有必要去重建的&lt;/strong&gt;，除了它们在AR上的一些应用？大家都说想要融合语义标签，物体分类与识别融合到三维图里面，有的人可能还想让地图进行实时更新，容纳动态物体（至少我曾经是这样想的），但是这样的做的目的和意义到底是什么？单纯说目的是为了让机器进一步理解环境是无法让我满足的，所以我也试图在寻找和思考这个问题的答案。遗憾的是，Davison在这篇文章里面也没有提到此类问题，不过也没有夸大三维地图的作用，而是强调图模型的作用，这一点我是赞同的。也就是说，类似人一样，我们利用视觉和计算完成任务时，”graph”这个东西是肯定发挥了很大的作用，但是却没必要“事无巨细”的记下来，我们大多是提取重要的特征，压缩下来，然后进行推断，从而得出各种预测和结论，而且事后也可以在脑海中回忆重建出场景的三维模型，此外，这些压缩信息也会随时间及时进行更迭，经常重访的则会记得牢固一些，调取起来也很快，那些不常去的可能就会进一步压缩或者删除了。&lt;/p&gt;
&lt;p&gt;Davison在论文是这样描述地图表征的利用形式的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In real-time, the system must maintain and update a world model, with geometric and semantic information, and estimate its position within that model, from primarily or only measurements from its on-board sensors.&lt;/p&gt;
&lt;p&gt;The system should provide a wide variety of taskuseful information about &lt;strong&gt;‘what’ is ‘where’ in the scene&lt;/strong&gt;. Ideally, &lt;strong&gt;it will provide a full semantic level model of the identities, positions, shapes and motion of all of the objects and other entities in the surroundings&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The representation of the world model will &lt;strong&gt;be close to metric&lt;/strong&gt;, at least locally, to enable rapid reasoning about arbitrary predictions and measurements of interest to an AI or IA system.&lt;/p&gt;
&lt;p&gt;It will probably &lt;strong&gt;retain a maximum quality representation of geometry and semantics only in a focused manner&lt;/strong&gt;; most obviously for the part of the scene currently observed and relevant for near-future interaction. &lt;strong&gt;The rest of the model will be stored at a hierarchy of residual quality levels, which can be rapidly upgraded when revisited.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The system will be generally alert, in the sense that every incoming piece of visual data is checked against a forward predictive scene model: for tracking, and for detecting changes in the environment and independent motion. The system will be able to &lt;strong&gt;respond to changes&lt;/strong&gt; in its environment.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以我的意思是，在进行视觉任务时，重建三维地图应该不是必要的，至少在实际任务上目前可能起不到很大的作用，可能需要的是一种更加简洁凝练的图表征模式，这种模式更适合机器去认识环境，去进行编码，解码，计算，存储以及维护，而不是像人一样以为这样看到的是环境的理解方式，毕竟我们看到的是已经经过大脑处理的“人机交互结果”，并不是最核心的表征方式。但是大家为什么现在都比较热衷与做3D视觉，3D重建，我想可能是计算机视觉的一个难题吧，毕竟计算机图形学还是很有魅力的，毕竟未来的应用谁也说不准，只希望最后不要让这些技术让人类迷失在虚拟世界里。。。在这个方面，还得多一些&lt;a href=&quot;https://www.technologyreview.com/s/613311/mapping-the-world-in-3d-will-let-us-paint-streets-with-augmented-reality/?utm_campaign=the_download.unpaid.engagement&amp;amp;utm_source=hs_email&amp;amp;utm_medium=email&amp;amp;utm_content=71836962&amp;amp;_hsenc=p2ANqtz-8xmRJCA-lRmkREUy6bMXIyqkH3NbAngJgynjDgBx2V-dGw-HLkRxMi3j1Z2izhsqPrpw6txfcuN5lVt9tU4FFzZ1ggiw&amp;amp;_hsmi=71836962&quot;&gt;产品层面的思考&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;以上只是我的一点不成熟的想法，还需要多去阅读思考和交流。&lt;/p&gt;
&lt;hr&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="SLAM" scheme="http://densecollections.top/tags/SLAM/"/>
    
    <category term="mapping" scheme="http://densecollections.top/tags/mapping/"/>
    
    <category term="AI system" scheme="http://densecollections.top/tags/AI-system/"/>
    
    <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
    <category term="hardware" scheme="http://densecollections.top/tags/hardware/"/>
    
  </entry>
  
  <entry>
    <title>visual SLAM by Gaoxiang(3)</title>
    <link href="http://densecollections.top/posts/visualSLAMbyGaoxiang-3/"/>
    <id>http://densecollections.top/posts/visualSLAMbyGaoxiang-3/</id>
    <published>2019-03-26T09:14:25.000Z</published>
    <updated>2021-01-03T03:28:48.434Z</updated>
    
    <content type="html"><![CDATA[<p>本次课程主要研究李群和李代数(Lie Group, Lie Algebra)，主要的目的是为了能够相机得旋转和平移进行微调。因为相机的运动估计可能不准确，而无法对旋转矩阵加上微小量之后依然是旋转矩阵（旋转矩阵无法定义加法，如果用四元数，必须是单位四元数，那么也无法定义加法）。李群李代数与后面的优化，流形都会有很大的联系。</p><blockquote><p>在视觉SLAM中，相机的位姿是未知的，而我们需要解决什么样的相机位姿最符合当前观测数据这样的问题。一种典型的方式是把其构建成一个优化问题，求解最优的 $R$ 和 $t$，使得误差最小化。</p><p>由于旋转矩阵自身带有约束，即必须正交且行列式为1，因此作为优化变量会引入额外的约束，使得优化变得困难。而通过李群李代数的转换关系，可以顺利求导，把位姿估计变成无约束的优化问题。</p></blockquote><h2 id="群"><a href="#群" class="headerlink" title="群"></a>群</h2><p>群(Group)是一种集合加上一种运算的代数结构，满足封闭性，结合律，<strong>幺元</strong>，逆。其中幺元可以认为是单位元，就是与其他元素作用不改变这个元素，逆是元素和和它的逆进行运算后得到了幺元。</p><p>三维旋转矩阵构成了三维正交群(special orthogonal group)</p><script type="math/tex; mode=display">SO(3) = \left \{ R \in {\mathbb R}^{3 \times 3} | RR^{T} =I, \det (R)=1 \right \}</script><p>三维变换矩阵构成了特殊欧式群(special euclidean group)</p><script type="math/tex; mode=display">SE(3) = \left \{T= \begin{bmatrix}R & t\\O^{T} & 1\end{bmatrix}\in {\mathbb R}^{4 \times 4} |R \in SO(3), t \in {\mathbb R}^{3} \right \}</script><p>旋转矩阵集合与矩阵乘法构成群，变换矩阵集合与矩阵乘法也构成了群，因此称它们为旋转矩阵群和变换矩阵群。</p><p>群结构保证了在群上的运算具有良好的性质。</p><a id="more"></a><h2 id="李群与李代数"><a href="#李群与李代数" class="headerlink" title="李群与李代数"></a>李群与李代数</h2><h3 id="李群"><a href="#李群" class="headerlink" title="李群"></a>李群</h3><p>具有连续（光滑）性质的群；</p><p>既是群也是流形；</p><p>直观上看，一个刚体能够连续地在空间中运动，因此$SO(3)$和$SE(3)$都是李群，然而，它们都没有定义加法，所以很难进行取极限和求导等操作；</p><h3 id="李代数"><a href="#李代数" class="headerlink" title="李代数"></a>李代数</h3><p>与李代数对应的一种结构，位于向量空间（李群单位元处的正切空间）$\mathfrak so(3)$，$\mathfrak se(3)$</p><p>从旋转矩阵引出旋转正交群的李代数：</p><p>对于相机的连续运动，旋转矩阵也随时间变化，则有：</p><script type="math/tex; mode=display">\begin{align*}& R(t)R(t)^{T} = I \\& \dot{R}(t)R(t)^{T}+R(t)\dot{R}(t)^{T} = 0  \quad \quad 对时间求导\\& \dot{R}(t)R(t)^{T} = - (\dot{R}(t)R(t)^{T})^{T} \quad \quad 反对称矩阵\\& 记\dot{R}(t)R(t)^{T} = \phi (t)^{\wedge} \implies \dot{R}(t) = \phi (t)^{\wedge} R(t)\end{align*}</script><p>符号$\wedge$看作是反对称矩阵的符号，在这里是指将向量$\phi$变成了反对称矩阵，这是由叉乘引申而来，在第二讲有提过。反过来符号$\vee$代表反对称矩阵到向量的变换。</p><p>上面的式子表示，对旋转矩阵求导，就是在其左侧乘以一个$\phi (t)$，类似于指数函数的求导。</p><p>下面进行进一步地近似，假设在单位元附近，$t_{0}=0, R(0)=I$，则：</p><script type="math/tex; mode=display">R(t) \approx R(t_{0}) + \dot {R}(t_{0})(t-t_{0}) = I + \phi (t_{0})^{\wedge} (t) \quad \quad 将R在t_{0}进行泰勒展开，并忽略二次及以上高阶项</script><p>进一步假设，在$t_{0}$附近，$\phi$不变，则$\dot{R}(t)=\phi(t_{0})^{\wedge}R(t)=\phi_{0}^{\wedge} R(t)$，再根据初值条件，得出：</p><script type="math/tex; mode=display">R(t) = \exp( \phi _{0} ^{\wedge} t)</script><p>在泰勒展开那一步可以看出，$\phi$反映的是一阶导数的性质，位于旋转正交群的正切空间上（tangent space，切平面上）。</p><p>上述证明提供了一种思路，可能不太严谨。实际上可以证明最后得出的式子在任意时间都适用，且该关系称为指数映射（exponential map），$\phi$称为$SO(3)$对应的李代数$\mathfrak so(3)$。</p><p><strong>李群是高维空间的低维曲面，或者说低维流形，在流形原点附近的切空间上的任意一个点，是李代数，可以通过指数映射映回李群上。李代数描述了李群单位元附近的正切空间的性质。</strong></p><script type="math/tex; mode=display">\begin{align*}& 李代数由一个集合\mathbb V，一个数域\mathbb F，和一个二元运算[,]（李括号，直观上说表示了两个元素的差异）组成。如果满足下面的四条性质，称\\& (\mathbb V, \mathbb F, [,])为一个李代数，记作 \mathfrak g。\\&1. 封闭性 \quad \quad \forall {\bf X, Y}\in \mathbb V, [{\bf X,Y}] \in \mathbb V\\&2. 双线性 \quad \quad \forall {\bf X, Y}\in \mathbb V, a,b \in \mathbb F,有：[a {\bf X} + b {\bf Y}, {\bf Z}] =a[{\bf X,Z}]+b[{\bf Y, Z}],[{\bf Z},a{\bf X}+b{\bf Y}] =a[{\bf Z,X}]+b[{\bf Z,Y}] \\&3. 自反性 \quad \quad \forall {\bf X} \in \mathbb V, [{\bf X,X}]={\bf 0}\\&4. 雅可比等价 \quad \quad \forall {\bf X,Y,Z} \in \mathbb V,[{\bf X},[{\bf Y,Z}]]+[{\bf Z},[{\bf Y,X}]]+[{\bf Y},[{\bf Z,X}]]= {\bf 0}\end{align*}</script><p>李代数$\mathfrak so(3)$可以看成是三维空间向量和叉积运算构成的，$so(3)=\left \{ \phi \in \mathbb R^{3}, \Phi = \phi ^{\wedge} \in \mathbb R^{3 \times 3} \right \}$，其中：</p><script type="math/tex; mode=display">\Phi = \phi ^{\wedge} =\begin{bmatrix}0 & - \phi_{3} & \phi _{2}\\\phi _{3} & 0 & -\phi _{1}\\-\phi _{2} & \phi _{1} & 0\end{bmatrix}\in \mathbb R^{3 \times 3}</script><p>李括号$[\phi_{1},\phi_{2}] = (\Phi_{1} \Phi_{2} - \Phi_{2} \Phi_{1})^{\vee}$,容易验证此李括号满足上述四条性质。</p><p>对于变换矩阵的特殊欧式群$SE(3)$，也有对应的李代数$\mathfrak se(3)$(6维的向量)</p><script type="math/tex; mode=display">\mathfrak se(3) = \left \{\xi = \begin{bmatrix}\rho \\\phi\end{bmatrix}\in \mathbb R^{6}, \rho \in \mathbb R^{3}, \phi \in \mathfrak se(3),\xi ^{\wedge} = \begin{bmatrix}\phi ^{\wedge} & \rho \\{\bf 0}^{T} & 0\end{bmatrix}\in \mathbb R^{4 \times 4}\right \}</script><p>此时还是以用符号$\wedge$来表示向量到矩阵的变换，只不过不再是限制于反对称矩阵。</p><script type="math/tex; mode=display">\begin{align*}& 设变换矩阵g(t)=\begin{bmatrix}R & \alpha \\{\bf 0}^{T} & 1\end{bmatrix} ，则g(t)^{-1} = \begin{bmatrix}R^{T} & -R^{T}\alpha \\{\bf 0}^{T} & 1\end{bmatrix}\\&有\dot{g}(t)g(t)^{-1}=\begin{bmatrix}\dot{R}R^{T} & \alpha- \dot{R}R^{T}\alpha \\{\bf 0}^{T} & 0\end{bmatrix}= \begin{bmatrix}\omega^{\wedge} & v\\{\bf 0}^{T} &0\end{bmatrix}，其中，\omega^{\wedge} \in \mathbb R^{3 \times 3},v \in \mathbb R^{3},记\xi^{\wedge}=\dot{g}(t)g(t)^{-1}\\& (\xi^{\wedge})^{\vee}=\begin{bmatrix}v \\\omega\end{bmatrix} \in \mathbb R^{6}\\&\dot{g}(t)=(\dot{g}(t)g(t)^{-1})g(t)=\xi^{\wedge}g(t),则g(t)=\exp(\xi^{\wedge}),假设g(0)=I\end{align*}</script><p>李括号$[\xi _{1}, \xi _{2}] = (\xi _{1} ^{\wedge} \xi_{2} ^{\wedge} - \xi _{2} ^{\wedge} \xi_{1} ^{\wedge}) ^{\vee}$</p><h2 id="指数映射和对数映射"><a href="#指数映射和对数映射" class="headerlink" title="指数映射和对数映射"></a>指数映射和对数映射</h2><p>指数映射反映了李代数到李群的关系，对于旋转矩阵$R$，有$R=\exp (\phi ^{\wedge})=\sum_{n=0}^{\infty} \frac{1}{n!}(\phi ^{\wedge})^{n}$</p><p>为了研究的方便，先将$\phi$写成旋转向量的形式，即设$\phi = \theta \vec{a}$,其中$\vec{a}$是单位向量，而且具有以下性质</p><script type="math/tex; mode=display">\begin{align*}&1.\vec{a}^{\wedge} \vec{a}^{\wedge} = \vec {a} \vec{a}^{T} - I \\&2.\vec {a}^{\wedge} \vec {a}^{\wedge} \vec {a}^{\wedge} = -\vec {a}^{\wedge}\end{align*}</script><p>下面利用上述性质对$\exp (\phi)^{\wedge}$进行Taylor展开</p><script type="math/tex; mode=display">\begin{align*}\exp (\phi)^{\wedge} & =\exp (\theta \vec{a}) = \sum _{n=0}^{\infty}\frac{1}{n!}(\theta \vec{a}^{\wedge})^{n}\\                     & = I+\theta \vec{a}^{\wedge}+\frac{1}{2!}\theta ^{2} \vec{a}^{\wedge}\vec{a}^{\wedge}+\frac{1}{3!} \theta ^{3} \vec{a}^{\wedge}\vec{a}^{\wedge}\vec{a}^{\wedge}+\frac{1}{4!} \theta ^{4} \vec{a}^{\wedge}\vec{a}^{\wedge}\vec{a}^{\wedge}\vec{a}^{\wedge}+ \cdots\\                     & = \vec{a}\vec{a}^{T}-\vec{a}^{\wedge}\vec{a}^{\wedge}+\theta \vec{a}^{\wedge}+\frac{1}{2!}\theta ^{2}\vec{a}^{\wedge}\vec{a}^{\wedge}-\frac{1}{3!} \theta^{3}\vec{a}^{\wedge}-\frac{1}{4!}\theta ^{4}\vec{a}^{\wedge}\vec{a}^{\wedge}+\cdots \\                     & = \vec{a}\vec{a}^{T}+(\theta -\frac{1}{3!}\theta^{3}+\frac{1}{5!}\theta^{5}-\cdots)\vec{a}^{\wedge}-(1-\frac{1}{2!}\theta^{2}+\frac{1}{4!}\theta^{4}-\cdots)\vec{a}^{\wedge}\vec{a}^{\wedge}\\                     & = \vec{a}^{\wedge}\vec{a}^{\wedge}+I+\sin \theta \vec{a}^{\wedge}-\cos \theta \vec{a}^{\wedge}\vec{a}^{\wedge}\\                     & = (1-\cos \theta)\vec{a}^{\wedge}\vec{a}^{\wedge}+I+\sin \theta \vec{a}^{\wedge}\\                     & = \cos \theta I+(1-\cos \theta)\vec{a}\vec{a}^{T}+\sin \theta \vec{a}^{\wedge}\end{align*}</script><p>这进一步说明了李代数$\mathfrak so(3)$的物理意义确实就是旋转向量。</p><p>反之，给定旋转矩阵亦可以求出对应的李代数，即对数映射$\phi = \ln(R)^{\vee}$。不过实际情况下可以通过旋转矩阵到向量的公式来得出李代数。</p><p>同理，可以得到$\mathfrak se(3)$到$SE(3)$的指数映射（具体推导过程会在作业中展示）</p><script type="math/tex; mode=display">\begin{align*}\exp(\xi^{\wedge}) & = \begin{bmatrix}\sum _{n=0}^{\infty}\frac{1}{n!}(\phi^{\wedge})^{n} & \sum _{n=0}^{\infty} \frac{1}{(n+1)!}(\phi ^{\wedge})^{n} \rho \\{\bf 0}^{T} & 1\end{bmatrix}\\& \triangleq \begin{bmatrix}R & J\rho \\{\bf 0}^{T} & 1\end{bmatrix}\\& = T\end{align*}</script><p>其中$J$为$SE(3)$的雅可比矩阵，$J=\frac{\sin \theta}{\theta} I + (1-\frac{\sin \theta} {\theta}) \vec{a} \vec{a}^{T} + \frac{1-\cos \theta} {\theta} \vec{a}^{\wedge}$</p><p>注意，这里的平移部分与变换矩阵的平移部分不完全相同，可以看出，指数映射会对李代数中的平移部分进行线性变换后才得到了真正的平移部分。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/visualSLAMbyGaoxiang-3/lie_group_and_lie_algebra.png" alt="李群李代数的对应关系"></p><h2 id="李代数求导与扰动模型"><a href="#李代数求导与扰动模型" class="headerlink" title="李代数求导与扰动模型"></a>李代数求导与扰动模型</h2><p>前面说过，视觉SLAM应用李群李代数的最初目的就是为了对相机的位姿进行优化。因为相机在观测世界的时候，会不可避免地引入噪声，而我们优化的目的就是会取N个观测，然后对其进行误差最小化，得到一个在这么多N个观测的过程中最优的变换关系。因此，针对优化（一般是最小化问题）问题，我们常用的手段就是求导，此时李代数的功能就体现出来了，因为李代数具有良好的加法运算，可以进行无约束优化问题分析。</p><p>不过问题是，李代数上的加法并不会对应李群上的乘法，即$\exp(\phi_{1}^{\wedge}) \exp(\phi_{2}^{\wedge}) \neq \exp ((\phi_{1}+\phi_{2})^{\wedge})$</p><p>该关系由BCH公式（Baker-Campbell-Hausdorff）给出：</p><script type="math/tex; mode=display">\ln (\exp (A) \exp(B)) =A +B + \frac{1}{2} [A,B] + \frac{1}{12}[A,[A,B]] - \frac{1}{12}[B.[A,B]]+\cdots</script><p>如果其中一个量为小量，则有下列的近似表达关系：</p><script type="math/tex; mode=display">\ln (\exp (\phi_{1}^{\wedge}) \exp (\phi_{2}^{\wedge}))^{\vee} \approx\begin{cases}J_{l} (\phi _{2})^{-1} \phi_{1}+\phi_{2} \quad \quad \text{if  $\phi_{1}$is small}\\J_{r}(\phi_{1})^{-1} \phi_{2} + \phi_{1} \quad \quad \text{if  $\phi_{2}$is small}\end{cases}</script><p>其中：</p><script type="math/tex; mode=display">\begin{align*}& J_{l}=J=\frac{\sin \theta}{\theta} I + (1-\frac{\sin \theta} {\theta}) \vec{a} \vec{a}^{T} + \frac{1-\cos \theta} {\theta} \vec{a}^{\wedge}  \quad左雅可比\\& J_{l}^{-1} = \frac{\theta}{2}\cot \frac{\theta}{2}I+(1-\frac{\theta}{2}\cot \frac{\theta}{2})\vec{a}\vec{a}^{T}-\frac{\theta}{2}\vec{a}^{\wedge}\\& J_{r}(\phi) = J_{l}(-\phi) \quad 右雅可比\end{align*}</script><p>一般来说，利用$T_{cw}$时会进行左乘，利用$T_{wc}$时会进行右乘，以左乘为例，直观的写法是</p><script type="math/tex; mode=display">\begin{align*}& \exp (\Delta \phi ^{\wedge}) \exp(\phi ^{\wedge}) = \exp ((J_{l}(\phi)^{-1}\Delta \phi +\phi)^{\wedge}) \quad \quad 李群上的微小量乘法，李代数上的加法相差雅可比矩阵的逆\\& \exp((\phi + \Delta \phi)^{\wedge})=\exp ((J_{l}\Delta \phi)^{\wedge})\exp(\phi^{\wedge})=\exp(\phi^{\wedge}) \exp((J_{r}\Delta \phi)^{\wedge})\quad \quad 李代数上的微小量加法，李群上要乘上雅可比矩阵\end{align*}</script><p>对于$SE(3)$和$\mathfrak se(3)$，关系要复杂一些（雅可比矩阵较为复杂，是个$6 \times 6$矩阵，但在实际计算中不用到该雅可比）：</p><script type="math/tex; mode=display">\begin{align*}& \exp(\Delta \xi ^{\wedge}) \exp(\xi ^{\wedge}) \approx \exp ((\mathcal J_{l}^{-1}\Delta \xi +\xi)^{\wedge})\\& \exp(\xi ^{\wedge}) \exp (\Delta \xi^{\wedge}) \approx \exp ((\mathcal J_{r}^{-1} \Delta \xi+\xi)^{\wedge})\end{align*}</script><p>有了上述公示后，开始对旋转矩阵进行求导，这里有两种方法，一种是导数定义求导，一种是通过扰动模型进行求导，实际中，扰动模型因为形式更加简单，因此采用的更多，先不严谨地记旋转后的点关于旋转矩阵的导数的求导式为 $\frac{\partial (Rp)}{\partial R}$</p><p>导数模型：</p><script type="math/tex; mode=display">\begin{align*}\frac{\partial(\exp (\phi^{\wedge})p)}{\partial \phi} & = \lim_{\delta \phi \to 0} \frac{\exp((\phi+\delta \phi)^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi} \\                                                      & = \lim_{\delta \phi \to 0} \frac{\exp ((J_{l}\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp (\phi^{\wedge})p}{\delta \phi}\\                                                      & \approx \lim_{\delta \phi \to 0} \frac{(I+(J_{l}\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi} \\                                                      & = \lim_{\delta \phi \to 0} \frac{(J_{l}\delta \phi)^{\wedge}\exp(\phi^{\wedge})p}{\delta \phi}\\                                                      & = \lim_{\delta \phi \to 0} \frac{-(\exp(\phi^{\wedge})p)^{\wedge} J_{l}\delta \phi}{\delta \phi} \\                                                      & = -(Rp)^{\wedge}J_{l}\end{align*}</script><p>扰动模型（左乘微小量）：</p><script type="math/tex; mode=display">\begin{align*}\frac{\partial(Rp)}{\partial R} & = \lim_{\delta \phi \to 0} \frac{\exp((\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi}\\                                & \approx \lim_{\delta \phi \to 0} \frac{(I+(\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi}\\                                & = \lim_{\delta \phi \to 0} \frac{(\delta \phi)^{\wedge}Rp}{\delta \phi} =  -(Rp)^{\wedge}\end{align*}</script><p>同理可得，$SE(3)$上的扰动模型（左乘微小量）为：</p><script type="math/tex; mode=display">\frac{\partial (Tp)}{\partial \delta \xi}=\begin{bmatrix}I & -(Rp+t)^{\wedge}\\{\bf 0}^{T} & {\bf 0}^{T}\end{bmatrix}\triangleq (Tp)^{\odot}</script><h2 id="相似变换"><a href="#相似变换" class="headerlink" title="相似变换"></a>相似变换</h2><p>对于单目视觉，由于存在尺度不确定性，因此不能使用$SE(3)$来表达位姿变化，而是利用相似变换群$Sim(3)$，也就是说要加一个尺度因子$s$，这个尺度因子会同时作用在变换的点$p$上，对其进行缩放，也就是在相机坐标系下进行了一次相似变换，而不是欧式变换，即：$p^{‘} = sRp+t$</p><script type="math/tex; mode=display">Sim(3)=\left\{\left[ \begin{array}{lll}{\boldsymbol{S}=} & {\boldsymbol{s} \boldsymbol{R}} & {\boldsymbol{t}} \\ {\boldsymbol{0}^{T}} & {1}\end{array}\right] \in \mathbb{R}^{4 \times 4}\right\}</script><script type="math/tex; mode=display">\mathfrak sim(3)=\left\{\zeta | \zeta=\left[ \begin{array}{l}{\rho} \\ {\phi} \\ {\sigma}\end{array}\right] \in \mathbb{R}^{7}, \zeta^{\wedge}=\left[ \begin{array}{cc}{\sigma I+\phi^{\wedge}} & {\rho} \\ {0^{T}} & {0}\end{array}\right] \in \mathbb{R}^{4 \times 4}\right\}</script><script type="math/tex; mode=display">\exp \left(\zeta^{\wedge}\right)=\left[ \begin{array}{cc}{e^{\sigma} \exp \left(\phi^{\wedge}\right)} & {J_{s} \rho} \\ {0^{T}} & {1}\end{array}\right]</script><script type="math/tex; mode=display">\begin{aligned} J_{s}=& \frac{e^{\sigma}-1}{\sigma} I+\frac{\sigma e^{\sigma} \sin \theta+\left(1-e^{\sigma} \cos \theta\right) \theta}{\sigma^{2}+\theta^{2}} a^{\wedge} \\ &+\left(\frac{e^{\sigma}-1}{\sigma}-\frac{\left(e^{\sigma} \cos \theta-1\right) \sigma+\left(e^{\sigma} \sin \theta\right) \theta}{\sigma^{2}+\theta^{2}}\right) a^{\wedge} a^{\wedge} \end{aligned}</script><script type="math/tex; mode=display">s=e^{\sigma}, \boldsymbol{R}=\exp \left(\boldsymbol{\phi}^{\wedge}\right), \boldsymbol{t}=\boldsymbol{J}_{s} \boldsymbol{\rho}</script><p>对于$Sim(3)$的求导，利用左扰动模型和BCH近似（这里的BCH近似与$SE(3)$公式不同）。假设点$p$经过相似变换$Sp$后，相对于$S$的导数为：</p><script type="math/tex; mode=display">\frac{\partial S p}{\partial \zeta}=\left[ \begin{array}{ccc}{\boldsymbol{I}} & {-\boldsymbol{q}^{\wedge}} & {\boldsymbol{q}} \\ {\mathbf{0}^{T}} & {\mathbf{0}^{T}} & {0}\end{array}\right]</script><p>其中$q$是$Sp$的前三维向量，最后的形式应该是$4 \times 7$的雅可比矩阵。</p><p>有关相似变换群的更为详细的理解和运用，等后面进行实际应用时再说，毕竟库已经提供好了，而且推导过程也与$SE(3)$类似。</p><h2 id="作业与实践"><a href="#作业与实践" class="headerlink" title="作业与实践"></a>作业与实践</h2><h3 id="群的性质"><a href="#群的性质" class="headerlink" title="群的性质"></a>群的性质</h3><p>群要满足封闭性，结合律，幺元和逆这四个性质。其中，满足前两个性质的叫半群，满足前三个性质的叫有单位元的半群，若满足了上述四个性质，还具有交换律的叫做阿贝尔群。</p><p>对于$\left \{ \mathbb Z, + \right \}$封闭性和结合律显然满足，幺元是0，逆为自身的相反数，因此是群，而且是阿贝尔群。</p><p>对于$\left \{ \mathbb N, +\right \}$，前三个性质都满足，幺元是0，但是除了0之外，其他的元素不存在逆，因此不是群，是有单位元的半群。</p><h3 id="验证向量叉乘的李代数性质"><a href="#验证向量叉乘的李代数性质" class="headerlink" title="验证向量叉乘的李代数性质"></a>验证向量叉乘的李代数性质</h3><p>设${\bf X}=a_{1}\vec{i}+b_{1}\vec{j}+c_{1}\vec{k}, {\bf Y}=a_{2}\vec{i}+b_{2}\vec{j}+c_{2}\vec{k}, {\bf Z}=a_{3}\vec{i}+b_{3}\vec{j}+z_{3}\vec{k}$</p><p>封闭性：</p><script type="math/tex; mode=display">[{\bf X},{\bf Y}] = {\bf X} \times {\bf Y} \in \mathbb R^{3}</script><p>双线性：</p><script type="math/tex; mode=display">\begin{align*}& [a{\bf X}+b{\bf Y},{\bf Z}]=(a{\bf X}+b{\bf Y}) \times {\bf Z}=a{\bf X}\times {\bf Z}+b{\bf Y} \times {\bf Z} = a[{\bf X,Z}]+b[{\bf Y,Z}]\\& [{\bf Z},a{\bf X}+b{\bf Y}]={\bf Z} \times (a{\bf X}+b{\bf Y})={\bf Z}\times a {\bf X}+{\bf Z} \times b{\bf Y}=a[{\bf Z,X}]+b[{\bf Z,Y}]\end{align*}</script><p>自反性：</p><script type="math/tex; mode=display">[{\bf X,X}]={\bf X} \times {\bf X}={\bf 0}</script><p>雅可比等价：</p><script type="math/tex; mode=display">\begin{align*}&[{\bf X},[{\bf Y,Z}]]+[{\bf Y},[{\bf Z,X}]]+[{\bf Z},[{\bf X,Y}]]\\& ={\bf X} \times {\bf Y} \times {\bf Z}+{\bf Y} \times {\bf Z} \times {\bf X}+{\bf Z} \times {\bf X} \times {\bf Y}\\& = {\bf X}^{\wedge} {\bf Z}^{\wedge} {\bf Y}+{\bf Y}^{\wedge} {\bf X}^{\wedge} {\bf Z}+{\bf Z}^{\wedge} {\bf Y}^{\wedge} {\bf X}\\& = \begin{bmatrix}0 & -c_{1} & b_{1}\\c_{1} & 0 & -a_{1}\\-b_{1} & a_{1} &0\end{bmatrix}\begin{bmatrix}0 & -c_{3} & b_{3}\\c_{3} & 0 & -a_{3}\\-b_{3} & a_{3} & 0\end{bmatrix}\begin{bmatrix}a_{2}\\b_{2}\\c_{2}\end{bmatrix}+\begin{bmatrix}0 & -c_{2} & b_{2}\\c_{2} & 0 & -a_{2}\\-b_{2} & a_{2} &0\end{bmatrix}\begin{bmatrix}0 & -c_{1} & b_{1}\\c_{1} & 0 & -a_{1}\\-b_{1} & a_{1} & 0\end{bmatrix}\begin{bmatrix}a_{3}\\b_{3}\\c_{3}\end{bmatrix}+\begin{bmatrix}0 & -c_{3} & b_{3}\\c_{3} & 0 & -a_{3}\\-b_{3} & a_{3} &0\end{bmatrix}\begin{bmatrix}0 & -c_{2} & b_{2}\\c_{2} & 0 & -a_{2}\\-b_{2} & a_{2} & 0\end{bmatrix}\begin{bmatrix}a_{1}\\b_{1}\\c_{1}\end{bmatrix}\\& = \begin{bmatrix}a_{3}(b_{1}b_{2}+c_{1}c_{2})-a_{2}(b_{1}b_{3}+c_{1}c_{3})\\b_{3}(a_{1}a_{2}+c_{1}c_{2})-b_{2}(a_{1}a_{3}+c_{1}c_{3})\\c_{3}(a_{1}a_{2}+b_{1}b_{2})-c_{2}(b_{1}b_{3}+a_{1}a_{3})\end{bmatrix}+\begin{bmatrix}a_{1}(b_{2}b_{3}+c_{2}c_{3})-a_{3}(b_{1}b_{2}+c_{1}c_{2})\\b_{1}(a_{2}a_{3}+c_{2}c_{3})-b_{3}(a_{1}a_{2}+c_{1}c_{2})\\c_{1}(a_{2}a_{3}+b_{2}b_{3})-c_{3}(b_{1}b_{2}+a_{1}a_{2})\end{bmatrix}+\begin{bmatrix}a_{2}(b_{1}b_{3}+c_{1}c_{3})-a_{1}(b_{2}b_{3}+c_{1}c_{3})\\b_{2}(a_{1}a_{3}+c_{1}c_{3})-b_{1}(a_{2}a_{3}+c_{1}c_{3})\\c_{2}(a_{1}a_{3}+b_{1}b_{3})-c_{1}(b_{2}b_{3}+a_{1}a_{3})\end{bmatrix}\\& = \begin{bmatrix}0\\0\\0\end{bmatrix}={\bf 0}\end{align*}</script><p>则$\mathfrak g =(\mathbb R^{3}, \mathbb R, \times)$构成李代数。</p><h3 id="推导-SE-3-的指数映射"><a href="#推导-SE-3-的指数映射" class="headerlink" title="推导$SE(3)$的指数映射"></a>推导$SE(3)$的指数映射</h3><p>推导$SE(3)$指数映射部分左雅可比的形式：</p><script type="math/tex; mode=display">\begin{align*}J & \triangleq \sum_{n=0}^{\infty} \frac{1}{(n+1)!}(\phi ^{\wedge})^{n}=\sum_{n=0}^{\infty} \frac{1}{(n+1)!}(\theta a^{\wedge})^{n}\\  & =I+\frac{1}{2!}\theta a^{\wedge}+\frac{1}{3!} \theta^{2}a^{\wedge}a^{\wedge}-\frac{1}{4!}\theta^{3}a^{\wedge}a^{\wedge}a^{\wedge}+\frac{1}{5!}\theta^{4}a^{\wedge}a^{\wedge}a^{\wedge}a^{\wedge}+\cdots\\  & =aa^{T}-a^{\wedge}a^{\wedge}+\frac{1}{2!}\theta a^{\wedge}+\frac{1}{3!}\theta^{2}a^{\wedge}a^{\wedge}-\frac{1}{4!}\theta^{3}a^{\wedge}-\frac{1}{5!}\theta^{4}a^{\wedge}a^{\wedge}+\cdots \\  & = \frac{1}{\theta}\left \{ aa^{T}\theta-a^{\wedge}a^{\wedge}\theta+a^{\wedge}-a^{\wedge}+\frac{1}{2!}\theta^{2}a^{\wedge}+\frac{1}{3!}\theta^{3}a^{\wedge}a^{\wedge}-\frac{1}{4!}\theta^{4}a^{\wedge}-\frac{1}{5!}\theta^{5}a^{\wedge}a^{\wedge}+\cdots \right \} \\  & = \frac{1}{\theta} \left \{ aa^{T}\theta - a^{\wedge}a^{\wedge}(\theta-\frac{1}{3!}\theta^{3}+\frac{1}{5!}\theta^{5}+\cdots)-a^{\wedge}(1-\frac{1}{2!}\theta^{2}+\frac{1}{4!}\theta^{4}+\cdots)+a^{\wedge}  \right\}\\  & = \frac{1}{\theta} \left \{ aa^{T}\theta - a^{\wedge}a^{\wedge}\sin \theta -a^{\wedge}\cos \theta +a^{\wedge}     \right \}\\  & = \frac{1}{\theta} \left \{ aa^{T}\theta-\sin \theta (aa^{T}-I)+a^{\wedge}(1-\cos \theta)   \right  \} \\  & = \frac{1}{\theta} \left \{  \sin \theta I +(\theta-\sin \theta)aa^{T}+(1-\cos \theta)a^{\wedge}   \right \} \\  & = \frac{\sin \theta}{\theta}I + (1-\frac{\sin \theta}{\theta})aa^{T}+\frac{1-\cos \theta}{\theta}a^{\wedge}\end{align*}</script><p>至于为什么雅可比矩阵是这种形式，即$J=\sum_{n=0}^{\infty} \frac{1}{(n+1)!}(\phi ^{\wedge})^{n}$是从何而来的，这个在<code>state estimation for robotics</code>一书的223-226有讲到，但是写的很抽象，下面给出证明：</p><script type="math/tex; mode=display">\begin{align*}& \xi ^{\wedge} = \begin{bmatrix}\phi ^{\wedge} & \rho \\{\bf 0}^{T} & 0\end{bmatrix}\\& \xi ^{\wedge} \xi^{\wedge}= \begin{bmatrix}\phi ^{\wedge} & \rho \\{\bf 0}^{T} & 0\end{bmatrix}\begin{bmatrix}\phi ^{\wedge} & \rho \\{\bf 0}^{T} & 0\end{bmatrix}=\phi ^{\wedge} \xi^{\wedge}\\& \xi^{\wedge}\xi^{\wedge}\xi^{\wedge}=(\phi ^{\wedge})^{2}\xi^{\wedge}\end{align*}</script><script type="math/tex; mode=display">\begin{align*}\exp(\xi ^{\wedge}) & = \sum_{n=0}^{\infty}\frac{1}{n!}(\xi ^{\wedge})^{n}\\                    & = I+\xi^{\wedge}+\frac{1}{2!}\xi^{\wedge}\xi^{\wedge}+\frac{1}{3!}\xi^{\wedge}\xi^{\wedge}\xi^{\wedge}+\cdots \\                    & =I+\xi^{\wedge}+\frac{1}{2!}\phi^{\wedge}\xi^{\wedge}+\frac{1}{3!}(\phi^{\wedge})^{2}\xi^{\wedge}+\cdots \\                    & = I+\sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n}\xi^{\wedge}\\                    & =                     \begin{bmatrix}                    I+\sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n+1} & \sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n}\rho \\                    {\bf 0}^{T} & 1                    \end{bmatrix}                    \\                    & =                     \begin{bmatrix}                    \sum_{n=0}^{\infty}\frac{1}{n!}(\phi^{\wedge})^{n} & \sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n}\rho \\                    {\bf 0}^{T} & 1                    \end{bmatrix}\end{align*}</script><h3 id="伴随"><a href="#伴随" class="headerlink" title="伴随"></a>伴随</h3><p>先证明$\forall a \in \mathbb R^{3}, Ra^{\wedge}R^{T}=(Ra)^{\wedge}$,高翔提供的网址的证明不严谨，即</p><script type="math/tex; mode=display">(Ra)^{\wedge}v=(Ra)\times v =(Ra) \times (RR^{-1}v)=R(a \times R^{-1}v)=Ra^{\wedge}R^{-1}v</script><script type="math/tex; mode=display">AB=BC,不能推导出A \neq B \quad\text{A,B,C是矩阵}</script><p>下面通过旋转矩阵正交的性质来证明该式成立。</p><script type="math/tex; mode=display">\begin{align*}& 设旋转矩阵R=\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\quad \text{这里的${\bf r}_{1},{\bf r}_{2},{\bf r}_{3}$既可以看作是$1 \times 3$的单位向量，也可以看作是矩阵，因此下面不再进行区分}\\& 对于Rp^{\wedge}R^{T} = (Rp)^{\wedge} \iff p^{\wedge}=R^{T}(Rp)^{\wedge}R，因此转为证明后式\\\end{align*}</script><p> 在证明之前，有两个事项需要注意：</p><p>1.${\bf r}_{i}^{T} {\bf r}_{j}={\bf r}_{i} \cdot {\bf r}_{j}$  前面可以看作矩阵，后面看作向量点乘，${\bf r}$是列向量</p><p>2.因为$R$是旋转矩阵，因此行向量和列向量都是单位向量且两两正交。为了后面叉乘运算的一致性，需要将${\bf r}_{1},{\bf r}_{2},{\bf r}_{3}$分别看成三维正交坐标系的$x,y,z$轴，即：</p><script type="math/tex; mode=display">\begin{cases}{\bf r}_{1} \times {\bf r}_{2} = {\bf r}_{1}^{\wedge}{\bf r}_{2}={\bf r}_{3}\\{\bf r}_{2} \times {\bf r}_{3} = {\bf r}_{2}^{\wedge}{\bf r}_{3}={\bf r}_{1}\\{\bf r}_{3} \times {\bf r}_{1} = {\bf r}_{3}^{\wedge}{\bf r}_{1}={\bf r}_{2}\end{cases}</script><script type="math/tex; mode=display">\begin{align*}p^{\wedge} & = R^{T}(Rp)^{\wedge}R \\& = \begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}(\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\begin{bmatrix}p_{1}\\p_{2}\\p_{3}\end{bmatrix})^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\\& =\begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}({\bf r}_{1}p_{1}+{\bf r}_{2}p_{2}+{\bf r}_{3}p_{3})^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\\& = \begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}({\bf r}_{1}^{\wedge}p_{1}+{\bf r}_{2}^{\wedge}p_{2}+{\bf r}_{3}^{\wedge}p_{3})\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\\& = p_{1}\begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}{\bf r}_{1}^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}+p_{2}\begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}{\bf r}_{2}^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}+p_{3}\begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}{\bf r}_{3}^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\\& = p_{1}\begin{bmatrix}{\bf r}_{1}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{1} & {\bf r}_{1}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{2} & {\bf r}_{1}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{3}\\{\bf r}_{2}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{1} & {\bf r}_{2}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{2} & {\bf r}_{2}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{3}\\{\bf r}_{3}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{1} & {\bf r}_{3}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{2} & {\bf r}_{3}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{3}\\\end{bmatrix}+p_{2}\begin{bmatrix}{\bf r}_{1}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{1} & {\bf r}_{1}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{2} & {\bf r}_{1}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{3}\\{\bf r}_{2}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{1} & {\bf r}_{2}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{2} & {\bf r}_{2}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{3}\\{\bf r}_{3}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{1} & {\bf r}_{3}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{2} & {\bf r}_{3}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{3}\\\end{bmatrix}+p_{3}\begin{bmatrix}{\bf r}_{1}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{1} & {\bf r}_{1}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{2} & {\bf r}_{1}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{3}\\{\bf r}_{2}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{1} & {\bf r}_{2}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{2} & {\bf r}_{2}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{3}\\{\bf r}_{3}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{1} & {\bf r}_{3}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{2} & {\bf r}_{3}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{3}\\\end{bmatrix}\\& = p_{1}\begin{bmatrix}0 & 0 & 0\\0 & 0 & -1\\0 & 1 & 0\end{bmatrix}+p_{2}\begin{bmatrix}0 & 0 & 1\\0 & 0 & 0\\-1 & 0 & 0\end{bmatrix}+p_{3}\begin{bmatrix}0 & -1 & 0\\1 & 0 & 0\\0 & 0 & 0\end{bmatrix}\\& =\begin{bmatrix}0 & -p_{3} & p_{2}\\p_{3} & 0 & -p_{1}\\-p_{2} & p_{1} & 0\end{bmatrix}\end{align*}</script><p>对于$SO(3)$上的伴随的证明只需进行泰勒展开即可，与上面的证明相同。</p><script type="math/tex; mode=display">\begin{align*}& \exp ((Ad(R)P)^{\wedge}) = \exp ((Rp)^{\wedge})=\exp (Rp^{\wedge}R^{T})=\exp (R\theta a^{\wedge}R^{T})=\sum _{n=0}^{\infty}\frac{1}{n!}(R\theta a^{\wedge}R^{T})^{n} \\& 令p=\theta a,a是单位向量，\theta为模长\\& Ra^{\wedge}R^{T} Ra^{\wedge}R^{T} =Ra^{\wedge}a^{\wedge}R^{T}=R(aa^{T}-I)R^{T}=Raa^{T}R^{T}-I\\& Ra^{\wedge}R^{T} Ra^{\wedge}R^{T} Ra^{\wedge}R^{T} =Ra^{\wedge}a^{\wedge}a^{\wedge}R^{T}=-Ra^{\wedge}R^{T}\end{align*}</script><p>泰勒展开后的式子利用正余弦函数表示，结果为：</p><script type="math/tex; mode=display">\begin{align*}\exp(Rp^{\wedge}R^{T}) & = \sum _{n=0}^{\infty}\frac{1}{n!}(\theta R a^{\wedge}R^{T})^{n}\\                       & = (1-\cos \theta)Raa^{T}R^{T}+\cos \theta I+\sin \theta Ra^{\wedge}R^{T}\\                       & =R((1-\cos \theta)aa^{T}+\cos \theta I+\sin \theta a^{\wedge})R^{T}\\                       & =R \exp(p^{\wedge})R^{T}\end{align*}</script><p>对于$SE(3)$的伴随$Ad(T)$有：</p><script type="math/tex; mode=display">\begin{align*}& T\exp (\xi ^{\wedge})T^{-1} = \exp((Ad(T)\xi )^{\wedge})\\& Ad(T)=\begin{bmatrix}R & t^{\wedge}R\\{\bf 0} & R\end{bmatrix}\end{align*}</script><p>$SO(3)$和$SE(3)$的伴随将在后面的位姿图优化中用到。</p><h3 id="轨迹的描绘"><a href="#轨迹的描绘" class="headerlink" title="轨迹的描绘"></a>轨迹的描绘</h3><p>1.$T_{WC}$是相机坐标系到世界坐标系的变换矩阵，其平移部分就是相机的移动距离，我们在解算位姿的时候是计算两帧之间的位姿，因此平移部分连起来就是相机的轨迹，即机器人的轨迹。</p><p>实际上，$T_{WC}$和$T_{CW}$之间只差了一个逆而已，都可以用来表示相机的位姿，但是实践当中使用$T_{CW}$更为常见，不过$T_{WC}$更为直观，因为$p_{W} =T_{WC}p_{C}=Rp_{C}+t_{WC}$对于相机原点来说，$p_{W}$就是在其对应于世界坐标系的点，而且正是$T_{WC}$的平移部分，那么连起来就可以看到相机的平移轨迹。</p><p>2.仿照ORB-SLAM2里的<code>CMakeLists.txt</code>写的CMakeLists.txt</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line">project(trajectorydrawing)</span><br><span class="line"></span><br><span class="line">set(CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br><span class="line"></span><br><span class="line"># check C++11 or C++0x support</span><br><span class="line">include(CheckCCompilerFlag)</span><br><span class="line">include(CheckCXXCompilerFlag)</span><br><span class="line">CHECK_CXX_COMPILER_FLAG(&quot;-std&#x3D;c++11&quot; COMPILER_SUPPORTS_CXX11)</span><br><span class="line">CHECK_CXX_COMPILER_FLAG(&quot;-std&#x3D;c++0x&quot; COMPILER_SUPPORTS_CXX0X)</span><br><span class="line">if(COMPILER_SUPPORTS_CXX11)</span><br><span class="line">   set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std&#x3D;c++11&quot;)</span><br><span class="line">   add_definitions(-DCOMPILEDWITHC11)</span><br><span class="line">   message(STATUS &quot;Using flag -std&#x3D;c++11.&quot;)</span><br><span class="line">elseif(COMPILER_SUPPORTS_CXX0X)</span><br><span class="line">   set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std&#x3D;c++0x&quot;)</span><br><span class="line">   add_definitions(-DCOMPILEDWITHC0X)</span><br><span class="line">   message(STATUS &quot;Using flag -std&#x3D;c++0x.&quot;)</span><br><span class="line">else()</span><br><span class="line">   message(FATAL_ERROR &quot;The compiler $&#123;CMAKE_CXX_COMPILER&#125; has no C++11 support. Please use a different C++ compiler.&quot;)</span><br><span class="line">endif()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">find_package(Eigen3 REQUIRED)</span><br><span class="line">find_package(Pangolin REQUIRED)</span><br><span class="line">find_package(Sophus REQUIRED)</span><br><span class="line"></span><br><span class="line">include_directories(</span><br><span class="line">    $&#123;EIGEN3_SOURCE_DIR&#125;</span><br><span class="line">    $&#123;Pangolin_INCLUDE_DIR&#125;</span><br><span class="line">    $&#123;Sophus_INCLUDE_DIR&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">add_executable(trajectorydrawing draw_trajectory.cpp)</span><br><span class="line">target_link_libraries(trajectorydrawing $&#123;EIGEN3_LIBRARIES&#125; $&#123;Pangolin_LIBRARIES&#125; $&#123;Sophus_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure><p>读取数据的代码如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 第一种方法，用fstream的getline分行读取</span><br><span class="line"></span><br><span class="line">    ifstream fin(trajectory_file); &#x2F;&#x2F;从文件中读取数据</span><br><span class="line">    if(!fin.is_open())&#123;</span><br><span class="line">        cout&lt;&lt;&quot;No &quot;&lt;&lt;trajectory_file&lt;&lt;endl;</span><br><span class="line">        return 0;</span><br><span class="line">    &#125;</span><br><span class="line">    double t,tx,ty,tz,qx,qy,qz,qw;</span><br><span class="line">    string line;</span><br><span class="line">    while(getline(fin,line))</span><br><span class="line">    &#123;</span><br><span class="line">       istringstream record(line); &#x2F;&#x2F;从string读取数据</span><br><span class="line">       record&gt;&gt;t&gt;&gt;tx&gt;&gt;ty&gt;&gt;tz&gt;&gt;qx&gt;&gt;qy&gt;&gt;qz&gt;&gt;qw;</span><br><span class="line">       Eigen::Vector3d p(tx,ty,tz);</span><br><span class="line">       Eigen::Quaterniond q &#x3D; Eigen::Quaterniond(qw,qx,qy,qz).normalized();</span><br><span class="line">       Sophus::SE3 SE3_qp(q,p);</span><br><span class="line">       poses.push_back(SE3_qp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;第二种方法</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; ifstream in(trajectory_file);&#x2F;&#x2F;创建输入流</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; if(!in)&#123;</span><br><span class="line">    &#x2F;&#x2F;     cout&lt;&lt;&quot;open posefile failture!!!&quot;&lt;&lt;endl;</span><br><span class="line">    &#x2F;&#x2F;     return 0;</span><br><span class="line">    &#x2F;&#x2F; &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; for(int i&#x3D;0; i&lt;620; i++)&#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;     double data[8]&#x3D;&#123;0&#125;;</span><br><span class="line">    &#x2F;&#x2F;     for(auto&amp; d:data) in&gt;&gt;d;&#x2F;&#x2F;按行依次去除数组中的值</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;     Eigen::Quaterniond q(data[7], data[8], data[5], data[6]);</span><br><span class="line">    &#x2F;&#x2F;     Eigen::Vector3d t(data[1], data[2], data[3]);</span><br><span class="line">    &#x2F;&#x2F;     Sophus::SE3 SE3(q,t);</span><br><span class="line">    &#x2F;&#x2F;     poses.push_back(SE3);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; &#125;</span><br><span class="line">    &#x2F;&#x2F; end your code here</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>生成的轨迹图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/visualSLAMbyGaoxiang-3/trajectory.png" alt="trajectory"></p><h3 id="轨迹的误差"><a href="#轨迹的误差" class="headerlink" title="轨迹的误差"></a>轨迹的误差</h3><p>CMakeLists.txt文件内容如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line">project(trajectory_error)</span><br><span class="line"></span><br><span class="line">set(CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br><span class="line"></span><br><span class="line"># check C++11 or C++0x support</span><br><span class="line">include(CheckCCompilerFlag)</span><br><span class="line">include(CheckCXXCompilerFlag)</span><br><span class="line">CHECK_CXX_COMPILER_FLAG(&quot;-std&#x3D;c++11&quot; COMPILER_SUPPORTS_CXX11)</span><br><span class="line">CHECK_CXX_COMPILER_FLAG(&quot;-std&#x3D;c++0x&quot; COMPILER_SUPPORTS_CXX0X)</span><br><span class="line">if(COMPILER_SUPPORTS_CXX11)</span><br><span class="line">   set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std&#x3D;c++11&quot;)</span><br><span class="line">   add_definitions(-DCOMPILEDWITHC11)</span><br><span class="line">   message(STATUS &quot;Using flag -std&#x3D;c++11.&quot;)</span><br><span class="line">elseif(COMPILER_SUPPORTS_CXX0X)</span><br><span class="line">   set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std&#x3D;c++0x&quot;)</span><br><span class="line">   add_definitions(-DCOMPILEDWITHC0X)</span><br><span class="line">   message(STATUS &quot;Using flag -std&#x3D;c++0x.&quot;)</span><br><span class="line">else()</span><br><span class="line">   message(FATAL_ERROR &quot;The compiler $&#123;CMAKE_CXX_COMPILER&#125; has no C++11 support. Please use a different C++ compiler.&quot;)</span><br><span class="line">endif()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">find_package(Eigen3 REQUIRED)</span><br><span class="line">find_package(Pangolin REQUIRED)</span><br><span class="line">find_package(Sophus REQUIRED)</span><br><span class="line"></span><br><span class="line">include_directories(</span><br><span class="line">    $&#123;EIGEN3_SOURCE_DIR&#125;</span><br><span class="line">    $&#123;Pangolin_INCLUDE_DIR&#125;</span><br><span class="line">    $&#123;Sophus_INCLUDE_DIR&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">add_executable(trajectory_error trajectory_error.cpp)</span><br><span class="line">target_link_libraries(trajectory_error $&#123;EIGEN3_LIBRARIES&#125; $&#123;Pangolin_LIBRARIES&#125; $&#123;Sophus_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure><p>trajectory_error.cpp文件内容如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;sophus&#x2F;se3.h&gt;</span><br><span class="line">#include &lt;string&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;fstream&gt;</span><br><span class="line">#include &lt;cmath&gt;</span><br><span class="line">#include &lt;pangolin&#x2F;pangolin.h&gt;</span><br><span class="line">#include &lt;Eigen&#x2F;Core&gt;</span><br><span class="line">#include &lt;Eigen&#x2F;Geometry&gt;</span><br><span class="line"> </span><br><span class="line">using namespace std;</span><br><span class="line">using namespace Eigen;</span><br><span class="line"> </span><br><span class="line">void ReadData(string FileName ,vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; &amp;poses);</span><br><span class="line">double ErrorTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g,</span><br><span class="line">        vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e);</span><br><span class="line">void DrawTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g,</span><br><span class="line">        vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e);</span><br><span class="line"> </span><br><span class="line">int main(int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">    string GroundFile &#x3D; &quot;.&#x2F;groundtruth.txt&quot;;</span><br><span class="line">    string ErrorFile &#x3D; &quot;.&#x2F;estimated.txt&quot;;</span><br><span class="line">    double trajectory_error_RMSE &#x3D; 0;</span><br><span class="line">    vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g;</span><br><span class="line">    vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e;</span><br><span class="line"> </span><br><span class="line">    ReadData(GroundFile,poses_g);</span><br><span class="line">    ReadData(ErrorFile,poses_e);</span><br><span class="line">    trajectory_error_RMSE &#x3D; ErrorTrajectory(poses_g, poses_e);</span><br><span class="line">    cout&lt;&lt;&quot;trajectory_error_RMSE &#x3D; &quot;&lt;&lt; trajectory_error_RMSE&lt;&lt;endl;</span><br><span class="line">    DrawTrajectory(poses_g,poses_e);</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">&#x2F;***************************读取文件的数据，并存储到vector类型的pose中**************************************&#x2F;</span><br><span class="line">void ReadData(string FileName ,vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; &amp;poses)</span><br><span class="line">&#123;</span><br><span class="line">    ifstream fin(FileName);  &#x2F;&#x2F;从文件中读取数据</span><br><span class="line">   &#x2F;&#x2F;这句话一定要加上，保证能够正确读取文件。如果没有正确读取，结果显示-nan</span><br><span class="line">    if(!fin.is_open())&#123;</span><br><span class="line">        cout&lt;&lt;&quot;No &quot;&lt;&lt;FileName&lt;&lt;endl;</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line">    double t,tx,ty,tz,qx,qy,qz,qw;</span><br><span class="line">    string line;</span><br><span class="line">    while(getline(fin,line)) </span><br><span class="line">    &#123;</span><br><span class="line">        istringstream record(line);    &#x2F;&#x2F;从string读取数据</span><br><span class="line">        record &gt;&gt; t &gt;&gt; tx &gt;&gt; ty &gt;&gt; tz &gt;&gt; qx &gt;&gt; qy &gt;&gt; qz &gt;&gt; qw;</span><br><span class="line">        Eigen::Vector3d p(tx, ty, tz);</span><br><span class="line">        Eigen::Quaterniond q &#x3D; Eigen::Quaterniond(qw, qx, qy, qz).normalized();  &#x2F;&#x2F;四元数的顺序要注意</span><br><span class="line">        Sophus::SE3 SE3_qp(q, p);</span><br><span class="line">        poses.push_back(SE3_qp);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">&#x2F;*******************************计算轨迹误差*********************************************&#x2F;</span><br><span class="line">double ErrorTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g,</span><br><span class="line">        vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e )</span><br><span class="line">&#123;</span><br><span class="line">    double RMSE &#x3D; 0;</span><br><span class="line">    Matrix&lt;double ,6,1&gt; se3;</span><br><span class="line">    vector&lt;double&gt; error;</span><br><span class="line">    for(int i&#x3D;0;i&lt;poses_g.size();i++)&#123;</span><br><span class="line">        se3&#x3D;(poses_g[i].inverse()*poses_e[i]).log();  &#x2F;&#x2F;这里的se3为向量形式，求log之后是向量形式</span><br><span class="line">        &#x2F;&#x2F;cout&lt;&lt;se3.transpose()&lt;&lt;endl;</span><br><span class="line">        error.push_back( se3.squaredNorm() );  &#x2F;&#x2F;二范数</span><br><span class="line">       &#x2F;&#x2F; cout&lt;&lt;error[i]&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    for(int i&#x3D;0; i&lt;poses_g.size();i++)&#123;</span><br><span class="line">        RMSE +&#x3D; error[i];</span><br><span class="line">    &#125;</span><br><span class="line">    RMSE &#x2F;&#x3D; double(error.size());</span><br><span class="line">    RMSE &#x3D; sqrt(RMSE);</span><br><span class="line">    return RMSE;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;*****************************绘制轨迹*******************************************&#x2F;</span><br><span class="line">void DrawTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g,</span><br><span class="line">        vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e) &#123;</span><br><span class="line">    if (poses_g.empty() || poses_e.empty()) &#123;</span><br><span class="line">        cerr &lt;&lt; &quot;Trajectory is empty!&quot; &lt;&lt; endl;</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    &#x2F;&#x2F; create pangolin window and plot the trajectory</span><br><span class="line">    pangolin::CreateWindowAndBind(&quot;Trajectory Viewer&quot;, 1024, 768);  &#x2F;&#x2F;创建一个窗口</span><br><span class="line">    glEnable(GL_DEPTH_TEST);   &#x2F;&#x2F;启动深度测试</span><br><span class="line">    glEnable(GL_BLEND);       &#x2F;&#x2F;启动混合</span><br><span class="line">    glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);&#x2F;&#x2F;混合函数glBlendFunc( GLenum sfactor , GLenum dfactor );sfactor 源混合因子dfactor 目标混合因子</span><br><span class="line"> </span><br><span class="line">    pangolin::OpenGlRenderState s_cam(</span><br><span class="line">            pangolin::ProjectionMatrix(1024, 768, 500, 500, 512, 389, 0.1, 1000),</span><br><span class="line">            pangolin::ModelViewLookAt(0, -0.1, -1.8, 0, 0, 0, 0.0, -1.0, 0.0) &#x2F;&#x2F;对应的是gluLookAt,摄像机位置,参考点位置,up vector(上向量)</span><br><span class="line">    );</span><br><span class="line"> </span><br><span class="line">    pangolin::View &amp;d_cam &#x3D; pangolin::CreateDisplay()</span><br><span class="line">            .SetBounds(0.0, 1.0, pangolin::Attach::Pix(175), 1.0, -1024.0f &#x2F; 768.0f)</span><br><span class="line">            .SetHandler(new pangolin::Handler3D(s_cam));</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    while (pangolin::ShouldQuit() &#x3D;&#x3D; false) &#123;</span><br><span class="line">        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</span><br><span class="line"> </span><br><span class="line">        d_cam.Activate(s_cam);</span><br><span class="line">        glClearColor(1.0f, 1.0f, 1.0f, 1.0f);</span><br><span class="line"> </span><br><span class="line">        glLineWidth(2);</span><br><span class="line">        for (size_t i &#x3D; 0; i &lt; poses_g.size() - 1; i++) &#123;</span><br><span class="line">            glColor3f(1 - (float) i &#x2F; poses_g.size(), 0.0f, (float) i &#x2F; poses_g.size());</span><br><span class="line">            glBegin(GL_LINES);</span><br><span class="line">            auto p1 &#x3D; poses_g[i], p2 &#x3D; poses_g[i + 1];</span><br><span class="line">            glVertex3d(p1.translation()[0], p1.translation()[1], p1.translation()[2]);</span><br><span class="line">            glVertex3d(p2.translation()[0], p2.translation()[1], p2.translation()[2]);</span><br><span class="line">            glEnd();</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        for (size_t j &#x3D; 0; j &lt; poses_e.size() - 1; j++) &#123;</span><br><span class="line">            &#x2F;&#x2F;glColor3f(1 - (float) j &#x2F; poses_e.size(), 0.0f, (float) j &#x2F; poses_e.size());</span><br><span class="line">            glColor3f(1.0f, 1.0f, 0.f);&#x2F;&#x2F;为了区分第二条轨迹，用不同的颜色代替,黄色</span><br><span class="line">            glBegin(GL_LINES);</span><br><span class="line">            auto p1 &#x3D; poses_e[j], p2 &#x3D; poses_e[j + 1];</span><br><span class="line">            glVertex3d(p1.translation()[0], p1.translation()[1], p1.translation()[2]);</span><br><span class="line">            glVertex3d(p2.translation()[0], p2.translation()[1], p2.translation()[2]);</span><br><span class="line">            glEnd();</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        pangolin::FinishFrame();</span><br><span class="line">        usleep(5000);   &#x2F;&#x2F; sleep 5 ms</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/visualSLAMbyGaoxiang-3/trajectory_error.png" alt="error"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本次课程主要研究李群和李代数(Lie Group, Lie Algebra)，主要的目的是为了能够相机得旋转和平移进行微调。因为相机的运动估计可能不准确，而无法对旋转矩阵加上微小量之后依然是旋转矩阵（旋转矩阵无法定义加法，如果用四元数，必须是单位四元数，那么也无法定义加法）。李群李代数与后面的优化，流形都会有很大的联系。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在视觉SLAM中，相机的位姿是未知的，而我们需要解决什么样的相机位姿最符合当前观测数据这样的问题。一种典型的方式是把其构建成一个优化问题，求解最优的 $R$ 和 $t$，使得误差最小化。&lt;/p&gt;
&lt;p&gt;由于旋转矩阵自身带有约束，即必须正交且行列式为1，因此作为优化变量会引入额外的约束，使得优化变得困难。而通过李群李代数的转换关系，可以顺利求导，把位姿估计变成无约束的优化问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;群&quot;&gt;&lt;a href=&quot;#群&quot; class=&quot;headerlink&quot; title=&quot;群&quot;&gt;&lt;/a&gt;群&lt;/h2&gt;&lt;p&gt;群(Group)是一种集合加上一种运算的代数结构，满足封闭性，结合律，&lt;strong&gt;幺元&lt;/strong&gt;，逆。其中幺元可以认为是单位元，就是与其他元素作用不改变这个元素，逆是元素和和它的逆进行运算后得到了幺元。&lt;/p&gt;
&lt;p&gt;三维旋转矩阵构成了三维正交群(special orthogonal group)&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
SO(3) = \left \{ R \in {\mathbb R}^{3 \times 3} | RR^{T} =I, \det (R)=1 \right \}&lt;/script&gt;&lt;p&gt;三维变换矩阵构成了特殊欧式群(special euclidean group)&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
SE(3) = 
\left \{
T= 
\begin{bmatrix}
R &amp; t\\
O^{T} &amp; 1
\end{bmatrix}
\in {\mathbb R}^{4 \times 4} |
R \in SO(3), t \in {\mathbb R}^{3} 
\right \}&lt;/script&gt;&lt;p&gt;旋转矩阵集合与矩阵乘法构成群，变换矩阵集合与矩阵乘法也构成了群，因此称它们为旋转矩阵群和变换矩阵群。&lt;/p&gt;
&lt;p&gt;群结构保证了在群上的运算具有良好的性质。&lt;/p&gt;</summary>
    
    
    
    <category term="科研记录" scheme="http://densecollections.top/categories/%E7%A7%91%E7%A0%94%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="visual SLAM" scheme="http://densecollections.top/tags/visual-SLAM/"/>
    
    <category term="Linux" scheme="http://densecollections.top/tags/Linux/"/>
    
    <category term="C++11" scheme="http://densecollections.top/tags/C-11/"/>
    
    <category term="Computer vision" scheme="http://densecollections.top/tags/Computer-vision/"/>
    
    <category term="Sophus" scheme="http://densecollections.top/tags/Sophus/"/>
    
  </entry>
  
  <entry>
    <title>visual SLAM by Gaoxiang(2)</title>
    <link href="http://densecollections.top/posts/visualSLAMbyGaoxiang-2/"/>
    <id>http://densecollections.top/posts/visualSLAMbyGaoxiang-2/</id>
    <published>2019-03-12T13:04:11.000Z</published>
    <updated>2021-01-02T12:18:34.707Z</updated>
    
    <content type="html"><![CDATA[<p>本次课程主要研究<strong>三维空间刚体运动</strong>，即visual slam的运动方程中的$x_{k}$如何表达。</p><h2 id="点与坐标系"><a href="#点与坐标系" class="headerlink" title="点与坐标系"></a>点与坐标系</h2><p>在2D情况下，物体可以通过两个坐标和一个旋转角进行表达，即$(x,y,\theta)$。</p><p> 在3D情况下，物体是6自由度的，包括平移和旋转，每个都得用三个变量表达，可以认为3D情况是包含着3个2D的情况，旋转轴不同。                </p><a id="more"></a><p>理清坐标系（参考系），点，向量，向量的坐标，运动变换之间的关系。相机会有相机坐标系，机器人会有机体坐标系（通常是运动的），空间会有世界坐标系（通常是固定的），这都是为了研究问题的方便，通过变换进行运动的表述。</p><p>熟悉向量的相关运算规则，比如，向量的加减法，向量的内积和外积。以及向量和矩阵的关系。</p><script type="math/tex; mode=display">\begin{align*}& 内积： \bf{a} \cdot \bf{b} =\bf{a^{T}} \bf{b}=\sum_{i=1}^{n}=|\bf{a}||\bf{b}| \cos \left< \bf{a}, \bf{b}\right> \\&外积： \bf{a} \times \bf{b}=\begin{bmatrix}\bf{i} & \bf{j} & \bf{k} \\a_{1} & a_{2} & a_{3} \\b_{1} & b_{2} & b_{3}\end{bmatrix}=\begin{bmatrix}a_{2}b_{3}-a_{3}b_{2}\\a_{3}b_{1}-a_{1}b_{3}\\a_{1}b_{2}-a_{2}b_{1}\end{bmatrix}=\begin{bmatrix}0 & -a_{3} & a_{2}\\a_{3} & 0 & -a_{1} \\-a_{2} & a_{1} & 0\end{bmatrix}\bf{b} \triangleq \bf{a} ^{\wedge} \bf{b}\end{align*}</script><p>建立了坐标系之后，如何表示同一个向量在不同坐标系之间的坐标，坐标系之间的变换关系又该如何描述。坐标系之间的变换可以分解成坐标系原点之间之间的平移和坐标轴之间的旋转，那么可以用一个平移向量和旋转矩阵来描述这样的变换。我们知道矩阵可以表述坐标系之间的变换，对于三维空间而言，可以用一个$4 \times 4$的矩阵来描述三维坐标系之间的变换，这也就是平移向量和旋转矩阵的合成矩阵，具体后面会细说。</p><h2 id="旋转矩阵"><a href="#旋转矩阵" class="headerlink" title="旋转矩阵"></a>旋转矩阵</h2><p>一个向量在坐标系进行旋转后不变，因此可以通过此推导出旋转矩阵$R$的表达式（即用变换前后的坐标系的基向量进行表述）。</p><script type="math/tex; mode=display">\begin{bmatrix}a_{1}\\a_{2}\\a_{3}\end{bmatrix}=\begin{bmatrix}e_{1}^{T}e_{1}^{'} & e_{1}^{T}e_{2}^{'} & e_{1}^{T}e_{3}^{'}\\e_{2}^{T}e_{1}^{'} & e_{2}^{T}e_{2}^{'} & e_{2}^{T}e_{3}^{'}\\e_{3}^{T}e_{1}^{'} & e_{3}^{T}e_{2}^{'} & e_{3}^{T}e_{3}^{'}\end{bmatrix}\begin{bmatrix}a_{1}^{'}\\a_{2}^{'}\\a_{3}^{'}\end{bmatrix}\triangleq R {\bf a}^{'}</script><p>旋转矩阵是正交矩阵（$RR^{T}=I$），且行列式为1（$\det (R)=1$）。旋转矩阵属于特殊正交群special orthogonal group，即：</p><script type="math/tex; mode=display">SO(n) = \lbrace R \in \mathbb{R}^{n \times n} | RR^{T}=I, \det(R)=1 \rbrace</script><p>对一个旋转矩阵进行转置就描述了一个相反方向的旋转。</p><p>欧拉旋转定理（Euler’s rotation theorem）：刚体在三维空间里的一般运动，可分解为刚体上方某一点的平移，以及绕经过此点的旋转轴的转动。</p><script type="math/tex; mode=display">{\bf a^{'}}= R \bf{a} + \bf{t}</script><p> 齐次形式（homogeneous）来更方便地表达变换，因为加上平移不满足线性性。定义变换矩阵$T$</p><script type="math/tex; mode=display">\begin{bmatrix}a^{'}\\1\end{bmatrix}=\begin{bmatrix}R & \bf{t}\\\bf{ 0^{T} } & 1\end{bmatrix}\begin{bmatrix}a \\1\end{bmatrix}\triangleq T\begin{bmatrix}a\\1\end{bmatrix}</script><p>齐次坐标认为其乘以任意非零常数时仍表达同一个坐标（归一化）。 变换矩阵的集合称为特殊欧式群special Euclidean Group:</p><script type="math/tex; mode=display">SE(3)=\lbrace T=\begin{bmatrix}R & \bf{t}\\\bf{ 0^{T}} & 1\end{bmatrix}\in \mathbb{R}^{4 \times 4} |R \in SO(3), \bf{t} \in \mathbb{R}^{3} \rbrace</script><script type="math/tex; mode=display">T^{-1}=\begin{bmatrix}R^{T} & -R^{T}{\bf t}\\{\bf 0^{T}} & 1\end{bmatrix}</script><p>定义了变化矩阵后，多次变换可以直接对变换矩阵直接相乘即可。</p><h2 id="旋转向量与欧拉角"><a href="#旋转向量与欧拉角" class="headerlink" title="旋转向量与欧拉角"></a>旋转向量与欧拉角</h2><p>一般来说视觉SLAM有旋转矩阵和平移向量就够了，一般也是用这样的方式表达。为了进行拓展，比如在航迹推算和组合导航中，通常用四元数来表述物体姿态。</p><p>旋转矩阵有9个元素但仅表示三个自由度，比较冗余，因此引入旋转向量(rotation vector)，方向为旋转轴，长度为转过的角度，又称为角轴或轴角（angle axis）。</p><p>旋转向量只有三个量，无约束，更加直观，但是旋转轴一般不容易得知，其中旋转向量和旋转矩阵的关系可以通过罗德里格斯公式得出（Rodrigues’ s Formula）：（假设旋转轴为$\bf{n}$，旋转角为$\theta$）</p><script type="math/tex; mode=display">\begin{align*}& R = \cos \theta {\bf I}+(1-\cos \theta){\bf n} {\bf n^{T}}+\sin \theta {\bf n^{\wedge}}\\& \theta = \arccos(\frac{tr(R)-1}{2}) \\& R{\bf n}=\bf{n} \quad特征值为1的特征向量\end{align*}</script><p>欧拉角（Euler Angles）将旋转分解成三个方向上的转动，最常见的是Z-Y-X，即yaw-pitch-roll（偏航-俯仰-横滚），不同领域习惯不同。但是欧拉角会有万向锁问题（<a href="https://krasjet.github.io/quaternion/bonus_gimbal_lock.pdf">gimbal lock</a>)，会在特定值丢失一个自由度，存在奇异性问题，因此欧拉角不适合插值和迭代。实际上，仅用三个实数表达旋转时，会不可避免地存在奇异性问题。视觉SLAM中一般不用欧拉角表达姿态，主要在人机交互中用。</p><h2 id="四元数（Quaternion）"><a href="#四元数（Quaternion）" class="headerlink" title="四元数（Quaternion）"></a>四元数（Quaternion）</h2><p>2D情况下，可以用单位复数表达旋转，$z=x+iy=\rho e^{i \theta}$，乘$i$代表转转90度。</p><p>3D情况下，类似地，四元数可作为复数地扩充。</p><p>四元数有三个虚部和一个实部，${\bf q}=q_{0}+q_{1}i+q_{2}j+q_{3}k$，虚部之间满足关系（自己和自己运算像复数，自己和别人运算像叉乘）:</p><script type="math/tex; mode=display">\begin{cases}i^{2}=j^{2}=k^{2}=-1 \\ij=k,jk=-k \\jk=i.kj=-i \\ki=j,ik=-j\end{cases}</script><p>单位四元数可以表达旋转，${\bf q}=q_{0}+q_{1}i+q_{2}j+q_{3}k=[s, {\bf v}], s=q_{0}\in \mathbb{R}, {\bf v}=[q_{1}, q_{2}, q_{3}]^{T}\in \mathbb{R}^{3}$，四元数有以下地运算规则：</p><script type="math/tex; mode=display">{\bf q}_{a}+{\bf q}_{b}=[s_{a}\pm s_{b}, {\bf v}_{a}\pm {\bf v}_{b}]\\\begin{align*}{\bf q}_{a}{\bf q}_{b}= &s_{a}s_{b}-x_{a}x_{b}-y_{a}y_{b}-z_{a}z_{b}\\& +(s_{a}x_{b}+x_{a}s_{b}+y_{a}z_{b}-z_{a}y_{b})i\\& +(s_{a}y_{b}-x_{a}z_{b}+y_{a}s_{b}+z_{a}x_{b})j\\& +(s_{a}z_{b}+x_{a}y_{b}-y_{b}x_{a}+z_{a}s_{b})k \end{align*}\\{\bf q}_{a}{\bf q}_{b}= [s_{a}s_{b}-{\bf v}_{a}^{T}{\bf v}_{b}, s_{a}{\bf v}_{b}+s_{b}{\bf v}_{a}+{\bf v}_{a} \times {\bf v}_{b}] \\{\bf q}^{*}=s_{a}-x_{a}i-y_{a}j-z_{a}k=[s_{a}, -{\bf v}_{a}]\\\left\| {\bf q}_{a} \right\|= \sqrt{s_{a}^{2}+x_{a}^{2}+y_{a}^{2}+z_{a}^{2}} \\{\bf q}^{-1}={\bf q}^{*}/ \left\| {\bf q} \right\| ^{2}\\k{\bf q}=[ks, k{\bf v}]\\{\bf q}_{a} \cdot {\bf q}_{b}=s_{a}s_{b}+x_{a}x_{b}i+y_{a}y_{b}j+z_{a}z_{b}k</script><p>四元数到角轴：${\bf q}=[\cos \frac{\theta}{2},n_{x}\sin \frac{\theta}{2}, n_{y}\sin \frac{\theta}{2}, n_{z}\sin \frac{\theta}{2}]^{T}$。</p><p>角轴到四元数：$\theta =2 \arccos q_{0}, [n_{x}, n_{y}, n_{z}]^{T} = [q_{1}, q_{2}, q_{3}]^{T}/  \sin \frac{\theta}{2}$</p><p>三维点$p(x,y,z)$经过一次以${\bf q}$表示的旋转后，得到了$p^{‘}$，${\bf p}=[0,x,y,z]=[0, {\bf v}]$，旋转之后的关系为${\bf p}^{‘}={\bf q} {\bf p} {\bf q}^{-1}$。四元数相比与角轴和欧拉角，形式上更加紧凑，也无奇异性。</p><p><strong>值得注意的是，${\bf q}$和$-{\bf q}$表示同一个旋转。</strong></p><hr><p>学习四元数的一个最直观的问题就是为什么三个变量来描述三维旋转，诸如欧拉角会出现奇异性的情况，而用四个变量的四元数就不会？也就说用高维的东西描述低维的东西更加有效。</p><p>知乎上有相关的<a href="https://www.zhihu.com/question/20962240/answer/33438846">回答</a>，写得还算不错，比较直观。不过依旧没有解决为什么三个变量描述三维旋转会出现奇异性的现象。</p><blockquote><p>利用四元数来对三维点的旋转进行操作，是通过纯四元数来进行的，即变换后的点可以表示为${\bf qwq^{-1} }$，这里的问题是为什么这种形式。（注意，这里的四元数是单位四元数）</p><p>汉密尔顿定义的性质：</p><p>1.运算产生的结果也要是三维向量<br>2.存在一个元运算，任何三维向量进行元运算的结果就是其本身<br>3.对于任何一个运算，都存在一个逆运算，这两个运算的积是元运算<br>4.运算满足结合律</p><p>其实，四元数有四个变量，完全可以被看作一个四维向量。单位四元数（norm=1）则存在于四维空间的一个球面上。${\bf q}_{a} {\bf q}_{b}$，四元数${\bf q}_{a}$乘以四元数${\bf q}_{b}$其实看作（1）对${\bf q}_{a}$进行${\bf q}_{b}$左旋转，或者（2）对${\bf q}_{b}$进行${\bf q}_{a}$右旋转。所以从始至终，四元数定义的都是四维旋转，而不是三维旋转！任意的四维旋转都可以唯一的拆分为一个左旋转和一个右旋转，表达出来就是${\bf q}_{L}{\bf p}{\bf q}_{R}$。这里，我们对四元数（四维向量）${\bf p}$进行了一个${\bf q}_{L}$左旋转和一个${\bf q}_{R}$右旋转。结果当然是一个四元数，符合性质1。这个运算也同时符合性质2，3，4。</p><p>为了进行三维旋转运算，汉密尔顿首先在四维空间里划出了一块三维空间。汉密尔顿定义了一种纯四元数（pure quaternion），其表达式为${\bf q}_{w}=(0,w_{x},w_{y},w_{z})$。纯四元数第一项为零，它存在于四维空间的三维超平面上，与三维空间中的三维向量一一对应。然后，就有了我们常见的${\bf q}  {\bf q}_{w} {\bf q}^{*}$这种左乘单位四元数，右乘其共轭的表达式。这个运算形式是为了限制其运算结果所在的空间。简单的说，当对一个三维向量进行三维旋转后，我们希望得到的是一个三维向量，而不是四维向量。</p><p>这也就解释了为什么四元数对应于角轴的关系式中是$\frac{\theta}{2}$，而不是$\theta$，这是因为${\bf q}$做的就是一个$\frac{\theta}{2}$的旋转，而${\bf q}^{-1}$也做了一个$\frac{\theta }{2}$的旋转。我们进行了两次旋转，而不是一次，这两次旋转的结果是一个旋转角为$\theta$的旋转。</p></blockquote><p>此外，还有一点是，四元数可以用2x2复数矩阵（特殊酉群）和4x4矩阵来描述，但是四元数之间不满足乘法交换律，即${\bf q}_{1} {\bf q}_{2} \ne {\bf q}_{2} {\bf q}_{1}$，但是二维平面里的复数相乘满足乘法交换律，这里我粗略地理解是，二维旋转只有角度，没有轴的概念，只有按照什么顺序旋转多少角度，因此先转$\theta$，再转$\alpha$，与先转$\alpha$，再转$\theta$的结果是一样的，但是四元数相乘不是，每次转都是有个旋转轴的，因此不可交换。</p><p>有关四元数，旋转，群的理解和证明，可以参考这篇<a href="https://krasjet.github.io/quaternion/quaternion.pdf">文章</a></p><script type="math/tex; mode=display">\begin {align*}{\bf v}^{'} & = {\bf v}_{||}^{'} + {\bf v}_{\bot}^{'}={\bf v}_{||} + {\bf q} {\bf v}_{\bot}\\             & = {\bf p} {\bf p}^{-1} {\bf v}_{\bot} + {\bf p} {\bf p} {\bf v}_{\bot}\\            & = {\bf p} {\bf p}^{*} {\bf v}_{\bot} + {\bf p} {\bf p} {\bf v}_{\bot}\\            & = {\bf p} {\bf v}_{||} {\bf p}^{*} + {\bf p} {\bf v}_{\bot} {\bf p}^{*}\\            & = {\bf p} ({\bf v}_{||}+{\bf v}_{\bot}) {\bf p}^{*}\\            & = {\bf p} {\bf v} {\bf p}^{*}\\            & = {\bf v}_{||} + {\bf p}^{2} {\bf v}_{\bot}\end{align*}\\{\bf q}=[\cos \theta, \sin \theta ({\bf u})], {\bf p}=[\cos \frac{\theta}{2}, \sin \frac{\theta}{2} ({\bf u})],{\bf q}={\bf p}^{2}</script><p><strong>对于平行的分量，变换完全抵消，对于垂直的分量，施加两次变换，这也就是为什么是</strong>$\frac {\theta}{2}$。</p><p><strong>四元数与群：</strong></p><p>单位四元数与 3D 旋转有一个<strong>2对1满射同态关系</strong>，或者说单位四元数<strong>双倍覆盖了3D旋转</strong>。因为这个映射是满射，我们可以说所有的单位四元数都对应着一个 3D 旋转。或者说，一个四维单位超球面（也叫做$\mathbb S^{3}$）上任意一点所对应的四元数（$∥q∥ = 1$）都对应着一个 3D 旋转。</p><p>四元数，旋转矩阵，群，geometric algebra都有着紧密的联系，因此可以先做好相关的功课，这里列出几篇阅读材料：</p><p><a href="https://alistairsavage.ca/mat4144/notes/MAT4144-5158-LieGroups.pdf">Introduction to Lie Groups</a></p><p><a href="http://www.jaapsuter.com/geometric-algebra.pdf">Geometric Algebra Primer</a></p><p><a href="https://www.youtube.com/watch?reload=9&amp;v=PNlgMPzj-7Q&amp;list=PLpzmRsG7u_gqaTo_vEseQ7U8KFvtiJY4K">youtube geometric algebra</a></p><blockquote><p><strong>A Quaternion is a scalar plus a bivector.</strong></p><p>Apart from the fact that quaternions have four components, there is nothing four-dimensional or imaginary about a quaternion. The first component is a scalar, and the other three components form the bivector-plane relative to which the rotation is performed.</p></blockquote><p>这似乎解释了之前我的疑问，为什么要用四个变量来描述三维，就是用得用高维的解释低维的，这里指出了四元数拥有四个变量，但是不是指四维或者虚数，而是通过标量和bivector的组合来描述旋转，几何代数的这种形式统一了不同维度的旋转，因此比较优雅。不过更深层次的东西和理解涉及到比较多的数学知识和空间理解，我暂时还达不到。。。。</p><hr><hr><h2 id="作业与实践"><a href="#作业与实践" class="headerlink" title="作业与实践"></a>作业与实践</h2><h3 id="熟悉Eigen运算"><a href="#熟悉Eigen运算" class="headerlink" title="熟悉Eigen运算"></a>熟悉Eigen运算</h3><p>对于线性方程组$Ax=b$，其中$A$是方阵。</p><p>1).什么条件下，$x$有解且唯一？</p><p>当矩阵$A$可逆的时候，也就是此时矩阵满秩。</p><p>2).高斯消元法原理</p><p>高斯消元法其实就是对系数矩阵作初等行变换，这不改变方程的解，将系数矩阵化成上三角矩阵的形式，然后从下往上依次解出方程组的每个分量解（回代）。如果是针对列主元的高斯消元法，需要加入方程组的右侧值，与系数矩阵组成增广矩阵，进而求解。</p><p>3). QR分解的原理</p><p>任意的$A \in {\bf C}^{n \times n}$都可以进行$QR$分解，即$A=QR$，$Q$为n阶酉矩阵($QQ^<br>{H}=I$)，$R$为n阶上三角矩阵。</p><p>$QR$分解与Gram-Schmidt正交化有关，即将$A$进行Gram-Schmidt正交化，化为$Q$，然后求出R。Q的列向量是A的列空间的标准正交基，R是一个非奇异可逆的上三角矩阵，即将矩阵每个列作为一个基本单元，将其化为正交的基向量与在这个基向量上的投影长度的积。</p><p>4).Cholesky分解的原理</p><p>Cholesky分解其实是矩阵Doolittle分解(三角分解的特例)的特例。三角分解是是将方针分解成同阶的下三角阵和上三角阵，其中上三角阵的主对角线元素全为1则为Doolittle分解。如果矩阵$A$既是方阵又是Hermite正定阵时($A=A^{H}$,且特征值全为正数)，则存在唯一分解$A=LL^{H}$，其中$L$是具有主对角元素为正数的下三角矩阵。</p><p>5).利用$QR$和Cholesky分解法分解随机矩阵$A \in {\bf C}^{100 \times 100}$求解$x$。</p><p>主要思路就是先用定义动态大小的矩阵，之后进行调用相关的函数处理。其中需要注意的是，Cholesky分解需要矩阵为正定阵，因此在矩阵定义上需要进行一些处理。</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">#include &lt;ctime&gt;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Eigen部分</span><br><span class="line">#include &lt;Eigen&#x2F;Core&gt;</span><br><span class="line">&#x2F;&#x2F;Eigen稠密矩阵的代数运算（逆和特征值等）</span><br><span class="line">#include &lt;Eigen&#x2F;Dense&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main (int argc, char** argv)</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double,Eigen::Dynamic, Eigen::Dynamic&gt; matrix_dynamic; &#x2F;&#x2F;Eigen固定大小矩阵最大支持到50</span><br><span class="line">Eigen::Matrix&lt;double,Eigen::Dynamic, Eigen::Dynamic&gt; matrix_A;</span><br><span class="line">Eigen::Matrix&lt;double,Eigen::Dynamic, 1&gt; x;</span><br><span class="line">Eigen::Matrix&lt;double,Eigen::Dynamic,1&gt; v_right;</span><br><span class="line"></span><br><span class="line">matrix_dynamic &#x3D; Eigen::MatrixXd::Random(100,100); &#x2F;&#x2F;随机化取值</span><br><span class="line"></span><br><span class="line">matrix_A &#x3D; matrix_dynamic.transpose()*matrix_dynamic; &#x2F;&#x2F;cholesky分解需要A为正定矩阵</span><br><span class="line"></span><br><span class="line">v_right &#x3D; Eigen::MatrixXd::Random(100, 1); &#x2F;&#x2F;方程右边的值随机取值</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;QR Decomposition</span><br><span class="line">clock_t time_stt &#x3D; clock();</span><br><span class="line"></span><br><span class="line">x &#x3D; matrix_A.colPivHouseholderQr().solve(v_right);</span><br><span class="line">cout&lt;&lt;&quot;the time used in QR decomposition is &quot;&lt;&lt; 1000* (clock() - time_stt)&#x2F;(double) CLOCKS_PER_SEC&lt;&lt;&quot;ms&quot;&lt;&lt; endl;</span><br><span class="line">cout&lt;&lt;x&lt;&lt;endl;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;Cholesky Decomposition</span><br><span class="line">time_stt &#x3D; clock();</span><br><span class="line"></span><br><span class="line">x &#x3D; matrix_A.llt().solve(v_right);</span><br><span class="line">cout&lt;&lt;&quot;the time used in Cholesky decomposition is &quot;&lt;&lt; 1000* (clock() - time_stt)&#x2F;(double) CLOCKS_PER_SEC&lt;&lt;&quot;ms&quot;&lt;&lt; endl;</span><br><span class="line">cout&lt;&lt;x&lt;&lt;endl;</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>CMakeLists.txt文件内容如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line"></span><br><span class="line">project(QR_cholesky)</span><br><span class="line"></span><br><span class="line">set(CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br><span class="line">set(CMAKE_CXX_FLAGS &quot;-O3&quot;) #Debug版会使用参数-g；Release版使用-O3 –DNDEBUG</span><br><span class="line"></span><br><span class="line">include_directories(&quot;&#x2F;usr&#x2F;include&#x2F;eigen3&quot;)</span><br><span class="line"></span><br><span class="line">add_executable(QR_cholesky QR_cholesky.cpp)</span><br><span class="line">#eigen3都是头文件，不需要target_link_libraries</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="几何运算练习"><a href="#几何运算练习" class="headerlink" title="几何运算练习"></a>几何运算练习</h3><p>基本思想就是$T_{cw} p=p^{‘}$，或者$R p + t=p^{‘}$，注意这里是世界坐标系的点变换到相机坐标系。</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;cmath&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;Eigen几何模块</span><br><span class="line">#include &lt;Eigen&#x2F;Core&gt;</span><br><span class="line">#include&lt;Eigen&#x2F;Geometry&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main(int argc, char** argv)</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Quaterniond q1(0.55,0.3,0.2,0.2);</span><br><span class="line">q1 &#x3D; q1.normalized();</span><br><span class="line"></span><br><span class="line">Eigen::Quaterniond q2(-0.1,0.3,-0.7,0.2);</span><br><span class="line">q2 &#x3D; q2.normalized();</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; t1;</span><br><span class="line">t1 &lt;&lt; 0.7,1.1,0.2;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; t2;</span><br><span class="line">t2 &lt;&lt; -0.1,0.4,0.8;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; p1;</span><br><span class="line">p1 &lt;&lt; 0.5,-0.1,0.2;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; p2;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; p; &#x2F;&#x2F;世界坐标系的点</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;利用变换矩阵的方法</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Eigen::Isometry3d Tcw1 &#x3D; Eigen::Isometry3d::Identity();&#x2F;&#x2F;变换矩阵1</span><br><span class="line">&#x2F;&#x2F; Tcw1.rotate(q1);</span><br><span class="line">&#x2F;&#x2F; Tcw1.pretranslate(t1);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Eigen::Isometry3d Tcw2 &#x3D; Eigen::Isometry3d::Identity();&#x2F;&#x2F;变换矩阵2</span><br><span class="line">&#x2F;&#x2F; Tcw2.rotate(q2);</span><br><span class="line">&#x2F;&#x2F; Tcw2.pretranslate(t2);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; p &#x3D; Tcw1.inverse()*p1;</span><br><span class="line">&#x2F;&#x2F; p2 &#x3D; Tcw2*p;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;直接利用旋转矩阵和平移向量进行组合运算</span><br><span class="line">Eigen::Matrix&lt;double,3,3&gt; R1_inverse;</span><br><span class="line">R1_inverse &#x3D; q1.matrix().inverse();</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;double,3,3&gt; R2;</span><br><span class="line">R2 &#x3D; q2.matrix();</span><br><span class="line"></span><br><span class="line">p2 &#x3D; R2 * R1_inverse *(p1 - t1) + t2;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cout &lt;&lt; &quot;p2 &#x3D; &quot; &lt;&lt; p2.transpose() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CMakeLists.txt文件内容如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line">set (CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br><span class="line">set (CMAKE_CXX_FLAGS &quot;-O3&quot;)</span><br><span class="line"></span><br><span class="line">project(Geometryusing)</span><br><span class="line">include_directories(&quot;&#x2F;usr&#x2F;include&#x2F;eigen3&quot;)</span><br><span class="line"></span><br><span class="line">add_executable(Geometryusing Geometryusing.cpp)</span><br><span class="line">#eigen3都是头文件，不需要target_link_libraries</span><br></pre></td></tr></table></figure><h3 id="旋转的表达"><a href="#旋转的表达" class="headerlink" title="旋转的表达"></a>旋转的表达</h3><p>1).旋转矩阵的正交性</p><p>坐标系中的某个单位正交基$(e_{1},e_{2},e_{3})$经过旋转变换后变为$(e^{‘}_{1},e^{‘}_{2},e^{‘}_{3})$，对于同一个向量在两个坐标系下的坐标分别为$a(a_{1},a_{2},a_{3})$和$(a^{‘}_{1},a^{‘}_{2},a^{‘}_{3})$，由于向量不会随着坐标系的旋转而发生变化，则有：</p><script type="math/tex; mode=display">\begin{bmatrix}e_{1} & e_{2} & e_{3}\end {bmatrix}\begin{bmatrix}a_{1} \\a_{2} \\a_{3}\end {bmatrix}=\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end {bmatrix}\begin{bmatrix}a^{'}_{1} \\a^{'}_{2} \\a^{'}_{3}\end {bmatrix}</script><p>等式两边同时左乘$[e^{T}_{1} \quad e^{T}_{2} \quad e^{T}_{3}]$，则得出旋转矩阵$R$为</p><script type="math/tex; mode=display">R=\begin{bmatrix}e^{T}_{1}\\e^{T}_{2} \\e^{T}_{3}\end{bmatrix}\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end{bmatrix}</script><p>则有：</p><script type="math/tex; mode=display">R^{T}R=\begin{bmatrix}e^{'}_{1}\\e^{'}_{2}\\e^{'}_{3}\end{bmatrix}\begin{bmatrix}e^{T}_{1} & e^{T}_{2} & e^{T}_{2}\end{bmatrix}\begin{bmatrix}e^{T}_{1}\\e^{T}_{2} \\e^{T}_{3}\end{bmatrix}\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end{bmatrix}=I</script><script type="math/tex; mode=display">\det R=\det(\begin{bmatrix}e^{T}_{1}\\e^{T}_{2} \\e^{T}_{3}\end{bmatrix}\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end{bmatrix})=det(\begin{bmatrix}e^{T}_{1}\\e^{T}_{2} \\e^{T}_{3}\end{bmatrix})det(\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end{bmatrix})=1</script><p>2).四元数的维度</p><p>易知$\varepsilon$是三维，$\eta$是一维的。</p><p>3).四元数相关证明(题目应该有错）</p><p>符号$x$和$\wedge$代表着反对称矩阵，代表着向量到反对称矩阵的变换，这是从叉乘引申而来的。即</p><script type="math/tex; mode=display">a^{\times}=(\begin{bmatrix}a_{1}\\a_{2}\\a_{3}\end{bmatrix})^{\times}=\begin{bmatrix}0 & -a_{3} & a_{2}\\a_{3} & 0 &-a_{1}\\-a_{2} & a_{1} & 0\end{bmatrix}</script><p>设$q_{1}=[x_{1},y_{1},z_{1},w_{1}]^{T}$,$q_{2}=[x_{2},y_{2},z_{2},w_{2}]^{T}$,</p><script type="math/tex; mode=display">\begin{align*}q_{1}q_{2} & = w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\\     & + (w_{1}x_{2}+x_{1}w_{2}+y_{1}z_{2}-z_{1}y_{2})i\\     & + (w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}+z_{1}x_{2})j\\     & + (w_{1}z_{2}+x_{1}y_{2}-y_{1}x_{2}+z_{1}w_{2})k\end{align*}</script><script type="math/tex; mode=display">q_{1}^{\bigoplus}q_{2}=\begin{bmatrix} w_{1} & -z_{1} & y_{1} & x_{1}\\ z_{1} & w_{1} & -x_{1} & y_{1}\\ -y_{1} & x_{1} & w_{1} & z_{1}\\ -x_{1} & -y_{1} & -z_{1} & w_{1}\end{bmatrix}\begin{bmatrix}x_{2}\\y_{2}\\z_{2}\\w_{2}\end{bmatrix}=\begin{bmatrix}w_{1}x_{2}-z_{1}y_{2}+y_{1}z_{2}+x_{1}w_{2}\\z_{1}x_{2}+w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}\\-x_{2}y_{1}+x_{1}y_{2}+w_{1}z_{2}+z_{1}w_{2}\\w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\end{bmatrix}\begin{align*} & = w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\\     & + (w_{1}x_{2}+x_{1}w_{2}+y_{1}z_{2}-z_{1}y_{2})i\\     & + (w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}+z_{1}x_{2})j\\     & + (w_{1}z_{2}+x_{1}y_{2}-y_{1}x_{2}+z_{1}w_{2})k\end{align*}=q_{1}q_{2}</script><script type="math/tex; mode=display">q_{2}^{+}q_{1}=\begin{bmatrix} w_{2} & z_{2} & -y_{2} & x_{2}\\ -z_{2} & w_{2} & x_{2} & y_{2}\\ y_{2} & -x_{2} & w_{2} & z_{2}\\ -x_{2} & -y_{2} & -z_{2} & w_{2}\end{bmatrix}\begin{bmatrix}x_{1}\\y_{1}\\z_{1}\\w_{1}\end{bmatrix}=\begin{bmatrix}w_{2}x_{1}+y_{1}z_{2}-y_{2}z_{1}+w_{1}x_{2}\\-x_{1}z_{2}+w_{2}y_{1}+x_{2}z_{1}+y_{2}w_{1}\\x_{1}y_{2}-x_{2}y_{1}+w_{2}z_{1}+w_{1}z_{2}\\w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\end{bmatrix}\begin{align*} & = w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\\     & + (w_{1}x_{2}+x_{1}w_{2}+y_{1}z_{2}-z_{1}y_{2})i\\     & + (w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}+z_{1}x_{2})j\\     & + (w_{1}z_{2}+x_{1}y_{2}-y_{1}x_{2}+z_{1}w_{2})k\end{align*}=q_{1}q_{2}</script><h3 id="罗德里格斯公式证明"><a href="#罗德里格斯公式证明" class="headerlink" title="罗德里格斯公式证明"></a>罗德里格斯公式证明</h3><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/visualSLAMbyGaoxiang-2/Rodrigues1.png" alt="旋转分解"></p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/visualSLAMbyGaoxiang-2/Rodrigues2.png" alt="分解的表达"></p><p>如上图所示，向量${\bf v}$绕单位旋转轴${\bf k}$旋转$\theta$角后，变换成了${\bf v}_{rot}$，分别将向量${\bf v}$和${\bf v}_{rot}$沿${\bf k}$平行和垂直的方向分解。</p><script type="math/tex; mode=display">\begin{align*}&{\bf v}={\bf v}_{||}+{\bf v}_{\bot}\\&{\bf v}_{||}=({\bf v} \cdot {\bf k}){\bf k}\\& {\bf v}_{\bot}={\bf v}-{\bf v}_{||}=-{\bf k}\times({\bf k} \times {\bf v})\end{align*}</script><p>根据图像，我们知道，平行于旋转轴的分量没有变化，只有垂直于旋转轴的分量进行了旋转，而且它们的模长是一样的，这就说明${\bf v}_{\bot}$和${\bf v}_{rot \bot}$是在一个圆上，长度相等，而且，${\bf v}_{\bot}$和${\bf k} \times {\bf v}_{\bot}$构成了该圆的两个正交坐标轴，因此${\bf v}_{rot \bot}$的坐标可以用这个坐标轴表示，或者说该向量可以被这两个正交坐标基线性表示，即：</p><script type="math/tex; mode=display">\begin{align*}{\bf v}_{rot \bot}&=\cos \theta{\bf v}_{\bot}+\sin \theta({\bf k}\times {\bf v}_{\bot})\\                  &=\cos \theta{\bf v}_{\bot}+\sin \theta({\bf k} \times {\bf v}-{\bf k} \times {\bf v}_{||})\\                  &=\cos \theta{\bf v}_{\bot}+\sin \theta({\bf k}\times {\bf v})\end{align*}</script><script type="math/tex; mode=display">\begin{align*}{\bf v}_{rot} & = {\bf v}_{rot ||}+{\bf v}_{rot \bot}\\              & = {\bf v}_{||}+\cos \theta {\bf v}_{\bot}+\sin \theta ({\bf k}\times{\bf v})\\              & = {\bf v}_{||}+\cos \theta({\bf v}-{\bf v}_{||}) +\sin \theta ({\bf k}\times{\bf v})\\              & = \cos \theta {\bf v}+(1-\cos \theta){\bf v}_{||}+\sin \theta ({\bf k}\times{\bf v})\\              & = \cos \theta {\bf v}+(1-\cos \theta)({\bf k}\cdot {\bf v}){\bf k}+\sin \theta ({\bf k}\times{\bf v})\end{align*}</script><p>下面将这些向量都看成矩阵的形式，将其运算化为矩阵的运算形式，假设${\bf k}$和${\bf v}$都是列向量的形式，则${\bf k}({\bf k}\cdot {\bf v})=kk^{T}v$，${\bf k}\times {\bf v}=k^{\wedge}v$，因此，得到了旋转矩阵的形式为：</p><script type="math/tex; mode=display">R=\cos \theta I+(1-\cos \theta)kk^{T}+\sin \theta k^{\wedge}</script><p>将$k$换成$n$即得罗德里格斯公式。</p><h3 id="四元数运算性质的验证"><a href="#四元数运算性质的验证" class="headerlink" title="四元数运算性质的验证"></a>四元数运算性质的验证</h3><p>设单位四元数$q=[\varepsilon \quad \eta]$(虚部 实部），点$p=[\zeta \quad 0]$，$q=x_{1}i+y_{1}j+z_{1}k+w_{1}, w_{1}^{2}+x_{1}^{2}+y_{1}^{2}+z_{1}^{2}=1$则：</p><script type="math/tex; mode=display">\begin{align*}p^{'}=qpq^{-1} & = q^{\bigoplus}pq^{-1}\\               & = q^{\bigoplus}q^{-1+}p\\               & = \begin{bmatrix}               \eta I+\varepsilon ^{\times} & \varepsilon \\               -\varepsilon^{T} & \eta               \end{bmatrix}               \begin{bmatrix}               \eta I-(-\varepsilon )^{\times} & -\varepsilon \\               -(-\varepsilon)^{T} & \eta               \end{bmatrix}               \begin{bmatrix}               \zeta\\               0               \end{bmatrix}               \\               & = \begin{bmatrix}               w_{1} & -z_{1} & y_{1} & x_{1} \\               z_{1} & w_{1} & -x_{1} & y_{1} \\               -y_{1} & x_{1} & w_{1} & z_{1}\\               -x_{1} & -y_{1} & -z_{1} & w_{1}               \end{bmatrix}               \begin{bmatrix}               w_{1} & -z_{1} & y_{1} & -x_{1} \\               z_{1} & w_{1} & -x_{1} & -y_{1} \\               -y_{1} & x_{1} & w_{1} & -z_{1}\\               x_{1} & y_{1} & z_{1} & w_{1}               \end{bmatrix}               \begin{bmatrix}               \zeta \\               0               \end{bmatrix}               \\               & = \begin{bmatrix}               R_{3 \times 3} & 0_{3 \times 1}\\               0_{1 \times 3} & 1               \end{bmatrix}               \begin{bmatrix}               \zeta \\               0               \end{bmatrix}\end{align*}</script><p>易知$p^{‘}$的实部为0，也就是上述形式限制了点的变换维度，虽然四元数是四个变量，但是不会把三维点变到四维去。</p><p>计算得出旋转矩阵$R$为:</p><script type="math/tex; mode=display">R=\begin{bmatrix}w_{1}^{2}+x_{1}^{2}-y_{1}^{2}-z_{1}^{2} & 2x_{1}y_{1}-2w_{1}z_{1} & 2x_{1}z_{1}+2y_{1}w_{1}\\2x_{1}y_{1}+2z_{1}w_{1} & w_{1}^{2}+y_{1}^{2}-x_{1}^{2}-z_{1}^{2} & 2y_{1}z_{1}-2x_{1}w_{1}\\2x_{1}z_{1}-2y_{1}w_{1} & 2y_{1}z_{1}+2x_{1}w_{1} & w_{1}^{2}+z_{1}^{2}-x_{1}^{2}y_{1}^{2}\end{bmatrix}</script><h3 id="C-11"><a href="#C-11" class="headerlink" title="C++11"></a><a href="https://blog.csdn.net/sinat_35297665/article/details/80101460">C++11</a></h3><ul><li>for(atuo&amp; a: avec)</li></ul><p><a href="https://blog.csdn.net/hailong0715/article/details/54172848/">范围for循环</a>，用a遍历avec中的每个量；基于范围的FOR循环的遍历是只读的遍历，除非将变量变量的类型声明为引用类型。</p><ul><li>for(atuo&amp; a: avec)</li></ul><p>自动类型推导，根据a获得的值，用auto自动推断出a的类型；</p><ul><li><a href="const A&amp;a1,const A&amp;a2"></a>{return a1.index&lt;a2.index;})</li></ul><p>运用了<a href="https://www.cnblogs.com/DswCnblog/p/5629165.html">lambda表达式</a>。</p><ul><li>begin()</li></ul><p><strong>begin</strong> 返回首元素的地址，<strong>end</strong> 返回尾元素的下一个地址。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本次课程主要研究&lt;strong&gt;三维空间刚体运动&lt;/strong&gt;，即visual slam的运动方程中的$x_{k}$如何表达。&lt;/p&gt;
&lt;h2 id=&quot;点与坐标系&quot;&gt;&lt;a href=&quot;#点与坐标系&quot; class=&quot;headerlink&quot; title=&quot;点与坐标系&quot;&gt;&lt;/a&gt;点与坐标系&lt;/h2&gt;&lt;p&gt;在2D情况下，物体可以通过两个坐标和一个旋转角进行表达，即$(x,y,\theta)$。&lt;/p&gt;
&lt;p&gt; 在3D情况下，物体是6自由度的，包括平移和旋转，每个都得用三个变量表达，可以认为3D情况是包含着3个2D的情况，旋转轴不同。                &lt;/p&gt;</summary>
    
    
    
    <category term="科研记录" scheme="http://densecollections.top/categories/%E7%A7%91%E7%A0%94%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="visual SLAM" scheme="http://densecollections.top/tags/visual-SLAM/"/>
    
    <category term="Linux" scheme="http://densecollections.top/tags/Linux/"/>
    
    <category term="C++11" scheme="http://densecollections.top/tags/C-11/"/>
    
    <category term="Computer vision" scheme="http://densecollections.top/tags/Computer-vision/"/>
    
    <category term="Eigen" scheme="http://densecollections.top/tags/Eigen/"/>
    
  </entry>
  
  <entry>
    <title>visual SLAM by Gaoxiang(1)</title>
    <link href="http://densecollections.top/posts/visualSLAMbyGaoxiang-1/"/>
    <id>http://densecollections.top/posts/visualSLAMbyGaoxiang-1/</id>
    <published>2019-03-12T01:10:21.000Z</published>
    <updated>2021-01-02T12:17:04.464Z</updated>
    
    <content type="html"><![CDATA[<h2 id="视觉SLAM概述"><a href="#视觉SLAM概述" class="headerlink" title="视觉SLAM概述"></a>视觉SLAM概述</h2><ul><li>simultaneous localization and mapping</li><li>仅使用相机进行室内/室外定位（有些情况下GPS会崩，IMU漂移随着时间误差增大）</li><li>机器人在未知环境进行导航—-建图(sparse/semi-dense/dense)</li><li>SLAM问题的本质是对运动主体自身和周围环境空间不确定性的估计（spatial uncertainty）</li><li>为了解决SLAM问题，我们需要状态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，估计状态的均值和不确定性（方差）</li></ul><hr><p>学术上研究视觉SLAM较多，尤其是monocular，但是应用上少了点，尤其是建图的作用目前很浅，而且很多人大部分现在在用深度学习搞3D重建。目前应用的地方有：</p><ul><li>手持设备定位</li><li>自动驾驶定位—比GPS的定位信息要丰富，甚至精度更好(可以达到厘米级)</li><li>AR 增强现实（定位，建图，深度学习结合）</li><li>清洁机器人</li></ul><hr><a id="more"></a><h2 id="学习研究步骤"><a href="#学习研究步骤" class="headerlink" title="学习研究步骤"></a>学习研究步骤</h2><p>第一部分是学习相关的数学知识，构建数学模型</p><ul><li>矩阵</li><li>概率论</li><li>李群李代数</li><li>微分几何</li><li>凸优化</li></ul><p>…</p><p>第二部分是计算机视觉的代码实践</p><ul><li>openCV</li><li>c++</li><li>python</li><li>Linux</li></ul><p>…</p><p>教材：</p><ul><li>Multiple view  Geometry in computer vision</li><li>State estimation for robotics</li><li>视觉SLAM十四讲-从理论到实践(<a href="https://github.com/AceCoooool/slambook">作业代码</a>)</li></ul><h2 id="本教程内容提纲"><a href="#本教程内容提纲" class="headerlink" title="本教程内容提纲"></a>本教程内容提纲</h2><ul><li>概述与预备知识</li><li>三维空间的刚体运动</li><li>李群李代数</li><li>相机模型与非线性优化</li><li>特征点法视觉里程计</li><li>直接法视觉里程计</li><li>后端优化</li><li>回环检测</li></ul><h2 id="视觉SLAM的基本框架和模型"><a href="#视觉SLAM的基本框架和模型" class="headerlink" title="视觉SLAM的基本框架和模型"></a>视觉SLAM的基本框架和模型</h2><p>定位与建图是相互关联的，准确的定位需要精确的地图，精确的地图也来自准确的定位。</p><p>为什么选择视觉传感器: 携带安装更加自由，成本更低，信息丰富，功能不单一，可开发性强，智能化程度高具有挑战性（单目monocular/双目stereo/深度相机RGB-D/鱼眼、全景、event-based相机等），而挑战是普通的相机会丢失世界的距离信息，因此需要从图像中进行恢复（相机运动，相机几何关系，物理测量等）。</p><p>视觉SLAM框架</p><ul><li><p>前端：visual odometry(估计邻帧相机相机运动/feature-based/direct-based)</p></li><li><p>后端：optimization(消除噪声，优化轨迹/最大后验概率估计/滤波器/图优化)</p></li><li><p>回环：loop closing(相机回到之前相同的位置，优化约束，消除累计误差/图像相似性/词袋模型)</p></li><li><p>建图：mapping(导航/路劲规划/人机交互/可视化/通讯/度量地图（稀疏地图/稠密地图）/拓扑地图)</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/visualSLAMbyGaoxiang-1/map%20categories.png" alt="map categories"></p><p> 视觉SLAM的数学模型：</p><script type="math/tex; mode=display">\begin{align*} & 运动方程 x_{k} = f(x_{k-1}, u_{k},w_{k}) \\ & 观测方程 z_{k,j} = h(y_{j}, x_{k}, v_{k,j}) \end{align*}</script><p>———-0——-1———2——-3——-4——-5——-6————-&gt; t</p><p>———$x_{0}$—-$x_{1}$——$x_{2}$——$x_{3}$——$x_{4}$———-              &gt;</p><p>​         $\searrow $       $\searrow $       $\searrow $       $\searrow $</p><p>​                        $\bullet y_{1}$             $\bullet y_{2}$</p><p>​                                                                      $\bullet y_{j}$</p><p>​                                                                                    $\cdots$</p><p>假设一个机器人在实际场景中运动，$x_{k}$是此时刻k的位置，$x_{k-1}$是上一个时刻k-1的，在这之间有个输入，使得机器人运动，我们设其为$u_{k}$，而同时运动也包含着噪声，机器人的运动也不完全受精确控制，假设噪声为$w_{k}$。 现在，机器人携带了相机，相机会拍到一系列现实场景的标志，我们称其为路标点（landmark），假设此场景各个路标点为$y_{1}, y_{2}, \cdots, y_{j}, \cdots$。容易知道，机器人在每个时刻所在的位置都会观测到这些路标点中的一部分或者全部，假设在$x_{0}$位置看到路标点$y_{1}$，我们记为$z_{0,1}$，看到了路标点$y_{2}$，记为$z_{0,2}$，…以此类推，这会反映在图像像素上，因此称为观测方程，同样观测也会有噪声$v_{k,j}$。</p><p>我们通过一系列运动方程和观测方程，利用已知的$u_{k}, z_{k,j}$来推断出$x_{k}, y_{j}$，也就是知道了机器人的位置，同时了解了环境的信息，因此也就是两大任务：定位和建图。</p><h2 id="本次作业与实践"><a href="#本次作业与实践" class="headerlink" title="本次作业与实践"></a>本次作业与实践</h2><p>首先大致了解下Linux下编写C++源码以及利用cmake编译可执行文件的步骤</p><ul><li>工具g++, cmake, VS code(optional IDE)</li><li>一个利用c++编写的简单工程包含着头文件文件夹incliude，源码文件夹src，cmake编译文件夹build，以及main.cpp和CMakeLists.txt等主要文件，其中CMakeLists.txt文件的语言格式要注意</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line">project(HelloSLAM)</span><br><span class="line"></span><br><span class="line">#指定程序编译的模式</span><br><span class="line">set(CMAKE_BUILD_TYPE Debug)</span><br><span class="line"></span><br><span class="line">#claim the include files&#39; directories</span><br><span class="line">include_directories(&quot;include&quot;)</span><br><span class="line"></span><br><span class="line"># only src files</span><br><span class="line">add_library(libHello src&#x2F;hello.cpp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">add_executable(sayHello main.cpp)</span><br><span class="line"></span><br><span class="line">target_link_libraries(sayHello libHello)</span><br></pre></td></tr></table></figure><p>利用vs code配合cmake进行<a href="https://zhuanlan.zhihu.com/p/52874931">配置</a>以进行代码的编写和调试，也可以选择KDEVELOP和其他的IDE。</p><h3 id="熟悉LIinux"><a href="#熟悉LIinux" class="headerlink" title="熟悉LIinux"></a>熟悉LIinux</h3><p>1.如何在 Ubuntu 中安装软件（命令⾏界⾯）？它们通常被安装在什么地⽅？</p><ul><li>apt-get 方式的安装；</li></ul><p>普通安装：<code>sudo apt-get install XXX</code></p><p>修复安装：<code>sudo apt-get -f install XXX</code></p><p>重新安装：<code>sudo apt-get -f reinstall XXX</code></p><ul><li>dpkg方式的安装</li></ul><p><code>sudo dpkg -i package_name.deb</code></p><ul><li>安装的地方</li></ul><p>通常被安装在<code>/usr/bin</code>，<code>/usr/local</code>这个目录下</p><p>系统安装软件一般在<code>/usr/share</code>，可执行的文件在<code>/usr/bin</code>，配置文件可能安装到了<code>/etc</code>下等。文档一般在 <code>/usr/share</code>，可执行文件 <code>/usr/bin</code>，配置文件 <code>/etc</code>，lib文件 <code>/usr/lib。</code></p><p>2.linux 的环境变量是什么？我如何定义新的环境变量？</p><ul><li><a href="http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap08.html">Environment variables</a> defined in this chapter affect the operation of multiple utilities, functions, and applications.</li></ul><p>linux是一个多用户的操作系统。每个用户登录系统后，都会有一个专用的运行环境。通常每个用户默认的环境都是相同的，这个默认环境实际上就是一组环境变量的定义。用户可以对自己的运行环境进行定制，其方法就是修改相应的系统环境变量。环境变量是一个具有特定名字的对象，它包含了一个或者多个应用程序所将使用到的信息</p><p>常见的环境变量：</p><p>PATH：决定了shell将到哪些目录中寻找命令或程序</p><p>HOME：当前用户主目录</p><p>MAIL：是指当前用户的邮件存放目录。</p><p>SHELL：是指当前用户用的是哪种Shell。</p><p>HISTSIZE：是指保存历史命令记录的条数</p><p>LOGNAME：是指当前用户的登录名。</p><p>HOSTNAME：是指主机的名称，许多应用程序如果要用到主机名的话，通常是从这个环境变量中来取得的。</p><p>LANG/LANGUGE：是和语言相关的环境变量，使用多种语言的用户可以修改此环境变量。</p><p> 使用修改.bashrc文件进行环境变量的编辑，只对当前用户有用。</p><p> 使用修改/etc/profile文件进行环境变量的编辑，是对所有用户有用。</p><p> 关于环境变量命令介绍：</p><p> echo显示某个环境变量值echo$PATH</p><p> <strong>export设置一个新的环境变量exportHELLO=”hello”(可以无引号)</strong></p><p> env显示所有环境变量</p><p> set显示本地定义的shell变量</p><p> unset清除环境变量unsetHELLO</p><p> readonly设置只读环境变量readonlyHELLO</p><ul><li>对所有用户生效的永久性变量（系统级）:</li></ul><p>这类变量对系统内的所有用户都生效，所有用户都可以使用这类变量。作用范围是整个系统。</p><p>设置方式： 用vim在/etc/profile文件中添加我们想要的环境变量,用export指令添加环境变量</p><p>当然，这个文件只有在root（超级用户）下才能修改。我们可以在etc目录下使用ls -l查看这个文件的用户及权限</p><p>【注意】：添加完成后新的环境变量不会立即生效，除非你调用source /etc/profile 该文件才会生效。否则只能在下次重进此用户时才能生效。</p><ul><li>对单一用户生效的永久性变量（用户级）:</li></ul><p>只针对当前用户，和上面的一样，只不过不需要在etc下面进行添加，直接在.bash_profile文件最下面用export添加就好了。</p><p>这里 .bashrc和.bash_profile原则上来说设置此类环境变量时在这两个文件任意一个里面添加都是可以的。</p><p>~/.bash_profile是交互式login方式进入bash shell运行。</p><p>~/ .bashrc是交互式non-login方式进入bash shell运行。</p><p>二者设置大致相同。</p><p>就是.bash_profile文件只会在用户登录的时候读取一次</p><p>而.bashrc在每次打开终端进行一次新的会话时都会读取。</p><ul><li>临时有效的环境变量（只对当前shell有效）:</li></ul><p>此类环境变量只对当前的shell有效。当我们退出登录或者关闭终端再重新打开时，这个环境变量就会消失。是临时的。 直接使用export指令添加。</p><p>3.linux 根⽬录下⾯的⽬录结构是什么样的？⾄少说出 3 个⽬录的⽤途。</p><ul><li>可通过终端查看/目录下的文件：</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;</span><br><span class="line">ls</span><br></pre></td></tr></table></figure><p><code>/bin</code>用户二进制文件<br>包含二进制可执行文件，系统所有用户可执行文件都在这个文件夹里，例如：ls，cp，ping等。</p><p><code>/sbin</code> 系统二进制文件<br>包含二进制可执行文件，但只能由系统管理员运行，对系统进行维护。</p><p><code>/etc</code>配置文件<br>包含所有程序配置文件，也包含了用于启动/停止单个程序的启动和关闭shell脚本。</p><p><code>/dev</code>设备文件<br>包含终端所有设备，USB或连接到系统的任何设备。例如：<code>/dev/tty1</code>、<code>dev/usbmon0</code></p><p><code>/proc</code>进程信息<br>包含系统进程的相关信息。<br>这是一个虚拟的文件系统，包含有关正在运行的进程的信息。例如：<code>/proc/&#123;pid&#125;</code>目录中包含的与特定pid相关的信息。<br>这是一个虚拟的文件系统，系统资源以文本信息形式存在。例如：<code>/proc/uptime</code></p><p><code>/var</code>变量文件<br>可以找到内容可能增长的文件。<br>这包括 - 系统日志文件<code>/var/log</code>;包和数据库文件<code>/var/lib</code>;电子邮件<code>/var/mail</code>;打印队列<code>/var/spool</code>;锁文件<code>/var/lock</code>;多次重新启动需要的临时文件<code>/var/tmp</code>;</p><p><code>/tmp</code>临时文件<br>包含系统和用户创建的临时文件。<br>当系统重新启动时，这个目录下的文件都将被删除。</p><p><code>/usr</code>用户程序<br>包含二进制文件、库文件、文档和二级程序的源代码。<br><code>/usr/bin</code>中包含用户程序的二进制文件。如果你在<code>/bin</code>中找不到用户二进制文件，到<code>/usr/bin</code>目录看看。例如：at、awk、cc、less、scp。<br><code>/usr/sbin</code>中包含系统管理员的二进制文件。如果你在<code>/sbin</code>中找不到系统二进制文件，到<code>/usr/sbin</code>目录看看。例如：atd、cron、sshd、useradd、userdel。<br><code>/usr/lib</code>中包含了<code>/usr/bin</code>和<code>/usr/sbin</code>用到的库。<br><code>/usr/local</code>中包含了从源安装的用户程序。例如，当你从源安装Apache，它会在<code>/usr/local/apache2</code>中。</p><p><code>/home</code> HOME目录<br>所有用户用来存档他们的个人档案。</p><p><code>/boot</code>引导加载程序文件<br>包含引导加载程序相关的文件。<br>内核的initrd、vmlinux、grub文件位于<code>/boot</code>下。</p><p><code>/lib</code>系统库<br>包含支持位于<code>/bin</code>和<code>/sbin</code>下的二进制文件的库文件.<br>库文件名为 ld或lib.so.*</p><p><code>/opt</code>可选的附加应用程序<br>opt代表opitional；<br>包含从个别厂商的附加应用程序。<br>附加应用程序应该安装在<code>/opt/</code>或者<code>/opt/</code>的子目录下。</p><p><code>/mnt</code>挂载目录<br>临时安装目录，系统管理员可以挂载文件系统。</p><p><code>/media</code> 可移动媒体设备<br>用于挂载可移动设备的临时目录。<br>举例来说，挂载CD-ROM的<code>/media/cdrom</code>，挂载软盘驱动器的<code>/media/floppy</code>;</p><p><code>/srv</code>服务数据<br>srv代表服务。<br>包含服务器特定服务相关的数据。<br>例如，<code>/srv/cvs</code>包含cvs相关的数据。</p><p>4.假设我要给 a.sh 加上可执⾏权限，该输⼊什么命令？</p><p><code>chmod 777 文件名</code> 将文件设置成对拥有者、组成员、其他人可读、可写、可执行。<br><code>chmod a+x 文件名</code>将文件在原来的配置上增加可执行权限。</p><p>5.假设我要将 a.sh ⽂件的所有者改成 xiang:xiang，该输⼊什么命令？</p><p><code>chown xiang:xiang 文件名</code>将文件的所有者改成xiang:xiang</p><h3 id="SLAM文献阅读"><a href="#SLAM文献阅读" class="headerlink" title="SLAM文献阅读"></a>SLAM文献阅读</h3><p>1.SLAM 会在哪些场合中⽤到？⾄少列举三个⽅向。</p><p>机器人定位导航、手持设备定位、增强现实、自动泊车、语义地图重建等</p><p>2.SLAM中定位与建图是什么关系？为什么在定位的同时需要建图？</p><p>定位需要精确的地图，详细的地图需要精确地定位，两者相辅相成，相互依存。重定位和局部建图是slam框架中很重要的两个线程。</p><p>定位：机器人必须<strong>知道自己在环境中位置</strong>；</p><p>建图：机器人必须<strong>记录环境中特征的位置</strong>（如果知道自己的位置）；</p><p>机器人在未知环境中从一个未知位置开始移动,在移动过程中根据位置估计和地图进行自身定位,同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。</p><p>3.SLAM发展历史如何？我们可以将它划分成哪⼏个阶段？</p><p>1985-1990：, Chatila 和Laumond (1985) and Smith et al. (1990)提出以建图和定位同时进行；</p><p><strong>单传感器为外部传感器：</strong></p><p>早期：</p><ul><li>声呐(Tardós et al. 2002; Ribas et al. 2008)；</li></ul><p>后期：</p><ul><li><p>激光雷达(Nüchter etal. 2007; Thrun et al. 2006)；</p></li><li><p>相机(Se et al. 2005; Lemaire et al. 2007; Davison 2003;Bogdan et al. 2009)；</p></li><li><p>GPS(Thrun et al. 2005a)；</p></li></ul><p><strong>多传感器融合</strong></p><p>三个发展阶段：</p><ul><li><p>初始阶段：二十世纪八十年代</p></li><li><p>发展阶段：二十世纪九十年代</p></li><li><p>快速发展阶段</p></li></ul><p>4.列举三篇在SLAM领域的经典⽂献。</p><ul><li><p>Smith, R.C. and P. Cheeseman, On the Representation and Estimation of Spatial Uncertainty. International Journal of Robotics Research, 1986. 5</p></li><li><p>Se, S., D. Lowe and J. Little, Mobile robot localization and mapping with uncertainty using scale­invariant visual landmarks. The international Journal of robotics Research, 2002. 21</p></li><li><p>Mullane, J., et al., A Random­Finite­Set Approach to Bayesian SLAM. IEEE Transactions on Robotics, 2011</p></li></ul><h3 id="CMake练习"><a href="#CMake练习" class="headerlink" title="CMake练习"></a>CMake练习</h3><blockquote><p>1.程序代码由头文件和源文件组成；</p><p>2.带有main函数的源文件编译成可执行程序，其他的编译成库文件；</p><p>3.如果可执行程序想调用库文件中的函数，它需要参考该库提供的头文件，以明白调用的格式。同时要把可执行程序链接到库文件上；</p></blockquote><p>在自己的工程目录里面建立子目录<code>build</code>文件夹和<code>src</code>文件夹和<code>libhello</code>文件夹，hello.cpp,hello.h放入<code>libhello</code>中,useHello.cpp放入<code>src</code>中。同时在工程目录下创建一个顶层的CMakeLists.txt文件，内容如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">project(sayHello)</span><br><span class="line"></span><br><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line"></span><br><span class="line">#ADD_SUBDIRECTORY(source_dir [binary_dir] [EXCLUDE_FROM_ALL])</span><br><span class="line">#这个指令用于向当前工程添加存放源文件的子目录，并可以指定中间二进制和目标二进制存放的位置</span><br><span class="line">add_subdirectory(src) </span><br><span class="line">#在 build 目录中将出现一个 src 目录，生成的目标代码 hello 将存放在 src 目录中</span><br><span class="line"></span><br><span class="line">add_subdirectory(libhello)</span><br><span class="line"></span><br><span class="line">set(CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br></pre></td></tr></table></figure><p>在<code>src</code>文件夹下新建CMakeLists.txt文件，输入如下内容：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line"></span><br><span class="line">#头文件目录</span><br><span class="line">include_directories($&#123;PROJECT_SOURCE_DIR&#125;&#x2F;libhello)</span><br><span class="line"></span><br><span class="line">set(APP_SRC useHello.cpp)</span><br><span class="line"></span><br><span class="line">#指定最终的目标二进制的位置</span><br><span class="line">#SET(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;&#x2F;bin)</span><br><span class="line">#SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;&#x2F;lib)</span><br><span class="line">set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;&#x2F;bin)</span><br><span class="line"></span><br><span class="line">add_executable(sayHello $&#123;APP_SRC&#125;)</span><br><span class="line"></span><br><span class="line">#为 target 添加需要链接的库</span><br><span class="line">target_link_libraries(sayHello libhello)</span><br></pre></td></tr></table></figure><p>最后在<code>libhello</code>文件夹下新建CMakeLists.txt文件，输入如下内容：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line"></span><br><span class="line">set(LIB_SRC hello.cpp)</span><br><span class="line"></span><br><span class="line">#向 C&#x2F;C++编译器添加-D 定义</span><br><span class="line">add_definitions(&quot;-DLIBHELLO_BUILD&quot;)</span><br><span class="line"></span><br><span class="line">#添加动态库</span><br><span class="line">add_library(libhello SHARED $&#123;LIB_SRC&#125;)</span><br><span class="line">#设置动态库输出位置</span><br><span class="line">set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;&#x2F;lib)</span><br><span class="line"></span><br><span class="line">#SET_TARGET_PROPERTIES(target1 target2 ...</span><br><span class="line">#PROPERTIES prop1 value1 </span><br><span class="line">#prop2 value2 ...)</span><br><span class="line">#该指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本</span><br><span class="line">set_target_properties(libhello PROPERTIES OUTPUT_NAME &quot;sayhello&quot;)</span><br></pre></td></tr></table></figure><p>接着在<code>build</code>文件夹下<code>cmake ..</code>, <code>make</code>即可，在biuld文件中的子问夹下的bin文件生成可执行文件sayHello;在lib中生成库文件libhello.so共享库文件。</p><p>对于安装路径，可以使用<code>CMAKE_INSTALL_PREFIX</code>命令。</p><h3 id="理解ORBSLAM2框架"><a href="#理解ORBSLAM2框架" class="headerlink" title="理解ORBSLAM2框架"></a>理解ORBSLAM2框架</h3><p>1.<code>git clone https://github.com/raulmur/ORB_SLAM2</code></p><p>2.(a) 6个可执行文件一个库文件 <code>set(CMAKE_LIBRARY_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/lib)</code></p><p>   (b)<code>include</code>文件夹包含：对应<code>src</code>中程序的代码函数头文件；<code>src</code>文件夹包含：相应程序的代码函数的c++文件；Examples文件夹包含RGB-D文件夹，Stereo文件夹， Monocular文件夹，具体的内容在总目录下的CMakeLists.txt文件中有写：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Build examples</span><br><span class="line"></span><br><span class="line">set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;&#x2F;Examples&#x2F;RGB-D)</span><br><span class="line"></span><br><span class="line">add_executable(rgbd_tum</span><br><span class="line">Examples&#x2F;RGB-D&#x2F;rgbd_tum.cc)</span><br><span class="line">target_link_libraries(rgbd_tum $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;&#x2F;Examples&#x2F;Stereo)</span><br><span class="line"></span><br><span class="line">add_executable(stereo_kitti</span><br><span class="line">Examples&#x2F;Stereo&#x2F;stereo_kitti.cc)</span><br><span class="line">target_link_libraries(stereo_kitti $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(stereo_euroc</span><br><span class="line">Examples&#x2F;Stereo&#x2F;stereo_euroc.cc)</span><br><span class="line">target_link_libraries(stereo_euroc $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;&#x2F;Examples&#x2F;Monocular)</span><br><span class="line"></span><br><span class="line">add_executable(mono_tum</span><br><span class="line">Examples&#x2F;Monocular&#x2F;mono_tum.cc)</span><br><span class="line">target_link_libraries(mono_tum $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(mono_kitti</span><br><span class="line">Examples&#x2F;Monocular&#x2F;mono_kitti.cc)</span><br><span class="line">target_link_libraries(mono_kitti $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(mono_euroc</span><br><span class="line">Examples&#x2F;Monocular&#x2F;mono_euroc.cc)</span><br><span class="line">target_link_libraries(mono_euroc $&#123;PROJECT_NAME&#125;)</span><br></pre></td></tr></table></figure><p>   (c) OPENCV_LIBS、EIGEN3_LIBS、Pangolin_LIBRARIES、/ORB-SLAM2/Thirdparty/DBoW2/lib/libDBoW2.so、/ORB-SLAM2/Thirdparty/g2o/lib/libg2o.so</p><h3 id="运行ORB-SLAM2"><a href="#运行ORB-SLAM2" class="headerlink" title="运行ORB-SLAM2"></a>运行ORB-SLAM2</h3><p>1.编译安装依赖项，可以按照<a href="https://github.com/raulmur/ORB_SLAM2">ORB-SLAM2源码</a>的指导进行配置。其他依赖项按照高翔书中所说的安装</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libopencv-dev libeigen3-dev libqt4-dev qt4-qmake libqglviewer-dev libsuitesparse-dev libcxsparse3.1.2 libcholmod[tab安装]</span><br></pre></td></tr></table></figure><p>其中需要注意Pangolin需要下载手动编译安装，步骤比较简单，不再赘述</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libglew-dev</span><br><span class="line">---</span><br><span class="line">sudo apt-get install libboost-dev libboost-thread-dev libboost-filesystem-dev</span><br><span class="line">---</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;stevenlovegrove&#x2F;Pangolin Pangolin</span><br><span class="line">--</span><br><span class="line">cd Pangolin</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make </span><br><span class="line">sudo make install</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>另外一个注意的点就是opencv库，我是手动下载编译安装的，网上的教程很多，主要是编译时注意cmake的设置，但是我在安装的时候无法被python链接，也就是没有生成cv2.so，导致没办法在Python2和python3中<code>import cv2</code>，估计可能是我之前先装了anaconda的原因，导致了opencv编译忽略了Python路径问题。。。尝试了几次，未果，等以后用到了python再解决这个问题（可以直接用<code>sudo pip3 install opencv-python</code>，如果不想麻烦的话，不过以后可能会出现一些问题）</p><p>2.如何将 myslam.cpp或 myvideo.cpp 加⼊到 ORB-SLAM2 ⼯程中？请给出你的 CMakeLists.txt 修改⽅案。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#add myvideo.cpp and myslam.cpp</span><br><span class="line">set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(myvideo myvideo.cpp)</span><br><span class="line">target_link_libraries(myvideo $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(myslam myslam.cpp)</span><br><span class="line">target_link_libraries(myslam $&#123;PROJECT_NAME&#125;)</span><br></pre></td></tr></table></figure><p><strong>将myvideo.cpp、myslam.cpp、myvideo.yaml、myslam.yaml以及myvideo.mp4都放到ORB-SLAM2目录下</strong>，再次编译ORB-SLAM2后，就会生成可执行文件，然后终端输入./myvideo和./myslam就可以直接运行了。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/visualSLAMbyGaoxiang-1/myvideo.png" alt="myvideo"></p><p>这里有个问题，一开始我是根据CMakeLits.txt中上面的语句来写的，也就是我把myvideo.cpp、myslam.cpp放在了、Examples/Monocular文件夹中，然后进行编译，结果虽然生成了可执行文件，但是程序无法运行，终端要么提示无法读取对应的.yaml的设置，要么出现segmentation fault错误，最后放到ORB-SLAM2总目录下就没问题了。。。目前不知道什么原因，等后面再看看。</p><hr><hr><p>因为ORB-SLAM2开源的版本没有稠密建图功能，因此如果想尝试稠密建图功能，可以参考高翔博士的ORB-SLAM2_modofied，利用PCL工具实时拼接RGB-D图像，效果还可以，不过没有原作者视频中最后的那个效果好，安装步骤在<a href="https://github.com/Richardyu114/ORBSLAM2_with_pointcloud_map-in-Ubuntu-16.04">这里</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;视觉SLAM概述&quot;&gt;&lt;a href=&quot;#视觉SLAM概述&quot; class=&quot;headerlink&quot; title=&quot;视觉SLAM概述&quot;&gt;&lt;/a&gt;视觉SLAM概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;simultaneous localization and mapping&lt;/li&gt;
&lt;li&gt;仅使用相机进行室内/室外定位（有些情况下GPS会崩，IMU漂移随着时间误差增大）&lt;/li&gt;
&lt;li&gt;机器人在未知环境进行导航—-建图(sparse/semi-dense/dense)&lt;/li&gt;
&lt;li&gt;SLAM问题的本质是对运动主体自身和周围环境空间不确定性的估计（spatial uncertainty）&lt;/li&gt;
&lt;li&gt;为了解决SLAM问题，我们需要状态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，估计状态的均值和不确定性（方差）&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;学术上研究视觉SLAM较多，尤其是monocular，但是应用上少了点，尤其是建图的作用目前很浅，而且很多人大部分现在在用深度学习搞3D重建。目前应用的地方有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;手持设备定位&lt;/li&gt;
&lt;li&gt;自动驾驶定位—比GPS的定位信息要丰富，甚至精度更好(可以达到厘米级)&lt;/li&gt;
&lt;li&gt;AR 增强现实（定位，建图，深度学习结合）&lt;/li&gt;
&lt;li&gt;清洁机器人&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</summary>
    
    
    
    <category term="科研记录" scheme="http://densecollections.top/categories/%E7%A7%91%E7%A0%94%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="visual SLAM" scheme="http://densecollections.top/tags/visual-SLAM/"/>
    
    <category term="Linux" scheme="http://densecollections.top/tags/Linux/"/>
    
    <category term="C++11" scheme="http://densecollections.top/tags/C-11/"/>
    
    <category term="Computer vision" scheme="http://densecollections.top/tags/Computer-vision/"/>
    
    <category term="OpenCV" scheme="http://densecollections.top/tags/OpenCV/"/>
    
  </entry>
  
</feed>
