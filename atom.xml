<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>自拙集</title>
  
  <subtitle>Work cures everything</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://densecollections.top/"/>
  <updated>2020-04-16T12:47:03.581Z</updated>
  <id>http://densecollections.top/</id>
  
  <author>
    <name>Richard YU</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[MIT]计算机科学课堂中学不到的知识</title>
    <link href="http://densecollections.top/2020/03/02/MIT-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E8%AF%BE%E5%A0%82%E4%B8%AD%E5%AD%A6%E4%B8%8D%E5%88%B0%E7%9A%84%E7%9F%A5%E8%AF%86/"/>
    <id>http://densecollections.top/2020/03/02/MIT-计算机科学课堂中学不到的知识/</id>
    <published>2020-03-02T09:56:00.000Z</published>
    <updated>2020-04-16T12:47:03.581Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><ul><li><a href="https://missing.csail.mit.edu/" target="_blank" rel="noopener">homepage</a>;</li><li><a href="https://www.bilibili.com/video/av86911412?p=1" target="_blank" rel="noopener">lecture video;</a></li><li>2020年初始刚放出的公开课，十分实用，千呼万唤始出来。主要是介绍一些CS领域常用的工具和技巧，这些对于平时科研，工作都有很大的帮助。正如在课程介绍和评论中说的，<strong>“It’s not computer science. It is computer literacy.”</strong></li><li>相关资源：阮一峰老师的<a href="https://wangdoc.com/bash/index.html" target="_blank" rel="noopener">bash脚本教程</a></li></ul><a id="more"></a><h2 id="Lecture-1-Course-overview-the-shell"><a href="#Lecture-1-Course-overview-the-shell" class="headerlink" title="Lecture 1. Course overview + the shell"></a>Lecture 1. Course overview + the shell</h2><p><a href="https://missing.csail.mit.edu/2020/course-shell/" target="_blank" rel="noopener">官方讲义</a>比较系统和简练，其中包含一些练习题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">date #显示时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># echo 后面跟上要输出的文本</span><br><span class="line">echo Hello</span><br><span class="line">echo &quot;Hello World&quot;</span><br><span class="line">echo Hello\ World</span><br><span class="line">echo $PATH</span><br><span class="line">which echo</span><br><span class="line">#讲解绝对路径和相对路径</span><br><span class="line">pwd # print working diretory</span><br><span class="line">cd # change working path</span><br><span class="line">cd .. #进入当前目录上一目录</span><br><span class="line"># 讲解输入代码要注意相对路径和绝对路径</span><br><span class="line"></span><br><span class="line">ls #当前路径下的文件</span><br><span class="line">ls .. # 当前路径前一层的文件夹下的文件</span><br><span class="line">cd ~ # 返回/home/username/</span><br><span class="line">cd - #往返当前和上一个文件夹路径</span><br><span class="line"># 讲解 - 加在某个命令后增加argument</span><br><span class="line"></span><br><span class="line">ls -l # 显示更加详细的文件信息，比如read, write, excute（对该文件或文件夹执行命令）</span><br><span class="line"></span><br><span class="line"># mv 文件名 文件名将源文件名改为目标文件名</span><br><span class="line"># mv 文件名 目录名将文件移动到目标目录</span><br><span class="line"># mv 目录名 目录名目标目录已存在，将源目录移动到目标目录；目标目录不存在则改名</span><br><span class="line"># mv 目录名 文件名出错</span><br><span class="line">mv filename1 filename2</span><br><span class="line">cp source dest #复制文或目录</span><br><span class="line">rm filename #删除文件或目录，不可恢复的操作</span><br><span class="line">rmdir # 删除空目录，但无法删除非空目录</span><br><span class="line">mkdir # 新建目录</span><br><span class="line">man ls #manual, 功能类似-help，但是更便于阅读，按q退出</span><br><span class="line"># CTRL + l清除面板记录</span><br></pre></td></tr></table></figure><p>在终端有个输入流input stream和输出流output stream，一般输出流就是执行输入的命令，打印在终端上，但是而可以通过&lt; 和 &gt;符号对input stream和output stream进行指定</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">echo hello &gt; hello.txt # 把文本hello存储在hello.txt中</span><br><span class="line">cat hello.txt or cat &lt; hello.txt # 显示file 内容</span><br><span class="line">cat &lt; hello.txt &gt; hello2.txt # 复制内容到hello2.txt中，而且是从头覆写</span><br><span class="line">cat &lt; hello.txt &gt;&gt; hello2.txt # append, 接着后面写</span><br><span class="line"></span><br><span class="line">ls -l | tail -n1 &gt;ls.txt # 显示最后一行并写入ls.txt</span><br><span class="line"># | 管道命令，是指 | 的左边运行结果 是|右边的 输入条件或者范围</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo su # 加入root权限，后续命令都以super user执行，exit命令推出</span><br><span class="line">cd sys # 进入系统设备，可以查看一些参数</span><br><span class="line"># 进入之后如果想改变某些系统参数，但是又没有使用sudo su，可以使用tee命令</span><br><span class="line">#  tee命令用于读取标准输入的数据，并将其内容输出成文件</span><br><span class="line"># 下面的命令不仅改变了brightness亮度值，还打印出结果在终端</span><br><span class="line">echo 1060 | sudo tee brightness</span><br><span class="line"></span><br><span class="line">xdg-open filename #以默认程序打开文件</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://missing.csail.mit.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;homepage&lt;/a&gt;;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/av86911412?p=1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;lecture video;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;2020年初始刚放出的公开课，十分实用，千呼万唤始出来。主要是介绍一些CS领域常用的工具和技巧，这些对于平时科研，工作都有很大的帮助。正如在课程介绍和评论中说的，&lt;strong&gt;“It’s not computer science. It is computer literacy.”&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;相关资源：阮一峰老师的&lt;a href=&quot;https://wangdoc.com/bash/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;bash脚本教程&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="课程记录" scheme="http://densecollections.top/categories/%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="computer science" scheme="http://densecollections.top/tags/computer-science/"/>
    
      <category term="public course" scheme="http://densecollections.top/tags/public-course/"/>
    
      <category term="MIT" scheme="http://densecollections.top/tags/MIT/"/>
    
      <category term="practical skills" scheme="http://densecollections.top/tags/practical-skills/"/>
    
  </entry>
  
  <entry>
    <title>Stanford CS231n 笔记</title>
    <link href="http://densecollections.top/2020/02/29/Stanford-CS231n-%E7%AC%94%E8%AE%B0/"/>
    <id>http://densecollections.top/2020/02/29/Stanford-CS231n-笔记/</id>
    <published>2020-02-29T04:44:28.000Z</published>
    <updated>2020-04-19T12:28:43.513Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><ul><li><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">homepage</a>;</li><li><a href="https://www.bilibili.com/video/av13260183?from=search&amp;seid=2308745029556209710" target="_blank" rel="noopener">2017 lecture video</a>;</li><li><a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">中文笔记</a></li><li>课程相关时间安排，课件，参考资料，作业详见homepage链接，以最新版为主；</li><li>官方的assignments和notes做得非常好，强烈推荐学习和反复观看；</li><li>时间原因，学习19版的时候恰好2020版春季课程将要开始，这一版换了team，里面加了不少新的research介绍，尤其是非监督，自监督之类的，因此会加入一下2020版的内容；</li><li>个人比较喜欢Justin Johnson的讲课风格和深度，因此配合他在UMICH (University of Michigan) 的 <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/" target="_blank" rel="noopener">EECS498: Deep Learning for Computer Vision</a>课一起看，可以相互补充（和CS231N有很大重叠， 但也些不同）；</li></ul><h2 id="Lecture1-Introduction"><a href="#Lecture1-Introduction" class="headerlink" title="Lecture1. Introduction"></a>Lecture1. Introduction</h2><p>Related courses in Stanford:</p><ul><li><p>CS131: Computer Vision: Foundations and Applications</p></li><li><p>CS231a: Computer Vision, from 3D Reconstruction to Recognition</p></li><li><p>CS 224n: Natural Language Processing with Deep Learning</p></li><li><p>CS 230: Deep Learning</p></li><li><p><strong>CS231n: Convolutional Neural Networks for Visual Recognition</strong></p></li><li><p>Andrew Ng的CS229虽然不在列表中，但个人觉得也值得一看，毕竟也属于经典ML课程。</p></li></ul><p>A Brief history of Computer Vision and CS 231n:</p><ul><li>从史前生物产生视觉开始，到后面的人类对于视觉的研究：相机，生物学研究，以及陆续地对于计算机视觉/机器视觉的系统研究，在社会各个领域结合产生的特定任务的建模设计等工作，这里再一次提到了David Marr的《vision》一书对整个计算机视觉领域的奠基于推动作用。</li><li>介绍卷积神经网络CNN对计算视觉的推动作用，简介了CNN的历史以及在各个视觉有关问题上的强大作用。</li><li>推荐课本教材：《Deep Learning》 by Goodfellow, Bengio, and Courville.</li></ul><a id="more"></a><h2 id="Lecture2-Image-Classification"><a href="#Lecture2-Image-Classification" class="headerlink" title="Lecture2. Image Classification"></a>Lecture2. Image Classification</h2><p>material: 主讲人<a href="http://cs.stanford.edu/people/jcjohns/" target="_blank" rel="noopener">Justin Johnson</a>编写的<a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="noopener">python numpy tutorial</a>，个人觉得是个很不错的教程。</p><p>官方的<a href="http://cs231n.github.io/classification/" target="_blank" rel="noopener">notes1—image classification</a>和<a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="noopener">notes2-linear classification</a>是对本次课的一个总结，补充和拓展（其中第二个笔记大部分是下次课的内容，放在这里我猜可能是预习吧），可以说写得十分详细了，而且常温常新。</p><ul><li>首先阐述对于计算机而言，为什么分类图片很难（光照，视点，类似纹理背景等）；</li><li>介绍了一些传统方法对此的努力，但是这些方法效果不好，具有单一性，鲁棒性和generalization都很差；</li><li>引入data-drive approcah, 也就是收集数据，训练，预测三段式方法；</li><li>介绍Nearest Neighbours分类器，并阐释不同K值和distance metric选取的不同performance（L1和L2的选取与坐标轴旋转对数据引起的改变）；</li><li>介绍交叉验证的作用，以及说明K-NN的维度诅咒（处理大数据吃力）和 predict速度慢的缺陷（需要一个个和训练集图片计算distance）；</li><li>引入线性分类器，权重矩阵<strong>W</strong>，编制矩阵<strong>b</strong>，并在第二个官网的notes中详细地介绍了SVM和Softmax两种loss衡量方式；</li></ul><p>官网推荐的一篇阅读review <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank" rel="noopener">A Few Useful Things to Konw About Machine Learnig</a>，写于2012年，但是仍不过时，可以一读。文中写到的12个”folk wisdom”:</p><ul><li>Learning = representation+evaluation+optimization;</li><li>It’s generalization that counts;</li><li>Data alone is not alone;</li><li>Overfitting has many faces;</li><li>Intuition falis in high dimensions;</li><li>Theoretical guarantees are not what they seem;</li><li>Feature engineering is the key;</li><li>More data beats a cleverer algorithm;</li><li>Learn many models, not just one;</li><li>Simplicity does not imply accuracy;</li><li>Representable does not imply learnable;</li><li>Gneralization does not imply causation;</li></ul><h2 id="Lecture3-Loss-Functions-and-Optimization"><a href="#Lecture3-Loss-Functions-and-Optimization" class="headerlink" title="Lecture3. Loss Functions and Optimization"></a>Lecture3. Loss Functions and Optimization</h2><p>官方笔记：<a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="noopener">linear classification</a>, <a href="http://cs231n.github.io/optimization-1/" target="_blank" rel="noopener">optimization</a>有更为详细的课程内容解释和总结;</p><p>其中linear classification笔记中有个交互式<a href="http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/" target="_blank" rel="noopener">demo</a>，主要用来体会SVM和Softmax来训练线性分类器的过程，包括调节正则项，学习率等超参数；</p><ul><li>上节课引入linear classification，现在需要loss function定量分类器对分类结果与training data的GT之间的差异，需要一些optimization方式去有效寻找模型参数可以最小化loss；</li><li>介绍两种loss衡量metric: Multiclass SVM和Softmax，并认为两种方式在训练分类器时差异不会很大，SVM是为了拉大正确的score与非正确score之间的差值到设定的margin，所以分的越差惩罚越重，分的越好基本上不再去多费精力，而Softmax则不断将正确的probability拉的更高，错误的拉的更低；</li><li><p>由于满足loss function的权重矩阵<strong>W</strong>有无数个（针对linear classification），之间可成正比例关系，因此需要引入正则项让模型自己适应数据，得到generalization比较高的模型参数；</p></li><li><p>针对优化参数方法，通过随机搜索过渡到梯度下降gradient descent，在官方笔记中更加仔细地介绍了针对实际工程应用的梯度下降法（数值梯度，分析梯度等），满足计算性能之间的trade-off</p></li><li><p>最后针对图像特征，介绍了color histogram, HoG (Histogram of Oriented Gradients), Bag of Words等方法。将feature映射到更高维的空间，使分类问题变得容易，这也是CNN的一大作用，从而引入下一节主题；</p></li></ul><p><img src="/2020/02/29/Stanford-CS231n-笔记/3_1.JPG" alt="特征映射"></p><p>在这里插一下sigmoid, softmax和cross entropy之间的联系，我们一般说的分类是单标签多分类，也就是说一张图片只会有一个标签，标签与标签之间是互斥的，而softmax计算的类别概率是类别互斥的，因为和为1，而针对多标签分类，也就是说一张图片可能有多个标签存在，标签之间不是非此即彼的，类别不互斥，此时用sigmoid激活函数，因为sigmoid是互相独立的计算，只判断此神经元是否属于该位置对应的类别，所以在计算多分类交叉熵时是每个神经元按照二分类交叉熵算然后求和平均。参考<a href="https://www.zhihu.com/question/341500352/answer/795497527" target="_blank" rel="noopener">blog1</a>, <a href="https://blog.csdn.net/tsyccnh/article/details/79163834" target="_blank" rel="noopener">blog2</a>.</p><p>梯度下降算法理解：</p><p>假设一个模型拟合表达式为：$h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$，其中$\theta_{i}, x_{i}$一般都是矩阵，对于一个输入，我们希望计算的值与真实函数$y$尽可能得接近，直接照着真实函数分布去“依葫芦画瓢”比较困难，所以来最小化他们之间得残差来搜索拟合函数得参数，假设我们用平方损失函数：$J(\theta)=\frac{1}{2} \sum \limits _{i=1}^{m}(h_{\theta}(x)-y)^{2}$，现在我们对每一个超参数$\theta_{j}$求偏导，得到关于它得梯度（函数上升最快得方向），由于我们是想找最小值，所以要反方向走（实际可以在损失函数前加个符号，转为找最大值）：</p><script type="math/tex;mode=display">\begin{array}{c}\frac{\partial}{\partial \theta_{j}} J(\theta)=\frac{\partial}{\partial \theta_{j}} \frac{1}{2}(h _{\theta}(x)-y)^{2}=(h _{\theta}(x)-y) \cdot \frac{\partial}{\partial \theta_{j}}(h _{\theta}(x)-y) \\=(h _{\theta}(x)-y) \cdot \frac{\partial}{\partial \theta_{j}}\left(\sum_{i=0}^{n} \theta_{i} x_{i}-y\right)=(h _{\theta}(x)-y) x_{j}\end{array}</script><p>根据梯度，加上步长learning rate, 对参数$\theta_{j}$进行更新：$\theta_{j}:=\theta_{j}-\alpha(h _{\theta}(x)-y) x_{j}$</p><p>通过不断地迭代，使函数往最小值逼近。在深度学习实际应用时，训练的数据样本可能很大，一次迭代要计算所有的样本会非常耗时，所以利用SGD方法，抽取一些数据作为batch，认为可以近似代表整体样本的分布，每次换不同的样本去学习，也可以得到近似的解（但这也只是工程近似）。深度学习中的模型拟合函数是非常庞大而又复杂的，结合BP加梯度下降可以使函数收敛到局部最小值。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/3_2.JPG" alt="模型优化过程"></p><h2 id="Lecture4-Introduction-to-Neural-Networks"><a href="#Lecture4-Introduction-to-Neural-Networks" class="headerlink" title="Lecture4. Introduction to Neural Networks"></a>Lecture4. Introduction to Neural Networks</h2><p>这一节官方指定阅读的笔记为：<a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="noopener">optimization-2-bp</a>, <a href="http://cs231n.stanford.edu/handouts/linear-backprop.pdf" target="_blank" rel="noopener">linear-classifier-bp</a>; 此外还给了一些推荐的阅读材料，以供选择，其中包括一份<a href="http://cs231n.stanford.edu/handouts/derivatives.pdf" target="_blank" rel="noopener">向量，矩阵，BP的导数推导示意笔记</a>，Yan leCun在1998年写的名为“Efficient BackProp”的<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">论文</a>，两篇解释BP的博客（<a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">1</a>, <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">2</a>）和MIT <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/" target="_blank" rel="noopener">Artificial Intelligence公开课</a>中有关BP讲解的课程视频，此外还有一篇ML中automatic differentiation的<a href="https://arxiv.org/abs/1502.05767" target="_blank" rel="noopener">survey</a>. 个人感觉如果对微积分和线性代数比较熟悉的话，看一下官方指定阅读的两篇笔记就差不多了，主要是为了深入理解BP，有时间的话也强烈推荐那篇推荐阅读的博客2，是<a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Micahel Nielsen</a>写的《nndl》教材中的一节，在入门DL的时候我就是看的这本书，十分经典耐读。BP的使用大多是对矩阵或者说tensor张量进行的，如果对矩阵求导不熟悉，或者想系统的学习巩固下，可以移步<a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术-上</a>和<a href="https://zhuanlan.zhihu.com/p/24863977" target="_blank" rel="noopener">矩阵求导术-下</a>，上-篇写的是标量对矩阵的求导方法，下-篇写的是矩阵对矩阵的求导方法，都是通过标量上全微分表达式来进行推广和计算的：</p><p>“<strong>导数与微分的联系是计算的枢纽</strong>，标量对矩阵的导数与微分的联系是$d f=\operatorname{tr}\left(\nabla_{X}^{T} f d X\right)$，先对$f$求微分，再使用迹技巧可求得导数，特别地，标量对向量的导数与微分的联系是$d f=\nabla_{x}^{T} f d \boldsymbol{x}$；矩阵对矩阵的导数与微分的联系是$\operatorname{vec}(d F)=\frac{\partial F^{T}}{\partial X} \operatorname{vec}(d X)$，先对$F$求微分，再使用向量化的技巧可求得导数，特别地，向量对向量的导数与微分的联系是$d \boldsymbol{f}=\frac{\partial \boldsymbol{f}^{T}}{\partial \boldsymbol{x}} d \boldsymbol{x}$。”（其中，$x,\boldsymbol {x}, X$分表代表标量，向量和矩阵）</p><p>我想对矩阵的求导可能是本节课的一个小重点以及后面进行DL理论学习的基石。</p><ul><li><p>该课延续上一讲提出的linear classifier,介绍了神经元的概念，历史和应用，引入fully-connected network；</p></li><li><p>指出如果层数加多，再加上激活函数，损失函数等，网络所代表的拟合函数表达式将非常庞大而复杂，采用梯度下降的方式训练时，推出分析表达式非常不现实和实际（如果想要零时改变一个函数就要重新推算）；</p></li><li><p>由于我们不关心函数梯度具体表达式，我们只是想通过计算梯度值来更新参数，所以采用computation graph的方式来逆推local gradient，forward + backpropagation形成一个完美体系；</p></li></ul><p>在此课之前我推导过全连接和卷积神经网络的BP，就是当成链式法则”chain rules“来对待，而这次课给我了一个数学公式之外一个更加形象化理解，也即是”computation graph“和”门电路“：</p><blockquote><p>Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.</p></blockquote><p>而网络通过一系列”add gate”, “mul gate”, “max gate”等”gate”一步步垒成复杂的函数，而且部分组织可以组合成一个特殊的”gate”，也可以被分解成其他简易的”gate”，方便我们进行梯度计算（注意variable分流forward之后bp要加起来+=）。通过简单的example可以了解到”gate” input和output之间的梯度分配关系，非常形象生动。</p><h2 id="Lecture-5-Convolutional-Neural-Networks"><a href="#Lecture-5-Convolutional-Neural-Networks" class="headerlink" title="Lecture 5. Convolutional Neural Networks"></a>Lecture 5. Convolutional Neural Networks</h2><p>该节课的<a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">官方笔记</a>对卷积神经网络CNN来龙去脉和计算都介绍得很详细，同时也对每个层（Conv layer, pooling layer, fc layer）都做了全面的回顾和介绍。</p><p>另有一篇<a href="https://www.matongxue.com/madocs/32.html" target="_blank" rel="noopener">博客</a>解释卷积神经网络中”卷积“一词和信号与系统中得卷积操作的联系，知乎也有个理解卷积的<a href="https://www.zhihu.com/question/22298352" target="_blank" rel="noopener">回答</a>，看完之后可以加深对卷积核的理解。</p><ul><li>首先介绍感知机，神经元和CNN的历史；</li><li>简介一个卷积神经网络的结构以及直觉上层级学习的内容和特征；</li><li>大部分时间讲解卷积的计算过程；</li></ul><p><a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="noopener">Convolution arithmetic</a></p><h2 id="Assignment-1"><a href="#Assignment-1" class="headerlink" title="Assignment 1"></a>Assignment 1</h2><p><a href="http://cs231n.github.io/assignments2019/assignment1/" target="_blank" rel="noopener">assignment page</a>;</p><p>reference：</p><ul><li><p><a href="https://github.com/Wangxb06/CS231n" target="_blank" rel="noopener">github-code</a>;</p></li><li><p>参考培训机构深度之眼的cs231n-camp的文档</p></li></ul><p>具体过程和结果参见这个<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments" target="_blank" rel="noopener">repository</a></p><p>注意：一开始运行<code>knn.ipynb</code>文件时可能显示<code>scipy.misc.imread</code>缺失，这是因为新版的<code>scipy</code>包去掉了<code>imread</code>这个module，解决方法是可以在<code>data_utils.py</code>中的<code>from scipy.misc import imread</code>改为<code>from imageio import imread</code>。</p><h2 id="Lecture-6-Deep-Learning-Hardware-and-Software"><a href="#Lecture-6-Deep-Learning-Hardware-and-Software" class="headerlink" title="Lecture 6. Deep Learning Hardware and Software"></a>Lecture 6. Deep Learning Hardware and Software</h2><p>讲义中包含的官方制作的两大深度学习框架（Pytorch, TensorFlow）的tutorial，我把它们直接下载下来放在<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments" target="_blank" rel="noopener">GitHub</a>便于观看。</p><p>2017版中该课在train neural network之后，19年改为之前，由于两年间框架格局也在发生改变，所以以最新版<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture06.pdf" target="_blank" rel="noopener">slides</a>为主（17版主推TensorFlow，也花了一大半时间介绍TF，19版偏向于PyTorch）。</p><p>结合前面的tutorial一起看，可以当作非常好的入门学习材料。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/dynamic_static_graph.JPG" alt="动态计算图和静态计算图"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/advice_for_frameworks.JPG" alt></p><h2 id="Lecture-7—8-Training-Neural-Networks-I-amp-II"><a href="#Lecture-7—8-Training-Neural-Networks-I-amp-II" class="headerlink" title="Lecture 7—8. Training Neural Networks I &amp; II"></a>Lecture 7—8. Training Neural Networks I &amp; II</h2><p>这一节课硬货非常非常的多，很多东西都值得深究。课程讲的不是很清楚，因为内容多，所以需要线下仔细看看笔记。笔记主要分为三个部分：</p><p>1.<a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="noopener">neural fc network model</a>: 介绍来自脑启发的神经元neuron，常用的激活函数（sigmoid, tanh, ReLU, Leaky ReLU, Maxout, ELU），全连接神经网络的一些结构问题，比如是否可以拟合任意连续函数，多少层足够，多大足够，以及表征能力和前向计算等。</p><p>Sigmoid在训练时除了会有梯度消失的情况外还有输出不是0均值（zero-centered）的情况：这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。以 $f=sigmoid(Wx+b)$为例， 假设输入均为正数（或负数），那么对$W$的导数（sigmoid的导数总是正的）总是正数（或负数），这样在反向传播过程中要么都往正方向更新，要么都往负方向更新，导致有一种捆绑效果，使得收敛缓慢。激活</p><blockquote><p><strong>TLDR</strong>: “<em>What neuron type should I use?</em>” Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.</p><p>larger networks will always work better than smaller networks, but their higher model capacity must be appropriately addressed with stronger regularization (such as higher weight decay), or they might overfit. We will see more forms of regularization (especially dropout) in later sections.</p></blockquote><p>2.<a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener">one time setup</a>: 介绍数据预处理（先减去mean image，或者是mean RGB vector中心化，然后除以std进行normalization；进行PCA降维数据，进行decorrelated或者利用whitening得到各向同性的Gaussian blob；一般利用CNN训练时不需要进行太复杂的数据预处理），<strong>权重初始化</strong>（非常重要和经验化的一点，不要以零对权重进行初始化，而是随机高斯分布采样初始化，“Xavier” initialization, 并且进行<code>w = np.random.randn(n) * sqrt(1/n)</code>对方差进行校准，为的是让输出的方差与输入的相同，后来何恺明的研究中表明，采用ReLU激活函数，初始化为<code>w = np.random.randn(n) * sqrt(2.0/n)</code>见论文”<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>“），batch normalization（直接作为一层插入FC/CNN与激活函数层之间，把batch数据进行标准高斯分布化，当然后面还有一个超参的逆向高斯化，主要是为了让网络自己去判断这那</p><p>种对自己适合是一种启发式处理），正则化（L1，L2正则化，一般L2更好些；Max norm constraints设置权重边界；Dropout进行组合训练或者是认为进行数据增强）和损失函数（classification, regression (最好不要硬来，因为没有规律，试着往分类转化), structured prediction）等问题。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/8_1.JPG" alt="网络权重初始化问题"></p><p>权重初始化中还有一个transfer learning技巧，利用ImageNet的预训练模型，再根据自己的数据进行层的选择性finetune.</p><p>Dropout: Forces the network to have a redundant representation; Prevents co-adaptation of features</p><p><a href="https://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">对Dropout起防过拟合作用的解释—组合派和噪声派</a>；<a href="https://blog.csdn.net/fu6543210/article/details/84450890" target="_blank" rel="noopener">对Inverted Dropout中神经元进行Dropout之后为何需要对余下工作的神经元进行rescale的一个解释—乘再除确保下一层输出期望不变</a></p><p>（除以p是为了保证训练和测试时输入给下一层的期望是一样的，那些失活的神经元损失的值会通过其他神经元补回来，这样就等于加强了某些神经元的鲁棒性，如果不除的话等于是随机挑选神经元训练，这样可能达不到预期的增强效果。）</p><p>Data Augmentation: translation, rotation, stretching（拉伸）, shearing（修剪）, lens distortions（镜头畸变）, color jitter （颜色抖动）等等。</p><p>Batch Normalization（类似于”白化“，实现数据独立同分布，但是BN并没有实现独立同分布，仅仅是压缩再变换）: 在前向传播时，分两种情况进行讨论：如果是在train过程，就使用当前batch的数据统计均值和标准差，并按照第二章所述公式对Wx+b进行归一化，之后再乘上gamma，加上beta得到Batch Normalization层的输出；如果在进行test过程，则使用记录下的均值和标准差，还有之前训练好的gamma和beta计算得到结果（作业中记录的值也是采用了指数权重滑动平均的方法）。</p><p>此外，CNN使用时叫spatial batchnorm，主要是考虑到卷积神经网络的参数共享特性。（<a href="https://www.zhihu.com/question/269658514" target="_blank" rel="noopener">知乎回答</a>)</p><p>之前大家对BN的理解就是为了解决ICS (internal covariate shift)，让每一层输入的分布不再变化莫测（网络需要不停调整参数适应数据分布变化），同时也要基上线性变换层保留一些模型需要的原始信息，也让输出的数据值落在激活函数的敏感区域，也可作为正则化的手段（进一步解读见：<a href="https://zhuanlan.zhihu.com/p/55852062" target="_blank" rel="noopener">1</a>, <a href="https://www.cnblogs.com/skyfsm/p/8453498.html" target="_blank" rel="noopener">2</a>, <a href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/" target="_blank" rel="noopener">3</a>）。</p><p>但是MIT 发表在NeurIPS 2018上的研究”<strong>How Does Batch Normalization Help Optimization</strong>“发现二者并无关系。研究者证明 BatchNorm 以一种基础的方式影响着网络的训练：它使相关优化问题的解空间更平滑了。这确保梯度更具预测性（见机器之心的<a href="https://www.jiqizhixin.com/articles/2018-11-13-8" target="_blank" rel="noopener">翻译</a>）。</p><p>另外，最近，2020 DeepMind刚出的论文”<strong>Batch Normalization Biases Deep Residual Networks Towards Shallow Paths</strong>“研究了BN为何可以提升深度残差网络的训练深度：”<strong>在初始化阶段，批归一化使用与网络深度的平方根成比例的归一化因子来缩小与跳跃连接相关的残差分支的大小</strong>。这可以确保在训练初期，深度归一化残差网络计算的函数由具备表现良好的梯度的浅路径（shallow path）主导“（见机器之心的<a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650783208&amp;idx=5&amp;sn=5ac1cbc22336a9e949e42a49310e5106&amp;chksm=871a7996b06df080454fe765caa7440b015329d4d67d3c70d5a2c807f928f8ec4151ac1ec075&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1585028895021&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=da21d41c58faea5bd0311609648073cf421257a6d02c4441b3868a2b447633270e7de10abb2aa824a08b5f711c6c34e277a648a52dd60752d3155ec6d6270b2dc5745360dae6f2aea673abbca5e3864c&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10&amp;version=62080079&amp;lang=zh_CN&amp;exportkey=AyJIB1yGzg%2B5xqUfbzluzjc%3D&amp;pass_ticket=IUc1Fn13evk5P8L18SlGS8pMmpxl8gKIhcPljl38AnLtjlv3XajT2eK9gnLXe6OU" target="_blank" rel="noopener">报导</a>）。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/7_1.JPG" alt="BatchNorm，CNN中常用"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/7_2.JPG" alt="LayerNorm，RNN中常用"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/7_3.JPG" alt="InstanceNorm，后面的style transfer会用到"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/7_4.JPG" alt="GroupNorm受HOG特征启发，有些特征是相互联系的，针对目标检测这样batch_size很小的情况做了优化，结合LN和IN"></p><p>出了上述几种变体外，还有个结合版本的Switchable Normalization，给IN, BN, LN分配权重，让网络自己去学习哪种归一化方式适合（<a href="https://blog.csdn.net/liuxiao214/article/details/81037416" target="_blank" rel="noopener">总结1</a>，<a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="noopener">总结2</a>，在NLP中也UCB也通过观测BN的异常情况提出了最新的PN，见<a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247486486&amp;idx=1&amp;sn=3901b342dab93dae889a744887cce454&amp;chksm=970c24c0a07badd676bcaf24833265eddede3fefc468c38031cf54c19f3d384233b1baf41375&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1587291237878&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=bdd3db2c3f0f72f7f2db9c82510b3febc816e0a31a5eec50b93b6b3bf0119676f36421d454c5468967fe63a0f4e502b7215e6264ee0cf4d8a2c1fb594d7b81f30102c4b35c6f73a07e6e38a5676d59c4&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6209005f&amp;lang=zh_CN&amp;exportkey=A4N3AlFSdyWYx%2FUgnxpPAk8%3D&amp;pass_ticket=gmEgMHc7Z0mPJZbQjqSRRs0scY2%2F8o3EWf%2FiXl5R0OEoJMA1VU0QqZve4JWTLBSa" target="_blank" rel="noopener">此</a>）。</p><p>在防止模型过拟合方面，正则化方法除了对大权重进行惩罚外，还有引入“random noise”的思路，具体有以下几种（如下图所示）。2019年 CS231N 跟进潮流，新加了Cutout和Mixup方法，其中MixUp方法本人在custom dataset上用过，对小数量数据确实有用。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/8_2.JPG" alt="对模型进行正则化的一些方法"></p><blockquote><ul><li>The recommended preprocessing is to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature</li><li>Initialize the weights by drawing them from a gaussian distribution with standard deviation of sqrt(2/n), where nn is the number of inputs to the neuron. E.g. in numpy: <code>w = np.random.randn(n) * sqrt(2.0/n)</code>.</li><li>Use L2 regularization and dropout (the inverted version)</li><li>Use batch normalization</li><li>We discussed different tasks you might want to perform in practice, and the most common loss functions for each task</li></ul></blockquote><p>3.<a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">training dynamics &amp; evaluation</a>: 梯度检查，一些网络学习的注意事项，参数更新，超参搜索，模型ensemble等。这里的重点在于optimizer优化器，从一开始的普通SGD到带<a href="http://zh.gluon.ai/chapter_optimization/momentum.html" target="_blank" rel="noopener">动量的SGD</a>，然后是一些自适应学习率的优化器，比如目前最好用的Adam等。</p><p>Momentum updateing: <a href="https://blog.csdn.net/dawningblue/article/details/89487418" target="_blank" rel="noopener">关于“动量”更新参数的动力学解释—“动量”应当是“阻尼系数”，点的质量和运动时间看成“1”，然后推导关于位置的更新</a>；</p><p>Nesterov Momentum: <a href="https://blog.csdn.net/dawningblue/article/details/89487480" target="_blank" rel="noopener">利用趋势点希望可以跳出“坑”的困境</a>；</p><p>Second order optimization: quasi-Newton methods, <a href="https://blog.csdn.net/Im_Chenxi/article/details/80546742" target="_blank" rel="noopener">数学公式</a>，<a href="https://zh.wikipedia.org/wiki/黑塞矩陣" target="_blank" rel="noopener">Hessian矩阵</a>，二阶方法是利用函数的曲率，也就是导数的导数来进行参数更新，速度会更快，但是Hessian阵难以计算，带数学推导的知乎推荐论文<a href="https://arxiv.org/abs/1401.7020" target="_blank" rel="noopener">A Stochastic Quasi-Newton Method for Large-Scale Optimization</a>. 2020.2.20，Google brain放出一篇二阶优化算法训练Transformer的论文<a href="https://arxiv.org/abs/2002.09018" target="_blank" rel="noopener">Second Order Optimization Made Practical</a>. 关于非线性优化方法的一些<a href="https://www.cnblogs.com/maybe2030/p/4751804.html" target="_blank" rel="noopener">区别</a>：一阶法（梯度下降，最速下降），二阶（牛顿法，拟牛顿法），共轭梯度等。</p><blockquote><p><strong>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）</strong></p><p><strong>根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</strong></p></blockquote><p><a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">Adam</a>优化算法: 其实就是momentum + RMSProp，减缓mini-batch训练中的抖动，同时自适应更新学习率，对超参的选择比较鲁棒。相关解释：<a href="https://blog.csdn.net/liuyuemaicha/article/details/52497512" target="_blank" rel="noopener">矩估计</a>，<a href="https://zhuanlan.zhihu.com/p/32335746" target="_blank" rel="noopener">指数加权移动平均与初始时刻阶段偏差修正</a>，<a href="https://moodle2.cs.huji.ac.il/nu15/pluginfile.php/316969/mod_resource/content/1/adam_pres.pdf" target="_blank" rel="noopener">a review ppt</a>.</p><blockquote><p>The two recommended updates to use are either SGD+Nesterov Momentum or Adam.（有时候带动量的SGD法可能比Adam训练出的效果更好，但是调参的难度可能大点）</p></blockquote><p>优化算法总结推荐阅读Juliuszh的知乎专栏：<a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">1</a>，<a href="https://zhuanlan.zhihu.com/p/32262540" target="_blank" rel="noopener">2</a>，<a href="https://zhuanlan.zhihu.com/p/32338983" target="_blank" rel="noopener">3</a></p><p>另有一篇大牛Sebastian Ruder写的<a href="https://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a>，最近一次更新在2020.3.20，加入了新的优化器，博客中也提供了arXiv版本。</p><p>超参Hyperparameters主要包括网络架构，学习率以及其deacy 机制，参数更新机制，和正则化regularization选择及其强度。面对这些超参，在实验时可以通过一下步骤来进行：首先检查模型初始化是的loss，一般权重对每一类都是零认知的，所以会给出平均猜测；然后拿出小部分样本进行实验，进行初始化，给出一些学习率然后对其训练，从中挑出靠谱的；接着再组合学习率和deacy参数，看看哪些不错；然后对上面最好的进行长时间训练，一言以蔽之就是先在数据中的小范围中进行预演。此外也要注意learning rate中的deacy参数选择和加入时间，否则可能使学习不充分。训练刚开始可以采取<a href="https://blog.csdn.net/sinat_36618660/article/details/99650804" target="_blank" rel="noopener">warm up</a>或者grad warm up处理，这个主要是针对大数据，大网络中权重的初始平稳问题，详见<a href="https://www.zhihu.com/question/338066667" target="_blank" rel="noopener">知乎回答</a>。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/8_3.JPG" alt="超参选择方法"></p><blockquote><p>To train a Neural Network:</p><ul><li>Gradient check your implementation with a small batch of data and be aware of the pitfalls.</li><li>As a sanity check, make sure your initial loss is reasonable, and that you can achieve 100% training accuracy on a very small portion of the data</li><li>During training, monitor the loss, the training/validation accuracy, and if you’re feeling fancier, the magnitude of updates in relation to parameter values (it should be ~1e-3), and when dealing with ConvNets, the first-layer weights.</li><li>The two recommended updates to use are either SGD+Nesterov Momentum or Adam.</li><li>Decay your learning rate over the period of the training. For example, halve the learning rate after a fixed number of epochs, or whenever the validation accuracy tops off.</li><li>Search for good hyperparameters with random search (not grid search). Stage your search from coarse (wide hyperparameter ranges, training only for 1-5 epochs), to fine (narrower rangers, training for many more epochs)</li><li>Form model ensembles for extra performance</li></ul></blockquote><h2 id="Lecture-9-CNN-Architecture"><a href="#Lecture-9-CNN-Architecture" class="headerlink" title="Lecture 9. CNN Architecture"></a>Lecture 9. CNN Architecture</h2><p>这一节主要讲CNN的几个经典结构（AlexNet, VGG, GoogLeNet, ResNet），课外阅读材料相关网络对应的论文。</p><p>推荐阅读Justin ho对CNN architecture的介绍文章，链接在<a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">此</a>。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_1.JPG" alt="2019较2017加了最新的NAS技术，自动搜索最优结构网络"></p><p><strong>AlexNet</strong>: 开启CNN风潮的奠基之作，规划了CNN网络该有的部件（data augmentation, filter, maxpool, normalization, non-linearity activation function, fc, dropout, gpu加速等）。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_2.JPG" alt="AlexNet结构（5Conv+3FC），初版由于GPU显存小所以一半一半丢到两块GPU训练，最后合并"></p><p><strong>VGG</strong>: 引入小卷积核，一方面小卷积核堆叠拥有大卷积核同样的感受野，另一方面还可以减小参数，同时加深网络，提高拟合精度。后续的网络基本吸取了VGG网络中小卷积核（3x3, p=1, s=1）提取特征，mxpool (s=2) downsample 特征图，同时在缩小后的特征图上加倍filter数量保持时间复杂度。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_3.JPG" alt="现在以16和19层为主，且只有3x3卷积核，和s=2的maxpool缩小特征图尺寸"></p><p><strong>GoogLeNet</strong>: 受到<a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">Network in Network</a>很大启发（一个是跨通道融合的1x1卷积核带来的MLP功能，提高局部特征的抽象表达能力，另一个是Global Average Pooling (GAP)，单独利用特征图分类），同时提升网络的深度和宽度。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_4.jpg" alt="GoogLeNet--22层，三个分类器，FC前面利用AP减少参数"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_5.JPG" alt="采用inception module进行堆叠，并增加网络宽度，同时采用1x1卷积核降维特征图层数，减少参数并增加非线性"></p><blockquote><p>为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉。</p></blockquote><p>之后Google对inception进行改进到v4版本(详见<a href="https://my.oschina.net/u/876354/blog/1637819" target="_blank" rel="noopener">blog</a>)，v2, v3主要是改小卷积核，v4是加入了Resnet的残差连接思想，然后将basic block或者bottleneck中的conv组合换成inception module。</p><p>GoogLeNet团队发表的有关GoogLeNet的论文：《Going deeper with convolutions》、《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》、《Rethinking the Inception Architecture for Computer Vision》、《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》</p><p><strong>ResNet</strong>：加深网络可以获得更好的性能，但是采用常规方法堆叠卷积加深优化困难，实际效果不好。</p><p>恒等映射，深网络中有不少是对浅网络的缝补，深层网络中，如果保留那些浅层网络的性能，同时又考虑采用恒等映射，那么效果应该不会比单独用浅层网络更差。但是直接学习恒等映射不如学习输出与恒等映射的残差有效（redidual模块会明显减小模块中参数的值从而让网络中的参数对反向传导的损失值有更敏感的响应能力，虽然根本上没有解决回传的损失小得问题，但是却让参数减小，相对而言增加了回传损失的效果，也产生了一定的正则化作用），因此引入跳级连接，ResNet诞生（参考<a href="https://blog.csdn.net/weixin_43624538/article/details/85049699?from=timeline&amp;isappinstalled=0" target="_blank" rel="noopener">PRIS-SCMonkey</a>，<a href="https://zhuanlan.zhihu.com/p/54289848" target="_blank" rel="noopener">Pascal</a>）。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_6.JPG" alt="basic block和bottleneck结构应对不同深度的网络，bottleneck结构应该受到GoogLeNet启发进行降维减少参数"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_7.JPG" alt="不同深度网络结构参数配置"></p><p>后续的研究者一直在思考为什么ResNet会如此简洁高效，它到底在解决网络学习中的什么问题？1.degradation问题，梯度有效性被跳级连接解决（何恺明在论文中指出梯度消失爆炸问题已经被BN等normalization手段解决得很好了，跳级连接是为了深层的网络参数对梯度更加敏感，保持前面梯度的相关性，因为前向传播的时候将原始的信息带出一部分。当然ResNet也可以在一定程度上缓解梯度弥散的问题，但是这不是最主要的，详见<a href="https://www.zhihu.com/question/64494691" target="_blank" rel="noopener">这个知乎回答</a>），且必须是1系数。多个浅层子网络代表的函数的组合函数，并不是真正意义上的深层特征网络。</p><blockquote><p><strong>神经网络的退化才是难以训练深层网络根本原因所在，而不是梯度消散。</strong>虽然梯度范数大，但是如果网络的可用自由度对这些范数的贡献非常不均衡，<strong>也就是每个层中只有少量的隐藏单元对不同的输入改变它们的激活值，而大部分隐藏单元对不同的输入都是相同的反应，此时整个权重矩阵的秩不高。并且随着网络层数的增加，连乘后使得整个秩变的更低。</strong>这也是我们常说的网络退化问题，虽然是一个很高维的矩阵，但是大部分维度却没有信息，表达能力没有看起来那么强大。<strong>残差连接正是强制打破了网络的对称性。</strong>(来自Orhan A E, Pitkow X. Skip connections eliminate singularities[J]. arXiv preprint arXiv:1701.09175, 2017.)</p><p>来自知乎专栏：<a href="https://zhuanlan.zhihu.com/p/42833949" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/42833949</a></p></blockquote><p>2.ResNet中恒等变换的意义目前有多种解释，我目前倾向的<a href="https://www.zhihu.com/question/293243905" target="_blank" rel="noopener">解释</a>是这种残差结构是为了动态自适应拟合训练数据集网络深度的一种方法，它除了可以平稳甚至加速训练外（跳级连接让不同分辨率的feature组合，梯度的相关性得到了保持，后续的DenseNet则用了更多的不同分辨率的特征组合），就是可以根据你的问题去慢慢补充浅层网络解决的最主要的拟合之外的小边边角角，让最后的函数更加平滑。网络输出的残差$F(x)$部分可能在某个block接近0，可能某个部分比较大，所以是一种自适应复杂度。</p><blockquote><p>ResNet是完成一个微分方程的差分形式，dx(t)/dt=H(t)=&gt;x(t+1)=x(t)+H(t). 所以ResNet就是用微分方程来描述一个动态系统，寻找合适的H(t)来构造系统。采用这个结构的好处是H(t)具有某种自适应特性（相比于普通的CNN），对网络的深度不会过于敏感（自适应差分方程的步长），同时简化网络的结构，让网络结构更加均匀化，从几何角度看就是制造了一个更为平直的流形并在其上进行优化，导致系统的收敛性更好。</p><p>不同的网络结构实际上构造了不同的解空间的流形，并且部分的定义了其上的度量，而不同的度量引出不同的曲率分布，这对系统的收敛性能影响重大。所以不同网络结构的差异表现在：（1）网络结构是否和问题匹配，是否是一个包含了可能解的足够紧致的空间；（2）由网络结构所部分定义的度量是否导致一个比较平直的均匀的流形（度量还被所采用的代价函数所部分定义）。从这两个角度就可以定性和半定量的分析不同网络结构的特性和关系。</p><hr><p>残差模块并不是就是恒等啊，我觉得可以理解为:当网络需要这个模块是恒等时，它比较容易变成恒等。而传统的conv模块是很难通过学习变成恒等的，因为大家学过信号与系统都知道，恒等的话filter的冲激响应要为一个冲激函数，而神经网络本质是学概率分布 局部一层不太容易变成恒等，而resnet加入了这种模块给了神经网络学习恒等映射的能力。</p><p>所以我个人理解resnet除了减弱梯度消失外，我还理解为这是一种自适应深度，也就是网络可以自己调节层数的深浅，不需要太深时，中间恒等映射就多，需要时恒等映射就少。当然了，实际的神经网络并不会有这么强的局部特性，它的训练结果基本是一个整体，并不一定会出现我说的某些block就是恒等的情况</p><p>来自知乎回答：<a href="https://www.zhihu.com/question/293243905" target="_blank" rel="noopener">https://www.zhihu.com/question/293243905</a></p></blockquote><p>在论文<a href="https://arxiv.org/abs/1603.09382" target="_blank" rel="noopener">Deep Networks with Stochastic Depth</a>中，作者设置概率p让residual block随机失活（均匀概率与线性概率，线性概率更有效些，符合ResNet的设计思想），只保留shortcut，结果精度也没有怎么下降，反而提高了训练速度，这也说明Identity Mapping的正确性。</p><p>对ResNet的改进以及后续其他网络：</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_8.jpg" alt="改进downsample部分，减小信息流失"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_9.JPG" alt="把激活函数和BN放在residual模块中，称其为full pre-activation，其效果最好。论文来自ResNet原班人马，Identity Mappings in Deep Residual Networks，在该论文中也分析讨论了有关ResNet的其他设置问题"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_10.JPG" alt="GoogLeNet引入ResNet结构，Inception-ResNet-v1"></p><p><a href="https://arxiv.org/pdf/1605.07146.pdf" target="_blank" rel="noopener">Wide Residual Networks</a>认为ResNet中有些block去掉也没有影响，所以residual是最主要的影响因素，作者尝试加倍block中的卷积核数量，而不是增加深度，结果发现效果更好，而且由于加深宽度可以由并行化计算解决，所以计算也更加快速。另外，作者也使用了卷积层上的dropout，结果也可以得到一些提升（论文指出何恺明他们是在identity part使用dropout效果不好，所以他们用在这，我想可能是因为拓宽了宽度，特征图增多，可能带来冗余信息，存在过拟合信息）。我没有仔细看论文，也没看源代码，不清楚作者是不是随机给特征图某个元素置0来进行的，实际上给卷积层做dropout的操作并不常见，一般都是BN就足够了，有部分研究是针对卷积层上或者特征图上的droput的，而且认为随机置某元素为0并不能起到作用，因为特征图一般是局部关联的，所以提出了区域置0，某个特征图置0等操作，见此知乎<a href="https://www.zhihu.com/question/52426832/answer/926104467" target="_blank" rel="noopener">回答</a>。</p><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Aggregated Residual Transformations for Deep Neural Networks</a>: ResNeXt，来自ResNet的作者，想法和上面的差不读，在一个block里采用多组重复卷积核操作，拓宽网络。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_11.JPG" alt="ResNet和ResNeXt结构差异"></p><p><a href="https://arxiv.org/pdf/1709.01507.pdf" target="_blank" rel="noopener">Squeeze-and-Excitation Networks</a> : SENet是在网络的特征通道之间进行建模，显示地通过FC去得到通道之间的依赖关系，然后利用最后的sigmoid激活函数给每个通道附一个新的权重，依照这个重要程度去提升有用的特征抑制用处不大的特征 (feature recalibration)。相关介绍见作者胡杰在机器之心的<a href="https://www.cnblogs.com/bonelee/p/9030092.html" target="_blank" rel="noopener">解释</a>。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_12.JPG" alt="在inception module和residual module中加入SE Block结构"></p><p><a href="https://arxiv.org/abs/1605.07648" target="_blank" rel="noopener">FractalNet: Ultra-Deep Neural Networks without Residuals</a>: 分形（从多个层次、多个角度、多个组成成分来共同解决一个问题）网络结构，类似于ResNet，但是认为深层网络的关键是浅层到深层的有效过度，residual representation不是必须的。分阶段组合不同分辨率特征，达到了类似于教师-学生机制、深度监督的效果。由于比较复杂，所以没有ResNet用的广发，但是思想很有深度，其中最突出的contribution就是drop path。在下图的示意中，从输入到预测有很多条路可以走通，drop path机制就是随机失活某些路径，假如其中一条承担的作用是比较大，那么一旦被失活，其他网络就得承担全部的责任，加强其学习力度，所以一方面防止过拟合，另一方面让整个网络变得非常鲁棒，不会出现退化问题，参考此<a href="https://blog.csdn.net/wspba/article/details/73251731" target="_blank" rel="noopener">博客</a>。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_13.JPG" alt="Fractal architecture"></p><p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">Densely Connected Convolutional Networks</a>: CVPR2017 Best paper，写的也非常简洁通透。其dense block的设计是ResNet的“完全版”，不仅提高了特征的利用率，减小了参数，也减轻了梯度弥散，每一dense block单独产生的特征图层数也相对较少，也便于训练，具体解读见此<a href="https://blog.csdn.net/u014380165/article/details/75142664" target="_blank" rel="noopener">博客</a>。但是一开始的DenseNet虽然参数少，但是由于复杂的的skip connection和concatenate操作，中间量的存在会占用很多显存，作者后续论文<a href="https://arxiv.org/abs/1707.06990" target="_blank" rel="noopener">Memory-Efficient Implementation of DenseNets</a>减缓了这个问题，现在PyTorch等框架也应该自动解决了这个问题，在一定程度上。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/9_14.JPG" alt="Dense Block结构，前面每一层都会和后面每一层沟通"></p><p>MobileNet, ShuffleNet等轻量级计算网络，通过引入depthwise separable convolution, pointwise group convolution, channel shuffle (顺序重分配每组的channel到新的组，减少每个组单独流动信息的局限性)等操作改变传统卷积计算方式，来加速计算。详见知乎白裳的<a href="https://zhuanlan.zhihu.com/p/35405071" target="_blank" rel="noopener">博客随笔</a>和AI之路的<a href="https://blog.csdn.net/u014380165/article/details/75137111" target="_blank" rel="noopener">详解</a>。</p><p>Meta-learning，NAS自动搜索合适架构，开山之作是Google的<a href="https://arxiv.org/abs/1611.01578" target="_blank" rel="noopener">Neural Architecture Search with Reinforcement Learning </a>，通过RNN确定架构中每个元素的参数，然后利用reinforcement learning奖励精度高的，惩罚精度差的，以此来学习。NAS领域交叉知识多，暂时无法深入理解。</p><h2 id="Assignment-2"><a href="#Assignment-2" class="headerlink" title="Assignment 2"></a>Assignment 2</h2><p>宝藏作业，值得认真做。</p><p><a href="http://cs231n.github.io/assignments2019/assignment2/" target="_blank" rel="noopener">assignment page</a>;</p><p>reference：</p><ul><li><p><a href="https://github.com/Wangxb06/CS231n" target="_blank" rel="noopener">github-code</a>;</p></li><li><p>参考培训机构深度之眼的cs231n-camp的文档</p></li></ul><p>具体过程和结果参见这个<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments" target="_blank" rel="noopener">repository</a></p><p>在做CNN那一节时（ubuntu环境，如果是Windows环境可能需要安装python for c++等库），如果在fast_layer.py环节出现<code>CS231n A2: Global name &#39;col2im_6d_cython&#39; is not defined解决</code>，请关掉jupyter notebook，然后重新编译就可以了。</p><h2 id="Lecture-10-Recurrent-Neural-Networks"><a href="#Lecture-10-Recurrent-Neural-Networks" class="headerlink" title="Lecture 10. Recurrent Neural Networks"></a>Lecture 10. Recurrent Neural Networks</h2><p>这一节课主要讲RNN的网络结构（recursive neural network是树结构，recurrent neural network是chain结构），设计文本生成，image caption, visual question之类的网路，后面通过training问题提及了下LSTM。课程的目的在于希望NLP和CV两者结合，相辅相成，解决更为智能化的问题。</p><p>参考阅读材料方面主要是花书《deep learning》中的<a href="http://www.deeplearningbook.org/contents/rnn.html" target="_blank" rel="noopener">第十章</a>: Sequence Modeling: Recurrent and Recursive Nets. 其内容不难理解，也都是介绍性文字，大部分细节都是列出论文便于感兴趣读者去进一步阅读。（个人觉得看好Justin Johnson的<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture10.pdf" target="_blank" rel="noopener">课件</a>就差不多了，主要内容都是RNN结构和LSTM）</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/10_1.JPG" alt="一个典型RNN结构，其中对每个类型变量的权重都是相同的，会重复使用"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/10_2.JPG" alt="原生RNN梯度回传会出现弥散或者爆炸现象，难以训练"></p><p><img src="/2020/02/29/Stanford-CS231n-笔记/10_3.JPG" alt="long-short-term-memory (LSTM)修正梯度回传问题"></p><p>这里有一篇讲解LSTM的<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">blog</a>（嫌麻烦的可以直接看<a href="https://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="noopener">中文翻译版本</a>）。</p><p>有关Justin Johnson课上提到的RNN的一些fun application可以在karpathy的这篇<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">blog</a>找到。</p><p>目前，有关机器翻译，文本生成等NLP问题自谷歌的”Attention is all you need“之后，基本上都直接采用self-attention机制了，但这并不代表RNN没有用，已被淘汰了，其在处理依赖时间序列问题还是有很多用武之地的。</p><h2 id="Lecture-11-Gnerative-Models"><a href="#Lecture-11-Gnerative-Models" class="headerlink" title="Lecture 11. Gnerative Models"></a>Lecture 11. Gnerative Models</h2><p>这节课没有官方推荐阅读材料，只有<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture11.pdf" target="_blank" rel="noopener">slides</a>首先介绍监督学习和分监督学习的一些区别（有无外部label带来的监督信号，前者着重利用数据做任务，后者着重学习数据内部的结构和分布等）。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/11_1.JPG" alt="supervised v.s. unsupervised learning"></p><p>后面主要讲无监督学习中的一个核心问题density estimation，介绍解决此问题的三类generative models：</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/11_2.JPG" alt="PixelRCNN/CNN, VAE, GAN"></p><p>目前在生成式模型方面还未进行研究，所以暂时也没有啥思考，也就单纯把这节课当成一次introduction。课程中有关这三类模型的概念的公式也不搬过来了，直接看课件就好。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/11_3.JPG" alt="三类生成式模型比较"></p><p>后续会好好研究一下GAN，那时会再专门写笔记讨论体会心得，先放几个不错的resources链接：</p><ul><li>review-<a href="https://arxiv.org/abs/2001.06937" target="_blank" rel="noopener">A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications</a></li><li>paper list-<a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">the-gan-zoo</a></li><li>how-to-train-gan-<a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">ganhacks</a></li><li>知乎文章-<a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN</a></li><li>researcher-<a href="https://people.csail.mit.edu/junyanz/" target="_blank" rel="noopener">Jun-Yan-Zhu</a></li><li>Stanford-cs236-<a href="https://deepgenerativemodels.github.io/" target="_blank" rel="noopener">deep generative models</a></li><li>UCB-cs294-<a href="https://sites.google.com/view/berkeley-cs294-158-sp19/home" target="_blank" rel="noopener">deep unsupervised learning</a></li></ul><h2 id="Lecture-12-Detection-and-Segmentation"><a href="#Lecture-12-Detection-and-Segmentation" class="headerlink" title="Lecture 12. Detection and Segmentation"></a>Lecture 12. Detection and Segmentation</h2><p>这节课介绍了计算机视觉中比较重要的四个任务：classification, semantic segmentation, object detection, instance segmentation, <a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture12.pdf" target="_blank" rel="noopener">slides</a>做的很不错。</p><p>此外，18版的课程里有个两个相关的discussion section：<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds06.pdf" target="_blank" rel="noopener">Practical Object Detection and Segmentation</a>，列出了经典的检测和分割结构，并给出了相应的代码实现链接和一些其他resources；<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds08.pdf" target="_blank" rel="noopener">Video Understanding</a>.</p><p>1.semantic segmentation主要通过FCN的结构介绍了downsampling—upsampling这种经典结构，重点引出一般的upsampling的方法，比如unpooling (一种是最近邻补值，一种是记下pooling的索引，然后其他补零)和转置卷积（利用权重矩阵乘以每个元素，然后得到大的特征图区域，overlap的地方累加&lt;累加只是一种处理手段，这方面或许可以有更好的办法恢复信息&gt;）</p><p>转置卷积transpose convolution： upsampling的一种 ，unpooling, max unpooling, 为什么叫转置卷积更好点，Justin Johnson在课上也给了数学解释。（<a href="https://www.zhihu.com/question/43609045/answer/132235276" target="_blank" rel="noopener">知乎问题</a>）</p><p>空洞卷积Dilated Convolution：源自于语义分割领域，目前也大多用于语义分割领域。初衷是为了解决downsample丢失的信息会影响pixel-wise的分类。“dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息”（<a href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener">知乎问题</a>）</p><p>2.object detection和instance detection是交叉介绍的，因为后者是前者的集成，也顺带提及了human pose estimation，keypoint。这一部分的介绍相比2017版多了一些细节（尤其是RoI pooling&lt;浮点数两次转整型处理，丢失proposal原始像素信息&gt;和RoI Align&lt;保留浮点数，利用双线性插值得到每个max pooling小区域的四个点，对小目标更好&gt;）和scene graph, 3D检测，3D shape prediction等部分（3D Machine Learning应该是目前工业界比较看重的一个领域，在无人驾驶，增强现实等领域可以发挥作用）。其中object detection部分着重介绍了R-CNN系列，更详细的解释可以阅读另一篇<a href="http://densecollections.top/2019/11/30/RCNN-series-in-object-detection/">博客</a>。课程中也推荐一篇综述类型的paper: <a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="noopener">Speed/accuracy trade-offs for modern convolutional object detectors</a>便于进一步了解（在我的另一篇R-CNN检测专题<a href="http://densecollections.top/2020/01/10/RCNN-series-in-object-detection-续/">博客</a>会有所提及）。</p><p>可变形卷积Deformable convolutional networks：受Andrew Zisserman的STN (Spatial Transformer Network) 启发，针对物体的形状自发的选择卷积的区域，而不是按照约定的矩形计算规则，更适合实际。在<a href="https://www.zhihu.com/question/57493889" target="_blank" rel="noopener">知乎回答</a>中，有人说可变性卷积是对空洞卷积的一种generalization，增大了感受野 。</p><h2 id="Lecture-13-Visualizing-and-Understanding"><a href="#Lecture-13-Visualizing-and-Understanding" class="headerlink" title="Lecture 13. Visualizing and Understanding"></a>Lecture 13. Visualizing and Understanding</h2><p>1.试图理解卷机神经网络是怎么做决策的，这些layers到底学了什么东西。因此引入了特征图可视化，看看网络对图像的那些区域或者blob甚至是pixel反应大些（感受野，遮挡，<strong>回传对pixel的梯度，gradient ascent</strong>等），激活的是哪些地方（这里面的针对像素的梯度上升方法与学习权重等参数的方法是对立统一的，固定神经元参数，给定zero 图像，加一些正则化方法，然后对图像进行优化，看看什么样的图像可以获得更高的类别分数），然后借此产生对抗样本adversarial example试图欺骗网络（17年的最后一课就是请Ian Goodfellow讲这个）。</p><p>2.利用可视化思想通过选定特定的layer对输入图像做gradient，然后更新图像，放大特征，产生<a href="https://github.com/google/deepdream" target="_blank" rel="noopener">DeepDream</a>效果；</p><p>3.Neural Texture Synthesis, 利用<em>Gram matrix</em>对小区域图像进行延展；</p><p>4.Neural Style Transfer, 对输入图像进行特定风格的渲染。这项技术现在已经被商业化了，有不少手机APP和其他应用都有此功能！Justin Johnson也有个torch的<a href="https://github.com/jcjohnson/neural-style" target="_blank" rel="noopener">实现</a>。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/13_1.JPG" alt="Neural Style Transfer model"></p><p>但是得到一张转换的图片很慢，因为有很多的forward和backword，因此J.J做了个<a href="https://github.com/jcjohnson/fast-neural-style" target="_blank" rel="noopener">Fast Style Transfer</a>, 训练另外一个网络来实现style transfer, 其中IN (instance norm)就是为了快速风格转换发明的。课程的<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture13.pdf" target="_blank" rel="noopener">slides</a>里也提供了很多相关的paper，对细节感兴趣的可以看看。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/13_2.JPG" alt></p><p>有关神经网络可视化，个人觉得是个很有意义也和好玩的一个领域，它在一定程度上解释了模型为什么work，也可以结合一些idea做出可消费的funny application出来。<del>Assignment3也有相关的作业，到时候应该可以深入学习理解下。</del>从assignment3来看，为了让输入图像的风格靠近目标图像 (style loss)，采用gram矩阵来编码模型各个层产生的通道之间的相关性，然后尽量让两幅图像的gram矩阵（以指定维度方向上的各个向量之间做内积）相近。gram矩阵代表图像特征中哪些特征是同时出现的，哪些是此消彼长，不相干的，而特征呢就相当于图像的某个特性，比如在修图的时候，我们会调调对比度，加些高光，调调饱和度等，为了得到我们想要的风格的图像，其中有些修图选项可以一起做，有些往大的值调，有些就得往小的调，gram矩阵也是类似的操作，缩小目标gram矩阵和输入gram矩阵之间的差异就等于是给图像加滤镜。此外，gram矩阵的对角线上还包含了每个特征图的原始信息，也就是说不仅有哪些特征（哪些修图选项），也有各个特征之间的联系（修图选项的调整值），因此可以大致代表图像的风格。（相关<a href="https://www.zhihu.com/question/49805962?sort=created" target="_blank" rel="noopener">知乎问题</a>)</p><p>相关blog: Hungryof的“谈谈图像的style transfer”系列—-<a href="https://blog.csdn.net/Hungryof/article/details/53981959" target="_blank" rel="noopener">1</a>, <a href="https://blog.csdn.net/Hungryof/article/details/71512406" target="_blank" rel="noopener">2</a>, <a href="https://blog.csdn.net/Hungryof/article/details/80310527" target="_blank" rel="noopener">3</a></p><h2 id="Lecture-14-Deep-Reinforcement-Learning"><a href="#Lecture-14-Deep-Reinforcement-Learning" class="headerlink" title="Lecture 14. Deep Reinforcement Learning"></a>Lecture 14. Deep Reinforcement Learning</h2><p>对于强化学习的印象只停留在一个框架的系统层面上：输入是状态（state），动作（action）和奖励（reward），输出时方案或者策略（policy），大意就是通过奖惩机制，在某个环境中为对象制定在初始状态实现目标的一系列策略。</p><p>强化学习的热度似乎没有神经网络，GAN等技术高，而且据我身边研究的朋友说，强化学习的学习难度有点高，具体没有细问，可能是模型，critic制作和训练的难度不低。一位bloger在他对强化学习简介的<a href="https://www.cnblogs.com/geniferology/p/what_is_reinforcement_learning.html" target="_blank" rel="noopener">blog</a>中说到：</p><blockquote><p>强化学习的领导研究者<a href="http://www.incompleteideas.net/" target="_blank" rel="noopener">Richard S. Sutton</a>认为，只有这种学习法才考虑到 <em>自主个体</em>、<em>环境</em>、<em>奖励</em> 等因素，所以它是人工智能中最 top-level 的 architecture，而其他人工智能的子系统，例如 logic 或 pattern recognition，都应该在它的控制之下，我觉得颇合理。</p></blockquote><p>个人认为，目前的强化学习方式确实是一个可以融合许多机器学习技术和模型的一个理论框架，视觉中的注意力机制可以用到，NAS中也可以用到，随着时间推移，可能会有更多的结合和应用会产生，甚至引导未来很多产业的落地。</p><p>一些相关resources:</p><ul><li><p><a href="https://www.zhihu.com/question/49230922/answer/115011594" target="_blank" rel="noopener">知乎—强化学习（reinforcement learning)有什么好的开源项目、网站、文章推荐一下</a></p></li><li><p><a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course" target="_blank" rel="noopener">Deep_reinforcement_learning_Course</a></p></li><li><p><a href="https://pathmind.com/wiki/deep-reinforcement-learning" target="_blank" rel="noopener">A Beginner’s Guide to Deep Reinforcement Learning</a></p></li><li><p><a href="http://www.incompleteideas.net/book/the-book.html" target="_blank" rel="noopener">textbook—Reinforcement Learning: An Introduction</a></p></li></ul><p>Besides，这<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture14.pdf" target="_blank" rel="noopener">课件</a>做得也挺好的，虽然Serena Yeung小姐姐讲的课让我一知半解，云里雾里的 :(</p><h2 id="Assignment-3"><a href="#Assignment-3" class="headerlink" title="Assignment 3"></a>Assignment 3</h2><p><a href="https://cs231n.github.io/assignments2019/assignment3/" target="_blank" rel="noopener">assignment page</a>;</p><p>reference：</p><ul><li><p><a href="https://github.com/FortiLeiZhang/cs231n/tree/master/code/cs231n/assignment3" target="_blank" rel="noopener">github-code</a>;</p></li><li><p>参考培训机构深度之眼的cs231n-camp的文档</p></li></ul><p>具体过程和结果参见这个<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments" target="_blank" rel="noopener">repository</a></p><p>COCO_Caption数据集下载速度很能很慢，将链接放至迅雷可能快点，我这边是1～2M/s。</p><p>做<code>RNN_Caption.ipynb</code>时出现类似assignment1的无法到处<code>imread,imresize</code>问题，原因就是<code>scipy</code>包现在不支持这个函数了，直接将<code>scipy</code>从1.4退回到1.2.0即可。</p><p>在执行<code>Look at the data</code>部分时出现<code>[WinError 10054] 远程主机强迫关闭了一个现有的连接</code>错误，按照<a href="http://188.131.244.232/article/10" target="_blank" rel="noopener">此博客</a>的解释，可能是链接存在一些无法访问的情况，将“http”改成”https”即可，<code>url = url.replace(http://&quot;, &#39;https://&#39;)</code></p><p>改完之后可以顺利访问，但是出现<code>进程被占用</code>情况，解决方法同上面博客，修改<code>image_utils.py</code>中的<code>image_from_url</code>函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def image_from_url(url):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Read an image from a URL. Returns a numpy array with the pixel data.</span><br><span class="line">    We write the image to a temporary file then read it back. Kinda gross.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    try:</span><br><span class="line">        f = urllib.request.urlopen(url)</span><br><span class="line">        #_, fname = tempfile.mkstemp()</span><br><span class="line">        # 为了解决“另一个程序正在使用此文件，进程无法访问的错误&quot;</span><br><span class="line">        fd, fname = tempfile.mkstemp(dir=&quot;G:\CS231N&quot;)</span><br><span class="line">        with open(fname, &apos;wb&apos;) as ff:</span><br><span class="line">            ff.write(f.read())</span><br><span class="line">        img = imread(fname)</span><br><span class="line">        # 在删除临时文件前关闭该文件，防止被占用</span><br><span class="line">        os.close(fd)</span><br><span class="line">        os.remove(fname)</span><br><span class="line">        return img</span><br><span class="line">    except urllib.error.URLError as e:</span><br><span class="line">        print(&apos;URL Error: &apos;, e.reason, url)</span><br><span class="line">    except urllib.error.HTTPError as e:</span><br><span class="line">        print(&apos;HTTP Error: &apos;, e.code, url)</span><br></pre></td></tr></table></figure><p>在<code>NetworkVisualization-Pytorch.ipynb</code>的<code>load some ImageNet images</code>中，运行报错<code>Object arrays cannot be loaded when allow_pickle=False</code>，通过<a href="https://blog.csdn.net/weixin_42096901/article/details/89855804" target="_blank" rel="noopener">这篇博客</a>发现是自己<code>numpy</code>版本过高问题，将其回退到1.16.1或1.16.2即可解决</p><p>在使用pytorch时，要注意对<code>requires_grad=True</code>的Tensor和求梯度阶段需要用到的Tensor都不能用inplace operation，也就是不能直接对这些张量进行数值操作，最好加上.detach()曲线救国，见<a href="https://zhuanlan.zhihu.com/p/38475183" target="_blank" rel="noopener">此</a></p><p>在<code>Generative_Adversarial_Networks_PyTorch.ipynb</code>中下载MNIST可能很慢，可以直接找到数据网址自己下载好数据集，然后放到代码自动创建的存放数据的文件夹中的<code>raw</code>中，再次运行就可以不用在线下载，而是直接读取在process了。还有一种办法是改变源代码的中的urls到本地，详情可见<a href="https://blog.csdn.net/york1996/article/details/81780065" target="_blank" rel="noopener">here</a>.</p><h2 id="2017-Invited-Talk-Efficient-Methods-and-Hardware-for-Deep-Learning-Song-Han"><a href="#2017-Invited-Talk-Efficient-Methods-and-Hardware-for-Deep-Learning-Song-Han" class="headerlink" title="2017 Invited Talk-Efficient Methods and Hardware for Deep Learning, Song Han"></a>2017 Invited Talk-Efficient Methods and Hardware for Deep Learning, Song Han</h2><p>CS231N每年都会邀请不同的专家来做一些专题介绍，由于2017年以后都没release视频了，所以就直接看17年的talk了，正好17年的这两个topic都比较不错，在应用上都很重要，因此也很值得学习下。</p><ul><li><p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture15.pdf" target="_blank" rel="noopener">课件</a></p></li><li><p><a href="https://songhan.mit.edu/" target="_blank" rel="noopener">Song Han</a></p></li><li><p>17年的旷视也有一节课专门讲神经网络压缩，<a href="https://www.bilibili.com/video/BV1E7411t7ay?p=9" target="_blank" rel="noopener">链接</a></p></li></ul><p>韩松在该领域算是比较又名的一位，他在此节课从算法讲到硬件（分inference和training两部分），非常系统。虽然大多是overview，但是包含的内容很多，后续研究压缩领域也会从他主页中列举的资料来入门和学习。</p><p>韩松教授2017年的phd thesis: <a href="[https://stacks.stanford.edu/file/druid:qf934gh3708/EFFICIENT%20METHODS%20AND%20HARDWARE%20FOR%20DEEP%20LEARNING-augmented.pdf](https://stacks.stanford.edu/file/druid:qf934gh3708/EFFICIENT METHODS AND HARDWARE FOR DEEP LEARNING-augmented.pdf">EFFICIENT METHODS AND HARDWARE FOR DEEP LEARNING</a>)</p><p><a href="https://songhan.github.io/DSD/" target="_blank" rel="noopener">DSD (Dense-Sparse-Dense Training) Model Zoo</a></p><h2 id="2017-Invited-Talk-Adversarial-Examples-and-Adversarial-Training-Ian-Goodfellow"><a href="#2017-Invited-Talk-Adversarial-Examples-and-Adversarial-Training-Ian-Goodfellow" class="headerlink" title="2017 Invited Talk-Adversarial Examples and Adversarial Training, Ian Goodfellow"></a>2017 Invited Talk-Adversarial Examples and Adversarial Training, Ian Goodfellow</h2><p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture16.pdf" target="_blank" rel="noopener">课件</a></p><p>Ian Goodfellow的语速很慢，然而我整节课却没怎么听懂他在讲什么。</p><ul><li><p>知乎文章<a href="https://zhuanlan.zhihu.com/p/42667844" target="_blank" rel="noopener">对抗样本Adversarial Examples</a></p></li><li><p>知乎问题<a href="https://www.zhihu.com/question/49129585" target="_blank" rel="noopener">如何看待机器视觉的“对抗样本”问题，其原理是什么？</a></p></li></ul><h2 id="Lecture-17-Human-Centered-AI"><a href="#Lecture-17-Human-Centered-AI" class="headerlink" title="Lecture 17. Human-Centered AI"></a>Lecture 17. Human-Centered AI</h2><p>李飞飞最后的总结性课程，描绘AI对社会和人类的影响和蓝图，提出HAI的概念。</p><p><img src="/2020/02/29/Stanford-CS231n-笔记/17_1.JPG" alt="HAI"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://cs231n.stanford.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;homepage&lt;/a&gt;;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/av13260183?from=search&amp;amp;seid=2308745029556209710&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2017 lecture video&lt;/a&gt;;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21930884&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;中文笔记&lt;/a&gt;&lt;/li&gt;&lt;li&gt;课程相关时间安排，课件，参考资料，作业详见homepage链接，以最新版为主；&lt;/li&gt;&lt;li&gt;官方的assignments和notes做得非常好，强烈推荐学习和反复观看；&lt;/li&gt;&lt;li&gt;时间原因，学习19版的时候恰好2020版春季课程将要开始，这一版换了team，里面加了不少新的research介绍，尤其是非监督，自监督之类的，因此会加入一下2020版的内容；&lt;/li&gt;&lt;li&gt;个人比较喜欢Justin Johnson的讲课风格和深度，因此配合他在UMICH (University of Michigan) 的 &lt;a href=&quot;https://web.eecs.umich.edu/~justincj/teaching/eecs498/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;EECS498: Deep Learning for Computer Vision&lt;/a&gt;课一起看，可以相互补充（和CS231N有很大重叠， 但也些不同）；&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;Lecture1-Introduction&quot;&gt;&lt;a href=&quot;#Lecture1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;Lecture1. Introduction&quot;&gt;&lt;/a&gt;Lecture1. Introduction&lt;/h2&gt;&lt;p&gt;Related courses in Stanford:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;CS131: Computer Vision: Foundations and Applications&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CS231a: Computer Vision, from 3D Reconstruction to Recognition&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CS 224n: Natural Language Processing with Deep Learning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CS 230: Deep Learning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Andrew Ng的CS229虽然不在列表中，但个人觉得也值得一看，毕竟也属于经典ML课程。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A Brief history of Computer Vision and CS 231n:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从史前生物产生视觉开始，到后面的人类对于视觉的研究：相机，生物学研究，以及陆续地对于计算机视觉/机器视觉的系统研究，在社会各个领域结合产生的特定任务的建模设计等工作，这里再一次提到了David Marr的《vision》一书对整个计算机视觉领域的奠基于推动作用。&lt;/li&gt;&lt;li&gt;介绍卷积神经网络CNN对计算视觉的推动作用，简介了CNN的历史以及在各个视觉有关问题上的强大作用。&lt;/li&gt;&lt;li&gt;推荐课本教材：《Deep Learning》 by Goodfellow, Bengio, and Courville.&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="课程记录" scheme="http://densecollections.top/categories/%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="public course" scheme="http://densecollections.top/tags/public-course/"/>
    
      <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
      <category term="Standford" scheme="http://densecollections.top/tags/Standford/"/>
    
  </entry>
  
  <entry>
    <title>旷视2017年深度学习实践课程</title>
    <link href="http://densecollections.top/2020/02/25/%E6%97%B7%E8%A7%862017%E5%B9%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E8%AF%BE%E7%A8%8B/"/>
    <id>http://densecollections.top/2020/02/25/旷视2017年深度学习实践课程/</id>
    <published>2020-02-25T03:57:44.000Z</published>
    <updated>2020-02-25T05:13:59.991Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><ul><li><a href="https://www.bilibili.com/video/av88056282/" target="_blank" rel="noopener">视频</a>;</li><li>课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取;</li><li>这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好。</li></ul><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul><li><p>computer vision 在AI中的地位属于感知（perception）智能（还包括speech），另外一块是认知（cognitive），包括NLP和AGI（通用人工智能）；</p></li><li><p>人使用眼睛和大脑认识世界，电脑使用图像传感器和算力来视觉感知周围环境；</p></li><li><p>大脑皮层的出现，灵活，结构化，计算处理；</p></li><li><p>CV和AI的关系：其中非常重要的一个task/不同的研究工作和成果/作为关键应用；</p></li><li><p>现阶段的CV任务：classification (image)/ detection (region) /segmentation (pixel，实例，语义，全景)/ sequence (video，spatial+temporal)；</p></li><li><p>David Marr 的《vision》一书，这在visual SLAM中也十分重要，视觉知识的表示，part representation (拆成块，用各种模型表示，举例关键点检测);</p></li><li><p>part representation存在局限，有些不可分，引发了神经网络第二次复兴，Yann Lecun 的卷积神经网络应用于手写字体识别和人脸检测。由于当时难以复现，且懂的人不多，加上小规模数据和SVM等模型流行，神经网络出现衰落；</p></li><li><p>learning-based representation/ feature-based representation，特征工程+分类器（handcraft features engineering+SVM/Random Forest），浅层学习pipeline a short sequence；</p></li><li><p>端到端学习，所有参数联合优化 ，a long or very long sequence实现高维非线性映射；</p></li><li><p>受感知机启发的多层感知机（multilayer perceptron，MLP），利用backpropagation (BP) 梯度训练逼近（局部最优解）任意非线性函数；</p></li><li>90年代的神经网络成果：CNN/ autocoder/ boltzmann machine/ belief nets/ RNN；</li><li>复兴：data+computing+industry competition+a few breakthrough；</li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/a_brief_history.JPG" alt="深度学习沉浮历史"></p><ul><li><p>Resnet的思想：由浅到深学习，保持梯度数值较大，防止梯度消失 ；</p></li><li><p>从以前的手工设计feature为重点到现在设计网络结构（2012-2017为止）为重点，不同的结构所需算力不同，现在轻量级网络是一个热点；</p></li><li>卷积核的方式：1x1，3x3，<a href="https://blog.csdn.net/tintinetmilou/article/details/81607721" target="_blank" rel="noopener">depthwise 3x3</a>等，网络layer连接的方式；</li></ul><h2 id="2-Math-in-DL-and-ML-Basics"><a href="#2-Math-in-DL-and-ML-Basics" class="headerlink" title="2. Math in DL and ML Basics"></a>2. Math in DL and ML Basics</h2><ul><li><p>深度学习的内涵：deep learning) representation learning) machine learning) AI;</p></li><li><p>Linear Algebra：</p><ul><li><p>向量，矩阵，集合，群，封闭性，矩阵乘法是为了表示一种变换关系，向量映射到另一个向量；</p><ul><li>方阵，正交矩阵，特征值，特征向量，实对称矩阵，二次型，正定矩阵，半正定矩阵，奇异值分解；</li></ul></li></ul></li><li><p>Probability：</p><ul><li><p>随机事件，随机变量，概率密度函数，联合分布，边缘分布，条件分布，独立变量；</p></li><li><p>贝叶斯法则，先验分布，后验分布，期望，方差，协方差矩阵（半正定）；</p></li><li><p>常见分布：二值分布，二项分布，多值/多相分布（图像分类问题），正态分布（高斯分布）；</p></li><li><p>信息熵（分类中的交叉熵损失函数，发生概率越大的事情信息越不值钱），交叉熵和KL-divergence，生成式模型中的wassertein distance；</p></li></ul></li><li><p>Optimize：</p><ul><li>minimization（最小化）— 梯度下降gradient descent（步长的选取很关键），stochastic gradient descent;</li></ul></li><li><p>机器学习基本知识（machine learning basics）— 定义，假设，模型，评估，supervised &amp; unsupervised learning (learning $p(y | x) \quad or \quad p(x,y)$，判别式模型，生成式模型&lt;目前都用判别式模型&gt;, learning $p(x)$，auto encoder，GAN)，“no free lunch theorem”（all learning algorithms are equal, but some algorithm are more equal than others），overfitting &amp; underfitting，model capacity vs. generalization error，regularization (正则项，数据增强，parameter reduce and tying);</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/supervised&amp;unsupervisedlearning.JPG" alt></p><h2 id="3-Neural-Networks-Basics-amp-Architecture-Design"><a href="#3-Neural-Networks-Basics-amp-Architecture-Design" class="headerlink" title="3. Neural Networks Basics &amp; Architecture Design"></a>3. Neural Networks Basics &amp; Architecture Design</h2><ul><li><p>Fundamental task in CV: classification, object detection, semantic segmentation, instance segmentation, keypoint detection, human pose estimation, VQA…</p></li><li><p>计算机识别图像的难点：图像内容的复杂性和多样性，比如姿势，光照，模糊等；</p></li><li><p>特征是计算机认识图像的一个灯塔，且应当使用非线性特征抽取器；</p></li><li><p>线性组合特征（kernel learning，boosting），缺点是需要大量的templates，对特征的利用性差；</p></li><li><p>特征层级组合，重复利用特征，更为高效 —-&gt; concepts reuse in DL，网络层级的特征也是由低到高，但是这样高度非线性的函数难以优化（目前采用收敛到局部最优值）；</p></li><li><p>key ideas of DL: nolinear system, learn it from data, feature hierarchies, end-to-end learning；</p></li><li><p>激活函数，神经元，全连接网络，训练决定网络参数（前向，反向，更新）；</p></li><li><p>针对图像的认识从locally-connected net到convolutional net的设计，参数共享；</p></li><li><p>卷积层的卷积操作，pooling layer等；</p></li><li>网络结构设计：网络拓扑结构，layer function，超参，优化算法等经验性的东西，手动/autoML；</li><li>简介AlexNet（包含LRN，加速收敛），VGG（发掘3x3小卷积核的显著作用，但并不代表最高效的做法），GoogleNet，ResNet（拟合残差而不是直接拟合原函数），Xception，ResNeXt (借鉴Xception在resnet基础改进)，ShuffleNet，DesneNet，SqueezeNet；</li><li>structure design: deeper and wider, ease of optimization, multi-path design, resdiual path, sparse connection；</li><li>简介部分layer design：SPP，batch normalization，parametric rectifiers，bilinear CNNs（做细粒度分类）；</li><li>针对特定任务的结构设计：Deepface (人脸识别)，Global Convolutional Networks (语义分割)，Hourglass Networks (沙漏结构，大的感受野，用于pose estimation或者关键点)；</li></ul><h2 id="4-Introduction-to-Computation-Technologies-in-Deep-Learning"><a href="#4-Introduction-to-Computation-Technologies-in-Deep-Learning" class="headerlink" title="4. Introduction to Computation Technologies in Deep Learning"></a>4. Introduction to Computation Technologies in Deep Learning</h2><p>该节课偏底层，听的不是很懂，权当了解。</p><ul><li><p>symbolic computation：</p><ul><li><p>深度学习框架overview—program, compilation, runtime mangement, kernels, hardware；</p></li><li><p>computing graph, graph structure—variable, operator, edge；</p></li><li><p>静态图和动态图；</p></li><li><p>执行和优化；</p></li></ul></li><li><p>dense numerical computation:</p><ul><li><p>CPU computation (机器码，流水线，超流水线，超标量，乱序执行/cache hierarchy/…)；</p></li><li><p>other computation devices (NVIDIA GPU&lt;单指令，多线程架构&gt;，Google TPU，Huawei NPU in Kirin 970，Mobile CPU+GPU+DSP)；</p></li><li><p>computation &amp; memory gap；</p></li></ul></li><li><p>distributed computation:</p><ul><li><p>system (communication，Remote Direct Memory Access);</p></li><li><p>optimization algorithm (synchronous SGD，asynchronous SGD)；</p></li><li><p>communication algorithm (MPI Primitives，An AllReduce Algorithm)；</p></li></ul></li></ul><h2 id="5-Neural-Network-Approximation-low-rank-sparsity-and-quantization"><a href="#5-Neural-Network-Approximation-low-rank-sparsity-and-quantization" class="headerlink" title="5. Neural Network Approximation(low rank, sparsity, and quantization)"></a>5. Neural Network Approximation(low rank, sparsity, and quantization)</h2><p>该节课着重神经网络压缩，for faster training，faster inference， smaller capacity;</p><p>convolution as matrix product，利用近似权重矩阵达到网络压缩的目的;</p><p>Low Rank (本质是对矩阵进行一系列分解变换近似操作，减小计算量和存储量):</p><ul><li><p>对权重矩阵进行奇异值分解，singular value decomposition；</p></li><li><p>SVD+Kronecker Product ——&gt; KSPD；</p></li><li><p>矩阵分解：C-HW-K====》C-HW-R-(1X1)-K，然后通过reshape进行重新分解，目前horizontal-vertical decomposition最好；</p></li><li><p>shared group convolution is a kronrcker layer；</p></li><li><p>CP-decomposition与depthwise；</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/CNN_decomposition.JPG" alt></p><p>Sparse Approximation:</p><ul><li><p>权重分布有点类似高斯分布 ，0附近很多，微调网络，weight pruning: 韩松博士的deepcompression，让为0的权重逐渐增多（掩模矩阵使权重为0），不让0附近的权重在训练时抖动，FC层效果压缩明显；</p></li><li><p>网络加速计算—稀疏矩阵计算，channel purning，sparse communication for distributed gradient descent；</p></li></ul><p>Quantization：</p><ul><li><p>用什么精度算；</p></li><li><p>参数的量化，激活的量化，梯度的量化；</p></li><li><p>二值化，binary network；</p></li><li><p>大容量模型利用小bit训练时掉点不明显，小容量模型视情况而定；</p></li></ul><p>主讲人<a href="http://zsc.github.io/" target="_blank" rel="noopener">周舒畅</a>推荐的几篇文章，其中XNOR-Net为课程阅读要求材料：</p><blockquote><p>Bit Neural Network<br>● Matthieu Courbariaux et al. BinaryConnect: Training Deep Neural Networks with binary<br>weights during propagations. <a href="http://arxiv.org/abs/1511.00363" target="_blank" rel="noopener">http://arxiv.org/abs/1511.00363</a>.<br>● Itay Hubara et al. Binarized Neural Networks <a href="https://arxiv.org/abs/1602.02505v3" target="_blank" rel="noopener">https://arxiv.org/abs/1602.02505v3</a>.<br>● Matthieu Courbariaux et al. Binarized Neural Networks: Training Neural Networks with<br>Weights and Activations Constrained to +1 or -1. <a href="http://arxiv.org/pdf/1602.02830v3.pdf" target="_blank" rel="noopener">http://arxiv.org/pdf/1602.02830v3.pdf</a>.<br>● Rastegari et al. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural<br>Networks <a href="http://arxiv.org/pdf/1603.05279v1.pdf" target="_blank" rel="noopener">http://arxiv.org/pdf/1603.05279v1.pdf</a>.<br>● Zhou et al. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with<br>Low Bitwidth Gradients <a href="https://arxiv.org/abs/1606.06160" target="_blank" rel="noopener">https://arxiv.org/abs/1606.06160</a>.<br>● Hubara et al. Quantized Neural Networks: Training Neural Networks with Low Precision<br>Weights and Activations <a href="https://arxiv.org/abs/1609.07061" target="_blank" rel="noopener">https://arxiv.org/abs/1609.07061</a>.</p></blockquote><h2 id="6-Modern-Object-Detection"><a href="#6-Modern-Object-Detection" class="headerlink" title="6. Modern Object Detection"></a>6. Modern Object Detection</h2><p><a href="https://zhuanlan.zhihu.com/p/62372897" target="_blank" rel="noopener">anchor-free与anchor-based的交替轮回</a>;</p><p>Representation:</p><ul><li><p>Bounding-box: face detection, human detection, vehicle detection, text detection, general object detection;</p></li><li><p>Point: semantic segmentation (下一节课);</p></li><li><p>Keypoint: face landmark, human keypoint;</p></li></ul><p>Evaluation Criteria：</p><ul><li>precision (预测为真的里面真正是真的比例), recall (所有是真的里面预测为真的比例), Average Prcision (AP), mean AP (mAP)，IoU, mmAP(coco);</li></ul><p>Perform a detection:</p><ul><li><p>之前是手工特征+图像金字塔+滑动窗口+分类器（robust real-time detection; IJCV 2001）；</p></li><li><p>通过Fully Convolutional Network进行计算共享;</p></li></ul><p>Deep Learning for Object Detetcion:</p><ul><li><p>proposal and refine;</p></li><li><p>one stage:</p><ul><li><p>example: Densebox, YOLO, SSD, Retina Net…</p></li><li><p>keyword: anchor, divide and conquer, loss sampling;</p></li></ul></li><li><p>two stage:</p><ul><li>example: Faster R-CNN, RFCN, FPN, Mask R-CNN;</li></ul></li><li>keyword: speed, performance;</li></ul><p>One Stage:</p><p><strong>Densebox:</strong></p><ul><li><p>流程：图—&gt;图像金字塔—&gt;卷积神经网络—&gt;upsampling—&gt;卷积神经网络—&gt;（4+1）通道—&gt;预测+threshold+NMS；</p></li><li><p>输入：$m \times n \times 3$，输出：$m/4 \times n/4 \times 5$；</p></li><li><p>输出的feature map每个像素对应一个带分数的边框：</p></li></ul><p>$t_{i}=\left\{s_{i}, d x^{t}=x_{i}-x_{t}, d y^{t}=y_{i}-y_{t}, d x^{b}=x_{i}-x_{b}, d y^{b}=y_{i}-y_{b},\right\}$</p><p>其中t和b分别代表左上角和右下角坐标；</p><ul><li>问题：回归的L2损失函数选的不好（不同程度scale的object学习程度不同），GT assignment也存在问题，object比较拥挤的情况下，多个物体可能缩小在最后特征图上的一个点上，FP比较多，回归变量选取问题，误差较大；</li></ul><p>UnitBox：</p><ul><li>把L2 loss换成IoU loss = $-\ln IoU$;</li></ul><p>YOLO：</p><ul><li>$7 \times 7$的grid，加了fc层可以覆盖到一些更全局的context，但是受限于固定输入尺寸，运行速度虽快但是拥挤场景检测不是很work；</li></ul><p><strong>SSD</strong>：</p><ul><li><p>引入不同scale和aspect ratio的anchor；</p></li><li><p>回归GT与anchor的offset；</p></li><li>不同layer检测不同尺寸的物体，小物体浅层出，大物体深层出（但是并没有直接证据证明此法可靠）；</li><li>loss sampling和OHEM；</li><li><a href="https://blog.csdn.net/qq_30815237/article/details/90292639" target="_blank" rel="noopener">blog</a>；</li></ul><p>DSSD:</p><ul><li><p>SSD利用浅层检测小目标，但是浅层语义信息少；</p></li><li><p>利用upsampling和融合加强语义信息；</p></li></ul><p>RON:</p><ul><li><p>reverse connect (similar to FPN)；</p></li><li><p>loss sampling: objectness prior (先做二分类在再细分)；</p></li></ul><p>RetainaNet:</p><ul><li><p>引入Focal loss；</p></li><li><p>FPN结构；</p></li></ul><p>One Stage Detector: Summary</p><ul><li><p>Anchor：</p><ul><li><p>No anchor: YOLO, densebox/unitbox/east；</p></li><li><p>Anchor: YOLOv2, SSD, DSSD, RON, RetinaNet；</p></li></ul></li><li><p>Divide and conquer：</p><ul><li>SSD, DSSD, RON, RetinaNet；</li></ul></li><li>loss sample：<ul><li>all sample: densebox；</li><li>OHEM: SSD；</li><li>focal loss: RetinaNet；</li></ul></li></ul><p>Two Stage:</p><p>RCNN:</p><ul><li>selective search+分类proposal；</li></ul><p>Fast RCNN:</p><ul><li>selective search对应到特征图，通过RoI pooling去分类；</li></ul><p>Faster RCNN:</p><ul><li>用预设的anchor去找proposal；</li></ul><p>RFCN，Deformable Convolutional Networks，FPN，Mask RCNN…</p><p>Two Stages Detector-Summary:</p><ul><li>Speed：<ul><li>RCNN -&gt; Fast RCNN -&gt; Faster RCNN -&gt; RFCN；</li></ul></li><li>Performance：<ul><li>Divide and conquer：<ul><li>FPN；</li></ul></li><li>Deformable Pool/ROIAlign；</li><li>Deformable Conv；</li><li>Multi-task learning；</li></ul></li></ul><p>Open Problem in Detection：</p><ul><li>FP；</li><li>NMS (detection in crowd)；</li><li>GT assignment issue；</li><li>Detection in video：<ul><li>detect &amp; track in a network；</li></ul></li></ul><p>Human Keypoint Task:</p><ul><li><p>Single Person Skeleton：</p><ul><li><p>CPM；</p></li><li><p>Hourglass；</p></li></ul></li><li><p>Multiple-Person Skeleton：</p><ul><li><p>top down:</p><ul><li><p>detect-&gt;single person skeleton；</p></li><li><p>Depends on the detector：</p><ul><li><p>Fail in the crowd case；</p></li><li><p>Fail with partial observation；</p></li><li>can detect the small-scale human；</li></ul></li><li><p>More computation；</p></li><li><p>Better localization when the input-size of single person skeleton is large；</p></li></ul></li><li><p>bottom up:</p><ul><li>Deep/Deeper cut, OpenPose, Associative Embedding；</li><li>Fast computational speed；</li><li>good at localizing the human with partial observation；</li><li>Hard to assemble human；</li></ul></li></ul></li></ul><h2 id="7-Scene-Text-Detection-and-Recognition"><a href="#7-Scene-Text-Detection-and-Recognition" class="headerlink" title="7. Scene Text Detection and Recognition"></a>7. Scene Text Detection and Recognition</h2><p>Background:</p><ul><li><p>文字的重要性：文明标志，携带高层语义信息，作为visual recognition的线索；</p></li><li><p>problem: scene text detection+scene text recognition；</p></li><li><p>challenge: 比OCR更复杂，比如背景，颜色，字体，方向，文字混杂等；</p></li><li><p>application: card recognition，图片定位，产品搜索，自动驾驶，工业自动化等；</p></li></ul><p>conventional methods：</p><ul><li><p>detection before deep learning: MSER (maximally stable extremal regions)，SWT (stroke width transform)，Multi-Oriented；</p></li><li><p>recognition: Top-down and bottom-up cues（滑窗+统计特性），Tree-structured Model (DPM+CRF)，Label embedding (另辟蹊)；</p></li><li><p>统一检测和识别：Lexicon Driven；</p></li></ul><p>Deep learning methods：</p><p>包含传统辅助方法的：</p><ul><li><p>end-to-end-recognition: PhotoOCR，Deep Features，Reading Text；</p></li><li><p>detection: MSER Trees；</p></li></ul><p>不包含传统辅助方法的：</p><ul><li>detection: Holistic (当作语义分割来做)，<a href="https://github.com/argman/EAST" target="_blank" rel="noopener">EAST</a> (旷视CVPR2017，多任务学习)，Deep Direct Regression (与EAST相似)，SegLink (多尺度特征图)，Synthetic Data (在图片上产生文字)；</li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/EAST.JPG" alt="EAST框架"></p><ul><li><p>recognition：$R^2AM$ (递归循环神经网络+soft-attention)，Visual Attention；</p></li><li><p>end-to-end recognition：<a href="https://github.com/MichalBusta/DeepTextSpotter" target="_blank" rel="noopener">Deep TextSpotter</a>；</p></li><li><p>summary: ideas from object detection and segmentation，end-to-end，use synthetic data；</p></li></ul><p>datasets and competitions：</p><ul><li>dataset: ICDAR 2103, MARA-TD500, ICDAR 2015, IIIT 5K-Word, COCO-Text, MLT, Total-Text；</li></ul><p>conclusion:</p><p><img src="/2020/02/25/旷视2017年深度学习实践课程/7_conclusion.JPG" alt></p><p>challenges:</p><ul><li>Diversity of text: language, font, scale, orientation, arrangement, etc；</li><li>Complexity of background: virtually indistinguishable elements (signs, fences, bricks and grasses, etc.)；</li><li><p>Interferences: noise, blur, distortion, low resolution, nonuniform illumination, partial occlusion, etc；</p><p>Trends:</p></li><li><p>Stronger models (accuracy, efficiency, <strong>interpretability</strong>)；</p></li><li>Data synthesis；</li><li>Muiti-oriented text；</li><li>Curved text；</li><li>Muiti-language text；</li></ul><p>References:</p><ul><li><p>Survey:</p><ul><li>Ye et al.. Text Detection and Recognition in Imagery: A Survey. TPAMI, 2015.</li><li>Zhu et al.. Scene Text Detection and Recognition: Recent Advances and Future Trends. FCS, 2015.</li></ul></li><li><p>Conventional Methods:</p><ul><li>Epshtein et al.. Detecting Text in Natural Scenes with Stroke Width Transform. CVPR, 2010.</li><li>Neumann et al.. A method for text localization and recognition in real-world images. ACCV, 2010.</li><li>Yao et al.. Detecting Texts of Arbitrary Orientations in Natural Images. CVPR, 2012.</li><li>Wang et al.. End-to-End Scene Text Recognition. ICCV, 2011.</li><li>Mishra et al.. Scene Text Recognition using Higher Order Language Priors. BMVC, 2012.</li><li>Busta et al.. FASText: Efficient Unconstrained Scene Text Detector. ICCV 2015.</li></ul></li><li><p>Deep Learning Methods:</p><ul><li>Bissacco et al.. PhotoOCR: Reading Text in Uncontrolled Conditions. ICCV, 2013.</li><li>Jaderberg et al.. Deep Features for Text Spotting. ECCV, 2014.</li><li>Gupta et al.. Synthetic Data for Text Localisation in Natural Images. CVPR, 2016.</li><li>Zhou et al.. EAST: An Efficient and Accurate Scene Text Detector. CVPR, 2017.</li><li>Busta et al.. Deep TextSpotter: An End-To-End Trainable Scene Text Localization and Recognition Framework. ICCV, 2017.</li><li>Ghosh et al.. Visual attention models for scene text recognition. 2017. arXiv:1706.01487.</li><li>Cheng et al.. Focusing Attention: Towards Accurate Text Recognition in Natural Images. ICCV, 2017.</li></ul></li></ul><p>Useful Resources:</p><ul><li>Laboratories and Papers<br><a href="https://github.com/chongyangtao/Awesome-Scene-Text-Recognition" target="_blank" rel="noopener">https://github.com/chongyangtao/Awesome-Scene-Text-Recognition</a></li><li>Datasets and Codes<br><a href="https://github.com/seungwooYoo/Curated-scene-text-recognition-analysis" target="_blank" rel="noopener">https://github.com/seungwooYoo/Curated-scene-text-recognition-analysis</a></li><li>Projects and Products<br><a href="https://github.com/wanghaisheng/awesome-ocr" target="_blank" rel="noopener">https://github.com/wanghaisheng/awesome-ocr</a></li></ul><h2 id="8-Image-Segmentation"><a href="#8-Image-Segmentation" class="headerlink" title="8. Image Segmentation"></a>8. Image Segmentation</h2><p>semantic segmentaion, instace segmentation, scene parsing, human parsing, stuff segmentation, UlrtraSound segmentation, selfie segmentation…</p><p>评价指标：</p><script type="math/tex;mode=display">\begin{array}{l}{\operatorname{Accuracy}(\mathbf{y}, \hat{\mathbf{y}})=\sum_{i=0}^{n} \frac{I\left[y_{i}=\hat{y}_{i}\right]}{n}} \\{\operatorname{mean} I O U(\mathbf{y}, \hat{\mathbf{y}})=\frac{\sum_{c}^{m} \sum_{i}^{n} I\left[y_{i}=c, \hat{y}_{i}=c\right]}{\sum_{c}^{m} \sum_{i}^{n} I\left[y_{i}=c \text { or } \hat{y}=c\right]}}\end{array}</script><p>Semantic Segmantation:</p><ul><li><p>FCN: 第一篇语义分割工作；</p></li><li><p>Learning Deconvolution Network for Semantic Segmentation，引入unpool和反卷积deconvolution；</p></li><li><p>DeepLab，引入空洞卷积<a href="https://www.jianshu.com/p/f743bd9041b3" target="_blank" rel="noopener">dilated-convolution</a>和DenseCRF；</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/CRF.JPG" alt></p><ul><li><p>CRF AS RNN;</p></li><li><p>Deeplab Attention;</p></li><li><p>PSPNet;</p></li><li><p>GCN (Global Convolutional Network，主讲人的工作，想要框住任意尺度的物体);</p></li><li><p>Deeplab V3;</p></li><li><p><strong>Deformable Convolution</strong>;</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/deformable_convolution.JPG" alt="deformable deconvolution"></p><p>Instance Segmentation:</p><p>Top-down pipeline (目前主流，依赖detection框架)：</p><ul><li><p>先detection再segmentation</p></li><li><p>FCIS (框得不准，但是分割依然准)；</p></li><li><p>Mask RCNN;</p></li></ul><p>Bottom-up pipeline (效果差，难实现，思考空间大):</p><ul><li><p>不出框分割;</p></li><li><p>Semantic instance segmentation via metric learning;</p></li></ul><hr><p>介绍旷视的框架：</p><ul><li>batch size in training:</li></ul><p>​ detection得batch size往往比分类小很多，主要是训练尺寸不同，另外可能一张图片会有很多proposal…</p><p>​ 小batch size 会导致：unstable gradient，inaccurate BN statistics， extremely imbalanced data, very</p><p>​ long training period…</p><ul><li><p>Multi-device BatchNorm;</p></li><li><p>Sublinear Memory;</p></li><li><p>Large Learning Rate;</p></li><li><p>打COCO instance segmentation比赛的一些tricks: precise RoI pooling, context extractor, mask generator；</p></li><li><p>keypoint比赛tricks；</p></li></ul><h2 id="9-Recurrent-Neural-Network"><a href="#9-Recurrent-Neural-Network" class="headerlink" title="9. Recurrent Neural Network"></a>9. Recurrent Neural Network</h2><p>RNN Bascis:</p><ul><li><p><a href="https://plato.stanford.edu/entries/turing-machine/" target="_blank" rel="noopener">Turning Machine</a>, RNN is Turing Complete, Sequence Modeling；</p></li><li><p>RNN Diagram,$(h_{i}, y_{i}) = F(h_{i-1},x_{i},W)$ ；</p></li><li><p>根据input/output分类：many-to-many, many-to-one, one-to-many, many-to-one+one-to-many;</p></li><li><p>many-to-many example: language model (predict next word by given previous words, tell story, write books in LaTex…);</p></li><li><p>many-to-one example: Sentiment analysis…</p></li><li><p>many-to-one+one_to_many exapmle: Neural Machine Translation (encoder+decoder)…</p></li><li><p>训练RNN，梯度爆炸和梯度消失: singular value &gt; 1 =&gt; explodes, singular value &lt; 1 =&gt; vanishes… LSTM (Long short-term memory) come to the resuce;</p></li><li><p>why LSTM works (input gate, forget gate, output gate, temp variable, memory cell);</p></li><li><p>GRU (similar to LSTM, let information flow without a separate memory cell);</p></li><li><p>Search for better RNN architecture;</p></li></ul><p>Simple RNN Extentsions:</p><ul><li><p>Bidirectional RNN (BDRNN)，预测未来；</p></li><li><p>2D-RNN: Pixel-RNN, each pixel depends on its top and left neighbor (补图，segmentation);</p></li><li><p>Deep RNN (stack more of them, harder to train);</p></li></ul><p>RNN with Attention:</p><ul><li><p>attention: differentiate entities by its importance, spatial attention is related to location; temporal attention is related to causality;</p></li><li><p>attention over input sequence: Neural Machine Translation (NMT);</p></li><li><p>Image Attention: Image Captioning (input image—&gt; Convolutional feature extraction—&gt;RNN with attention over the image—&gt;Word by word generation);</p></li></ul><p>RNN with External Memory:</p><ul><li>copy a sequence: Neural Turning Machines (NTM);</li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/NTM.JPG" alt></p><p>More Applications:</p><ul><li><p>RNN without a sequence input: read house numbers from left to right, generate images of digits by learning to sequentially add color to canvas;</p></li><li><p>generalizing recurrence (a computation unit with shared parameter occurs at multiple places in the computation graph);</p></li><li><p>apply when there’s tree structure in data;</p></li><li><p>bottom-up aggregation of information;</p></li><li><p>speech recognition;</p></li><li><p>generating sequence;</p></li><li><p>question answering;</p></li><li><p>visual question answering;</p></li><li><p>combinatorial problems;</p></li><li><p>learning to excute;</p></li><li><p>compress image;</p></li><li><p>model architecture search;</p></li><li><p>meta-learning;</p></li></ul><p>…</p><p><img src="/2020/02/25/旷视2017年深度学习实践课程/RNN_pros_cons.JPG" alt></p><p>RNN’s RIval:</p><ul><li><p>WaveNet: causal dilated convolution, Oord, Aaron van den, et al. “Wavenet: A generative model for raw audio.” arXiv preprint arXiv:1609.03499 (2016).</p></li><li><p>Attention is All You Need (Transformer) ;</p></li></ul><h2 id="10-Introduction-to-Generative-Models-and-GANs"><a href="#10-Introduction-to-Generative-Models-and-GANs" class="headerlink" title="10. Introduction to Generative Models (and GANs)"></a>10. Introduction to Generative Models (and GANs)</h2><p>Basics:</p><ul><li><p>Generative Models: Learning the distributions;</p></li><li><p>Discriminative: learn the likelihood;</p></li><li><p>Generative: performs Density Estimation (learns the distribution) to allow sampling;</p></li><li><p>回归建模的话会取平均值，回归的是最可能情况的平均值，显得不真实，a driscrminative model just smoothes all possibilities, ambiguity and “blur” effect;</p></li><li><p>application of generative models: image generation from sketch, interactive editing, image to image translation;</p></li></ul><p>How to train generative models:</p><ul><li>给出一系列样本点，模型生成符合预期分布的输出；</li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/taxonomy_of_generative_models.JPG" alt="从左往右方法逐渐work"></p><ul><li><p>exact model: NVP (non-volume preserving), real NVP: invertible no-linear transforms, 理论要求过于严格（Restriction on the source domain: must be of the <strong>same</strong> as the target.），效果不好（人脸稍微好点，因为其structure比较规矩）；</p></li><li><p>Variational Auto-Encoder (VAE): encoder 做density estimation的过程， decoder做sampling的过程。</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/VAE.JPG" alt></p><ul><li><p>Generative Adversarial Networks (GAN): 生成器和判别器相互学习进步，交替训练；</p></li><li><p>DCGAN: example of feature manipulation （人脸加眼镜，变性别之类的的操作）；</p></li><li><p>conditional, cross-domain generation (genenative adversarial text to image synthesis);</p></li><li><p>GAN training problems: unstable losses（训练时应该G和D应该处于动态平衡）, mini-batch fluctuation （每个batch之间生成的图像不同），model collapse (lack of diversity in generated results);</p></li><li><p>improve GAN training: label smoothing, <strong>Wasserstein GAN (WGAN)</strong> (stabilized taining curve, non-vanishing gradient), loss sensitive GAN (LS-GAN)… <a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">The GAN Zoo</a>;</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/WGAN.JPG" alt></p><p>举一些有名的GAN例子：</p><ul><li><p>zhu junyan—-Cycle GAN :correspondence from unpaired data;</p></li><li><p>DiscoGAN: cross-domain relation;</p></li><li><p>GeneGAN: shorter pathway improves training (cross breeds and reproductions, 生成笑容)，object transfiguration (变发型)，interpolation in object subspace (改变发型方向)；</p></li></ul><p>Math behind Generative Models:</p><ul><li><p>formulation: sampling vs. density estimation;</p></li><li><p>RBM (现在已经不怎么使用)；</p></li><li><p>from density to sample: 给定概率密度方程，无法有效采样；</p></li><li><p>from sample to density: 给定black-box sampler，是否可以估计概率密度（频率）；</p></li></ul><p>​ Given samples, some properties of the distribution can be learned, while others cannot.</p><p><img src="/2020/02/25/旷视2017年深度学习实践课程/game_G_D.JPG" alt></p><ul><li><p>the future of GANs: guaranteed stabilization (new distance), broader application (apply adversarial loss in xx/ different type of data);</p></li><li><p>GAN tutorial from Ian Goodfellow: <a href="https://arxiv.org/abs/1701.00160" target="_blank" rel="noopener">https://arxiv.org/abs/1701.00160</a>;</p></li></ul><h2 id="11-Persom-Re-Identification"><a href="#11-Persom-Re-Identification" class="headerlink" title="11. Persom Re-Identification"></a>11. Persom Re-Identification</h2><p>ReID: from face to person;</p><ul><li><p>face recognition (verification, size: $32 \times 32$, horizontal: -30~30, vertical: -20~20, little occlusion);</p></li><li><p>person Re-Identification (trcaking in cameras, searching person in videos, clustering person in photos, challenges: inaccurate detection, misalignment, illumination difference, occlusion…);</p></li><li><p>common in FR &amp; ReID: deep metric learning, mutual learning, re-ranking;</p></li><li><p>special in ReID: feature alignment, ReID with pose estimation, ReID with human attributes;</p></li></ul><p>from classification to metric learning:</p><ul><li><p>classification network只能辨别那些“见过的”物体，没见过的物体就要重训练，对于人脸识别部署来说，不现实。为了克服这点，加入metric learning，拿pre-train过的classification网络在metric learning中finetune (similar feature);</p></li><li><p>有些工作是fusing intermediate feature maps, 但是计算量和存储都加大，拖慢了速度，不实用；</p></li></ul><p>Metric Learning:</p><ul><li><p>Learn a function that measures how similar two objects are. Compared to classification which works in a closed-word, metric learning deals with an open-world.</p></li><li><p>contrastive loss: $L_{\text {pairwise}}=\delta\left(I_{A}, I_{B}\right) \cdot\left|f_{A}-f_{B}\right|_{2}+\left(1-\delta\left(I_{A}, I_{B}\right)\right)\left(\alpha-\left|f_{A}-f_{B}\right|_{2}\right)_{+}$ （最后一项有focus困难样本的作用，$\delta$ is Kronecker Delta，$\alpha$ is the margin for different identities），让有相同identity的图像距离变小，反之变大，$\alpha$被用来略掉那些“naive”的negative pairs；</p></li><li><p>triplet loss: $L_{t r p}=\frac{1}{N} \sum \limits ^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha\right)_{+}$ (The distance of A and A’ should be smaller than that of A and B. $\alpha$ is the margin between negative and positive pairs. Without $\alpha$, all distance converge to zero.);</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/closs_tloss.JPG" alt></p><ul><li><p>improved triplet loss: $ L_{i m t r p}=\frac{1}{N} \sum^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha\right)_{+} +\frac{1}{N} \sum^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\beta\right)_{+} $ ($\beta$ penalizes distance between features of $A$ and $A^{\prime}$), only consider image pairs with the same identity;</p></li><li><p>quadruplet loss: $\begin{aligned} L_{q u a d} &amp;=\frac{1}{N} \sum^{N}(\overbrace{\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha}^{\text {relative distance}}) \\ &amp;+\frac{1}{N} \sum^{N}(\overbrace{\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{C}-f_{B}\right|_{2}+\beta}^{\text {absolute distance }}) \end{aligned}$, 结合了triplet loss和pairwise loss，任何有着相同identity的image之间的distance都要比不同不同image之间的distance小；</p></li><li>triplet loss较contrastive loss提升明显，后面的quadruplet loss较triplet提升不多，而带来了计算量和搜索空间的提升，因此常用triplet loss;</li></ul><p>Hard Sample Mining:</p><ul><li><p>triplet hard loss: $ L_{\text {trihard}}=\frac{1}{N} \sum_{A \in \text {batch}}(\overbrace{\max _{A^{\prime}}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}\right)}^{\text {hard positive pair }} -\overbrace{\min \left(\left|f_{A}-f_{B}\right|_{2}\right)}^{\text {hard negative pair }}+\alpha) $, 找出矩阵中相同identity images中最不像的（the largest distance in the diagonal block）和不同identity images中最像的（The smallest distance in other places）;</p></li><li><p>soft triplet hard loss: 不用一个个找出来，而是利用softmax自动去分配大权重给harder samples;</p></li><li><p>margin sample mining: $L_{e m l}=(\overbrace{\max _{A, A^{\prime}}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}\right)}^{\text {hardest positive pair }}-\overbrace{\min _{C, B}\left(\left|f_{C}-f_{B}\right|_{2}\right)}^{\text {hardest negative pair }}+\alpha)_{+}$;</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/conclu_dmetriclea.JPG" alt></p><p>Mutual Learning:</p><ul><li><p>knowledge distill: 知识蒸馏，学生网络学习老师网络的输出；</p></li><li><p>mutual learning: 几个学生网络自己相互学习，利用KL散度算各个网络output pro之间的接近程度；</p></li><li><p>metric mutual learning: $ L_{M}=\frac{1}{N^{2}} \sum_{i}^{N} \sum_{j}^{N} \left(\left[Z G\left(M_{i j}^{\theta_{1}}\right)-M_{i j}^{\theta_{2}}\right]^{2}+\left[M_{i j}^{\theta_{1}}-Z G\left(M_{i j}^{\theta_{2}}\right)\right]^{2}\right) $, ZG代表zero gradient，不计算梯度，不进行反向传播，学习distance matrix;</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/framework_MML.JPG" alt></p><ul><li>re-ranking: 对initial ranking list进行再ranking，使其smooth，on Supervised Smoothed Manifold/ by K-reciprocal Encoding;</li></ul><p>Person Re-Identification:</p><ul><li><p>difficulties: inaccurate detection, misalignment, illumination difference, occlusion, non-rigid body deformation, similar apperance…</p></li><li><p>evaluation criteria: CMC (Cumulative Math Characteristic)<rank-1 , rank-5, rank-10>, mAP (based on rank);</rank-1></p></li><li><p>datasets: Marke1501, CUHK03, DukeMTMC-reid, MARS;</p></li></ul><p>Feature Alignment:</p><p>motivations:</p><ul><li>Person is highly structured;</li><li>Local similarity plays a key role to decide the identity;</li></ul><p>methods:</p><ul><li>Local Features from local regions<ul><li>Traditional Methods (colors, texture…);</li><li>Deep Learning Methods;</li></ul></li><li>Local Feature Alignment<ul><li>Fusion by LSTM (RNN cannot fuse local features properly);</li><li>Alignment in PL-Net (Part Loss Network, unsupervised);</li><li>Alignment in AlignedReID (Face++出品，性能超越人类，global feature+7个local feature，代表人的7个部分，横向pool，只拿对应的边，使用动态规划);</li></ul></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/AlignedReID.JPG" alt></p><p>ReID with Extra Information:</p><p>ReID with Pose Estimation:</p><ul><li>Providing explicit guidance for alignment;</li><li>Global-Local Alignment Descriptor (GLAD);<ul><li>Vertical alignment by pose estimation;</li></ul></li><li>SpindleNet;<ul><li>Fusing local features from regions proposed by pose estimation;</li></ul></li></ul><p>ReID with Human Attributes:</p><ul><li>Attributes is critical in discriminating different persons;</li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/11_conclusion.JPG" alt></p><h2 id="12-Shape-from-X-3D-reconstruction-传统和DL"><a href="#12-Shape-from-X-3D-reconstruction-传统和DL" class="headerlink" title="12. Shape from X (3D reconstruction: 传统和DL)"></a>12. Shape from X (3D reconstruction: 传统和DL)</h2><ul><li><p>Structure from Motion (SfM): the most easy-to-understand approach, triangulation gets depth;</p></li><li><p>triangulation: the epipolar constraint对极约束，单目；</p></li><li><p>stereo, rectification (更正), disparity (视差，depth): correspondence, 不能远距离测量；</p></li><li><p>3D point cloud: paper-building Rome in one day, 多视角图片SfM重建，3D geometry；</p></li><li><p>surface reconstruction: integration of oriented point;</p></li><li><p>SfM scanning: SLAM based, positioning, 华为手机发布会实现静止小熊猫玩偶重建；</p></li><li><p>depth sensing: active sensors, structured light, ToF (Time of Flight);</p></li><li><p>short baseline stereo: phase detection autofous;</p></li><li><p>shape from shading: shading as cue of 3D shape (the Lambertian law);</p></li><li><p>photometric stereo;</p></li><li><p>shape from texture, depth from focus, depth from defocus, shape from shadows, shape from sepcularities, object priors paper-<a href="https://arxiv.org/abs/1612.00603" target="_blank" rel="noopener">A point set generation network for 3D object reconstruction from single image</a>;</p></li></ul><p>3D reconstruction from single image:</p><ul><li><p>the ShapeNet dataset;</p></li><li><p>depth map;</p></li><li><p>volumetric occupancy;</p></li><li><p>XML file;</p></li><li><p>ponit-based represenation;</p></li></ul><p>A neural method to stereo matching:</p><ul><li><p>Flownet &amp; Dispnet (using raw left and right images as input, output disparity map);</p></li><li><p>stereo matching cost convolutional neural network—Yan lecun;</p></li><li><p>MRF (马尔可夫随机场) stereo methods;</p></li><li><p>global local stereo neural network;</p></li><li><p>PatchMatch Communication Layer;</p></li></ul><h2 id="13-Visual-Object-Tracking"><a href="#13-Visual-Object-Tracking" class="headerlink" title="13. Visual Object Tracking"></a>13. Visual Object Tracking</h2><p>Motion estimation/ Optical flow:</p><ul><li><p>motion field: the projection of the 3D motion onto a 2D image;</p></li><li><p>optical flwow: the pattern of apparent motion in images, $I(x, y, t)=I(x+d x, y+d y, t+d t)$, 在adjacent frames中像素的运动；</p></li><li><p>motion field与optical flow不是完全相等；</p></li><li><p>KLT feature tracker (找点，计算光流，更新点)，比较成熟，available in OpenCV；</p></li><li><p>optical flow with CNN: FlowNet / <a href="https://github.com/lmb-freiburg/flownet2" target="_blank" rel="noopener">FlowNet 2.0</a>, lack of training data (Flying Chairs / ChairsSDHom, Flying Things 3D);</p></li><li><p>optical flow长距离跟踪和复杂场景跟踪容易失效，不建议采用；</p></li></ul><p>Single object tracking:</p><ul><li><p>model free: nothing but a single training example is provided by the bounding box in the first frame;</p></li><li><p>short term and subject to causality;</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/single_object_tracking.JPG" alt></p><ul><li><p><a href="https://github.com/foolwood/benchmark_results" target="_blank" rel="noopener">paper list</a>;</p></li><li><p>correlation fiter: 模板匹配，similar to convolution;</p></li><li><p>MOSSE (Minimum Output Sum of Squared Error) Filter;</p></li><li><p>KCF (Kernelized Correlation Filter);</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/KCF.JPG" alt></p><ul><li><p>from KCF to Discriminative CF Trackers: Martin Danelljan, 从Deep SRDCF开始利用CNN feature;</p></li><li><p><a href="https://github.com/martin-danelljan/Continuous-ConvOp" target="_blank" rel="noopener">Continous-Convolution Operator Tracker</a>: very slow (~1fps) and easy to overfitting;</p></li><li><p>Efficient Convolution Operators: based (factorized convolution operator + Guassian mixture model) on C-COT, ~15fps on GPU;</p></li><li><p><a href="https://github.com/HyeonseobNam/MDNet" target="_blank" rel="noopener">Multi-Domain Convolutional Neural Network Tracker</a>: online tracking, bounding box regression, ~1fps;</p></li><li><p><a href="http://davheld.github.io/GOTURN/GOTURN.html" target="_blank" rel="noopener">GOTURN</a>: ~100fps;</p></li><li><p><a href="https://github.com/bertinetto/siamese-fc" target="_blank" rel="noopener">SiameseFC</a>: ~60fps, a deep FCN is trained to address a more general similarity learning problem in an initial offline phase;</p></li><li><p>Benchmark: VOT (accuracy, robustness, EAO-expect average overlap), OTB(one pass evaluation, spatial robustness evaluation);</p></li></ul><p>Multiple object tracking:</p><p><img src="/2020/02/25/旷视2017年深度学习实践课程/multi_object_tracking.JPG" alt></p><ul><li><p>tracking by detection: assocation based on location (IoU, L1/L2 distance), motion (modeling the movement of objects, Kalman filter), apperance (feature) ans so on;</p></li><li><p>association:</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/association.JPG" alt></p><ul><li><p>association as optimization: local method (Hungarian algorithm), global methods (clustering, network flow, minimum cost multi-cut problem), do optimization in a window (trade off speed against acc);</p></li><li><p>Benchmark: MOT, KITTI, ImageNet VID;</p></li><li><p>evaluation metrics: Multiple object tracking accuracy (MOTA);</p></li></ul><p>Other:</p><ul><li><p>fast moving object (FMO): an object that moves over a distance exceeding its size within the<br>exposure time;</p></li><li><p>multiple camera tracking;</p></li><li><p>tracking with multiple cues: with multiple detectors, with key points, with semantic segmentation, with RGBD camera;</p></li><li><p>multiple object tracking with NN:</p><ul><li>Milan, Anton, et al. “Online Multi-Target Tracking Using<br>Recurrent Neural Networks“. AAAI. 2017.</li><li>Son, Jeany, et al. “Multi-Object Tracking with Quadruplet<br>Convolutional Neural Networks.” CVPR. 2017.</li></ul></li></ul><h2 id="14-Neural-Network-in-Computer-Graphics"><a href="#14-Neural-Network-in-Computer-Graphics" class="headerlink" title="14. Neural Network in Computer Graphics"></a>14. Neural Network in Computer Graphics</h2><p>计算机视觉是将图像信息转换成抽象的语义信息等，而计算机图形学是将抽象的语义信息转换成图像信息。</p><ul><li><p>Graphics: rendering, 3D modeling, visual media retouching （图像修整）；</p></li><li><p>Neural Network for graphics: faster, better, more robust;</p></li><li><p>NN rendering:</p><ul><li><p>Monte Carlo ray tracing （光线追踪，寻找光源），paper-[SIGGRAPH17] Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings ( utilize CNN to predict de-noising kernels, thus enhance ray tracing rendering result);</p></li><li><p>volume rendering;</p></li><li>NN shading (real-time rendering), paper-Deep shading: Convolutional Neural Networks for Screen-space shading (2016);</li><li>goal is to accelerate, all training data can be gathered virtually;</li></ul></li><li><p>NN 3D modeling:</p><ul><li>shape understanding:<ul><li>3D ShapeNets: A Deep Representation for Volumetric Shapes (2015).</li><li>VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition (2015).</li><li>DeepPano: Deep Panoramic Representation for 3-D Shape Recognition (2015).</li><li>FusionNet: 3D Object Classification Using Multiple Data Representations (2016).</li><li>OctNet: Learning Deep 3D Representations at High Resolutions (2017).</li><li>O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis (2017).</li><li>Orientation-boosted voxel nets for 3D object recognition (2017).</li><li>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (2017).</li></ul></li><li>shape synthesis: 3D-conv, also use GAN</li><li>from 2D to 3D, data becomes harder to handle, design of mesh representation, high-resolution 3D problem;</li></ul></li><li><p>NN visual retouching:</p><ul><li>tone mapping: paper-Deep Bilateral Learning for Real-Time Image Enhancement (2017), it can handle high-resolution images relatively fast;</li><li>automatic enhancement: paper- Exposure: A white-box Photo Post-processing Framework (2017);</li></ul></li></ul><p>Example-NN 3D Face:</p><ul><li><p>given a face RGB/RGBD still/sequence, reconstruct for each frame (intrinsic image or inverse rendering):</p><ul><li>Inner/outer camera matrix;</li><li>Face 3D pose;</li><li>Face shape;</li><li>Face expression;</li><li>Face albedo;</li><li>lighting;</li></ul></li><li><p>3D face priors: shape &amp; albedo, paper-A 3D Morphable Model learnt from 10,000 faces (2016);</p></li><li><p>3D face priors: expression: paper- FaceWarehouse: a 3D Facial Expression Database for Visual Computing (2012);</p></li><li><p>optimization: based 3D face fitting;</p></li><li><p>Coarse Net, Fine Net;</p></li><li><p>3D Face-without prior: paper-DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild(2017);</p></li></ul><p><img src="/2020/02/25/旷视2017年深度学习实践课程/3D_face.JPG" alt></p><ul><li><p>render for CV:</p><ul><li>Synthesizing Training Data for Object Detection in Indoor Scenes, (2017);</li><li>Playing for Data: Ground Truth from Computer Games (2016)</li><li>Learning from Synthetic Humans (2017);</li></ul></li><li><p>demo: Face2Face, Real-Time high-fidelity facial performance capture, DenseReg;</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/av88056282/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;视频&lt;/a&gt;;&lt;/li&gt;&lt;li&gt;课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取;&lt;/li&gt;&lt;li&gt;这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好。&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="课程记录" scheme="http://densecollections.top/categories/%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
      <category term="Face++" scheme="http://densecollections.top/tags/Face/"/>
    
  </entry>
  
  <entry>
    <title>2019-2020:漫长的告别</title>
    <link href="http://densecollections.top/2020/01/10/2019-2020-%E6%BC%AB%E9%95%BF%E7%9A%84%E5%91%8A%E5%88%AB/"/>
    <id>http://densecollections.top/2020/01/10/2019-2020-漫长的告别/</id>
    <published>2020-01-10T02:52:06.000Z</published>
    <updated>2020-02-03T02:28:47.284Z</updated>
    
    <content type="html"><![CDATA[<p>搭博客的时间大概只有一年，当初是因为受到<a href="http://pluskid.org/" target="_blank" rel="noopener">pluskid网站</a>和博客的影响，觉得做一个有趣的人，学习自己想了解的，放淡自己的心态是是每天很重要的事，因此想记录下来，有些规划，也好作为充实自己的见证，搁以前每年都来一个总结这件事我是不怎么做的。求学生涯没有结束，每年都是反复地上课，做项目，似乎都是浑浑噩噩度过，科研中的磕碰和一些不甘心也会时不时消磨自己的意志，让自己在怀疑，焦虑，麻木，强迫的交织中蹒跚着。我之前也知道很多励志结论，他们说的都很对，但每个人的生活总是悲喜交加的，而大多数情况下都是在平平无奇中暗生悲戚，羁绊越多的人似乎悲戚越浓。</p><p>2019年的寒假里，我翻看了很多<a href="http://freemind.pluskid.org/" target="_blank" rel="noopener">pluskid的博客</a>内容，其中以年终总结最多。一开始我是被博主优秀的履历所吸引，后来在那些博客里我读到了对于生活细节的热爱，以及对于艺术，真理和其他人类活动的浓郁兴趣，真诚，质朴同时又很能温暖人，所以算是“始于履历，陷于才华”吧！（厚脸皮地说，有一种惺惺相惜之感）。罗曼罗兰在《米开朗琪罗传》中有句很出名的话：<em>“Il n’ya qu’un héroïsme au monde : c’est de voir le monde tel qu’il est et de l’aimer.（世界上只有一种真正的英雄主义，那就是认识生活的真相后依然热爱它）”</em>，我开始渐渐明白其中的血肉故事，目前虽然谈不上热爱，但可以说是慢慢从走出到走入，慢慢进入状态。</p><p>接下来，我想还是先从自己的学校科研生活讲起，然后再去讲讲自己看过的书，去过的地方，听过的音乐，拍过的照片，看过的电影等，一步步勾勒出自己的故事，就像是在索拉里斯星上模拟出的记忆花园，感想估计无法给出多少，倒是想能抓住几分情绪便好。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/电影大师剧照.jpg" alt="电影&#39;大师(the Master)&#39;剧照"></p><a id="more"></a><h2 id="研究生科研工作"><a href="#研究生科研工作" class="headerlink" title="研究生科研工作"></a>研究生科研工作</h2><p>2019年开始慢慢接受18年底国外读博计划失败的经历，审视自己这一个大时间段的失意，认清自己的能力和所需。前半年一半时间在慢慢恢复神志，试图理清自己后面的研究方向，还是想坚持SLAM+DL的路线。从头开始看高翔在深蓝学院的《视觉 SLAM 14讲》，看书，推公式，理解代码，再总结下来写到博客上，当然也时不时关注DL+CV相关论文的最新进展。大概是在3月底的时候，老板受人所托，给了我个在校外一家初创AI医疗研究院实习的机会，当时正好毫无头绪， 像个无头苍蝇到处乱撞，便答应下来。</p><p>为了能够顺利拿到实习机会（小型团队，气氛很好。mentor是海归，前百度员工，有梦想的工作狂），开始正式去看深度学习的理论，同时也参加了一个小型的“蔚蓝杯”人工智能 竞赛（赛题是检测图片中的商品，给出商品数量），熟悉一下数据分析，pytorch掉包和调参。趁着研一上课空余时间，突击学习理论，撕撕代码，看完了<a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielsen</a>(这个人好像是个物理学家，写了一本关于量子通信，量子计算机方面的书籍，最近又搞了个网站专门介绍量子通信)的<a href="http://www.liuxiao.org/2016/10/dnn-《神经网络与深度学习》中文版及代码下载/" target="_blank" rel="noopener">nndl</a>，<a href="http://www.weixiushen.com/" target="_blank" rel="noopener">魏秀参</a>的《解析卷积神经网络》和一部分<a href="https://github.com/scutan90/DeepLearning-500-questions" target="_blank" rel="noopener">深度学习500问</a>，本来计划接着看<a href="https://xpqiu.github.io/" target="_blank" rel="noopener">邱锡鹏</a>教授的新书《神经网络与深度学习》，不过时间没赶得上就去面试了，个人感觉这些都是不错的参考资料。</p><p>实习面试的时候理论问题问得比较浅，可能对方知道我是个DL新手，只有训练CNN的一些防止过拟合技巧，BP中的链式法则等等；编程题就比较惨了，题目大概是在一个包含”x”和“o”的矩阵（”o”是少量元素）中找出起始“o”和终止”o”之间的最短距离（只能顺着“o”走）。很久没撕过代码， 加上第一次工作面试有点紧张，连暴力搜索都没能写出来，场面一度十分尴尬。后来我觉得可能看在老板的面子上，还是让我过了。。。实习期是6月底到9月底，期间做的一些<a href="http://densecollections.top/2019/10/04/实习痰涂片项目总结/">工作</a>和<a href="http://densecollections.top/2019/10/03/实习见闻及其他/">感想</a>我都放在了之前的博客里，认识的师兄，了解到的新知识都打开了我的思路，也培养了我一些工作习惯，锻炼了自己的代码能力，总之还是十分感谢这段实习经历的。</p><p>回来之后就在准备研究生毕业开题的事情，同时也赶着实习工作总结出的论文。当然要不是因为组里小老师给的项目，也不会三个月实习期结束就回来。年初的时候想着拿深度视觉里程计开题，后来实习的时候一度打算一边实习，一边拿实习的AI医疗项目做毕设，最后，还是综合考虑答应了做小老师的项目，以此作为毕设。毕设的大致内容其实就是检测无人机，但是由于要求检测距离远，所以可能常规的Faster R-CNN等框架的检测效果可能不太好，要引入GAN和超分辨率等手段。目前最大的坑处应该是数据集的收集了，既要拍摄RGB，也要拍摄红外；既要考虑不同出现场景和环境，还要引入多形态的无人机，鸟，风筝类别，此外还得保证将近1km的拍摄距离。估计数据拍完了，毕设时间也差不多到了(&gt;_&lt;|||)。</p><p>2019年的学习与科研不算太顺利，自己也经常没进入状态，算是那种没别人努力，却还时常抱怨，焦虑的人。希望2020年能加把劲，不管是工作还是继续申请读博，都得把自己整硬实起来才行啊（@_@）！</p><p><img src="/2020/01/10/2019-2020-漫长的告别/work-harder.jpg" alt="Photo by Jordan Whitfield on Unsplash"></p><h2 id="探访之境"><a href="#探访之境" class="headerlink" title="探访之境"></a>探访之境</h2><p>今年南京之外去过的其他城市不多，大多是借着出差机会逛逛，不像本科时期年年都会找几个地方专程去看看。出走对于我来说已经成为生活中必不可少的一部分了，以为我是个很容易“习惯”的人，日常生活中的每件事都可以成为惯例，从不会厌倦，直到自己觉得到了需要进行调整或者改变的时刻。人在一个熟悉的封闭的空间里待久了，容易怀疑自己和这个世界，容易陷入空想而无所为的状态，这种感觉最能吞噬人的精神和智力。可能是我自己心浮气躁的原因，有时候会觉得内心烦闷，精气神不足，以前就任其发展，以无所事事应对，等待其消失。现在，随着这种情况出现次数的增多，我开始体会到这可能是科技异化人性和人类自身特质共同作用导致的，为了应对，我强制给自己找合适的，能辅助静下心来的事情做：“要么学习，要么运动“。其中”学习“的方式有很多种，不仅仅是科技理论知识，还可以是培养兴趣爱好，发展艺术视野和思维，出走旅游也是其中一种形式。</p><p>我学校处于靠市中心的位置，离很多景区都比较近，偶尔朋友来金陵的时候便会一起去玩玩，比如夫子庙，中山陵，老门东等地方。自己平时有点累或者烦躁的时候，通常会选择傍晚围中山陵绿道或者体育公园骑一圈，或者偶尔跟朋友师兄们相约爬紫金山，逛灵谷寺看萤火虫等。一年四季，这些地方都有很多”隐秘之地“，树草虫鸣，风拂月泻，令人舒心。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/夫子庙夜景1.jpg" alt="夏天夫子庙的傍晚"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/秦淮河.jpg" alt="秦淮河夜景"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/夫子庙中的公园.jpg" alt="夫子庙中的一个公园，具体名字我记不得了。"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/夫子庙鲤鱼.jpg" alt="夫子庙中一个公园里的一大群人工饲养鲤鱼"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/公园中的鹅.jpg" alt="公园中的一只鹅，对游客已经免疫了"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/中山陵绿道.jpg" alt="傍晚骑行的中山陵绿道"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/体育公园.jpg" alt="傍晚的南京体育公园，挨着中山陵。颜色有种克莱因蓝的感觉"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/紫金山大路.jpg" alt="和朋友一起爬紫金山路上"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/紫金山头陀岭俯瞰.jpg" alt="站在紫金山山群头陀岭俯瞰南京城，有些霾，拍得不是很清楚"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/中山陵公园一隅.jpg" alt="去灵谷寺路上的一汪水池"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/看萤火虫校外的马路.jpg" alt="灵谷寺看完萤火虫回来的路上。第一次去拍萤火虫，相机不太好，光圈不够没拍出来萤火虫，囧x_x"></p><p>本科的时候有一些喜欢骑行的小伙伴，经常会约出去骑车到处逛，自从毕业读了研以后，朋友都各自纷飞，自己的圈子也越来越小，大家也都在实验室忙着各自的事。我偶尔一个人出去，也少了好几分乐趣。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/教研室外的天空.jpg" alt="实验室外的天空"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/理想骑行1.jpg" alt="日本一个小哥推特发的骑行照片，很是理想中的样子了"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/理想骑行2.jpg" alt="这样的骑行环境国内似乎很少"></p><p>南京不像上海深圳，文化艺术多样性方面还是有所欠缺的，好歹有个南京博物院会时不时联合办些展览，可以饱饱眼福。2019年除了专程去看了固定的艺术馆的书画展，就是三次合作的特展：世界巨匠-意大利文艺复兴三杰；从毕加索到基弗-路德维希的艺术课；仰之弥高-二十世纪中国画大家展。前两次展品不多，毕竟大作是不太可能借过来的，最近一次的书画展规模比较大，看得也比较过瘾，将近百幅的画作看了两三个小时。</p><p><strong><a href="https://mp.weixin.qq.com/s/xrxMLSUhycpbtMBk53Q2yg" target="_blank" rel="noopener">世界巨匠-意大利文艺复兴三杰</a></strong></p><p>展览内容是文艺复兴三杰达芬奇，米开朗琪罗，拉斐尔和他们继承人的一些作品，一开始我还妄想会有“蒙娜丽莎”，实际上三个人的真迹不多，其中有一些还只是复制品。不过，能看到这样的一些合作已经很满足了(&gt;～&lt;)。</p><p>拍的作品不多，有几幅是自己心水的。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/世界巨匠3.jpg" alt="展览门票"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/世界巨匠6.jpg" alt="米开朗琪罗的“哀悼基督”，展览中最喜欢的作品之一，可惜只是复制品"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/世界巨匠1.jpg" alt="展览入门处矗立的雕塑，米开朗琪罗的“倚靠十字架的基督”。然鹅展览中没有“大卫”和“拉奥孔”的身影 Q_Q"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/世界巨匠4.jpg" alt="米开朗琪罗画像"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/世界巨匠5.jpg" alt="达芬奇的“女孩头部像”，自己比较喜欢的一幅作品，还专程买了相似的纪念品"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/世界巨匠2.jpg" alt="达芬奇的“美丽公主”"></p><p><strong><a href="https://mp.weixin.qq.com/s?__biz=MzA5Mzk1NzQxNQ==&amp;mid=2656021627&amp;idx=1&amp;sn=54a8be2efaa5acd0c40f8da4850b43ee&amp;chksm=8beef033bc997925f6f4e35c4c417c9972b2bafd903674a3b2b26070eb1c53bfa1440b3d6960&amp;mpshare=1&amp;scene=1&amp;srcid=0128YyCtgogxHWDWaC7K1g3a&amp;sharer_sharetime=1580211377613&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=008658d21f0cfb81aea856e3b585c2ee570e286de35cc8bd4d066218d574c041f1493ecdcc46b9cd65bfba07cee59c36a74b56a57911c6c0e9b4292c04f2c18bb7c2103218fe57a9e9d788fa2cbec43f&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10&amp;version=6208006f&amp;lang=zh_CN&amp;exportkey=A%2FAhzHlLl0jDCA0j%2FJwrG58%3D&amp;pass_ticket=2hEidokwNVuWD7Y6FL6DoxqyMg6FNzZEiYvkngh8pdxJI2%2Bneh%2F9h8tqW9J5PVnV" target="_blank" rel="noopener">从毕加索到基弗一一路德维希的艺术课</a></strong></p><p>路德维希夫妇是非常有名的艺术收藏家，曾收集了很多毕加索的名画。这次展览也展出了毕加索的几幅真迹，比如“带鸟的步兵”等，不过鉴于规模，大部分还是我这个圈外人士不曾了解过的艺术家。画作风格上涵盖了立体主义，表现主义，抽象派，野兽派和波普艺术等，神秘，荒诞，又虚无。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/路德维希4.jpg" alt="贝尔纳德·舒尔茨，“伟大的母亲”。此次展览中最喜欢的一幅抽象画，我来回看了很久，但是没看出“母亲”的含义"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/路德维希5.jpg" alt="安塞尔姆·基弗，“阿拉里克的坟墓”"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/路德维希7.jpg" alt="娜塔丽亚·内斯特洛娃，“鸟”"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/路德维希1.jpg" alt="名字不记得了，画作内容好像是描绘新疆某个地方的烧烤摊的场景"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/路德维希2.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/路德维希3.jpg" alt="这幅画挺有意思的，人类拥有小范围太空探索的能力，与造物神之间的对峙和连接"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/路德维希6.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/路德维希8.jpg" alt></p><p>画作和诗歌一样，都得参照下创作背景，虽然有些艺术家可能是天赋异禀，像小松美羽，画作直接在脑海呈现，一气呵成。一千个读者，有一千个哈姆雷特，画作也是这样，那些吸引我的画，都有一种能攫住人的思绪得魅力，触发观赏者的千万感受。很抱歉我自己读书和思考比较少，鉴赏水平有限，没能做出比较完整的评价来。(+_+)</p><p><strong><a href="https://mp.weixin.qq.com/s?__biz=MzA5Mzk1NzQxNQ==&amp;mid=2656022782&amp;idx=1&amp;sn=e98882a7deac21af575b8298d24490e3&amp;chksm=8beeecb6bc9965a01af713ba2fd6a3618a1faaa24735cdf478f15dbf9bc548cd0cb2690f3f9b&amp;mpshare=1&amp;scene=1&amp;srcid=0128xHFfNOS2JdO2U7K6n5hW&amp;sharer_sharetime=1580211431161&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=1f3bd73ca2cb19588251434376e9c167a19004fc86747f5e66073c286ed4bf8a62151c98501fd2d8d6e7bb1bec11003fad8faf0ac6ae80516b1fbe2235c1e9f02f2d8dbe4128d1bd26a95a185e174974&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10&amp;version=6208006f&amp;lang=zh_CN&amp;exportkey=AzstWxsx9pxr1OrJifPhDaI%3D&amp;pass_ticket=2hEidokwNVuWD7Y6FL6DoxqyMg6FNzZEiYvkngh8pdxJI2%2Bneh%2F9h8tqW9J5PVnV" target="_blank" rel="noopener">仰之弥高-二十世纪中国画大家展</a></strong></p><p>20世纪中国画作继往开来，有种特别的魅力。南京博物院这一次的联合展览规模据说是国内之最，由于展品较多，分为上下半场，上半场在2019末尾，下半场在2020开年，我只看了上半场，就已经觉得展品数量多且质量高，2个多小时下来后意犹未尽，又去买了配套的书珍藏。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/仰之弥高1.png" alt="展览的八大家"></p><p>国画当中，我最喜爱也是唯一爱的是山水画，尤其以写意为主，创造了与西方讲究逼真复刻思想截然不同的心境体验。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/仰之弥高2.jpg" alt="展览封面是张大千的江岸图"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/仰之弥高3.jpg" alt="展览门票"></p><p>齐白石挑了几张图，从左到右分别是山溪群虾图，秋荷图，松寿图，寻旧图。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/齐白石.jpg" alt></p><p>展出的黄宾虹作品中，狮子林望松谷园，练江南岸图，栖霞山居图，天目奇峰图，湖滨山居图，江村图，黄山记游图，九子山，练滨草堂图个人都非常喜欢，可惜手机里只翻出其中三张。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/黄宾虹.jpg" alt></p><p>徐悲鸿：双马图|侧目图|飞鹰图，愚公移山图，鹰图，山鬼图，九方皋相马图。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/徐悲鸿.jpg" alt></p><p>张大千的画作想必很多人都喜欢，毕竟颇有古人风骨。下面挑出的几张是江岸图，泼墨荷花图，荷花图，仿杨昇笔意设色山水图，夏山高隐图。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/张大千.jpg" alt></p><p>潘天寿：青山白云图，雄视图，雨后千山铁铸成，记写雁荡山花。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/潘天寿.jpg" alt></p><p>林风眠，中西集合的一位画家：夜枭图，劈山救母图，早春暮色图，风景图，灰鹭图，鱼鹰小舟图。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/林风眠.jpg" alt></p><p>傅抱石：万竿烟雨图，待细把江山图画，兰亭修褉图，龙蟠虎踞今胜昔。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/傅抱石.jpg" alt></p><p>李可染：苏州虎丘图，万山红遍 层林尽染，崇山茂林源远流长图，无尽江山入画图，树百重泉图。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/李可染.jpg" alt></p><p>近日受到2019-ncov病毒影响，学校也不准提前到校，下半场的展览不知还能不能赶得上了。</p><p>19年除南京城外，只到访过西安和成都。西安是“故地重游”，当时本科毕业时去了甘肃，正好顺路就去西安找朋友玩了趟。当时正值夏季，大西北天气炎热，白天毒辣的阳光炙烤得人很难受，不过晚上舒服，没有南京那么闷热，有些凉爽。西安的夜景确实很不错，市民们的夜生活也丰富多彩。大雁塔，大唐芙蓉园周边的柔和灯光映衬着来回的身着汉服的姑娘，恍如隔世。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/西安夜景1.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/西安夜景2.jpg" alt></p><p>第一次玩那会没吃上臊子面，算是一大憾事，19年年底开会的时候正好借着机会找了一家“乡党臊子面”体验了一把一顿“几十碗”的快感。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/臊子面.jpg" alt="疯狂吃各种口味的臊子面 &gt;_&lt;"></p><p>吃撑了肚子，跟朋友一起到了附近的城墙走了一圈，夜景还是像以前来那样令人惊喜。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/西安城墙1.jpg" alt="城墙的夜景"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/西安城墙3.jpg" alt="站在小桥上拍，护城河？"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/西安城墙2.jpg" alt="走着走着发现一条走廊里挂着古风似的红灯笼，觉得非常喜欢，就拍下来了，但是我的GR II好像没对上焦"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/西安城墙4.jpg" alt="河边上的小人雕塑"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/西安城墙5.jpg" alt="商店旁一只会变色的鹿"></p><p>两次的西安之行，没有很长时间，达不到能够感受一个城市风格的力度。粗略的印象是面很好吃，夜景很好看，人民生活丰富也比较健谈，可惜大西北灰尘太多，空气质量差了点，嗓子实在吃不消。</p><p>大学本科四年来，成都是我一直想去但没能去成的地方，要么是没钱，要么是没时间。自西安开会结束回来后，正好收到要赶去四川成都陪师兄们做oral的消息，甚是激动。4天的学术会议，只花了几个小时参加作报告就溜了，剩下的时间都和师兄，朋友们在成都吃喝玩乐闲逛了。刚下飞机的那天就跑去谭鸭血吃了火锅，和热情的成都朋友们喝得酩酊大醉，半夜回旅馆路上还看到街上人来人往，分外悠闲。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/谭鸭血.jpg" alt="谭鸭血火锅店。不得不说成都的火锅确实很好吃，佐料也非常可口"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/成都1.jpg" alt="晚上喝晕了，第二天起来已经是中午了，参会前就近找了一家小吃店，点了老妈蹄花，重庆小面和夫妻肺片"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/成都2.jpg" alt="汇报结束去文殊院闲逛，吃了正宗的龙抄手，晚上被朋友拉去川菜馆"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/成都3.jpg" alt="吃罢晚饭到春熙路溜达"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/成都4.jpg" alt="听说成都人民的一大爱好是喝下午茶"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/成都5.jpg" alt="到朋友的学校川师大参观"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/成都6.jpg" alt="宽窄巷子"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/成都7.jpg" alt="人民广场。大爷大妈聚集在此开展各种文艺活动，喝茶闲聊，掏耳朵打牌，应有尽有。我们待在一处啃着兔头"></p><p>尽管只有将近4天的短暂游玩，但成都却给我留下了与众不同的印象，特色美食，热情的小伙伴，悠然的生活方式，以及温润的气候让我一度不想归宁。临行的出租车司机大叔对我们说“你们年轻人不要随便来四川工作生活”，估计也是怕缓慢慵懒的生活节奏让我们丧失奋斗的意志了吧。</p><h2 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h2><p>19年的阅读量比18年稍多了些，主要是在L同学的影响下，翻了很多画册或者手绘书。这些书看起来快，又不需要花时间思考，正好适合我现在的碎片阅读时间。本来今年打算着手学习一下画画，不管是什么绘画分支，得画起来才行，硬盘里也收藏了很多很多绘画教材和一些作品集，然而到现在都几乎没怎么翻过。</p><p>19年让我感触最大的两本书是两部小说，一本是科幻小说，《索拉里斯星》，塔可夫斯基也曾将其拍成电影《飞向太空》；另一本是写实结构主义小说，《城市与狗》，拉美文学作品，写作方式不怎么规则，但是阅读体验很好。我在豆瓣也建了个<a href="https://www.douban.com/photos/album/1686128240/" target="_blank" rel="noopener">相册</a>存放自己有关看过的书的一些手拍照片，算是思考总结，也算是一种鞭策自己多阅读的手段吧。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/阅读.jpg" alt="一大半的阅读量是绘画有关的书籍"></p><ul><li><p><a href="https://book.douban.com/subject/3618253/" target="_blank" rel="noopener">和空姐同居的日子</a>是初中是很喜欢的一部广播小说，作者三十也居住在南京。内容虽然有些yy的成分，但是总体还算是清新自然，单纯美好。19年年初那会觉得心情不好，就重听了一遍，个人觉得第一部（季）质量最好，第二部可能是因为大家不喜欢第一部的结局而写出来的。现在喜马拉雅还有第一部的广播<a href="https://www.ximalaya.com/youshengshu/3087022/" target="_blank" rel="noopener">资源</a>。</p></li><li><p><a href="https://book.douban.com/subject/26651221/" target="_blank" rel="noopener">线性代数的几何意义</a>是19年年初看MIT 线性代数公开课阅读的课外材料。学过线代后看一遍会有不小的收获，也算是解释数学意义的一个体现吧，尤其是在实际工程上。</p></li><li><p><a href="https://book.douban.com/subject/5908654/" target="_blank" rel="noopener">给青年人的信</a>是在自己的kindle上当作睡前读物看完的。大一时候听老师讲里尔克时心潮澎湃，专程去图书馆借了里尔克的诗集，囫囵吞枣地看完了，现在几乎对他的诗歌也没什么印象了，除了经典的几首。实际上，这本书作为枕边读物，常拿出来翻翻还是很有裨益的。里尔克写信的文字十分真诚，字里行间透露出对生活的热爱，对细节，真善美的观察和思考，不仅是对诗歌创作者，即使是对忧郁迷茫中人，也有不小的指导和振奋作用。</p></li><li><a href="https://book.douban.com/subject/25898192/" target="_blank" rel="noopener">索拉里斯星</a>是波兰科幻作家斯塔尼斯瓦夫·莱姆的代表作，也是我目前最喜欢的科幻小说之一。它不同于一般的大格局的科幻小说，关心宇宙和人类的命运，或者讽刺人性等等，而是着眼于人的记忆和内心之中的隐秘之痛，和特德·姜的《你一生的故事》有些异曲同工之妙。这篇小说的故事内容很简单：人类在一个遥远的星系里发现了一个特别的天体，称之为索拉里斯，人类派出了空间站去探测研究这颗特别的天体，地球上还出现相关的研究学科。这颗天体本身就是一个生命体，表面几乎都被”海洋“覆盖，那些奇怪的液体可以随便被天体操作，模拟出各种形态，甚至是有自己意识的人类实体。男主到达空间站后发现几位同事都被索拉里斯星折磨得濒临崩溃，自己也随着索拉里斯星的实验慢慢揭开其中的秘密。实际上，索拉里斯星的这些实验都是处于自己对宇宙好奇的探索，空间站的人类在其面前没有任何秘密可言，包括记忆。记忆和亡妻之痛（或者说是每个人内心独有的隐秘之痛）是最近很着迷的点，总觉得它们代表着人类特有的印记，以此为内容的电影总能让我感触良多，弥漫着无法抗拒的悲痛感，像是“海边的曼彻斯特”，“登月第一人”等都可以说这种类型电影的代表。莱姆的索拉里斯星恰好也是有着这两个点，但是又不全是，书中包含着对人类道德，思想和精神世界的审视，虽然也有那种宇宙的孤独感，但是却又不全是那种冰冷的感觉。索拉里斯星上的大海，不像地球上孕育着生命，却表现出一种宇宙的隐士般的好奇，无欲，与人类触碰却又收回了手。书中有关男主的心理描写和索拉里斯星的描写都十分出色，在一定程度上缓解了阅读的枯燥。</li><li><a href="https://book.douban.com/subject/26827211/" target="_blank" rel="noopener">小北野武</a>是导演北野武的童年生活记录，同时也亲自手绘了插图。北野武的真诚，诙谐，充满童趣的特点在这本书中表露无疑，以孩子天真无邪口吻说出的故事，表面让人发笑，背后实则充满辛酸之感，尤其读到最后“父亲的遗物”那里，看到北野武老爸叫菊次郎，突然觉得有点抑郁~。书里的插画我也上传到<a href="https://www.douban.com/photos/album/1686213156/?m_start=0" target="_blank" rel="noopener">豆瓣相册</a>了。</li><li><a href="https://book.douban.com/subject/26863095/" target="_blank" rel="noopener">素描的艺术：席勒</a>这本书是我在图书馆找书时无意发现的，看到书名时翻了一下，非常惊喜，国内有关埃·贡席勒作品介绍的书籍似乎不多，这本作品集编排得很用心，就借了出来。此外，前几年有部关于他的电影<a href="https://movie.douban.com/subject/26421474/" target="_blank" rel="noopener">埃贡·席勒：死神和少女</a>，感兴趣可以看看。埃贡·席勒是个很有艺术天赋的艺术家，但是画作不走寻常路，人物通常瘦骨嶙峋，不符合正常规律的细长，大多都是反映内在世界得敏感，孤独，暧昧，神经质等，作品风格好像叫立体解构主义或表现主义？他也有很多自画像，评论家认为其有着明显的自恋和自我剖析倾向。作为作品集，自然文字内容会少很多，主要是不做二次加工展示给读者，让其自己去感受自己所看到的内容。书中大部分作品传到了我的<a href="https://www.douban.com/photos/album/1686395459/" target="_blank" rel="noopener">豆瓣相册</a>上。</li></ul><p><img src="/2020/01/10/2019-2020-漫长的告别/埃贡席勒1.jpg" alt="埃贡·席勒作品"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/埃贡席勒2.jpg" alt="埃贡·席勒自画像"></p><ul><li><p><a href="https://book.douban.com/subject/30257644/" target="_blank" rel="noopener">念书还是工作，这是一个问题</a>是法国一位女博士根据自己的经历画出的漫画故事。虽然是文学博士，但是很真实了，特别是在无穷的资料和方向中不断摸索，在碰壁，消沉，以及自我安慰中挣扎，有时还不得不面对社会和家庭的一些压力……在理工科界也有一本phd博士写的书，叫”<a href="http://pgbovine.net/PhD-memoir/pguo-PhD-grind.pdf" target="_blank" rel="noopener">the phd grind</a>”。读不读博这个问题，感觉好难回答，那些读过了的人和没迈进去步子的人，感觉精神上应该有很大不同吧～或许那些有心得人会在读博期间不断地审视自己吧。</p></li><li><p><a href="https://book.douban.com/subject/30358084/" target="_blank" rel="noopener">线条：斯坦伯格的世界</a>是漫画家斯坦伯格的一部作品集，这也是我偶然在图书馆发现的一本书。斯坦伯格的线条漫画作品简洁机智，有的讽刺意味很强，有的妙趣横生。他也曾经为杂志”New Yorker“作过不少次封面插画，在国外知名度很高。原书收集的作品比较多，我从书里挑了一些，放在了<a href="https://www.douban.com/photos/album/1686466375/" target="_blank" rel="noopener">豆瓣相册</a>里。</p></li></ul><p><img src="/2020/01/10/2019-2020-漫长的告别/索尔斯坦伯格.jpg" alt="斯坦伯格作品"></p><ul><li><a href="https://book.douban.com/subject/27154489/" target="_blank" rel="noopener">蓝色小药丸</a>是一部讲述HIV携带者相爱故事的漫画，作者画风凌厉，内容却细腻感人。难以想象，这样的事会如何发生在自己身上。</li></ul><p><img src="/2020/01/10/2019-2020-漫长的告别/蓝色小药丸.jpg" alt="蓝色小药丸部分内容"></p><ul><li><a href="https://book.douban.com/subject/26597979/" target="_blank" rel="noopener">梁山伯与祝英台</a>是一部似皮影戏般的绘本，当初从多抓鱼凑单买的，薄薄的一本，可以拿来收藏。</li></ul><p><img src="/2020/01/10/2019-2020-漫长的告别/梁山伯与祝英台.PNG" alt="梁山伯与祝英台插图"></p><ul><li><a href="https://book.douban.com/subject/25982252/" target="_blank" rel="noopener">蝙蝠侠：致命玩笑</a>。在凤凰叔“Joker”资源未出来之前解渴用的。</li><li><a href="https://book.douban.com/subject/26930483/" target="_blank" rel="noopener">英国插画师</a>里面介绍了10位不同风格的英伦插画师，有卡通的，也有成人暗黑的。当中有位Aubrey Beardsley(奥伯利·比亚兹莱)曾是奥斯卡·王尔德的情人，曾为其作品莎乐美绘制插图，带有日本浮世绘风格，算是这10位插画师当中风格最与众不同的一位，可惜英年早逝，20几岁就陨落了。此外，奥伯利在英国<a href="https://www.bl.uk/collection-items/the-yellow-book" target="_blank" rel="noopener">the yellow book</a>期刊中的插画作品也非常经典，我之前还专门google找到了几期数字化的pdf，现在不知道链接存哪而去了/_\。</li></ul><p><img src="/2020/01/10/2019-2020-漫长的告别/奥伯利1.jpg" alt="书中有关奥伯利的介绍"></p><p><img src="/2020/01/10/2019-2020-漫长的告别/奥伯利2.jpg" alt="奥伯利作品"></p><p>除了奥伯利之外，Arthur Rackham(亚瑟·拉克姆)的童话插画作品也十分梦幻，甚至带有些诡异。他为《爱丽丝梦游仙境》和《安徒生童话》绘制的插画神秘阴暗，带有一丝哥特风，风格十分独特。后来我在网上搜索，希望能翻到他的数字化作品，可惜童话插画作品太多，不如奥伯利的那么好找，只好放弃。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/拉克姆.jpg" alt="拉克姆及其童话插画作品"></p><p>这本书的人物介绍和作品选取都很棒，但我还是没看过瘾，要是编辑能放多点图就好了。书中提到的10位插画师中我个人比较喜欢Kate Greenaway(凯特·格林纳威 )，Heath Robinson(希思·罗宾逊)，Edward Lear(爱德华·李尔)， Aubrey Beardsley(奥伯利·比亚兹莱)，John Millais(约翰·米莱斯)，Arthur Rackham(亚瑟·拉克姆)和Walter Crane(沃尔特·克莱恩)这7位，我也挑选了他们的一些作品放在<a href="https://www.douban.com/photos/album/1686128240/?m_start=36" target="_blank" rel="noopener">豆瓣相册</a>。</p><ul><li><a href="https://book.douban.com/subject/26904293/" target="_blank" rel="noopener">素描的艺术：毕加索</a>这本书与席勒那本同属一个系列。由于内容限制在素描，所以很多作品都是平时不常见的。这本书展示了毕加索自己平常生活中的速写，寥寥几笔，形神兼备，让我感觉比那些挂在博物馆里的大作更有温度和情感。美中不足的是，作品排列似乎没有按照一定的顺序，比较杂乱，也缺乏一定的说明。部分作品上传到了<a href="https://www.douban.com/photos/album/1686128240/?m_start=18" target="_blank" rel="noopener">豆瓣相册</a>。</li></ul><p><img src="/2020/01/10/2019-2020-漫长的告别/毕加索.jpg" alt="毕加索素描作品"></p><ul><li><a href="https://book.douban.com/subject/30304849/" target="_blank" rel="noopener">拍电影时我在想的事</a>是日本导演是枝裕和的随笔集，交织了电影拍摄历程，人生感悟。是枝裕和导演和村上春树的随笔风格很像，真诚又细腻，谦逊又不乏思考，没任何架子，感觉与这样的人聊天不仅很舒服，而且还会获益良多。不过说来惭愧，是枝裕和的影视作品看得不多，因此这本书也就是快速翻阅般地看完了，对他的作品有些了解之后再去看这本书或许阅读体验会更好。</li><li><a href="https://book.douban.com/subject/30228730/" target="_blank" rel="noopener">观山海</a>是杉泽根据《山海经》所画的异兽部分。杉泽是微博上很火的手绘古风神话鬼怪艺术家，托L同学强烈推荐，我才开始慢慢了解到此类画风的作品和艺术家。总的来说，第一次看很新鲜，色彩很美，梦幻又诡谲。但是看多了有些审美疲劳，尤其是人物的脸部造型，觉得没什么变化，可能是一种想象和代表吧。同样，我也拍了些放在<a href="https://www.douban.com/photos/album/1688487996/" target="_blank" rel="noopener">豆瓣相册</a>上。</li></ul><p><img src="/2020/01/10/2019-2020-漫长的告别/观山海.jpg" alt="随意放几张"></p><ul><li><a href="https://book.douban.com/subject/26727997/" target="_blank" rel="noopener">Neural Network and Deep Learning</a>和魏秀参的<a href="https://book.douban.com/subject/30381203/" target="_blank" rel="noopener">卷积神经网络</a>是当初准备实习面试补的书，写得都还可以，不过魏秀参博士那本不太适合刚入门卷积神经网络的小白看。</li><li><a href="https://book.douban.com/subject/10750155/" target="_blank" rel="noopener">数学之美</a>应当属于理工科内，尤其是是计算机专业圈内久负盛名的一本书，主要是以NLP自然语言处理，搜索引擎算法等讲述一些具体数学算法在工程中的实际应用。没有研究过相关领域，作为科普读物看看。</li><li><a href="https://book.douban.com/subject/26695174/" target="_blank" rel="noopener">灯塔</a>是一本讲述孤独忧伤的“畸形儿”困在灯塔几十年，渴望自由和情感的漫画故事，故事很短很有想象力，像电影分镜一般，推荐阅读。其实19年也有一部电影也叫<a href="https://movie.douban.com/subject/30143336/" target="_blank" rel="noopener">灯塔</a>（罗伯特·帕丁森和威廉·达福主演，导演之前的<a href="https://movie.douban.com/subject/26276364/?from=subject-page" target="_blank" rel="noopener">女巫</a>我也很喜欢），不过故事截然不同，要暗黑惊悚得多，目前还没来得及看，在此不再多说啦。</li><li><a href="https://book.douban.com/subject/26780984/" target="_blank" rel="noopener">城市与狗</a>算是今年读到的比较惊喜的一部小说。因为APP MONO中的一个海报特地从图书馆借来，阅读之前也并不知道是结构主义小说，刚开始的时候没有准备，确实被混乱的时间线叙述，不同的人物视角搞得有点蒙，直到第一部分一半，整体故事脉络就慢慢缕清了。后面的阅读体验极佳，略萨的对话和心理描写别具一格，画面感很强，读者自己很容易就代入进去，成为旁观者或主角，很难想象20出头的略萨就已经有如此高的写作技巧。“我曾有过二十岁，我也不同意任何人说那是最美好的年华。”这句话算是全书最无奈的最忧伤的精髓了。</li></ul><p><img src="/2020/01/10/2019-2020-漫长的告别/城市与狗.jpg" alt></p><h2 id="音乐，电影，公开课以及其他"><a href="#音乐，电影，公开课以及其他" class="headerlink" title="音乐，电影，公开课以及其他"></a>音乐，电影，公开课以及其他</h2><p><img src="/2020/01/10/2019-2020-漫长的告别/音乐专辑.png" alt>19年听的除了一般的流行歌曲之外，主要是古典和爵士。听古典是由于一方面可以培养和提升个人品味，另一方面是可以放松舒缓心情，帮助大脑思考；听爵士呢主要是之前受村上春树和伍迪·艾伦的影响，自己也很喜欢爵士乐营造出的那种氛围，那种音乐里的情感和故事性，很能激发人的想象力。</p><p>目前两种类型的音乐领域我都处于泛听阶段，当然也有一些比较喜好的作曲家和演奏家等，像德彪西，萨蒂，肖斯塔科维奇，鲁宾斯坦，查里贝克，路易斯·阿姆斯特朗，Karen Souza等。接下来还会多去探索，等到了一个适合的饱和点，然后再去慢慢整理自己在这方面的喜好，同时我也在看耶鲁大学<a href="https://www.bilibili.com/video/av16809187?from=search&amp;seid=10378298280102696019" target="_blank" rel="noopener">聆听音乐</a>这门公开课，当作一个参考的鉴赏指导，希望明年总结时也能给其他想入门的人一些听的建议。</p><p>观影上，19年看的总量不多，主要是当时的爆米花商业片加上一些偶尔看到的冷门片。以前都是专门找自己喜欢的口碑片来看，现在基本都是想找些没什么情感压抑的爽片，可能是19年比较劳累了吧:-}。电视剧方面倒是看了好几部韩剧，比如《春夜》，《我的鬼神大人》等，另外还有一部木村拓哉的《东京大饭店》，看完之后学做菜的热情猛地高涨起来，木村拓哉的”带货能力“是真的强:-D。美剧里由于《权力的游戏》最终季和《黑镜》第五季都让我很失望，后面就没再看其他的了。</p><p>我从自己的观影记录中挑出了10佳，其中昆汀的《好莱坞往事》是最喜欢的，而且还非常喜欢前面很长时间的絮絮叨叨，后面的”曼森杀人案“高潮也是一贯的昆汀式B级风格；马丁·斯科塞斯的《爱尔兰人》也还可以，更像是一曲挽歌，满怀忧伤唏嘘之感；凤凰叔的《小丑》中规中矩，凤凰叔的表演加上剧本上的点到为止成就了这部高分片；最惊艳的当属于基努·李维斯的《John Wick 3》了，续集不失前作的水准，反而让故事中的杀手世界变得越来越丰满…</p><ul><li><a href="https://movie.douban.com/subject/27087724/" target="_blank" rel="noopener">好莱坞往事</a></li><li><a href="https://movie.douban.com/subject/26909790/" target="_blank" rel="noopener">疾速备战</a></li><li><a href="https://movie.douban.com/subject/27119724/" target="_blank" rel="noopener">小丑</a></li><li><a href="https://movie.douban.com/subject/33415943/" target="_blank" rel="noopener">我失去了身体</a> 比较意识流的一部法国动画片</li><li><a href="https://movie.douban.com/subject/27138615/" target="_blank" rel="noopener">自卫的艺术</a> 一部诡异黑色片，如果不是卷西，效果可能会大打折扣</li><li><a href="https://movie.douban.com/subject/30165034/" target="_blank" rel="noopener">昨日奇迹</a> 纪念致敬披头士乐队</li><li><a href="https://movie.douban.com/subject/6538866/" target="_blank" rel="noopener">极速车王</a> 改编自福特与法拉利的勒芒大赛竞争，没有落入俗套的热血，也顺带讽刺了系统和机构以及对其的反抗</li><li><a href="https://movie.douban.com/subject/4185834/" target="_blank" rel="noopener">丧失乐园2</a> 时隔10年的续作，一如既往的脑洞大开</li><li><a href="https://movie.douban.com/subject/27089612/" target="_blank" rel="noopener">纽约的一个雨天</a> 伍迪·艾伦的近作，甜茶饰演的男主彷佛就是老爷子本尊</li><li><a href="https://movie.douban.com/subject/6981153/" target="_blank" rel="noopener">爱尔兰人</a></li></ul><p><img src="/2020/01/10/2019-2020-漫长的告别/电影.jpg" alt></p><p>戏剧方面，除了一如既往地搜寻NT Live的资源，一边也慢慢探索其他类型的舞台剧，比如音乐剧，芭蕾舞剧等，不过19年还没完整看过其中的任何一部。对于舞台剧，要想有好的观感体验，获得共情能力，是必须要去现场的，可惜资源和机会有限，能偶尔盼到一两部电子拷贝也是极好的。</p><p>听公开课是19年下半年开展的一个计划，每天都拿出30min~60min的时间花在一些有意义的课程上，不管是艺术类，理工科类，不管是通识概括还是深入讲解都可能拿来学习。但是自己的自律性还不够，没能每天坚持，尤其是年末那段时间基本属于自我放纵阶段了，20年里这个计划要得严格执行点才行。</p><ul><li><a href="https://www.bilibili.com/video/av68107287?from=search&amp;seid=15101072632370554317" target="_blank" rel="noopener">梁老师的爱情课</a> 这个是一席 万象组织的一个演讲，内容比较充实，聊的也很透彻，需要观众在课后反复思考。里面的很多价值观我是持赞同态度的，然而爱情是人类中最复杂的问题之一，感性的，理性的手段都不一定能很好处理其中的纠葛联系。这个课更多地是让我看到了爱情中一些行为背后的人性因素，权当一种拓展知识面的材料了。</li><li><a href="https://www.bilibili.com/video/av18102433" target="_blank" rel="noopener">戏剧入门-张先</a> 这个是在B站上发现的为数不多的国内戏剧入门公开课之一，是中央戏剧学院开设的。课程内容个人觉得不是很充足，学习体验不是很好，推荐后面Crash Course中的剧院公开课系列。</li><li><a href="https://www.bilibili.com/video/av21376839" target="_blank" rel="noopener">Crash Course-Computer Science</a> 看的第一门Crash Course公开课，后面准备把整个系列中自己喜欢的系列都刷一遍。这门课我看很多微信公众号都推送了，内容上确实做得很用心，字幕组<a href="https://github.com/1c7/crash-course-computer-science-chinese" target="_blank" rel="noopener">翻译</a>的也很棒。</li><li><a href="https://www.bilibili.com/video/av13762839" target="_blank" rel="noopener">Crash Course-Philosophy</a> 课程基本上是以提出哲学问题形式展开的，看完之后可能需要看一些书籍辅助理解（这里有一个推荐的<a href="https://www.zhihu.com/question/19588342/answer/786026336" target="_blank" rel="noopener">知乎回答</a>)，然后再去找自己感兴趣的哲学家的专著阅读。</li><li><a href="https://www.bilibili.com/video/av19463816?from=search&amp;seid=1384032167897391415" target="_blank" rel="noopener">Crash Course-Theatre</a> 这个课看了一大半了，字幕组还没翻译完，内容属于通识类，老师的讲说风格有种”学术幽默“，挺有趣，只不过东西太多，记不住，有点伤脑筋@_@。</li></ul><p>我非常频繁的找有关戏剧的东西看是因为它这种融合了人性的考察、无处不在的想象力、轮回式的角色扮演的艺术形式让我深深着迷，我越深入其中，越发现它其实探讨的是人的命运问题，通过把虚构的/非虚构的人的过去、现在、和未来展现在观众面前，让我们发现生活的真相。</p><p>什么是戏剧：</p><blockquote><p>Percy Bysshe Shelley The Cenci, ACT1. SCENE1</p><p>The highest moral purpose aimed at in the highest species of the drama, is the teaching the human heart, through its sympathies and antipathies, the knowledge of itself.</p></blockquote><p>亚里士多德关于悲剧的描述：</p><blockquote><p>An imitation of an action that is serious, complete, and of a certain magnitude; in language embellished with each kind of artistic ornament, the several kinds being found in separate parts of the play; … in the form of action, not of narrative; through pity and fear effecting the proper purgation.</p></blockquote><p>这些公开课在看的时候我是没有记笔记的，后来发现几乎全都忘光了，而且看的时候难免会分心走神，因此还是推荐做一些总结，加强记忆和理解，否则看公开课就成了一种形式了，起不到扩展知识面的作用。</p><h2 id="摄影捕捉笔记"><a href="#摄影捕捉笔记" class="headerlink" title="摄影捕捉笔记"></a>摄影捕捉笔记</h2><p>19年的摄影陷入了瓶颈，同时拍摄热情也大大减少了。在写这一版块之前，我翻了翻手机相册，19年的作品少得可怜，更何况在这些片子之中基本上没有满意的作品。专门出去街拍的情况几乎没有，只是有时候碰巧出去会携带下GR II，因此相机SD卡里面的照片就更糟糕了。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/相机.jpg" alt="对于器材的热忱远比拍摄本身要多得多。19年在B站看的相机测评视频占据了总量的一大半，虽然我极力抑制，但那种狂热很快又会卷土重来"></p><p>为了寻求突破，我开始陆续看一些大师的摄影作品，而且尽量按照他们出版的摄影集的顺序看，毕竟目前还买不起纸质摄影出版物。从森山大道，到荒木经惟，再到深濑昌久，从薇薇安，罗伯特·卡帕，到罗伯特·弗兰克，再到亚历克斯·韦伯，平卡索夫，Stephen Gill，只要是有照片看出了某种意义，就会专门花时间找来看看该摄影师的作品集。下半年的时候，渐渐在B站上发现了一些“宝藏UP主 ”，他们有的将自己的作品剪成视频，有的讲解自己的拍摄心得和后期技巧，有的介绍摄影大师和他们的作品集，还有的是有了很高摄影鉴赏水平和摄影理念，直接在视频中讲解属于自己的摄影笔记，这些对我都有或多或少的帮助，非常感谢他们的分享。</p><p>回到我自己的摄影作品上来，拍景的明显多于拍人的，实际上，我个人一开始是倾向于拍人的，尤其是人景交织的作品对我最具有吸引力，人的独特性会让景更加生动化，富有情感。不过慢慢地，我不再执着于此，在开始拍之前，我不应该对将要拍到的内容做任何预期的规划，我觉得那样会破坏掉我拍摄时候的“捕捉力”，尤其对是任何未知的，不可预知的细节的捕捉，这些细节不局限于任何物和人，不局限于何种情绪，何种故事，恰是由于这些细节让观者解读到了属于他们每个人的独特的世界，成就了作品的魅力。</p><p>诚然，我私以为摄影理念应该会随着自己的拍摄历程的积累而不断地发生进化，这种进化可能是完善性的，也可能是转变性的，具体可能会取决于人的想象力。等到写20年总结的时候，或许自己又是另一番说法了。</p><p>我挑了几张自己的图片，基本上都是19年拍的，少数几张是19年之前的。自己的作品数量不多，质量也不高，希望20年能作为一个新的起点，多多去实践和思考。</p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄1.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄2.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄3.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄4.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄5.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄6.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄7.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄8.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄9.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄10.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄11.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄12.jpg" alt></p><p><img src="/2020/01/10/2019-2020-漫长的告别/自摄13.jpg" alt></p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>其实我在写总结之前，一直在想该取什么题目，由于第一次这样写年终总结，很多事情拿捏不准，有点小紧张。“漫长的告别”这个名字来源于美国侦探小说家雷蒙德·钱德勒的同名小说，书里面有句经典的话是“To say goodbye is to die a little.”，不过我取这个名字没有这么重的情怀，也没有这么哀伤，我只是觉得19年让我明白了很多事情，真正地认识到”这个充满偏见的，残酷的真实的世界“和渺小的自己，所以我想以此为人生的一个分割线，整理一下，告别20多年的过去，然后继续下一段路。</p><p>现在2020年已经过去一个月了，我希望这年可以：</p><ul><li><p>把自己未来3，4年的路明确下来，不管是读博还是工作，都希望能追求到一个满意的结果；</p></li><li><p>现场看一次音乐会和戏剧或者舞台剧；</p></li><li><p>好好写一篇论文；</p></li><li><p>多多看书，看不同类型的书，开卷有益；</p></li><li><p>坚持摄影，日常摄影；</p></li><li><p>多陪陪、关心身边爱我的人；</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;搭博客的时间大概只有一年，当初是因为受到&lt;a href=&quot;http://pluskid.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pluskid网站&lt;/a&gt;和博客的影响，觉得做一个有趣的人，学习自己想了解的，放淡自己的心态是是每天很重要的事，因此想记录下来，有些规划，也好作为充实自己的见证，搁以前每年都来一个总结这件事我是不怎么做的。求学生涯没有结束，每年都是反复地上课，做项目，似乎都是浑浑噩噩度过，科研中的磕碰和一些不甘心也会时不时消磨自己的意志，让自己在怀疑，焦虑，麻木，强迫的交织中蹒跚着。我之前也知道很多励志结论，他们说的都很对，但每个人的生活总是悲喜交加的，而大多数情况下都是在平平无奇中暗生悲戚，羁绊越多的人似乎悲戚越浓。&lt;/p&gt;&lt;p&gt;2019年的寒假里，我翻看了很多&lt;a href=&quot;http://freemind.pluskid.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pluskid的博客&lt;/a&gt;内容，其中以年终总结最多。一开始我是被博主优秀的履历所吸引，后来在那些博客里我读到了对于生活细节的热爱，以及对于艺术，真理和其他人类活动的浓郁兴趣，真诚，质朴同时又很能温暖人，所以算是“始于履历，陷于才华”吧！（厚脸皮地说，有一种惺惺相惜之感）。罗曼罗兰在《米开朗琪罗传》中有句很出名的话：&lt;em&gt;“Il n’ya qu’un héroïsme au monde : c’est de voir le monde tel qu’il est et de l’aimer.（世界上只有一种真正的英雄主义，那就是认识生活的真相后依然热爱它）”&lt;/em&gt;，我开始渐渐明白其中的血肉故事，目前虽然谈不上热爱，但可以说是慢慢从走出到走入，慢慢进入状态。&lt;/p&gt;&lt;p&gt;接下来，我想还是先从自己的学校科研生活讲起，然后再去讲讲自己看过的书，去过的地方，听过的音乐，拍过的照片，看过的电影等，一步步勾勒出自己的故事，就像是在索拉里斯星上模拟出的记忆花园，感想估计无法给出多少，倒是想能抓住几分情绪便好。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/2020/01/10/2019-2020-漫长的告别/电影大师剧照.jpg&quot; alt=&quot;电影&amp;#39;大师(the Master)&amp;#39;剧照&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="纪录与总结" scheme="http://densecollections.top/categories/%E7%BA%AA%E5%BD%95%E4%B8%8E%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="learning" scheme="http://densecollections.top/tags/learning/"/>
    
      <category term="movies" scheme="http://densecollections.top/tags/movies/"/>
    
      <category term="reflection" scheme="http://densecollections.top/tags/reflection/"/>
    
      <category term="summary" scheme="http://densecollections.top/tags/summary/"/>
    
      <category term="memory" scheme="http://densecollections.top/tags/memory/"/>
    
  </entry>
  
  <entry>
    <title>RCNN-series-in-object-detection(续)</title>
    <link href="http://densecollections.top/2020/01/10/RCNN-series-in-object-detection-%E7%BB%AD/"/>
    <id>http://densecollections.top/2020/01/10/RCNN-series-in-object-detection-续/</id>
    <published>2020-01-10T02:51:19.000Z</published>
    <updated>2020-04-07T07:31:53.184Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>自Faster R-CNN后，基于深度学习的目标检测框架大致形成，且精度也较为不错。在这之后，围绕着对图像数据更深层次理解，以及根据现有结构进行改进成为了一个主流点。</p><p><a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="noopener">Speed/accuracy trade-offs for modern convolutional object detectors</a></p><p><a href="https://arxiv.org/abs/1912.05190" target="_blank" rel="noopener">IoU-uniform R-CNN: Breaking Through the Limitations of RPN</a></p><h2 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a><a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="noopener">R-FCN</a></h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>内容有参考：</p><ul><li><a href="https://arleyzhang.github.io/articles/7e6bc4a/" target="_blank" rel="noopener">blog1</a></li></ul><a id="more"></a><h3 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h3><h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a><a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">FPN</a></h2><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>Feature Pyramid Networks(FPN)考虑到了卷积神经网络中各个尺度特征图的作用，认为像Fast R-CNN和Faster R-CNN这样的目标检测网络只用了一个特征图去来做RoI 和搜寻proposal，不一定能很好地处理全部尺寸的物体，虽然这种方式是为了speed-acc之间的trade off。作者认为，网络越深层产出的特征图往往语义信息越高，但是位置信息比较模糊，对小目标检测来说不太好；浅层的特征图提取出来的特征都是比较低级的边缘，纹理信息，但是分辨率好，位置信息得到了保留，因此将这些特征图结合起来，充分利用到高层语义信息，同时也不丢掉位置信息，应当能很大程度上提高检测的精度和鲁棒性。</p><p>博客内容有参考：</p><ul><li><a href="https://vision.cornell.edu/se3/wp-content/uploads/2017/07/fpn-poster.pdf" target="_blank" rel="noopener">FPN在CVPR的poster</a></li><li><a href="https://zhuanlan.zhihu.com/p/34144226" target="_blank" rel="noopener">blog1</a>, <a href="https://www.jiqizhixin.com/articles/2017-07-25-2" target="_blank" rel="noopener">blog2</a>, <a href="https://zhuanlan.zhihu.com/p/61536443" target="_blank" rel="noopener">blog3</a></li></ul><h3 id="Content-1"><a href="#Content-1" class="headerlink" title="Content"></a>Content</h3><p>为了让各个尺寸的物体都能很好的检测到，以往的工作提出了图像金字塔，利用不同大小的图像尺寸进行滑窗，到了深度学习时代，直接通过神经网络输出的高层特征图，进行对应特征上的检测分类，之后考虑到尺度问题，开始挑选网络中产出的几个level的feature map，分别进行检测，然后合并筛选给出最后的结果。FPN认为，既然检测既需要高层的特征便于分类和统筹全局观念，又需要特征图具有一定的分辨率去定位物体的图像位置，那么应该想个办法将这两个重要信息结合起来。但是在网络的前向传播中，这两者是矛盾的，低层的特征图特征抽象度不够，高层的特征图物体分辨率过低。我想，作者应该是受到当时resnet等跳级连接和FCN，U-Net等语义分割模型的启发，通过下采样提取高级特征，上采样恢复尺度，同时侧级连接补充位置信息，然后在每个上采样的特征图上进行检测来覆盖到各个物体（这一点借鉴了SSD）。</p><p><img src="/2020/01/10/RCNN-series-in-object-detection-续/various_pyramid_ways_in_cv.PNG" alt="目标检测中的金字塔模型"></p><p>如果没有每个特征图的预测，乍看就是FCN的经典结构。不过FPN的侧重点是为了结合高级语义特征和位置信息，因此加了一些额外的卷积操作，让网络在梯度下降中去focus这一点。下采样过程属于正常的网络操作，上采样时每个特征图进行2倍放大（最近邻插值），当然这个2倍是根据你的下采样倍数来的，一般都是2倍，然后侧向对应的不是直接加过来（FCN），也不是叠操作（U-Net），而且先用个$1 \times 1$卷积处理下采样的特征图（我想可能是为了，然后加在一起，最后再做个$3 \times 3$的卷积。$1 \times 1$的卷积是为了减少通道数（上采样的通道数是固定的），否则不能相加，同时我想可能也是去提取一下位置信息，$3 \times 3$的卷积是为了处理一下加在一起后的特征图的混叠效应，提取出两者的有用信息。</p><p>值得一提的是，作者在论文中也说了，按照解决问题的思想，这样的金字塔形式应当是最简单的，没有加入任何其他的复杂技巧，实验效果证明效果也足够好，简单又有效。</p><blockquote><p>Simplicity is central to our design and we have found that our model is robust to many design choices. We have experimented with more sophisticated blocks (e.g., using multilayer residual blocks [16] as the connections) and observed marginally better results. Designing better connection modules is not the focus of this paper, so we opt for the simple design described above.</p></blockquote><p><img src="/2020/01/10/RCNN-series-in-object-detection-续/FPN_architecture.PNG" alt="FPN的结构示意"></p><p>实验方面，FPN主要针对RPN和Fast R-CNN进行，通过ablation study论证了FPN结构的有效性，同时结构之间的各个部件都是必要的。其中 RPN 和 Fast RCNN 分别关注的是召回率和正检率，在这里对比的指标分别为 Average Recall(AR) 和 Average Precision(AP)，分别对比了不同尺度物体检测情况。</p><p>以resnet为例，不管第一次feature map的缩小（$7 \times 7$的卷积），用后面的$\left\{ C_{2}, C_{3}, C_{4}, C_{5} \right\}$。对RPN来说，只用了$C_{4}$去生成预选框，然后利用pyramid of anchor去搜区域，由于FPN已经有了多尺度的作用，因此每个上采样的特征图中，会根据其特点设置一个固定size的anchor area，anchor aspect ratio还是保持[0.5, 1, 2]不变。为了照应到RPN中的512大小的anchor，FPN实验时多加了一个特征图（在$P_{5}$上下采样2倍），对应的区域大小和特征图level分别是：$\left\{ 32^{2}, 64^{2}, 128^{2}, 256^{2}, 512^{2} \right\} \longleftrightarrow \left\{ P_{2}, P_{3}, P_{4}, P_{5}, P_{6} \right\}$，用$P$代表上采样的特征图，便于区分下采样的$C$。从设置里可以看出，大特征图里找小物体，小特征图里找大物体。</p><p>实验Fast R-CNN时，固定FPN+RPN提取的proposal结果，在其中加入也加入FPN分别在$\left\{ P_{2}, P_{3}, P_{4}, P_{5} \right\}$中找物体做RoI Pooling，一样的，对于大尺度的RoI就用小的特征图，小尺度的RoI就用大的特征图。为了安排每个RoI Pooling尺度对应的特征图，作者给出如下公式：</p><script type="math/tex;mode=display">k=\left\lfloor k_{0}+\log _{2}(\sqrt{w h} / 224)\right\rfloor</script><p>其中，224是ImageNet的标准输入尺寸，$k_{0} = 5$是基准值，代表最后一个特征图，$w, h$分别代表RoI区域（RPN+FPN给的原图的proposal）的宽和高。假设RoI大小是$112 \times 112$，那么$k=5-1=4$，就在$P_{4}$特征图上找，做RoI Pooling。一般来说proposal大小不固定，所以应该取整处理。</p><p>因为resnet的Conv5也作为特征金字塔的一部分，而原先的Fast R-CNN和Faster R-CNN在RoI Pooling后面才接上Conv5继续提取特征，所以论文简单的加了两个1024维的fc层在分类器和回归器之前，代替一下原先Conv5的工作。</p><p>最后在加入FPN的Faster R-CNN中进行参数共享，检测精度也得到了一定的提升。具体实验结果直接看图，不再赘述。</p><p><img src="/2020/01/10/RCNN-series-in-object-detection-续/FPN_experiments.PNG" alt="部分实验结果"></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p><img src="/2020/01/10/RCNN-series-in-object-detection-续/FPN_QA.PNG" alt="FPN在CVPR现场QA"></p><p>FPN的贡献思想在于向上采样，融合了特征信息和位置信息，而且简洁有效。在这之后，何恺明率先利用FPN实现了Mask R-CNN，一统检测和实例分割，斩获马尔奖。现在FPN也被广泛使用，成为检测的必备组件（R-FCN由于自身设计缘故，无法加入FPN）。</p><p>但是，FPN设计中的上采样和侧向连接，其实主要是给小目标检测提供了帮助，因为主要是引入位置信息，然后放大特征图（实验结果也说明小目标检测精度提升多）。对于大目标来说，顶层特征图的高级语义固然重要，位置信息肯定还是没有底层特征图的多的，因此可以对一开始网络产出的浅层特征图跳级连接到顶层特征图，类似下面的结构（参考自blog3，原论文PAN没找到，但是看结构也就是堆砌了几个FPN，没必要再看了）：</p><p><img src="/2020/01/10/RCNN-series-in-object-detection-续/FPN_for_bigobject.jpg" alt="FPN针对大物体检测改进"></p><h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><p><a href="https://arxiv.org/pdf/1912.04488.pdf" target="_blank" rel="noopener">SOLO: Segmenting Objects by Locations</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;自Faster R-CNN后，基于深度学习的目标检测框架大致形成，且精度也较为不错。在这之后，围绕着对图像数据更深层次理解，以及根据现有结构进行改进成为了一个主流点。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.10012&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Speed/accuracy trade-offs for modern convolutional object detectors&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.05190&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;IoU-uniform R-CNN: Breaking Through the Limitations of RPN&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;R-FCN&quot;&gt;&lt;a href=&quot;#R-FCN&quot; class=&quot;headerlink&quot; title=&quot;R-FCN&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://arxiv.org/abs/1605.06409&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;R-FCN&lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;内容有参考：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://arleyzhang.github.io/articles/7e6bc4a/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;blog1&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
      <category term="object detection" scheme="http://densecollections.top/tags/object-detection/"/>
    
      <category term="semantic segmentation" scheme="http://densecollections.top/tags/semantic-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>尝试远程控制Ubuntu服务器</title>
    <link href="http://densecollections.top/2020/01/05/%E5%B0%9D%E8%AF%95%E8%BF%9C%E7%A8%8B%E6%8E%A7%E5%88%B6Ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://densecollections.top/2020/01/05/尝试远程控制Ubuntu服务器/</id>
    <published>2020-01-05T11:45:06.000Z</published>
    <updated>2020-02-03T02:28:47.257Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>暑假实习用公司的显卡的经历很舒服，所以寒假回家之前，想在教研室的工作站上搭个ssh，方便回家也可以用显卡跑网络。但是自己对这方面知识不是很了解，尤其是端口映射之类的操作，所以导致外网登陆服务器的时候折腾了一点时间，最后勉强利用ngork进行了内网穿透。。。使用过程中发现，ngork似乎有些延迟，加上scp传文件问题（我是win利用git bash登Ubuntu服务器）一直没解决，还是放弃了ssh转战了teamviewer（还是有点香的。。）</p><a id="more"></a><h2 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h2><p>我安装的时候没关心密匙之类的问题，也没去修改ssh 的config文件，直接在Ubuntu 18.04上安装openssh-server就好了。</p><p>在Ubuntu终端上输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get openssh-server</span><br><span class="line"># 启动ssh</span><br><span class="line">sudo service ssh start</span><br><span class="line"># 重启</span><br><span class="line">sudo service ssh restart</span><br><span class="line"># 查看有没有启动</span><br><span class="line">ps -aux | grep &apos;&apos;ssh&apos;&apos;</span><br><span class="line"># 关闭ssh</span><br><span class="line">sudo /etc/init.d/ssh stop</span><br></pre></td></tr></table></figure><p>然后我在windows上打开git bash，输入：</p><p><img src="/2020/01/05/尝试远程控制Ubuntu服务器/log_command.PNG" alt="登陆命令"></p><p><code>-p</code>代表端口，默认是22，如果需要修改可以去<code>/etc/ssh/sshd_config</code>修改（改过记得source一下生效），参考<a href="https://blog.mythsman.com/post/5d2d65fca2005d74040ef873/" target="_blank" rel="noopener">blog1</a>和<a href="https://www.jianshu.com/p/7028e5fecf2b" target="_blank" rel="noopener">blog2</a>。端口后面跟的是<code>uername@ip_of_server</code>，密码输入服务器用户名密码，如果和服务器处于一个局域网，你的windows是可以无缝登陆的。服务器的ip地址可以通过<code>ifconfig</code>查询。</p><p>但是只能在一个局域网登陆没什么用处。。我们需要随时随地登陆服务器，这时候就需要科普一些公网，内网，端口映射等知识，看得我也有点眼花…后来我就找了个比较简单的内网穿透方法，也没有自己买服务器降低延迟啥的，基本上按照<a href="https://zhuanlan.zhihu.com/p/60962957" target="_blank" rel="noopener">blog3</a>的教程，去<a href="https://ngrok.com/" target="_blank" rel="noopener">ngork</a>下载软件，然后解压，在输入命令<code>./ngork tcp 22</code>，这个<code>tcp 22</code>默认端口22。在<code>Forwarding</code>一行会出现地址（<code>0.tcp.ngrok.io</code> ）和端口号，同样地在git bash输入即可。</p><h2 id="Teamviewer"><a href="#Teamviewer" class="headerlink" title="Teamviewer"></a>Teamviewer</h2><p>没自己买服务器做内网穿透的结果就是卡！卡！延迟让我有点受不了，而且scp一直搞不到win上来，也懒得查是不是自己命令哪里错了。当然除了上面的ngork外还可以搞个VPN，让你的电脑连上服务器所在的网，然后就可以按照最开始的方式登陆和传输了，暑假实习的公司就是这么干的，不过他们有专人维护这块，稳定性以及文件传输速度都很快。</p><p>最后…..</p><p>我还是转向了teamviewer，设置了固定密码。</p><p><img src="/2020/01/05/尝试远程控制Ubuntu服务器/teamviewer.PNG" alt="这延迟还能接受，传输速度也不赖，还要啥自行车..."></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;暑假实习用公司的显卡的经历很舒服，所以寒假回家之前，想在教研室的工作站上搭个ssh，方便回家也可以用显卡跑网络。但是自己对这方面知识不是很了解，尤其是端口映射之类的操作，所以导致外网登陆服务器的时候折腾了一点时间，最后勉强利用ngork进行了内网穿透。。。使用过程中发现，ngork似乎有些延迟，加上scp传文件问题（我是win利用git bash登Ubuntu服务器）一直没解决，还是放弃了ssh转战了teamviewer（还是有点香的。。）&lt;/p&gt;
    
    </summary>
    
      <category term="技术支持" scheme="http://densecollections.top/categories/%E6%8A%80%E6%9C%AF%E6%94%AF%E6%8C%81/"/>
    
    
      <category term="Ubuntu" scheme="http://densecollections.top/tags/Ubuntu/"/>
    
      <category term="ssh" scheme="http://densecollections.top/tags/ssh/"/>
    
      <category term="git" scheme="http://densecollections.top/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>RCNN series in object detection</title>
    <link href="http://densecollections.top/2019/11/30/RCNN-series-in-object-detection/"/>
    <id>http://densecollections.top/2019/11/30/RCNN-series-in-object-detection/</id>
    <published>2019-11-30T07:30:42.000Z</published>
    <updated>2020-04-07T07:05:53.512Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>Faster R-CNN是目标检测领域中”two-stage”的代表性方法，其精度高，适应性强，兼具学术和工程价值。整个框架由于吸取了很多先前工作的经验，因此比较庞大，而且细节很多，因此需要认真研读下相关paper和Faster R-CNN的python代码。</p><p>在此之前，先贴上一位博主做的“<a href="https://nikasa1889.github.io/2017/05/02/The-Modern-History-of-Object-Recognition-—-Infographic-1/" target="_blank" rel="noopener">The Modern History of Object Recognition — Infographic</a>”，其中也包括了“one-stage”的方法，不过2017年以后的没再更新了。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/HistoryOfObjectRecognition.png" alt="modern history of object recognition"></p><a id="more"></a><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">R-CNN</a></h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>内容有参考<a href="https://zhuanlan.zhihu.com/p/23006190?refer=xiaoleimlnote" target="_blank" rel="noopener">blog1</a>和<a href="https://blog.csdn.net/WoPawn/article/details/52133338" target="_blank" rel="noopener">blog2</a>。</p><p>R-CNN(Rich feature hierarchies for accurate object detection and semantic segmentation)是将深度学习应用于目标检测的开山之作，以前传统的目标检测算法使用滑动窗口法依次判断所有的可能区域，在该文章中，采用selective search方法先预先提取一系列可能是物体的候选区域（foreground），之后将这些候选区域（proposals）整合成固定的大小（$227 \times 227$， 采用AlexNet）送到预训练的CNN模型上提取特征然后进行fine-tuning迁移学习，然后通过fc6和fc7层，得到了针对目标检测任务的特征向量，然后再过SVM二分类器得到物体的每个类别分数（至于为什么用了两层fc，不直接用三层fc得到softmax分数，Ross他也在论文中作了说明和实验，主要原因就是proposal的筛选更加严格点，mAP更高点），得到2K个左右proposal的所属类别之后再做非极大抑制(NMS)，去掉那些多余的框，留下局部最优的建议框，然后根据这些剩下的框再去训练20个类别（针对pascal VOC数据）的regression器，最后终于得到图像中物体的种类和矩形框坐标信息。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/RCNN-overview.PNG" alt="R-CNN architecture overview"></p><p>下面对一些重点内容进行分析，同时由于是object detection based CNN开篇，所以也顺便加上评价指标等细节。</p><h3 id="评价指标IoU和AP"><a href="#评价指标IoU和AP" class="headerlink" title="评价指标IoU和AP"></a>评价指标IoU和AP</h3><p><strong>IoU(insertion of union).</strong>中文名为交并比，主要用来衡量框与框的重合程度，计算公式为$IoU=\frac{area(A \cap B)}{area(A \cup B)}$</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/IOU.png" alt="IoU"></p><p><strong>AP(Average Precision)与mAP(mean Average Precision).</strong>这是目标检测中常用的评价指标，AP代表每一类的检测精度，mAP代表所有类的整体检测精度，也就是所有类AP的平均值。AP一般是二分类问题的P-R曲线（precision-recall curve）与x轴围成的面积。</p><p>accuracy（准确率）=$(TP+TN)/(TP+FP+TN+FN)$，预测对的样本/所有样本；</p><p>precision（精准率）=$TP/(TP+FP)$，预测对的正样本/预测出的正样本；</p><p>recall（召回率）=$TP/(TP+FN)$，预测对的正样本/真正的正样本；</p><p><a href="https://blog.csdn.net/u013249853/article/details/96132766" target="_blank" rel="noopener">如何绘制PR曲线?</a>首先将样本按照置信度从大到小排列，然后设置一个从高到低的阈值，大于该阈值的才能认定为正样本，否则为负样本，在该阈值下就能得到一组（P,R）值，阈值设的越细，点对就越多，这样连接成线就得到了P-R曲线。</p><p><a href="https://arleyzhang.github.io/articles/c521a01c/" target="_blank" rel="noopener">AP的计算</a>一般是通过插值或者估算的方式进行的，并不是直接积分。</p><p>针对Pascal VOC数据来说，2010前后有两种计算方式，现在主要是<a href="https://www.zhihu.com/question/53405779/answer/419532990" target="_blank" rel="noopener">第二种</a>，计算方法如下（Pascal VOC给出的评价文本结果是img_name+置信度+x1_lefttop+y1_lefttop+x2_rightbottom+y2_rightbottom，通过坐标计算IoU，大于0.5的认为是TP，小于等于0.5或者检测到同一个GT的多余框认为是FP，没有检测到GT认为是FN）：</p><blockquote><p>假设样本中有M个正样本，且每个样本都有预测的类别置信度和预测的GT（IoU大于0.5），根据置信度顺序给出各处的P-R值，画出曲线，根据样本设定[0/M, 1/M, 2/M, 3/M, …, M/M]这几个recall点，找出每个点之后最大的precision值（如果曲线不能全部遍历完则只遍历到最后出现的recall值，没有的对应点，precision做0处理），以precision为高，相邻recall点之间的距离为宽，相乘相加近似得到曲线与X轴的面积，即为AP</p></blockquote><p>算出所有类的AP，再做平均就得到了mAP。</p><p>举个例子：</p><p>我现在用训练好的Faster R-CNN测试自己的数据集，总共有三类：drone，bird，kite，最后会在<code>results</code>文件夹下生成三类检测的.txt文件。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/faster rcnn eval results.jpg" alt="三类检测的结果文件"></p><p>比如在<code>comp4_det_test_bird.txt</code>里面，前面几行是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">njust_drone2videotest_0119 0.002 536.2 359.2 564.2 378.1</span><br><span class="line">njust_drone2videotest_0119 0.000 563.2 404.0 580.2 413.3</span><br><span class="line">njust_drone2videotest_0119 0.000 522.2 338.8 539.9 346.3</span><br><span class="line">njust_drone2videotest_0053 0.983 663.7 407.7 671.3 416.6</span><br><span class="line">njust_drone2videotest_0053 0.939 259.6 358.7 267.8 367.3</span><br><span class="line">njust_drone2videotest_0053 0.077 667.9 410.4 675.7 420.0</span><br><span class="line">njust_drone2videotest_0053 0.005 265.2 359.2 272.8 368.0</span><br><span class="line">njust_drone2videotest_0053 0.003 591.5 400.0 619.1 419.3</span><br><span class="line">njust_drone2videotest_0053 0.002 657.2 406.7 667.6 416.6</span><br><span class="line">njust_drone2videotest_0053 0.002 261.8 362.8 269.0 372.3</span><br><span class="line">njust_drone2videotest_0053 0.000 259.6 353.4 271.0 361.2</span><br><span class="line">njust_drone2videotest_0053 0.000 280.1 399.3 292.0 407.7</span><br><span class="line">njust_drone2videotest_0053 0.000 561.4 439.9 576.8 446.5</span><br><span class="line">njust_drone2videotest_0053 0.000 666.5 415.0 672.5 424.4</span><br><span class="line">njust_drone2videotest_0053 0.000 662.8 402.8 673.9 411.6</span><br><span class="line">njust_drone2videotest_0053 0.000 647.6 444.0 659.6 461.4</span><br><span class="line">njust_drone2videotest_0053 0.000 269.3 390.4 278.0 400.9</span><br><span class="line">njust_drone2videotest_0278 0.001 686.4 397.4 711.0 415.7</span><br><span class="line">njust_drone2videotest_0280 0.001 678.2 399.2 703.1 416.0</span><br><span class="line">njust_drone2videotest_0280 0.000 710.2 437.4 725.8 446.0</span><br><span class="line">njust_drone2videotest_0280 0.000 654.5 382.8 673.3 391.2</span><br><span class="line">njust_drone2videotest_0280 0.000 704.2 454.3 719.0 463.2</span><br></pre></td></tr></table></figure><p>之后需要对上述文本就行处理，主要是筛选检测框，比如设置置信度阈值，使用NMS等，然后根据推测的坐标和真值坐标计算IOU，大于0.5的设置框的GT为1，否则为0。值得注意的是，即使做过筛选后，最后得到的检测框还是可能误检（FP)，多检（FP，一个物体框了多个），漏检（FN），大致流程如下：</p><blockquote><p>首先对所有的结果按置信度排序，从高到低，然后根据坐标判断检测是否成功。排好序的坐标每一组都得和真实标注的GT进行比较计算IoU,大于0.5的认为检测成功，那么赋给他的标签就是1（TP)，否则是0(FP)，同时，每当一个GT被成功检测了，那么就会被标记，后续如果发现有其他的检测与这个GT重复，那么就把IoU最大的当作TP，其他多检的当作FP，遍历完成之后，如果存在没有匹配到的标注GT，也就是漏检，认定为FN。这样一整个流程下来就会得到TP,FP和FN。</p></blockquote><p>借用上头<a href="https://www.zhihu.com/question/53405779/answer/419532990" target="_blank" rel="noopener">知乎链接</a>的答案，具体讲下怎么计算AP：</p><p>在筛选，NMS，比较等步骤之后，假设对于bird这一类，有如下检测结果(IoU&gt;0.5时GT=1，按照置信度排好了顺序)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">BB  | confidence | GT</span><br><span class="line">----------------------</span><br><span class="line">BB1 |  0.9       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB2 |  0.9       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB1 |  0.8       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB3 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB4 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB5 |  0.7       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB6 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB7 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB8 |  0.7       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB9 |  0.7       | 1</span><br><span class="line">----------------------</span><br></pre></td></tr></table></figure><p>其中BB代表bounding box，其中两个BB1代表一个物体被框了两次，则应属于FP，因此在计算的时候要注意，虽然给出的GT是1，此外还有两个bird没有被检测出来，那么属于FN，整体上实际是正样本的有5+2=7个。现在从上到下以此按置信度大小计算（recall,precison）点对。（$recall \in [0/7, 1/7, 2/7, 3/7, 4/7, 5/7, 6/7, 7/7]) $</p><blockquote><p>1.首行以下认为预测值全为0，则TP=1(BB1)，BB2,BB5,BB8,BB9实际是1，但是预测为0，所以FN=4+2=6(两个漏检的)，FP=0，所以recall=1/(1+6)=0.14,precision=1/(1+0)=1.00;<br>2.第二行一下认为预测值全为0，则TP=2(BB1,BB2),BB5,BB8,BB9实际是1，但是预测0，所以FN=3+2=5,FP=0，所以recall=2/(2+5)=0.29,precision=2/(2+0)=1.00;<br>3.第三行一下认为预测值全为0，则TP=2(BB1,BB2,BB1为多余的，算FP)，BB5,BB8,BB9实际是1，但是预测是0，所以FN=3+2=5,FP=1,所以recall=2/(2+5)=0.29,precision=2/(2+1)=0.67;<br>4.第四行以下认为预测值全为0，则TP=2(BB1,BB2),BB5,BB8,BB9实际是1，但是预测是0，所以FN=3+2=5,BB3实际为0，预测为1，BB1多检一个，所以FP=2，则recall=2/(2+5)=0.29,precision=2/(2+2)=0.50;<br>5.第五行以下认为预测值全为0，则TP=2,FN=5,FP=3,所以recall=2/(2+5)=0.29,precision=2/(2+3)=0.40;<br>6.第六行以下认为预测值全为0，则TP=3,FN=4,FP=3,所以recall=3/(3+4)=0.43,precision=3/(3+3)=0.50;<br>7.第七行以下认为预测值全为0，则TP=3,FN=4,FP=4,所以recall=3/(3+4)=0.43,precision=3/(3+4)=0.43;<br>8.第八行以下认为预测值全为0，则TP=3,FN=4,FP=5,所以recall=3/(3+4)=0.43,precision=3/(3+5)=0.38;<br>9.第九行以下认为预测值全为0，则TP=4,FN=3,FP=5,所以recall=4/(4+3)=0.57,precision=4/(4+5)=0.44;<br>10.第十行以下认为预测值全为0，则TP=5,FN=2,FP=5,所以recall=5/(5+2)=0.71,precision=5/(5+5)=0.50</p></blockquote><p>接着根据每个不同的recall值去找对应的<strong>最大的precision值</strong>，即：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">recall&gt;=0.00,precision_max=1.00</span><br><span class="line">recall&gt;=0.14,precision_max=1.00;</span><br><span class="line">recall&gt;=0.29,precision_max=1.00;</span><br><span class="line">recall&gt;=0.43,precision_max=0.50;</span><br><span class="line">recall&gt;=0.57,precision_max=0.50;</span><br><span class="line">recall&gt;=0.71,precision_max=0.50;</span><br><span class="line">recall&gt;=1.00,precision_max=0.00;</span><br></pre></td></tr></table></figure><p>则</p><script type="math/tex;mode=display">AP=(0.14-0) \times 1 + (0.29-0.14) \times 1 + (0.43-0.29) \times 0.5 + \\(0.57-0.43) \times 0.5 + (0.71-0.57) \times 0.5 + (1-0.71) \times 0 = 0.50</script><p>实际上，这就是在P-R曲线上找出一些特定的recall点，然后利用多个矩形的面积和来近似代替曲线与X轴围成的面积。</p><p>如果是VOC2010之前，recall值的选取是固定的，即$recall \in [0, 0.1, 0.2, …, 1]$，对应的最大precision为1，1，1，0.5，0.5，0.5，0.5，0.5，0，0，0，此时AP的计算公式是11个precision的和的平均值，为0.5。</p><p>在COCO数据集中，IoU要求在[0,5, 0.95]区间每隔0.05取一次作为正样本判断阈值，然后计算出10个类似Pascal VOC的mAP，然后再做平均，作为最后的AP。COCO并不将AP和mAP做区分，COCO中的AP@0.5等同于Pascal中的mAP。</p><p>AP是衡量检测器性能的一个综合指标，但是对于你的数据集，可能并不是最适合的，因为有的数据场景认为误检几个影响不大，主要是都能检测出来，那么这时候recall值就大点好；有的数据场景呢，认为漏检几个没问题，但是不能检测错了，那么precision大点好。而AP是对recall和precision做了一个整体的评估，是检测器对数据普遍场景下的检测性能的打分。后续的一些研究工作发现了此评价指标在一些场景下水土不服，以及没有考虑具体物体检测的置信度得分情况而导致泛化能力失衡的情况，并对此做了一些改进，有兴趣的可以阅读下南京旷视研究院的两篇博文做个大概的了解。（<a href="https://zhuanlan.zhihu.com/p/55575423" target="_blank" rel="noopener">blog1</a>, <a href="https://zhuanlan.zhihu.com/p/56899189" target="_blank" rel="noopener">blog2</a>）</p><h3 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h3><p><strong>Region prposals—Selective Search(<a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">paper</a>, <a href="https://blog.csdn.net/mao_kun/article/details/50576003" target="_blank" rel="noopener">blog</a>,<a href="https://www.cnblogs.com/zyly/p/9259392.html" target="_blank" rel="noopener">code</a>).</strong>最开始的阶段，算法要通过该方法提取出每张图像上存在物体的可能区域，即建议框，每张图片大概会提取2k左右。Selective Search算法属于经典计算机视觉范畴，主要是利用不同的尺度（颜色，纹理，大小等），采用图像分割，层次算法等，先分割成小区域，然后通过颜色直方图相近（颜色相似），梯度直方图相近（纹理相似）等规则合并，得到最终的proposals，由于这个方法严重限制了整体框架的速度，而且后面也被Faster R-CNN中的RPN取代，因此我也没有兴趣深究，感兴趣的读者可以通过原论文和代码进一步了解。</p><p><strong>CNN feature extraction.</strong>利用selective search得到的预选框由于大小不一，不方便过卷积层后reshape成统一维度的矩阵，因此需要进行resize等预处理。变形操作方面主要就是拉伸和填充的组合。</p><p>如下图所示，(A)是原图，(B)，(C)是各向同性变形，(D)是各项异性变形。(B)考虑了proposal周围的纹理内容（context），利用其扩充到$227 \times 227$，如果遇到了图像边界就利用proposal的像素均值填充；(C)不考虑proposal周围的像素信息，直接用其像素均值填充到$227 \times 227$；(D)是直接对proposal resize到$227 \times 227$，不过在此之前先进行padding处理，padding的尺寸分别为0和16，像素值为proposal的像素均值。论文给出的结论是padding=16加各向异性缩放的效果最好。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/different object proposal transformations.PNG" alt="different object proposal transformations"></p><p>网络层面，R-CNN选用了Alexnet和VGG这两个网络，由于VGG采用了更小的卷积核和小的stride，因此最后的检测mAP也更高。在训练方面，R-CNN利用的是在ILSVC数据集上的预训练分类模型，等于是进行了较好的初始化，有了一个比较通用的特征提取器，然后在Pascal VOC这样的小数据集上进行fine-tuning，这样的迁移学习也算当时的一个contribution，但是现在来看已经属于比较正常的操作了。以Alexnet为例，proposal经过其中的5个卷积层之后，得到的是$6 \times 6 \times 256$的特征图矩阵，再经过fc6和fc7之后得到了4096个神经元，最后再经过一个有21个神经元的fc，即分类数，最终得到了适合该数据样本的分类器（和GT的IOU大于0.5的那些proposals都会标为正样本，否则为负样本）,但是并不能准确预测位置。</p><p><strong>SVM classification.</strong>一般来说，根据图像分类的操作，都是将最后的全连接层改成需要分类个数的神经元，因此针对Pascal VOC来说，可以将最后的fc进行softmax，但是R-CNN却在fc7后面加了个SVM分类器，是什么原因导致这样做精度更高呢？根据论文的说法，Ross在微调CNN的时候，和GT的IOU大于0.5的那些proposals都会标为正样本，否则为负样本，这样的话对bndbox的限制非常宽松，也就是说监督信息不是严格有效的，因为CNN对小样本容易过拟合，因此这样的操作只是为了微调网络，尤其是最后的fc6和fc7，使其能够得到针对Pascal VOC数据的分类特征。采用SVM是因为SVM适用于少样本训练，而且由于是最后的分类，所以IOU的阈值设定也比较严格，目的是为了提供尽量正确的监督信息（GT为正样本，IOU小于0.3的负样本，这个0.3也是调参试出来的）。SVM分类器（<a href="https://blog.csdn.net/luoshixian099/article/details/51073885" target="_blank" rel="noopener">可以与sigmoid函数结合，进行概率输出，在scikit-learn package中的SVM函数可以直接输出概率</a>，<a href="https://blog.csdn.net/v_JULY_v/article/details/7624837" target="_blank" rel="noopener">理解SVM</a>）其实就是$4096 \times N$的权值矩阵，最后得到了每类的置信值，这样就得到了类别结果。此外还需注意的是，由于GT只有一个，而IOU小于0.3的proposal可能会有很多，这样就导致了训练SVM分类器的时候正负样本不均衡，论文中也提到，SVM的负样本是经过hard negative mining筛选的（负样本的处理是object detection中的一个技术细节和难点），具体怎么做的，我没有继续深究，我准备在Ross的下一篇文章“OHEM”进行梳理。</p><p><strong>NMS非极大值抑制。</strong>SVM分类器输出的是2k个proposal的类别置信矩阵，后面需要对每一类做NMS处理，去除掉无用的proposals，留下最接近的proposals（一个物体可能被selective search提取出了很多类似的proposals）。具体步骤如下：</p><p>假设是2000个proposals，然后是21类（加一个background），那么一张图片最后过SVM得到了一个$2000 \times 21$的矩阵，每一列代表这2k个proposals的每个类别的置信度，那么：</p><blockquote><p>1.对此矩阵按列按从大到小的顺序排列；<br>2.对每列，先选取该列最大的那么proposal，然后与该列后面每个得分对应的proposals计算IoU,如果大于设定阈值，则剔除该proposal，否则认定是这张图片存在该类别的多个物体；<br>3.对这一列剩下的次大的proposal进行2的操作，并不断重复，直到该列遍历完；<br>4.对21列（所有类）进行步骤2，3的操作</p></blockquote><p>经过NMS之后，就可以得到位置比较准确且类别置信度较高的一些proposals。</p><p><strong>Bounding box regression.</strong>分类完成并且NMS之后需要对proposals的位置进行精修，因为selective search得到的proposal并不是精确的目标检测器，因此还需要对物体的位置做进一步的修正。</p><p>a).如何设计回归？首先是挑选与GT比较接近的proposal（框）进行回归，如果差的太远，是没办法进行学习的。其次也不是直接回归矩形框的四个坐标，而是学习一种框的变换，即从检测框到GT的平移和缩放，这样的话会使网络学习比较稳定，直接回归无规律的坐标可能导致网络不稳定。如下图所示，P代表送入网络的region proposal，G是标注的GT，$\hat{G}$是学习过后的regression模型预测出的更接近GT的bounding box。其中$P=(P_{x},P_{y},P_{w},P_{h}), G=(G_{x},G_{y},G_{w},G_{h}), \hat{G}=(\hat{G_{x}},\hat{G_{y}},\hat{G_{w}},\hat{G_{h}})$，下标$x,y$代表矩形框中心坐标，$w,h$代表矩形框的宽和高。$d_{x}(P),d_{y}(P)$为待学习的平移变换，$d_{w}(P),d_{h}(P)$为待学习的缩放变换，即：</p><script type="math/tex;mode=display">\begin{array}{l}{\hat{G}_{x}=P_{w} d_{x}(P)+P_{x}} \\ {\hat{G}_{y}=P_{h} d_{y}(P)+P_{y}} \\ {\hat{G}_{w}=P_{w} \exp \left(d_{w}(P)\right)} \\ {\hat{G}_{h}=P_{h} \exp \left(d_{h}(P)\right)}\end{array}</script><p>(这里缩放变换用了exp可能还是因为网络回归小的值容易些，变化大的值不稳定）P实际上代表着proposal的信息，因为是对proposal进行修正，因此在回归学习时候，都可以看作是Alexnet最后一个pool层的线性函数，目的是学习对最后的feature map使其可以变换到真值附近的变换组合，即：</p><script type="math/tex;mode=display">d_{*}(P), *=x, y, w, h</script><script type="math/tex;mode=display">d_{\star}(P)=\mathbf{w}_{\star}^{\mathrm{T}} \boldsymbol{\phi}_{5}(P)</script><script type="math/tex;mode=display">Loss = \underset{\hat{\mathbf{w}}_{\star}}{\operatorname{argmin}} \sum_{i}^{N}\left(t_{\star}^{i}-\hat{\mathbf{w}}_{\star}^{\mathrm{T}} \boldsymbol{\phi}_{5}\left(P^{i}\right)\right)^{2}+\lambda\left\|\hat{\mathbf{w}}_{\star}\right\|^{2}</script><p>$\mathbf{w}_{\star}^{\mathrm{T}}$就是要间接梯度下降学习的参数，$\boldsymbol{\phi}_{5}(P)$代表最后pool层出来的结果，$\lambda$是正则项系数，防止过拟合，在论文中Ross提到这个参数很关键，否则效果不好，论文中设的值是1000。$t_{\star}$是要去学习的准确的变换，即从P到G的变换：</p><script type="math/tex;mode=display">\begin{aligned} t_{x} &=\left(G_{x}-P_{x}\right) / P_{w} \\ t_{y} &=\left(G_{y}-P_{y}\right) / P_{h} \\ t_{w} &=\log \left(G_{w} / P_{w}\right) \\ t_{h} &=\log \left(G_{h} / P_{h}\right) \end{aligned}</script><p><img src="/2019/11/30/RCNN-series-in-object-detection/bbox regression.png" alt="bounding box regerssion--learning the transformation"></p><p>b).怎么训练？为了回归器有效训练，每类样本只采取与GT之间IoU最大的且大于0.6的region proposal，输入的是P和G的坐标信息，以及Alexnet的$pool_{5}$层特征，然后根据loss函数对每一类单独训练回归器。</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>1.虽然整体繁琐，占用内存大，速度也慢，但是技术框架和细节为后面的改进工作奠定了整体基础，具有开创意义；</p><p>2.论文写得非常详实，没有难懂的地方和句子，通读以后对其工作和贡献了解得很清楚；</p><p>3.考虑问题很全面，实验的各个因素都考虑到了，而且做了很多对比试验（Ablation Study），让人信服，有理有据。Ross的写作技巧和框架对自己写论文有很大的借鉴意义。</p><p>4.Ross论文中的推荐的<a href="http://dhoiem.web.engr.illinois.edu/publications/eccv2012_detanalysis_derek.pdf" target="_blank" rel="noopener">object detection errors analysis </a>，进一步了解目标检测的错误分析。</p><h2 id="SPPNet"><a href="#SPPNet" class="headerlink" title="SPPNet"></a><a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="noopener">SPPNet</a></h2><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>内容有参考<a href="https://zhuanlan.zhihu.com/p/24774302" target="_blank" rel="noopener">blog1</a>和<a href="https://zhuanlan.zhihu.com/p/24780433" target="_blank" rel="noopener">blog2</a>。</p></li><li><p>何恺明在ICCV2015上作的tutorial，<a href="http://kaiminghe.com/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf" target="_blank" rel="noopener">Convolutional Feature Maps —Elements of efficient (and accurate) CNN-based object detection</a>，讲解了SPPNet的一些内容，同时对比了R-CNN, Fast R-CNN和Fsater R-CNN。</p></li></ul><p>SPPNet是在R-CNN的基础上进一步提高目标检测的速度和精度。何恺明等作者认为，R-CNN每次都要将Selective Search提取出的2k左右的region proposals进行crop和warp，然后分别过卷积这样的操作太费时间而且可能做了很多重复的计算，因此他们认为可不可以直接将full image过一次卷积网络，然后在feature map进行操作，毕竟图像的特征在卷积提取之后都是一样的，由此SPPNet应运而生。</p><p>SPPNet的好处是速度得到了大幅提升，而且简化了部分操作，但是随之而来的问题是：</p><ol><li><p>region proposal的大小不一，但是conv之后的fc层是固定的向量长度，怎么去适应？</p></li><li><p>解决这个固定尺度的问题后，那么怎么找region proposal对应的feature map区域？</p></li></ol><p>对于第一个问题，SPPNet提出了spatial pyramid pooling（SPP, 空间金字塔池化），通过多次pooling输出不同尺度（预设好尺寸）的特征图并进行concatenate叠操作（类似U-Net），得到了固定长度的特征向量，这也是该论文的主要contribution；对于第二个问题，SPPNet通过简化receptive field和对应中心坐标计算，来近似得到region proposal图像的top-left, right-bottom坐标对应在最后feature map上的坐标，从而确定区域。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/RCNN-SPP-1.PNG" alt="R-CNN和SPPNet的结构比较"></p><p><img src="/2019/11/30/RCNN-series-in-object-detection/RCNN-SPP-2.PNG" alt="R-CNN和SPPNet的不同region proposal处理方式比较"></p><h3 id="Content-1"><a href="#Content-1" class="headerlink" title="Content"></a>Content</h3><p><strong>SPP空间金字塔池化</strong></p><p>不同的regional proposal对应的feature map区域尺寸看成一个个不同尺寸的小feature map，然后在该map上做几次不同的maxpooling，得到尺度依次变小的特征图，不改变channel数，然后将这些不同尺度的金字塔特征图reshape成一维的向量，然后合并在一起形成固定维度的fc层。由于特征金字塔的尺度是预设的，所以不管region proposal或者image的尺寸如何，都不会影响最后的分类。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/SPP-object-detection.PNG" alt="基于R-CNN的SPP示意图"></p><p>此外，由于可以tolerate multi-scale，论文也采用了多尺度的图像去训练，并做了实验，结果也证明确实比单一尺度要好。</p><blockquote><p>We develop a simple multi-size training method. For a single network to accept variable input sizes, we approximate it by multiple networks that share all parameters, while each of these networks is trained using a fixed input size. In each epoch we train the network with a given input size, and switch to another input size for the next epoch. Experiments show that this multi-size training converges just as the traditional single-size training, and leads to better testing accuracy.</p><p>Note that the above single/multi-size solutions are for training only. At the testing stage, it is straightforward to apply SPP-net on images of any sizes</p><p>SPP is better than no-spp, and full-image representation is better than crop</p></blockquote><p><strong>region proposal 映射</strong></p><p>在进行SPP之前，网络需要知道最后卷出来的feature map中哪些部分和最初Selective Search得到的原始region proposal是对应的。论文对此方法的解释比较简略，只在附录最后做了提及，估计也是个工程性，实验性的处理。其大致思路前面也已经说过，就是找左上和右下的对应点，而这种对应的映射关系主要由网络的感受野（receptive field）决定。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/mapping-a-window-to-feature-maps.PNG" alt="论文附录中提到的映射思路"></p><p><em>感受野</em></p><blockquote><p><em>The</em> receptive field is defined as the region in the input space that a particular CNN’s feature is looking at (i.e. be affected by)</p></blockquote><p>简而言之，感受野就是当前你的特征图上的像素点对应的是前面特征图的哪些部分区域（这个点是从多少视野中抽象出来的），这个区域一般是矩形大小的。对应分类来说，一般网络越深、感受野越大越好（高层语义信息越准确），对于目标检测，如果感受野太大，那么小目标的信息可能丢失，因此需要针对具体的任务去分析调节。</p><p>对于卷积之后特征图大小计算，感受野概念和推导来源，可以参考下面的references，我在之后的内容中就直接给出公式和结论了。</p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/23185164?refer=xiaoleimlnote" target="_blank" rel="noopener">卷积神经网络(CNN)简介</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/44106492" target="_blank" rel="noopener">卷积神经网络的感受野</a></p></li><li><p><a href="https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807" target="_blank" rel="noopener">A guide to receptive field arithmetic for Convolutional Neural Networks—需要梯子</a></p></li><li><p><a href="https://fomoro.com/research/article/receptive-field-calculator#3,1,1,VALID;2,2,1,VALID;3,1,1,VALID;2,2,1,VALID;3,1,1,VALID;3,1,1,VALID;2,2,1,VALID" target="_blank" rel="noopener">感受野网页计算器</a></p></li><li><p><a href="https://arxiv.org/pdf/1603.07285.pdf" target="_blank" rel="noopener">A guide to convolution arithmetic for deep learning</a></p></li></ul><p>假设特征图是正方形的，特征图尺寸是$F$，感受野尺寸是$RF$，卷积核kernel_size是$k$，步长stride是$s$，填充padding大小是$p$，感受野中心坐标为$C$，下标$i$代表从上到下的顺序标号，则：</p><p>经过一次卷积后特征图大小变为（向下取整，比如22.5取22）：</p><script type="math/tex;mode=display">F_{i+1} = \lfloor (F_{i}+2p_{i}-k_{i}) / s_{i} \rfloor + 1</script><p>当前特征图的像素点对应上一特征图的感受野尺寸是（根据上面的公式逆推，下标意义不完全准确）：</p><script type="math/tex;mode=display">RF_{i}=(RF_{i+1}-1) * s_{i} + k _{i}</script><p>当前特征图在上一个特征图的感受野大小就是卷积核大小，即$k$。感受野计算一般不加padding，因为感受野是指在原图上的感受野，与填充无关，虽然逆推公式时候padding会有影响，但是将其忽略然后近似计算。如果要计算最后特征图在原图上的感受野，依次递归即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">举个例子：</span><br><span class="line">原图输入</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">k1=3, s1=2;</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">特征图1</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">k2=3, s2=2;</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">特征图2</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">k3=3, s3=1;</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">特征图3</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line"></span><br><span class="line">特征图3在原图上的感受野尺寸是多大？</span><br><span class="line">特征图3在特征图2上的感受野就是卷积核大小，即3；</span><br><span class="line">特征图3在特征图1上的感受野尺寸：(3-1)*2+3=7;</span><br><span class="line">特征图3在原图上的感受野尺寸：(7-1)*2+3=15;</span><br></pre></td></tr></table></figure><p>特征图像素点对应上一个特征图的感受野中心坐标：</p><script type="math/tex;mode=display">C_{i}=s_{i} * C_{i+1} + ((k_{i}-1)/2-p_{i})</script><p>这时候要考虑padding，因为坐标有影响。尺寸加上坐标就可以定位region proposal在特征图与特征图之间的映射关系。</p><blockquote><p>NIPS 2016的论文<a href="https://arxiv.org/abs/1701.04128" target="_blank" rel="noopener">Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</a>提出了有效感受野的概念，也就是说感受野内部的每个像素的作用和贡献不是相同的，有效感受野仅占理论感受野的一部分，一般中心较多，属高斯分布影响。</p></blockquote><p><img src="/2019/11/30/RCNN-series-in-object-detection/receptive-field-solution.PNG" alt="How to compute the center of the receptive field"></p><p>上图是何恺明简化的计算中心点的公式，也就是计算region proposal左上和右下坐标点的方法。</p><p>设$p_{i} = \lfloor (k_{i} /2) \rfloor$，当$k_{i}$为奇数时，$p_{i}=(k_{i}-1)/2$，则$C_{i}=s_{i}\times C_{i+1}$；当$k_{i+1}$为偶数时，$p_{i}=k_{i}/2$，则$C_{i}=s_{i}\times C_{i+1}-0.5$，由于坐标取的都是整数，所以近似认为$C_{i}=s_{i} \times C_{i+1}$，也就是说感受野的中心坐标只跟步长以及后面的中心坐标有关，因此通过这种关系一步一步将原图的region proposal坐标映射到feature map上。另外，可能考虑到近似处理的原因，论文最后对映射到feature map上的坐标值做了进一步处理：</p><p>左上坐标值：$x^{‘}=\lfloor x/s \rfloor + 1, y^{‘} = \lfloor y/s \rfloor + 1$；</p><p>右下坐标值：$x^{‘}=\lfloor x/s \rfloor - 1, y^{‘} = \lfloor y/s \rfloor - 1$；</p><p>也就是说，把区域缩小一点（左上点下移，右下点上移），应该是何恺明考虑到最后feature map上的点计算感受野映射回去的时候扩大了区域，所以做了一个这么经验化的处理。</p><p><strong>最后总结一下SPP方法，通过原图region proposal的左上和右下坐标，分别以各自作为中心坐标扩展出感受野大小的区域，然后映射到feature map上，找到对应的像素点，这样就定位出了原图region proposal对应的特征，然后对该特征进行预设金字塔尺寸的池化，最后对不同的特征图reshape，“叠”在一起给fc层，进行分类。</strong></p><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Fast R-CNN</a></h2><h3 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h3><p>内容有参考：</p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/24780395" target="_blank" rel="noopener">blog1</a>, <a href="https://blog.csdn.net/WoPawn/article/details/52463853" target="_blank" rel="noopener">blog2</a>, <a href="https://blog.deepsense.ai/region-of-interest-pooling-explained/" target="_blank" rel="noopener">blog3-RoI pooling</a></p></li><li><p>Ross Girshick在ICCV 2015上对自己论文的讲解，在他的个人<a href="https://www.rossgirshick.info/" target="_blank" rel="noopener">主页</a>可以找到slide</p></li></ul><p>Fast R-CNN吸收了SPPNet的优点，并针对R-CNN冗余的的多推理结构，以及带来的占用内存资源多，推理速度慢的问题进行了改进，采用shared computation和mutli-task learning将整个流程（原始图像region proposal提取除外）合在了一个网络中，最终使得推理速度得到了显著提升，占用内存减小，训练也更加容易便捷。</p><p><strong>R-CNN的问题</strong></p><p><img src="/2019/11/30/RCNN-series-in-object-detection/slow-RCNN.PNG" alt="R-CNN结构示意"></p><ul><li>分阶段训练，多个步骤，首先是softmax分类网络，然后时linear SVM分类器，最后是bounding-box regression，而且每个阶段的训练样本都得按照各自的规则重新选取；</li><li>训练时间很长（84h），region proposals 要一个个地进行resize，然后一个个送进网络，占用大量硬盘空间；</li><li>训练好的结构推理时间长（利用vgg16 backbone一张图片47s）；</li></ul><p><strong>SPPNet的问题</strong></p><p><img src="/2019/11/30/RCNN-series-in-object-detection/SPP-net.PNG" alt="SPPNet结构示意"></p><p>SPP进行了图像级别的计算共享，只需在最后的feature map上进行region-wise computation计算，加速了推理时间。但是由于SPPNet的其他部分完全照搬R-CNN，所以也存在训练繁琐，训练时间长，占用磁盘空间大的问题。此外，Ross认为，SPPNet引入了一个新的问题：在训练时，SPP layer以下的卷积层等layer的参数无法更新。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/sppnet-problem.PNG" alt="训练时SPPNet的spp layer以下的层参数无法更新"></p><p>其原因主要跟mini-batch sample样本的策略有关，Ross也在其ICCV 2015的oral中解释了。SPPNet在训练时，一个batch随机采样128个regional proposal，这样的话样本有很大概率是来自于不同的图片，特征图上的一个proposal对应到原图上的感受野通常很大，最坏的情况是感受野是整个图像，那么每个batch可能就要去计算很多个几乎是整幅图像的梯度，而且不同图像间不能共享卷积计算和内容，反向传播会很慢且很耗内存。</p><p><strong>Fast R-CNN的优缺点</strong></p><ul><li><p>单个网络训练，占用内存少；</p></li><li><p>推理速度快，精度更高；</p></li><li><p>region proposal依然耗时；</p></li></ul><p><img src="/2019/11/30/RCNN-series-in-object-detection/fast-RCNN-testing.PNG" alt="Fast R-CNN网络结构示意图"></p><h3 id="Content-2"><a href="#Content-2" class="headerlink" title="Content"></a>Content</h3><p>如上图，Fast R-CNN的推理流程如下：</p><p>输入任意size的图片和Selective Search产生的region proposals；原图经过一系列卷积层得到最终的特征图，并根据原region proposal找到feature map上的对应特征区域；对特征区域进行RoI pooling，得到固定大小的特征图，并经过fc层得到固定大小的特征向量（可由SVD分解加速推理）；特征向量继续向后传递产生两个branch，一个用softmax估计region proposal的类别（N+1个神经元），另一个回归bounding-box的平移和伸缩变换系数（4个神经元）；最后利用NMS剔除多余的框。</p><p><strong>RoI pooling</strong></p><p>RoI pooling是特殊的SPP，因为只有一个尺度的pooling，目的是为了不同尺寸的region proposal产生固定size的输出，兼容后面的fc层。论文中提到使用多尺度pooling对精度会有较小的提升，但是会带来成倍的计算量，所以只用单尺度是一种trade-off。此外RoI pooling的网格尺寸是自适应的，利用原proposal大小和期望输出的大小进行计算。</p><p>由于RoI pooling采用max pooling，是可微分的，用于BP进行反向传播更新参数。但是由于一幅图像中不同proposal之间内容会有重叠，所以特征图上一个比较大的值可能出现在不同的pooling后的proposal上，因此论文提出直接将其相加。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/RoI-BP.PNG" alt="论文对RoI pooling 梯度计算的描述"></p><p><img src="/2019/11/30/RCNN-series-in-object-detection/differentiable-RoI.PNG" alt="梯度计算示意（前面提到的blog2也有相关内容的详细描述）"></p><p><strong>one-stage training—SGD &amp; multi-task loss</strong></p><p>如前所述，如果采取SPPNet mini-batch sample样本的方式，梯度计算会很慢，将会很难训练网络。Fast R-CNN采用了向同幅图片随机采多个region proposals（hierarchical sampling）来加快梯度计算和传递：先随机采两幅图片，然后从这两幅图像中分别再各自采取64（128/2）个proposals，这样的话感受野会有很大重叠，梯度计算就不会很慢，占用内存也不会太大。但是Ross认为这种方式可能会导致网络收敛很慢，因为proposals很多都是相关的，不过实际中并没有出现这个问题。</p><p>分类损失函数：$L_{c l s}(p, u)=-\log p_{u}$，依然是交叉熵，分类采用softmax，未继续采用svm，实验证明采用前者精度更高，原因可能是因为多任务学习以及RoI pooling的作用。</p><p>回归损失函数不再是平方损失函数，而换成了$smooth_{L1}$损失函数：</p><script type="math/tex;mode=display">L_{l o c}\left(t^{u}, v\right)=\sum_{i \in\{x, y, w, h\}} \operatorname{smooth}_{L_{1}}\left(t_{i}^{u}-v_{i}\right)</script><script type="math/tex;mode=display">\operatorname{smooth}_{L_{1}}(x)=\left\{\begin{array}{ll}{0.5 x^{2},} & {|x|<1} \\ {|x|-0.5,} & {\text { otherwise }}\end{array}\right.</script><p>回归分支输出预测的平移伸缩变换参数：$t^{u}=\left(t_{x}^{u}, t_{y}^{u}, t_{w}^{u}, t_{h}^{u}\right)$，根据预测结果得到一个预测框，计算出真实平移伸缩参数：$v=\left(v_{x}, v_{y}, v_{w}, v_{h}\right)$，然后计算误差不断逼近即可。</p><p>训练中分类正样本是与GT的IoU属于[0.5,1]之间的proposal，负样本是与所有类别GT的IoU的最大值落在[0.1, 0.5)之间的proposal（0.1来自困难样本挖掘，论文中一笔带过），两者按照1：3配比（没有这个约束来平衡正负样本，训练出的模型精度会下降）。</p><p>Ross也在论文中做了对比实验，验证多任务学习确实多两者都有提升，也进一步说明这种一个流程下来的结构是符合期望也有不错的效果。</p><p>此外Ross也研究了多尺度训练和单尺度训练的问题（曾作为SPPNet的一个小contribution），也就是输入的原始图像的size不同，实验证明，deep neural network善于学习尺度不变性，对目标的scale不敏感，虽然多尺度效果效果确实好一点点，但是没必要以时间和硬件资源的牺牲来换取。</p><p><strong>SVD分解或许对提高目标检测的实时性有潜在的帮助</strong></p><p>由于开始提取出的region proposal比较多，所以大部分时间都会消耗在全连接层上（一张图片只要过一整遍卷积），为了提高速度，Ross实验了SVD分解全连接层，结果证明时间可以得到不小的提升，同时只损失很小的精度。</p><p>假设全连接层输入为$x$，输出为$y$，权重矩阵为$W$，尺寸为$u \times v$，则：</p><script type="math/tex;mode=display">y = Wx</script><p>其计算复杂度为$u \times v$，现对$W$进行SVD分解，并用前$t$个特征值近似代替，则：</p><script type="math/tex;mode=display">W=U_{u \times t} \sum \nolimits_{t \times t}  V_{v \times t}^{T}</script><script type="math/tex;mode=display">y=W x=U \cdot\left(\sum \cdot V^{T}\right) \cdot x=U \cdot z</script><p>实际上上述操作将一个全连接层拆成两个小的全连接，$z$是中间的全连接层。经过SVD分解后的计算复杂度变为$u \times t + v \times t$，如果$t$比$min(u,v)$小不少的话，可以显著减少计算量。</p><p><strong>more proposals is harmful</strong></p><p>由于region proposal的提取直接影响着后面检测的准确度，为了更高的recall，就要尽可能让region proposal覆盖到所有图像中的object，所以生成越多可能越好，但是Ross在实验中发现并不是这样，逐步增多Selective Search生成的region proposal的数量，发现mAP先增大后减小，我猜想可能是因为太多的proposal导致了相似信息增多，使得网络训练出现过拟合。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/more-proposals-is-harmful.PNG" alt="多数量的proposal并不会增高检测精度"></p><h3 id="Discussion-1"><a href="#Discussion-1" class="headerlink" title="Discussion"></a>Discussion</h3><p>Ross官方吐槽：一体化proposal提取。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/further-work-fast-rcnn.PNG" alt="Fast R-CNN进一步可以改进的地方"></p><h2 id="Faster-R-NN"><a href="#Faster-R-NN" class="headerlink" title="Faster R-NN"></a><a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">Faster R-NN</a></h2><h3 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h3><p>内容有参考：</p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">blog1</a>, <a href="https://zhuanlan.zhihu.com/p/24916624" target="_blank" rel="noopener">blog2</a>, <a href="https://blog.csdn.net/WoPawn/article/details/52223282" target="_blank" rel="noopener">blog3</a>, <a href="https://arleyzhang.github.io/articles/21c44637/" target="_blank" rel="noopener">blog4</a>, <a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">blog5</a>, <a href="https://blog.csdn.net/XiangJiaoJun_/article/details/85008477" target="_blank" rel="noopener">blog6</a>;—-时间短直接看blog1和blog5</p></li><li><p>Ross Girshick 在ICCV2015tutorial上的演讲<a href="http://mp7.watson.ibm.com/ICCV2015/slides/iccv15_tutorial_training_rbg.pdf" target="_blank" rel="noopener">Training R-CNNs of Various Velocities: Slow, Fast, and Faster</a></p></li></ul><p>Faster R-CNN的主要贡献在于提出anchor机制，利用RPN网络和Fast R-CNN一体化检测框架，实现一个完全end-to-end的目标检测网络。因此，阅读该论文需要解决两个主要问题，而且大部分内容应该着重解决第一个问题：</p><ul><li><p>RPN是如何工作的，具体设计流程和实现细节？</p></li><li><p>PRN加上Fast R-CNN怎么训练？</p></li></ul><h3 id="Content-3"><a href="#Content-3" class="headerlink" title="Content"></a>Content</h3><p><strong>Faster R-CNN是个非常复杂和精细的结构，包含了很多细节和参数，只通过读论文去完全了解是不现实的，想真正理解必须要去读下源码，如果能手撕出来的话就更好了。</strong></p><p><img src="/2019/11/30/RCNN-series-in-object-detection/FasterRCNN_architecture.png" alt="FasterRCNN_architecture，图来自blog5"></p><p>上图是Faster R-CNN的整体网络框架，图片在进入网络前要减去RGB三通道像素均值，并适当resize处理。处理好的图像矩阵送入一系列卷积层进行特征提取，过max pooling缩小特征图尺寸，在特定的卷积层后，特征图送入RPN网络，再过一个$3\times 3$的卷积层，然后分别送入两个$1 \times 1$的卷积层对生成的anchor进行前景/背景的二分类，并对其进行靠近GT的坐标回归修正；之后从中挑出topN的已经修正过坐标的anchor给RoI pooling从最后的特征图（输入PRN的特征图和RoI pooling的特征图是一样的）中抠出相应的feature region然后合成固定长度的向量送给最后的fc层去分类所有的类别，回归所有类别的坐标，进行二次修正。</p><p><strong>RPN</strong></p><p>RPN的全称是Region Proposal Network，是用以替代R-CNN和Fast R-CNN中的Selective Search方法，实现端对端的object预选框的提取工作。神经网络擅长于分类，同时我们结合R-CNN等前人的工作，要想通过网络去提取出一系列预选框该怎么做？以往经典视觉是通过滑动窗口或者图像金字塔的方法，但是类似的方法移植到网络上太费时间：可以用不同的卷积核代表滑动窗口，然后卷完再分类；用不同尺寸的特征图代表图像金字塔，然后再分类，这样的话就等于几乎复制了一遍R-CNN这样的架构，速度可能会更慢。因此，任少卿采取了pyramids-of-filter，pyramids-of-images之外的另一种替代的方法：pyramids-of-reference-boxes，也就是说，在RPN里面，我们自己预先根据一定的规则，生成许多的矩形框，覆盖住整个图像的几乎每个object的内容，然后再用网络去识别哪些是“好的”，哪些是“坏的”，然后让“好的”尽量好点，送给Fast R-CNN去进一步识别检测，“坏的”直接丢弃掉。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/pyramids_of_achors.PNG" alt="pyramids-of-images, pyramids-of-filters, pyramids-of-reference-boxes"></p><p>这些参考框称为anchor，具体生成方式如下：</p><p>参照Faster R-CNN的网络结构图，假设输入给RPN的特征图尺寸比原图缩小了16倍，特征图上的每一个像素点，或者说grid，可以看作是原图的$16 \times 16$的gird的缩小版，然后以每个原图上的这些$16 \times 16$的grid的中心为参考点，画出预设面积和宽高比的矩形框，这就作为一个初始的预选框。论文设置了三个anchor scale（代表面积）和三个宽高比，分别是[8, 16, 32]，[0.5, 1, 2]，其中anchor scale的base是16，也就是说预设的面积分别是：$16 \times 8 = 128, 16 \times 16 = 256, 32 \times 16 = 512$，宽高为：$area \times \sqrt{aspect ratio}, area \times \sqrt{1/aspectratio}$（这里说个题外话，在训练自己的数据集的时候，可能需要修改anchor scale和anchor aspect ratio，但是我自己平时实验发现，改anchor scale的作用稍微大点，比如检测小的目标的时候，改ratio效果就不怎么明显，我曾经统计了标注数据的长宽比例，然后根据此比例当作anchor aspect ratio训练，结果效果差不多，我猜想这可能就是二次回归修正的功劳，只要你覆盖到了，挑选到了就好说。）</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/anchors.png" alt="生成anchor示意图"></p><p>在第一次读论文的时候我以为是根据送进RPN的那个特征图的每个grid的感受野去得到原图的位置，然后画框，但是这样的话，每个grid在原图的位置就固定了，而每个grid代表的感受野不能像anchor那样去覆盖几乎所有的object，数量远不如anchor那么多，那么必然是会漏掉的。另外一点是跟PRN的网络结构有关。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/RPN_architecture.PNG" alt="RPN_architecture"></p><p><img src="/2019/11/30/RCNN-series-in-object-detection/RPN_Network.png" alt="详细的RPN_Network示意图"></p><p>看上面两个图，RPN不是用fc来分类和回归的，而是直接用全卷积来完成，有点类似FCN。一开始是对输入的特征图进行$3 \times 3$卷积，完成进一步特征提取，非线性话之类的工作，然后再用两个$1 \times 1$卷积完成分类和回归，值得注意的是，里面的卷积操作不改变feature map的尺寸，也就是grid的数量不变，最后分类和回归卷积层输出的tensor的尺寸分别是$(9 \times2, h, w), (9 \times 4, h, w)$，通道数分别为18（前景\背景2类，每个grid有9个anchor）和64（每个grid有9个anchor，每个anchor回归4个坐标），其后的reshape是便于softmax和回归处理。</p><p>也就是说，RPN通过全卷积，强制让网络去学习每个grid生成的9个预设anchor的质量，并进行适当的修正。实际上，我个人还是比较疑惑的，我不理解为什么这种形式可以去学习特征图在原图对应的anchor哪些是promising的，因为anchor只在训练时分配GT时有用，网络学习时并没有把anchor作为输入的一部分，但是事实证明，RPN确实可以学到，毕竟Faster R-CNN的检测效果摆在那儿。我大概猜测，特征图的每个grid经过$3 \times 3$卷积后，包含了这9个anchor的一些信息，再分别经过$1 \times 1$卷积后又分别提取出了类别和坐标信息，网络隐式地学习到了anchor的存在以及他们的信息。</p><p>此外，论文中还提到anchor这种方式也具有平移不变性，就是你物体动了，我anchor也会跟着动，这对于图像分类问题来说是很好的一个特性，不管物体在哪个位置都要求较高的准确性。但是对于目标检测来说，还需要定位出物体在图片上的位置，因此平移不变性会抹掉这点。</p><p>RPN的训练和推理不是完全一样的。训练RPN时，预先产生的anchor数量很多，可能有上万个，这么多的框确实会包含到几乎所有的object信息，但是很多都是冗余的，也有很多是背景，因此不能全部拿去训练，否则不仅速度慢，效果可能还不好。</p><p>论文中训练的anchor总量只有256个（也就是说每次训练只有256个位置有GT，没有GT的就不去计算误差），但是每张图片生成的训练anchor的位置是不一样的，所以图片比较多，训练比较充足的情况下，基本上整个网络应该都是可以训练到的（但是我感觉有些anchor位置可能就是比较容易选中训练，比如原图中心部分）。</p><p>训练RPN时候需要先计算anchor的label和坐标变换系数：</p><ul><li><p>foreground认定：anchor box和GT box的IoU超过了设定阈值（论文是0.7），为了防止没有超过的，对于每一个GT box，anchor box跟它有最大IoU的也认为是前景框；</p></li><li><p>background认定：anchor box和GT box的IoU低于设定阈值（论文是0.3）；</p></li><li><p>落于两个阈值之间的认为是“don’t care”。此外，还有一个<code>TRAIN.RPN_FG_FRACTION</code>参数，一般是0.5，也就是每个batch size中采取的前景anchor只有$256 \times 0.5 =128$个，如果超过了，就随机挑128个。</p></li></ul><p>训练好的RPN需要给Fast R-CNN提供一些可靠的RoI，这时RPN进入推理阶段：</p><ul><li><p>将PRN输出的偏移量给原始的anchor，得到新的区域，同时带有置信度score；</p></li><li><p>对超出图像范围的框进行裁剪，保证框都在图像内部，这一步没有改变框的数量；</p></li><li><p>丢掉小于设定的最小尺寸(<code>TRAIN.RPN_MIN_SIZE = 8</code>)的anchor；</p></li><li><p>根据置信度，选择topK(<code>TRAIN.RPN_PRE_NMS_TOP_N = 12000</code>)个框，和设定的NMS(<code>TRAIN.RPN_NMS_THRESH = 0.7</code>)阈值，通过非极大抑制去除冗余的框；</p></li><li><p>从NMS之后的框中，选择得分为topN（<code>TRAIN.RPN_POST_NMS_TOP_N = 2000</code>）的框作为region proposal输入给Faster R-CNN；</p></li></ul><p>到此为止，RPN完成了类似Selective Search的工作。</p><p>类似于Fast R-CNN的操作，这些2000个region proposal并不会全部进行RoI pooling，而是从中挑出128个样本进行训练：</p><p>region proposal与任一GT box之间的IoU 大于阈值(<code>TRAIN.FG_THRESH=0.5</code>)认为是前景，在0.1(<code>TRAIN.BG_THRESH_LO</code>)和0.5(<code>TRAIN.BG_THRESH_HI</code>)之间认为是负样本；</p><p>从前景中挑出一定数量（batch size乘以比例）作为正样本，比例为<code>TRAIN.FG_FRACTION = 0.25</code>，如果正样本没有这么多，则全部选出来，其他的用负样本凑。</p><p>如果使用VGG16作为特征提取网络，那么送入RPN的特征图就是最后的特征图，然后利用RoI找到基于原图的128个region proposal对应在这个特征图上的特征向量，最后送到fc层进行分类和回归即可（回归坐标的时候有个bbox_inside_weights，只有前景的时候为1，背景为0，计算loss时候通过这个weight来抹掉背景）。</p><p>对于resnet101而言，网络相对而言已经很深了，上面提到的平移不变性此时在最后输出的特征图上基本已经消失了，包含的都是一些高层的语义信息，如果还是把最后的特征图给RPN，那么特征图再去卷积学习anchor的信息，然后提取，将会很难，最后的检测结果也不一定理想，所以此时送入RPN的特征图需要稍微往前些。一般我看的都是把Conv4出来的特征图送给RPN，然后RoI pooling提取出对应的特征区域后再过最后的Conv5，最后送入fc。</p><p>此外，RoI pooling也是影响Faster R-CNN精度的一个部分，后面出现了RoI Align（Mask R-CNN）, Crop pooling（<a href="https://arxiv.org/pdf/1506.02025.pdf" target="_blank" rel="noopener">Spatial Transformer Networks</a>, <a href="https://zhuanlan.zhihu.com/p/37110107" target="_blank" rel="noopener">blog</a>）, RoI warp（<a href="https://blog.csdn.net/Julialove102123/article/details/80567827" target="_blank" rel="noopener">blog</a>）等取代，目的都是想尽可能地提高回归精度。这些细节我们后续再谈。</p><p>在利用Faster R-CNN进行预测的时候，跟训练Fast R-CNN部分差不多，但是region proposal的数量等参数会有所不同，比如PRN在NMS之前的nchor数量设置为6000，在NMS之后设置为300，然后Fast R-CNN直接对这300个进行推理，NMS之后输出预测框即可。</p><p><strong>如何训练Faster R-CNN?</strong></p><p>RPN网络的loss函数是二分类交叉熵和$soomth \quad L_{1}$回归损失：</p><script type="math/tex;mode=display">\begin{aligned} L\left(\left\{p_{i}\right\},\left\{t_{i}\right\}\right)=& \frac{1}{N_{c l s}} \sum_{i} L_{c l s}\left(p_{i}, p_{i}^{*}\right) +\lambda \frac{1}{N_{r e g}} \sum_{i} p_{i}^{*} L_{r e g}\left(t_{i}, t_{i}^{*}\right) \end{aligned}​</script><p>$\frac{1}{N_{c l s}}, \frac{1}{N_{r e g}}, \lambda$分别是分类正负样本mini bacth size（256），anchor数量（大约2400）正则化系数和两个loss的权重系数，论文中说明前两个正则化系数可以不用，效果没差，$\lambda$也不是敏感系数，取值扩大100倍也不会太影响精度。</p><p>回归部分依然是学习平移和缩放变换系数，按照我前面的说法，RPN是强制是学习有anchor这么个东西，然后再去学习辨别anchor，修正其位置。论文中给出的公式也说明，RPN应当是先学到anchor的位置，再去学GT的位置。当然，给回归部分的GT是anchor和对应GT box之间的变换系数。</p><script type="math/tex;mode=display">t_{\mathrm{x}}=\left(x-x_{\mathrm{a}}\right) / w_{\mathrm{a}}, \quad t_{\mathrm{y}}=\left(y-y_{\mathrm{a}}\right) / h_{\mathrm{a}} \\t_{\mathrm{w}}=\log \left(w / w_{\mathrm{a}}\right), \quad t_{\mathrm{h}}=\log \left(h / h_{\mathrm{a}}\right) \\t_{\mathrm{x}}^{*}=\left(x^{*}-x_{\mathrm{a}}\right) / w_{\mathrm{a}}, \quad t_{\mathrm{y}}^{*}=\left(y^{*}-y_{\mathrm{a}}\right) / h_{\mathrm{a}} \\t_{\mathrm{w}}^{*}=\log \left(w^{*} / w_{\mathrm{a}}\right), \quad t_{\mathrm{h}}^{*}=\log \left(h^{*} / h_{\mathrm{a}}\right)</script><p>Fast R-CNN部分的loss在前一节已经说了，就不再赘述，所以Faster R-CNN一共有rpn_cls, rpn_box, rcnn_cls, rcnn_box这四个loss。</p><p>最后需要解决的是Faster R-CNN是如何联合训练的问题。RPN结合Fast R-CNN有点像GAN，但是两者之间不是对抗的关系，而是相互依存的，而且Fast R-CNN更依赖具有良好性能的RPN多一点。作者在论文中分别提到了3种方法，分别是：<em>Alternating training, Approximate joint training, Non-approximate training</em></p><p><em>Alternating training</em>是分别训练两个网络，比如用CNN特征提取网络VGG16去初始化RPN，训练让其收敛，再让RPN产生的proposal去训练Fast R-CNN，让其收敛…循环往复；<em>Approximate joint training, Non-approximate training</em>都是将其当成一体的，直接利用SGD训练，两种的区别在于前者丢弃RPN的bbox预测梯度，后者则不丢弃，这两点论文都未细说具体细节，比如丢弃RPN的回归梯度的话，anchor还需要向GT box修正吗？</p><p>根据其他博客的内容，官方源码给出的训练思路是这样的（类似第一种交替迭代训练方法），估计这也是目前常用的流程（可能要看完源码之后才清楚这件事）：</p><ul><li><p>利用在ImageNet上预训练好的CNN模型，训练RPN；</p></li><li><p>利用上一步训练好的RPN产生的proposals训练已被ImageNet预训练模型初始化的Fast R-CNN：</p></li><li><p>利用上一步的Fast R-CNN初始化RPN，不更新共享的特征提取网络，仅仅更新RPN独有的卷积层，重新训练RPN；</p></li><li><p>加入Fast R-CNN，形成一个整体，但是只训练Fast R-CNN特有的卷积层和fc层，共享卷积层参数冻结，proposals来自上一步训练好的RPN；</p></li></ul><p>上述过程类似迭代训练并且进行了两次，作者提到循环多次没有更多的提升。</p><h3 id="Discussion-2"><a href="#Discussion-2" class="headerlink" title="Discussion"></a>Discussion</h3><p>Faster R-CNN中最大的contribution可能就是anchor的使用了，解决了物体中尺度的多样性问题。此外，根据这篇<a href="https://zhuanlan.zhihu.com/p/73024408" target="_blank" rel="noopener">博客</a>的内容，anchor的使用还顺带解决了<strong>gt box与gt box之间overlap过大导致gt box丢失问题</strong>。大意就是两个不同的物体标的框重合度很大，导致CNN特征图里面也分不清了，这样的话这两个物体可能一起存在特征图的一个grid里面，就会丢掉一个物体，但是anchor是在原图搜的，每个grid都有不同尺度的内容，因此可以一定程度缓解recall降低（漏检）的问题。</p><p>（最近的新文章: <a href="https://arxiv.org/pdf/1912.05190.pdf" target="_blank" rel="noopener">IoU-uniform R-CNN: Breaking Through the Limitations of RPN</a>，还没来得及看，后面搞懂了再来开一篇说下，顺便也加上anchor free）</p><p>基于pytorch的<a href="https://github.com/jwyang/faster-rcnn.pytorch/tree/pytorch-1.0" target="_blank" rel="noopener">faster rcnn</a>中的<a href="https://github.com/jwyang/faster-rcnn.pytorch/blob/pytorch-1.0/lib/model/utils/config.py" target="_blank" rel="noopener">config</a>文件，里面设置了几乎所有的参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br></pre></td><td class="code"><pre><span class="line"># Training options</span><br><span class="line">#</span><br><span class="line">__C.TRAIN = edict()</span><br><span class="line"></span><br><span class="line"># Initial learning rate</span><br><span class="line">__C.TRAIN.LEARNING_RATE = 0.001</span><br><span class="line"></span><br><span class="line"># Momentum</span><br><span class="line">__C.TRAIN.MOMENTUM = 0.9</span><br><span class="line"></span><br><span class="line"># Weight decay, for regularization</span><br><span class="line">__C.TRAIN.WEIGHT_DECAY = 0.0005</span><br><span class="line"></span><br><span class="line"># Factor for reducing the learning rate</span><br><span class="line">__C.TRAIN.GAMMA = 0.1</span><br><span class="line"></span><br><span class="line"># Step size for reducing the learning rate, currently only support one step</span><br><span class="line">__C.TRAIN.STEPSIZE = [30000]</span><br><span class="line"></span><br><span class="line"># Iteration intervals for showing the loss during training, on command line interface</span><br><span class="line">__C.TRAIN.DISPLAY = 10</span><br><span class="line"></span><br><span class="line"># Whether to double the learning rate for bias</span><br><span class="line">__C.TRAIN.DOUBLE_BIAS = True</span><br><span class="line"></span><br><span class="line"># Whether to initialize the weights with truncated normal distribution</span><br><span class="line">__C.TRAIN.TRUNCATED = False</span><br><span class="line"></span><br><span class="line"># Whether to have weight decay on bias as well</span><br><span class="line">__C.TRAIN.BIAS_DECAY = False</span><br><span class="line"></span><br><span class="line"># Whether to add ground truth boxes to the pool when sampling regions</span><br><span class="line">__C.TRAIN.USE_GT = False</span><br><span class="line"></span><br><span class="line"># Whether to use aspect-ratio grouping of training images, introduced merely for saving</span><br><span class="line"># GPU memory</span><br><span class="line">__C.TRAIN.ASPECT_GROUPING = False</span><br><span class="line"></span><br><span class="line"># The number of snapshots kept, older ones are deleted to save space</span><br><span class="line">__C.TRAIN.SNAPSHOT_KEPT = 3</span><br><span class="line"></span><br><span class="line"># The time interval for saving tensorflow summaries</span><br><span class="line">__C.TRAIN.SUMMARY_INTERVAL = 180</span><br><span class="line"></span><br><span class="line"># Scale to use during training (can list multiple scales)</span><br><span class="line"># The scale is the pixel size of an image&apos;s shortest side</span><br><span class="line">__C.TRAIN.SCALES = (600,)</span><br><span class="line"></span><br><span class="line"># Max pixel size of the longest side of a scaled input image</span><br><span class="line">__C.TRAIN.MAX_SIZE = 1000</span><br><span class="line"></span><br><span class="line"># Trim size for input images to create minibatch</span><br><span class="line">__C.TRAIN.TRIM_HEIGHT = 600</span><br><span class="line">__C.TRAIN.TRIM_WIDTH = 600</span><br><span class="line"></span><br><span class="line"># Images to use per minibatch</span><br><span class="line">__C.TRAIN.IMS_PER_BATCH = 1</span><br><span class="line"></span><br><span class="line"># Minibatch size (number of regions of interest [ROIs])</span><br><span class="line">__C.TRAIN.BATCH_SIZE = 128</span><br><span class="line"></span><br><span class="line"># Fraction of minibatch that is labeled foreground (i.e. class &gt; 0)</span><br><span class="line">__C.TRAIN.FG_FRACTION = 0.25</span><br><span class="line"></span><br><span class="line"># Overlap threshold for a ROI to be considered foreground (if &gt;= FG_THRESH)</span><br><span class="line">__C.TRAIN.FG_THRESH = 0.5</span><br><span class="line"></span><br><span class="line"># Overlap threshold for a ROI to be considered background (class = 0 if</span><br><span class="line"># overlap in [LO, HI))</span><br><span class="line">__C.TRAIN.BG_THRESH_HI = 0.5</span><br><span class="line">__C.TRAIN.BG_THRESH_LO = 0.1</span><br><span class="line"></span><br><span class="line"># Use horizontally-flipped images during training?</span><br><span class="line">__C.TRAIN.USE_FLIPPED = True</span><br><span class="line"></span><br><span class="line"># Train bounding-box regressors</span><br><span class="line">__C.TRAIN.BBOX_REG = True</span><br><span class="line"></span><br><span class="line"># Overlap required between a ROI and ground-truth box in order for that ROI to</span><br><span class="line"># be used as a bounding-box regression training example</span><br><span class="line">__C.TRAIN.BBOX_THRESH = 0.5</span><br><span class="line"></span><br><span class="line"># Iterations between snapshots</span><br><span class="line">__C.TRAIN.SNAPSHOT_ITERS = 5000</span><br><span class="line"></span><br><span class="line"># solver.prototxt specifies the snapshot path prefix, this adds an optional</span><br><span class="line"># infix to yield the path: &lt;prefix&gt;[_&lt;infix&gt;]_iters_XYZ.caffemodel</span><br><span class="line">__C.TRAIN.SNAPSHOT_PREFIX = &apos;res101_faster_rcnn&apos;</span><br><span class="line"># __C.TRAIN.SNAPSHOT_INFIX = &apos;&apos;</span><br><span class="line"></span><br><span class="line"># Use a prefetch thread in roi_data_layer.layer</span><br><span class="line"># So far I haven&apos;t found this useful; likely more engineering work is required</span><br><span class="line"># __C.TRAIN.USE_PREFETCH = False</span><br><span class="line"></span><br><span class="line"># Normalize the targets (subtract empirical mean, divide by empirical stddev)</span><br><span class="line">__C.TRAIN.BBOX_NORMALIZE_TARGETS = True</span><br><span class="line"># Deprecated (inside weights)</span><br><span class="line">__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)</span><br><span class="line"># Normalize the targets using &quot;precomputed&quot; (or made up) means and stdevs</span><br><span class="line"># (BBOX_NORMALIZE_TARGETS must also be True)</span><br><span class="line">__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = True</span><br><span class="line">__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)</span><br><span class="line">__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)</span><br><span class="line"></span><br><span class="line"># Train using these proposals</span><br><span class="line">__C.TRAIN.PROPOSAL_METHOD = &apos;gt&apos;</span><br><span class="line"></span><br><span class="line"># Make minibatches from images that have similar aspect ratios (i.e. both</span><br><span class="line"># tall and thin or both short and wide) in order to avoid wasting computation</span><br><span class="line"># on zero-padding.</span><br><span class="line"></span><br><span class="line"># Use RPN to detect objects</span><br><span class="line">__C.TRAIN.HAS_RPN = True</span><br><span class="line"># IOU &gt;= thresh: positive example</span><br><span class="line">__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7</span><br><span class="line"># IOU &lt; thresh: negative example</span><br><span class="line">__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3</span><br><span class="line"># If an anchor statisfied by positive and negative conditions set to negative</span><br><span class="line">__C.TRAIN.RPN_CLOBBER_POSITIVES = False</span><br><span class="line"># Max number of foreground examples</span><br><span class="line">__C.TRAIN.RPN_FG_FRACTION = 0.5</span><br><span class="line"># Total number of examples</span><br><span class="line">__C.TRAIN.RPN_BATCHSIZE = 256</span><br><span class="line"># NMS threshold used on RPN proposals</span><br><span class="line">__C.TRAIN.RPN_NMS_THRESH = 0.7</span><br><span class="line"># Number of top scoring boxes to keep before apply NMS to RPN proposals</span><br><span class="line">__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000</span><br><span class="line"># Number of top scoring boxes to keep after applying NMS to RPN proposals</span><br><span class="line">__C.TRAIN.RPN_POST_NMS_TOP_N = 2000</span><br><span class="line"># Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)</span><br><span class="line">__C.TRAIN.RPN_MIN_SIZE = 8</span><br><span class="line"># Deprecated (outside weights)</span><br><span class="line">__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)</span><br><span class="line"># Give the positive RPN examples weight of p * 1 / &#123;num positives&#125;</span><br><span class="line"># and give negatives a weight of (1 - p)</span><br><span class="line"># Set to -1.0 to use uniform example weighting</span><br><span class="line">__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0</span><br><span class="line"># Whether to use all ground truth bounding boxes for training,</span><br><span class="line"># For COCO, setting USE_ALL_GT to False will exclude boxes that are flagged as &apos;&apos;iscrowd&apos;&apos;</span><br><span class="line">__C.TRAIN.USE_ALL_GT = True</span><br><span class="line"></span><br><span class="line"># Whether to tune the batch normalization parameters during training</span><br><span class="line">__C.TRAIN.BN_TRAIN = False</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Testing options</span><br><span class="line">#</span><br><span class="line">__C.TEST = edict()</span><br><span class="line"></span><br><span class="line"># Scale to use during testing (can NOT list multiple scales)</span><br><span class="line"># The scale is the pixel size of an image&apos;s shortest side</span><br><span class="line">__C.TEST.SCALES = (600,)</span><br><span class="line"></span><br><span class="line"># Max pixel size of the longest side of a scaled input image</span><br><span class="line">__C.TEST.MAX_SIZE = 1000</span><br><span class="line"></span><br><span class="line"># Overlap threshold used for non-maximum suppression (suppress boxes with</span><br><span class="line"># IoU &gt;= this threshold)</span><br><span class="line">__C.TEST.NMS = 0.3</span><br><span class="line"></span><br><span class="line"># Experimental: treat the (K+1) units in the cls_score layer as linear</span><br><span class="line"># predictors (trained, eg, with one-vs-rest SVMs).</span><br><span class="line">__C.TEST.SVM = False</span><br><span class="line"></span><br><span class="line"># Test using bounding-box regressors</span><br><span class="line">__C.TEST.BBOX_REG = True</span><br><span class="line"></span><br><span class="line"># Propose boxes</span><br><span class="line">__C.TEST.HAS_RPN = False</span><br><span class="line"></span><br><span class="line"># Test using these proposals</span><br><span class="line">__C.TEST.PROPOSAL_METHOD = &apos;gt&apos;</span><br><span class="line"></span><br><span class="line">## NMS threshold used on RPN proposals</span><br><span class="line">__C.TEST.RPN_NMS_THRESH = 0.7</span><br><span class="line">## Number of top scoring boxes to keep before apply NMS to RPN proposals</span><br><span class="line">__C.TEST.RPN_PRE_NMS_TOP_N = 6000</span><br><span class="line"></span><br><span class="line">## Number of top scoring boxes to keep after applying NMS to RPN proposals</span><br><span class="line">__C.TEST.RPN_POST_NMS_TOP_N = 300</span><br><span class="line"></span><br><span class="line"># Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)</span><br><span class="line">__C.TEST.RPN_MIN_SIZE = 16</span><br><span class="line"></span><br><span class="line"># Testing mode, default to be &apos;nms&apos;, &apos;top&apos; is slower but better</span><br><span class="line"># See report for details</span><br><span class="line">__C.TEST.MODE = &apos;nms&apos;</span><br><span class="line"></span><br><span class="line"># Only useful when TEST.MODE is &apos;top&apos;, specifies the number of top proposals to select</span><br><span class="line">__C.TEST.RPN_TOP_N = 5000</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># ResNet options</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">__C.RESNET = edict()</span><br><span class="line"></span><br><span class="line"># Option to set if max-pooling is appended after crop_and_resize.</span><br><span class="line"># if true, the region will be resized to a square of 2xPOOLING_SIZE,</span><br><span class="line"># then 2x2 max-pooling is applied; otherwise the region will be directly</span><br><span class="line"># resized to a square of POOLING_SIZE</span><br><span class="line">__C.RESNET.MAX_POOL = False</span><br><span class="line"></span><br><span class="line"># Number of fixed blocks during training, by default the first of all 4 blocks is fixed</span><br><span class="line"># Range: 0 (none) to 3 (all)</span><br><span class="line">__C.RESNET.FIXED_BLOCKS = 1</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># MobileNet options</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">__C.MOBILENET = edict()</span><br><span class="line"></span><br><span class="line"># Whether to regularize the depth-wise filters during training</span><br><span class="line">__C.MOBILENET.REGU_DEPTH = False</span><br><span class="line"></span><br><span class="line"># Number of fixed layers during training, by default the first of all 14 layers is fixed</span><br><span class="line"># Range: 0 (none) to 12 (all)</span><br><span class="line">__C.MOBILENET.FIXED_LAYERS = 5</span><br><span class="line"></span><br><span class="line"># Weight decay for the mobilenet weights</span><br><span class="line">__C.MOBILENET.WEIGHT_DECAY = 0.00004</span><br><span class="line"></span><br><span class="line"># Depth multiplier</span><br><span class="line">__C.MOBILENET.DEPTH_MULTIPLIER = 1.</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># MISC</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># The mapping from image coordinates to feature map coordinates might cause</span><br><span class="line"># some boxes that are distinct in image space to become identical in feature</span><br><span class="line"># coordinates. If DEDUP_BOXES &gt; 0, then DEDUP_BOXES is used as the scale factor</span><br><span class="line"># for identifying duplicate boxes.</span><br><span class="line"># 1/16 is correct for &#123;Alex,Caffe&#125;Net, VGG_CNN_M_1024, and VGG16</span><br><span class="line">__C.DEDUP_BOXES = 1. / 16.</span><br><span class="line"></span><br><span class="line"># Pixel mean values (BGR order) as a (1, 1, 3) array</span><br><span class="line"># We use the same pixel mean for all networks even though it&apos;s not exactly what</span><br><span class="line"># they were trained with</span><br><span class="line">__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])</span><br><span class="line"></span><br><span class="line"># For reproducibility</span><br><span class="line">__C.RNG_SEED = 3</span><br><span class="line"></span><br><span class="line"># A small number that&apos;s used many times</span><br><span class="line">__C.EPS = 1e-14</span><br><span class="line"></span><br><span class="line"># Root directory of project</span><br><span class="line">__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), &apos;..&apos;, &apos;..&apos;, &apos;..&apos;))</span><br><span class="line"></span><br><span class="line"># Data directory</span><br><span class="line">__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, &apos;data&apos;))</span><br><span class="line"></span><br><span class="line"># Name (or path to) the matlab executable</span><br><span class="line">__C.MATLAB = &apos;matlab&apos;</span><br><span class="line"></span><br><span class="line"># Place outputs under an experiments directory</span><br><span class="line">__C.EXP_DIR = &apos;default&apos;</span><br><span class="line"></span><br><span class="line"># Use GPU implementation of non-maximum suppression</span><br><span class="line">__C.USE_GPU_NMS = True</span><br><span class="line"></span><br><span class="line"># Default GPU device id</span><br><span class="line">__C.GPU_ID = 0</span><br><span class="line"></span><br><span class="line">__C.POOLING_MODE = &apos;crop&apos;</span><br><span class="line"></span><br><span class="line"># Size of the pooled region after RoI pooling</span><br><span class="line">__C.POOLING_SIZE = 7</span><br><span class="line"></span><br><span class="line"># Maximal number of gt rois in an image during Training</span><br><span class="line">__C.MAX_NUM_GT_BOXES = 20</span><br><span class="line"></span><br><span class="line"># Anchor scales for RPN</span><br><span class="line">__C.ANCHOR_SCALES = [8,16,32]</span><br><span class="line"></span><br><span class="line"># Anchor ratios for RPN</span><br><span class="line">__C.ANCHOR_RATIOS = [0.5,1,2]</span><br><span class="line"></span><br><span class="line"># Feature stride for RPN</span><br><span class="line">__C.FEAT_STRIDE = [16, ]</span><br><span class="line"></span><br><span class="line">__C.CUDA = False</span><br><span class="line"></span><br><span class="line">__C.CROP_RESIZE_WITH_MAX_POOL = True</span><br></pre></td></tr></table></figure><h2 id="OHEM-Online-Hard-Example-Mining"><a href="#OHEM-Online-Hard-Example-Mining" class="headerlink" title="OHEM(Online Hard Example Mining)"></a><a href="https://arxiv.org/abs/1604.03540" target="_blank" rel="noopener">OHEM(Online Hard Example Mining)</a></h2><h3 id="Introduction-4"><a href="#Introduction-4" class="headerlink" title="Introduction"></a>Introduction</h3><p>论文全称是”Training Region-based Object Detectors with Online Hard Example Mining”，<a href="https://github.com/abhi2610/ohem" target="_blank" rel="noopener">code</a></p><p>内容有参考：<a href="https://blog.csdn.net/u012905422/article/details/52760669" target="_blank" rel="noopener">blog1</a>, <a href="https://zhuanlan.zhihu.com/p/58162337" target="_blank" rel="noopener">blog2</a></p><p>对样本进行挖掘，自动选出困难样本让网络学习，提高检测精度。</p><h3 id="Content-4"><a href="#Content-4" class="headerlink" title="Content"></a>Content</h3><p>传统机器学习方法，比如SVM，在进行分类训练时，会采取 ”hard negative mining“这样的bootstrapping策略对难以学习的样本进行选取。大致流程是先用正样本和随机采样的负样本组成初始训练数据集，训练一会之后<br>（比如差不多收敛了），去掉容易分类的样本，然后加入一些现有模型不能很好判断的样本，重新进行训练，迭代几轮之后得到最后的分类模型。</p><p>Ross等人受到此方法的启发，想在基于region-conv-based这样的object detection方法上也进行困难样本的挖掘。</p><blockquote><p>Our motivation is the same as it has always been – detection datasets contain an overwhelming number of easy examples and a small number of hard examples</p></blockquote><p>原因有以下几点：</p><ul><li><p>基于深度学习的目标检测网络中，通常forground和background的region proposal数量很不均衡，负样本要比正样本多得多，这样会导致网络学习的focus被淹没；</p></li><li><p>每个样本对网络学习的贡献是不同的，学习过程中肯定出现容易和困难的样本，因此也需要网络自己去调节对他们的学习权重；</p></li><li><p>Fast R-CNN中对图像搜索出来的region proposal进行可正负样本1：3的配比，同时根据传统的hard negative mining得到了人工设置的IoU阈值0.1，去挑选负样本，虽然不是最优的方法，但是也是不能省去的部分，否则会掉点；</p></li></ul><p>因此，Ross等人专门在这篇paper中探讨样本选择问题，根据模型测试样本得到的误差大小来针对所有类别的样本自动进行挖掘（loss越大说明样本对该模型越难判断），故取名”online hard example mining“。但是不同于传统机器学习，此方法在神经网络上直接实施会存在以下问题:</p><ul><li><p>神经网络基于SGD随机梯度下降，在固定数据集上进行很多次迭代后才可以训练好的，如果先训练几个batch，然后froze参数，再去测试，然后挑出困难的样本，然后再去训练更新参数会大大拖慢模型产出的速度；</p></li><li><p>常规选择样本的方法是，首先将loss排序，然后选择loss比较大的一部分，其他的置0，这样就等于把其他样本认定为简单样本”丢掉了“，但是由于当时深度学习框架的限制，即使这些样本是0，但是依然占据空间，依然需要参与反向传播，并不能很好实现速加快和空间节省；</p></li></ul><p>考虑到这些现实问题，作者采取了折中的方法，由于计算loss和卷积层无关，卷积层只负责提取原始图像特征，loss计算只由RoI pooling+fc层决定，所以网络在卷积之后设置了两个RoI pooling+fc层，一个只读（读所有的proposal），用于计算loss，挑选样本（样本数量为B，类似Fast R-CNN的计算方式），不梯度下降，另一个负责正常的训练，样本来自于只读网络。两个之间权重参数是共享的，整体结构如下图所示。</p><p><img src="/2019/11/30/RCNN-series-in-object-detection/OHEM_architecture.PNG" alt="Fast R-CNN+OHEM结构流程示意图"></p><p>考虑到RoI中可能存在一些具有重复区域的proposal，而这些样本可能会造成相同的loss，所以如果一个是困难样本，其他的IoU比较高的重复proposal也会被选进来，造成了样本的冗余，所以不仅需要排序loss，还需要对样本进行NMS处理。</p><blockquote><p>Given a list of RoIs and their losses, NMS works by iteratively selecting the RoI with the highest loss, and then removing all lower loss RoIs that have high overlap with the selected region. We use a relaxed IoU threshold of 0.7 to suppress only highly overlapping RoIs.</p></blockquote><p>作者根据上述思想在Pascal VOC和COCO数据集上对了一系列对比试验，包括对比hard negative mining(原Fast R-CNN设置的阈值0.1，用于选择background)，batch_size，B，以及加入了一些”bells and whistles”（多尺度和迭代式检测框回归），结果都证明OHEM的效果良好，而且也容易实现，占用空间也不会很大，在某些类别的检测上有很大的精度提升（可能每个类的学习程度也不同，可能也可以挖掘下，open problem）。</p><p>此外，作者还提到如果不去选择的话，把样本全部都倒进去，那些容易的样本的loss很低，这样的话对梯度就没什么贡献，网络应该可以自己去focus那些困难的样本。作者据此也设计了实验，对不同方法的loss做了可视化，最终发现，OHEM的效果确实最好，lower loss，higher mAP。</p><h3 id="Discussion-3"><a href="#Discussion-3" class="headerlink" title="Discussion"></a>Discussion</h3><p>OHEM是在two-stage的检测框架上提出和实现的，主要关注困难的样本，抛弃容易的样本。这种主动选择样本的方式，一方面可能间接平衡了训练的样本比例，另一方面提升了网络学习的针对性。在one -stage检测框架上提出的focal loss，也是为了处理相似的问题，通过权重分配，加大对分错样本的惩罚力度，让网络主动挖掘那些样本比例较少的类别。不过focal loss接受了所有样本，并没有完全抛弃容易的样本，而且也更加容易部署，因此两者是否可以结合，产出更有效的学习criterion，也值得去思考一下。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;Faster R-CNN是目标检测领域中”two-stage”的代表性方法，其精度高，适应性强，兼具学术和工程价值。整个框架由于吸取了很多先前工作的经验，因此比较庞大，而且细节很多，因此需要认真研读下相关paper和Faster R-CNN的python代码。&lt;/p&gt;&lt;p&gt;在此之前，先贴上一位博主做的“&lt;a href=&quot;https://nikasa1889.github.io/2017/05/02/The-Modern-History-of-Object-Recognition-—-Infographic-1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The Modern History of Object Recognition — Infographic&lt;/a&gt;”，其中也包括了“one-stage”的方法，不过2017年以后的没再更新了。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/2019/11/30/RCNN-series-in-object-detection/HistoryOfObjectRecognition.png&quot; alt=&quot;modern history of object recognition&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
      <category term="object detection" scheme="http://densecollections.top/tags/object-detection/"/>
    
      <category term="semantic segmentation" scheme="http://densecollections.top/tags/semantic-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>FutureMapping2</title>
    <link href="http://densecollections.top/2019/11/17/FutureMapping2/"/>
    <id>http://densecollections.top/2019/11/17/FutureMapping2/</id>
    <published>2019-11-17T08:24:29.000Z</published>
    <updated>2020-02-03T02:28:47.256Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>继第一篇<a href="https://arxiv.org/abs/1803.11288" target="_blank" rel="noopener">FutureMapping</a>之后，视觉SLAM领域内的奠基者Andrew Davison最近又将他的和别人讨论的有关未来空间AI对地图构建，机器人协同定位，以及动态问题等新想法撰写成了新的论文<a href="https://arxiv.org/abs/1910.14139" target="_blank" rel="noopener">FutureMapping2</a>，置顶在了自己的推特上，表示欢迎大家交流自己的想法。</p><p><img src="/2019/11/17/FutureMapping2/Andrew-twitter.PNG" alt="Andrew-twitter"></p><p>总的来说，这篇文章干货还算是很多的，主要是着眼于factor graph（因子图）和Gaussian Belief Propagation（GBP，高斯置信传播）对整体，动态机器人建图等问题的潜力和前景，不仅在数学上进行一些tutorial，还给出了三个python demos。由于我对视觉SLAM只是了解整体特点，其中各种计算细节和优化方法并没有认真看过和代码书写过，因此读完这篇充满amazing reflections的文章之后我只能把握到整体的idea和一些实现方法。对于GBP和factor graph 结合构建的数学模型，我将在后续花时间弄懂后再对该blog进行补充。</p><p><strong>非常建议对SLAM或者机器视觉领域感兴趣的同行阅读此论文，相信会帮助您开阔思路！</strong></p><a id="more"></a><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h3><blockquote><p>Abstract: We argue the case for Gaussian Belief Propagation (GBP) as a strong algorithmic framework for the distributed, generic and incremental probabilistic estimation we need in Spatial AI as we aim at high performance smart robots and devices which operate within the constraints of real products. Processor hardware is changing rapidly, and GBP has the right character to take advantage of highly distributed processing and storage while estimating global quantities, as well as great flexibility. We present a detailed tutorial on GBP, relating to the standard factor graph formulation used in robotics and computer vision, and give several simulation examples with code which demonstrate its properties.</p></blockquote><p>关键词：分布式，边缘计算，局部估计，GBP，因子图，概率模型</p><p><strong>（边缘计算，分布式的，局部计算和存储，整体计算和构建以一种‘graph’的方式展开）</strong></p><p>首先，背景是现在的spatial AI系统要试着处理异质的数据，通过不同的估计手段将其转换成一致性的表示方式，但是这受限于现在的处理器性能（实时，持续地处理带来计算负担，存储负担和转换负担）。Andrew认为目前有两种方式可以对此进行提升：</p><ul><li>One is to focus on scene representation, and to find new parameterisations of world models which allow high quality scene models to be built and maintained much more efficiently. 这是表征方式的问题，这也是学术界都在解决的问题，representation learning. 机器人，计算机如何利用自己的硬件特点来认识和表征周围的世界，这跟人认识世界不一定是一样的。</li><li>The other is to look towards the changing landscape in computing and sensing hardware. 另一个就是硬件设计，比如现在视觉SLAM中的“event camera”，利用事件来记录。这里Andrew也推荐了ETH苏黎世联邦理工大学 J. Martel的phD thesis，<a href="https://www.research-collection.ethz.ch/handle/20.500.11850/362900" target="_blank" rel="noopener">传送门</a></li></ul><p>硬件问题上，Andrew并没有说太多，大都还是一些常见的设想，在后续的section中，Andrew主要是是针对第一点来说的。</p><blockquote><p>The purest representation of the knowledge in a <strong>Spatial AI problem is the factor graph itself, rather than probability distributions derived from it, which will always have to be stored with some approximation</strong>. What we are really seeking is an algorithm which implements Spatial AI in a distributed way on a computational resource like a graph processor, by storing the factor graph as the master representation and operating on it in place using local computation and message passing to implement estimation of variables as needed but taking account of global influence.</p></blockquote><p>(有关因子图的资料，推荐<a href="http://www.cs.cmu.edu/~kaess/pub/Dellaert17fnt.pdf" target="_blank" rel="noopener">Factor Graphs for Robot Perception</a>， 关于高斯置信传播，Andrew是根据bishop的PRML一书来的，此外他在文中推荐的是比较老的参考文献<a href="http://web.cs.iastate.edu/~honavar/factorgraphs.pdf" target="_blank" rel="noopener">Factor Graphs and the Sum-Product Algorithm</a>，我自己也在网上发现了一篇伯希来大学的phD论文<a href="https://arxiv.org/pdf/0811.2518.pdf" target="_blank" rel="noopener"> Gaussian Belief Propagation: Theory and Application </a>，希望对后续理解有所帮助。）</p><p>Andrew认为可以通过因子图来进行局部估计，进行边缘计算，然后进行信息传递，通过这种方法来构建分布式和全局上的计算，从而达到认知和执行任务的目的。</p><p>—-to be continued</p><h3 id="Mathematics-models"><a href="#Mathematics-models" class="headerlink" title="Mathematics models"></a>Mathematics models</h3><h4 id="factor-graph"><a href="#factor-graph" class="headerlink" title="factor graph"></a>factor graph</h4><h4 id="GBP"><a href="#GBP" class="headerlink" title="GBP"></a>GBP</h4><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;继第一篇&lt;a href=&quot;https://arxiv.org/abs/1803.11288&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FutureMapping&lt;/a&gt;之后，视觉SLAM领域内的奠基者Andrew Davison最近又将他的和别人讨论的有关未来空间AI对地图构建，机器人协同定位，以及动态问题等新想法撰写成了新的论文&lt;a href=&quot;https://arxiv.org/abs/1910.14139&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FutureMapping2&lt;/a&gt;，置顶在了自己的推特上，表示欢迎大家交流自己的想法。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/2019/11/17/FutureMapping2/Andrew-twitter.PNG&quot; alt=&quot;Andrew-twitter&quot;&gt;&lt;/p&gt;&lt;p&gt;总的来说，这篇文章干货还算是很多的，主要是着眼于factor graph（因子图）和Gaussian Belief Propagation（GBP，高斯置信传播）对整体，动态机器人建图等问题的潜力和前景，不仅在数学上进行一些tutorial，还给出了三个python demos。由于我对视觉SLAM只是了解整体特点，其中各种计算细节和优化方法并没有认真看过和代码书写过，因此读完这篇充满amazing reflections的文章之后我只能把握到整体的idea和一些实现方法。对于GBP和factor graph 结合构建的数学模型，我将在后续花时间弄懂后再对该blog进行补充。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;非常建议对SLAM或者机器视觉领域感兴趣的同行阅读此论文，相信会帮助您开阔思路！&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="SLAM" scheme="http://densecollections.top/tags/SLAM/"/>
    
      <category term="mapping" scheme="http://densecollections.top/tags/mapping/"/>
    
      <category term="AI system" scheme="http://densecollections.top/tags/AI-system/"/>
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="hardware" scheme="http://densecollections.top/tags/hardware/"/>
    
  </entry>
  
  <entry>
    <title>实习痰涂片项目总结</title>
    <link href="http://densecollections.top/2019/10/04/%E5%AE%9E%E4%B9%A0%E7%97%B0%E6%B6%82%E7%89%87%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/"/>
    <id>http://densecollections.top/2019/10/04/实习痰涂片项目总结/</id>
    <published>2019-10-04T07:47:17.000Z</published>
    <updated>2020-02-03T02:28:47.281Z</updated>
    
    <content type="html"><![CDATA[<h2 id="classification"><a href="#classification" class="headerlink" title="classification"></a>classification</h2><p>在上篇博客提到，该任务就是将原始数据的每张图片（256x256）进行grid级别的label预测，思路很简单，就是最后卷出的feature map是4x4的，不要过average global pooling layer，直接拉成1x16的向量过sigmoid激活函数即可（label也要变成16个字符，有点类似OCR)。</p><h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p>原始数据给的标注是json格式的框标注，但是框不是杆菌的具体位置，而是代表这个grid里面存在杆菌：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;frames&quot;:&#123;&quot;0_grid.png&quot;:[&#123;&quot;x1&quot;:162.42542787286064,&quot;y1&quot;:25.34963325183374,&quot;x2&quot;:170.24938875305622,&quot;y2&quot;:34.11246943765281,&quot;width&quot;:256,&quot;height&quot;:256,&quot;box&quot;:&#123;&quot;x1&quot;:162.42542787286064,&quot;y1&quot;:25.34963325183374,&quot;x2&quot;:170.24938875305622,&quot;y2&quot;:34.11246943765281&#125;,&quot;UID&quot;:&quot;fd548124&quot;,&quot;id&quot;:0,&quot;type&quot;:&quot;Rectangle&quot;,&quot;tags&quot;:[&quot;TB01&quot;],&quot;name&quot;:1&#125;,&#123;&quot;x1&quot;:214.3765281173594,&quot;y1&quot;:24.410757946210268,&quot;x2&quot;:224.07823960880197,&quot;y2&quot;:34.11246943765281,&quot;width&quot;:256,&quot;height&quot;:256,&quot;box&quot;:&#123;&quot;x1&quot;:214.3765281173594,&quot;y1&quot;:24.410757946210268,&quot;x2&quot;:224.07823960880197,&quot;y2&quot;:34.11246943765281&#125;,&quot;UID&quot;:&quot;5318dd81&quot;,&quot;id&quot;:1,&quot;type&quot;:&quot;Rectangle&quot;,&quot;tags&quot;:[&quot;TB01&quot;],&quot;name&quot;:2&#125;,...&#125;</span><br></pre></td></tr></table></figure><p>部分标注内容如上，主要包含了对应的文件夹下有哪些图片，图片上有无杆菌，杆菌的位置在哪个格子（要自己判断），以及一张图片有杆菌的话共有几个（”name”）。</p><a id="more"></a><p>首先找出哪些是positive的图片，并且根据坐标位置写出标签:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def find_write_positive_imgs(src_json_path, src_imgs_path, dst_csv_path, dst_imgs_path):</span><br><span class="line">    </span><br><span class="line">    data_csv = open(dst_csv_path, &apos;a+&apos;, newline=&apos;&apos;)</span><br><span class="line">    csv_writer = csv.writer(data_csv)</span><br><span class="line">    csv_writer.writerow([&quot;ImageName&quot;, &quot;LabelId&quot;])</span><br><span class="line">    </span><br><span class="line">    with open(src_json_path,&apos;r&apos;) as load_json:</span><br><span class="line">         load_dict = json.load(load_json)</span><br><span class="line"></span><br><span class="line">         img_names = load_dict[&apos;visitedFrames&apos;]</span><br><span class="line"></span><br><span class="line">         for img_name in img_names:</span><br><span class="line">         </span><br><span class="line">             #n_name represents the boxes quantities of the img &lt;&quot;name&quot; attribute in .json file&gt;</span><br><span class="line">             n_name=len(load_dict[&apos;frames&apos;][img_name])</span><br><span class="line">             </span><br><span class="line">             if n_name &gt; 0:</span><br><span class="line">                src_img_path = os.path.join(src_imgs_path, img_name)</span><br><span class="line">                img = cv2.imread(src_img_path)</span><br><span class="line">                H = img.shape[0]</span><br><span class="line">                W = img.shape[1]</span><br><span class="line">                dst_img_path = os.path.join(dst_imgs_path, img_name.replace(&apos;.png&apos;, &apos;_22.png&apos;))</span><br><span class="line">                cv2.imwrite(dst_img_path, img)</span><br><span class="line">                </span><br><span class="line">                labelid = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] </span><br><span class="line">                for i in range(0,n_name):</span><br><span class="line">                    x1 = load_dict[&apos;frames&apos;][img_name][i][&apos;x1&apos;]</span><br><span class="line">                    y1 = load_dict[&apos;frames&apos;][img_name][i][&apos;y1&apos;]</span><br><span class="line">                    area_h0 = 0</span><br><span class="line">                    area_w0 = 0</span><br><span class="line">                    for area_h1 in range(H//4, H+1, H//4):</span><br><span class="line">                        if y1 &gt; area_h0 and y1 &lt; area_h1:</span><br><span class="line">                           row_id = (area_h1 * 4 / H) - 1</span><br><span class="line">                           for area_w1 in range(W//4, W+1, W//4):</span><br><span class="line">                               if x1 &gt; area_w0 and x1 &lt; area_w1:</span><br><span class="line">                                  col_id = (area_w1 * 4 / W) - 1</span><br><span class="line">                                  id = int(col_id + 4 * row_id)</span><br><span class="line">                                  labelid[id] = 1</span><br><span class="line">                                  break</span><br><span class="line">                               else:</span><br><span class="line">                                    area_w0 = area_w1</span><br><span class="line">                           break</span><br><span class="line">                        else:</span><br><span class="line">                            area_h0 = area_h1</span><br><span class="line">                  </span><br><span class="line">                csv_writer.writerow([img_name.replace(&apos;.png&apos;, &apos;_22.png&apos;), </span><br><span class="line">                                     &apos;&apos;.join(str(k) for k in labelid)])</span><br></pre></td></tr></table></figure><p>此外，由于最后找出的positive图片很少（好像只有320张），我又对其进行了数据扩增，先是原始旋转一圈，然后right-left翻转后又旋转了一圈，因此总共扩增到了8倍大小。之后进行一下train-val-test set的划分，一般生成随机数就可以按自己的意愿划分，也有专门的库，具体划分代码就不上了。</p><p>另外数据增强方面也考虑过rgb转hsv或者ycrcb的，但是我试了一个样例之后效果不是很好，毕竟这样做的目的就是为了将主要的前景和特征显示出来，奈何我的数据太差了些，不好操作，于是作罢。</p><p>准备好数据之后，要对数据进行抽取，我用的是pytorch，直接继承Dataset类就好：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class SSDataset(Dataset):</span><br><span class="line">      </span><br><span class="line">      def __init__(self, imgs_path, csv_path, </span><br><span class="line">                    img_transform=None, loader=default_loader):</span><br><span class="line">          with open(csv_path, &apos;r&apos;) as f:</span><br><span class="line">               #这里一定要按字符串读取，否则前面的0会丢掉</span><br><span class="line">               #类似于OCR的labe读取</span><br><span class="line">               data_info = pd.read_csv(f, dtype=str) </span><br><span class="line">               #第一列是image name</span><br><span class="line">               self.img_list = list(data_info.iloc[:,0])</span><br><span class="line">               #第二类是labelid</span><br><span class="line">               self.label_list = list(data_info.iloc[:,1])</span><br><span class="line">          self.img_transform = img_transform</span><br><span class="line">          #loader用PIL.Image.open()</span><br><span class="line">          #不要用cv2.imread()</span><br><span class="line">          #pytorch默认PIL格式</span><br><span class="line">          self.loader = loader</span><br><span class="line">          self.imgs_path = imgs_path</span><br><span class="line">      </span><br><span class="line">      def __getitem__(self, index):</span><br><span class="line">          img_path = os.path.join(self.imgs_path, self.img_list[index])</span><br><span class="line">          img = self.loader(img_path)</span><br><span class="line">          label = self.label_list[index]</span><br><span class="line">          if self.img_transform is not None:</span><br><span class="line">             img = self.img_transform(img)</span><br><span class="line">          return img, label</span><br><span class="line"></span><br><span class="line">      def __len__(self):</span><br><span class="line">         return len(self.label_list)</span><br></pre></td></tr></table></figure><p>但是定义的labelid是str，还需要转成tensor去计算loss:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def labelid_switch(labels_str):</span><br><span class="line">    b_s = len(labels_str)</span><br><span class="line">    pad_label = []</span><br><span class="line">    for i in range(0, b_s):</span><br><span class="line">        temp_label = [0]* 16</span><br><span class="line">        temp_label[:16] = labels_str[i]</span><br><span class="line">        temp_label = list(map(int, temp_label))</span><br><span class="line">        pad_label.append(temp_label)</span><br><span class="line">    pad_label = torch.Tensor(pad_label)</span><br><span class="line">    labels_float = pad_label.view(b_s, 16)</span><br><span class="line">    return labels_float</span><br></pre></td></tr></table></figure><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><p>训练模型是主要用的是resnet和vgg，这部分代码可以直接参考torchvision，然后改改后面的layer就好了。</p><p>loss function上我试了binary cross entropy和focal loss（毕竟整体上positive grids还是少于negative grids的），此外我也试了下<a href="https://github.com/facebookresearch/mixup-cifar10" target="_blank" rel="noopener">mixup</a>，就是随机把batch里面的图片两两混合，计算loss的时候按照混合的比例分别计算相加，这也是一种应对过拟合，降低模型复杂度的办法（还有一种类似的方法叫sample pairing，只混合图片，不管label，我也试了，不过实际好像没mixup顶用）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class BFocalLoss(nn.Module):</span><br><span class="line"> </span><br><span class="line">    def __init__(self, gamma=1,alpha=0.8):</span><br><span class="line">        super(BFocalLoss, self).__init__()</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">    def forward(self, inputs, targets):</span><br><span class="line">        p = inputs</span><br><span class="line">        loss = -self.alpha*(1-p)**self.gamma*(targets*torch.log(p+1e-12))-\</span><br><span class="line">               (1-self.alpha)*p**self.gamma*((1-targets)*torch.log(1-p+1e-12))</span><br><span class="line">        loss = torch.sum(loss)</span><br><span class="line">        return loss</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def mixup_data(in_img, in_label, alpha=1.0):</span><br><span class="line">    #alpha in [0.1,0.4] in paper has better gain(for imagenet)</span><br><span class="line">    #for cifar-10 is 1.</span><br><span class="line">    if alpha &gt; 0:</span><br><span class="line">       lam = np.random.beta(alpha, alpha)</span><br><span class="line">    else:</span><br><span class="line">       lam = 1</span><br><span class="line">    </span><br><span class="line">    Batch_Size = in_img.size()[0]</span><br><span class="line">    Index = torch.randperm(Batch_Size)</span><br><span class="line">    mixed_x = lam * in_img + (1 - lam) * in_img[Index, :]</span><br><span class="line">    y_a, y_b = in_label, in_label[Index]</span><br><span class="line">    return mixed_x, y_a, y_b, lam</span><br><span class="line">    </span><br><span class="line">#计算loss  </span><br><span class="line">loss_mixup =  lam * criterion(pred, labels_a) + \</span><br><span class="line">                   (1 - lam) * criterion(pred, labels_b)</span><br></pre></td></tr></table></figure><p>接下来的事就是调参，对比实验，开tensorboard看loss的趋势了。（这里有一个现象，前期有一部分时间loss难以下降，总是在一个范围内波动，我猜想可能是因为数据扩增的原因。）</p><h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>这部分就是加载模型，一张张图片测试，然后写出预测的csv即可，然后给出grid acc</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">#部分代码如下：</span><br><span class="line">file_pre = open(PRE_TEST_CSV, &apos;w&apos;, newline=&apos;&apos;)</span><br><span class="line">pre_writer = csv.writer(file_pre)</span><br><span class="line">pre_writer.writerow([&quot;ImageName&quot;, &quot;LabelId&quot;])</span><br><span class="line"></span><br><span class="line">with open(SRC_TEST_CSV, &apos;r&apos;) as f_test:</span><br><span class="line">     test_data = pd.read_csv(f_test, dtype=str)</span><br><span class="line">     img_name = list(test_data.iloc[:,0])</span><br><span class="line">     labelid = list(test_data.iloc[:,1])</span><br><span class="line">     test_data_len = len(test_data.index)</span><br><span class="line"></span><br><span class="line">     num_right = 0 </span><br><span class="line">     positive_num = 0 </span><br><span class="line">     positive_num_right = 0</span><br><span class="line">     for i in range(0,test_data_len):</span><br><span class="line">         img_path = os.path.join(TEST_DATA_PATH, img_name[i])</span><br><span class="line">         img = Image.open(img_path)</span><br><span class="line">         img_tensor = transformations(img).float()</span><br><span class="line">         img_tensor = img_tensor.unsqueeze_(0)</span><br><span class="line">         </span><br><span class="line">         temp_label = [0]*16</span><br><span class="line">         temp_label[:16] = labelid[i]</span><br><span class="line">         temp_label = list(map(int, temp_label))</span><br><span class="line">         for temp in temp_label:</span><br><span class="line">             if temp &gt; 0:</span><br><span class="line">                positive_num += 1</span><br><span class="line">         label = torch.FloatTensor(temp_label)</span><br><span class="line">         label = label.view(1, 16)</span><br><span class="line">          </span><br><span class="line">         input = Variable(img_tensor)</span><br><span class="line">         input = input.to(device)</span><br><span class="line">         pred = net(input).data.cpu() #在CPU中比较</span><br><span class="line">         output = pred</span><br><span class="line">         pred_len = pred.size()[1]</span><br><span class="line">         out = []</span><br><span class="line">         for j in range(0, pred_len):</span><br><span class="line">             if pred[0][j] &lt; 0.5:</span><br><span class="line">                output[0][j] = 0</span><br><span class="line">                out.append(0)</span><br><span class="line">                if output[0][j] == label[0][j]:</span><br><span class="line">                   num_right += 1</span><br><span class="line">             else:</span><br><span class="line">                 output[0][j] = 1</span><br><span class="line">                 out.append(1)</span><br><span class="line">                 if output[0][j] == label[0][j]:</span><br><span class="line">                    num_right += 1</span><br><span class="line">                    positive_num_right += 1</span><br><span class="line">         pre_writer.writerow([img_name[i],&apos;&apos;.join(str(k) for k in out)]) </span><br><span class="line">    </span><br><span class="line">     print(&apos;test acc is: &apos;, num_right/(test_data_len*16))</span><br><span class="line">     print(&apos;postivite acc is: &apos;, positive_num_right, &apos;/&apos;, positive_num)</span><br></pre></td></tr></table></figure><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><p>实际上这个代码下来，调参还是挺费劲的，尤其是对我这种刚开始搞深度学习，经验还不够的新手来说，着实走了不少弯路。可是数据集实在太差，实在是想不出什么招。。所以硬撑了快两个月（实际上前大半个月我是直接分割grid成单独的图片，然后全部丢进去训练的。。这样搞不仅正负样本差距极大，而且切断了图片的连续性，效果奇差也在意料之中了，基本训不动，即使加了focal loss也没什么卵用）最后最高也才得到90%的acc。</p><h2 id="weakly-semantic-segmentation"><a href="#weakly-semantic-segmentation" class="headerlink" title="weakly semantic segmentation"></a>weakly semantic segmentation</h2><p>好歹8月下旬那会找到了一个公开的sputum smear的数据集，还带着框的标注：</p><ul><li>Makerere University, Uganda<ul><li><a href="http://air.ug/microscopy/" target="_blank" rel="noopener">homepage</a></li><li><a href="http://proceedings.mlr.press/v56/Quinn16.pdf" target="_blank" rel="noopener">paper</a></li><li><a href="https://github.com/jqug/microscopy-object-detection/blob/master/CNN%20training%20%26%20evaluation%20-%20tuberculosis.ipynb" target="_blank" rel="noopener">code</a></li></ul></li></ul><p>跟CTO交流后，他觉得这数据集质量不错，干脆就提议做弱监督分割，毕竟object detection现在都做烂了，而且开源这数据集的小哥自己也把object detection的acc刷的不错了，所以没必要再调包重复同样的事情了。我当时其实没啥思路，但是觉得应该挺有意思的，于是就接了下来。</p><p>后来通过调研发现，原来在自然图像上早就有人做了weakly segmentation(又是我恺明哥那些人…)，而且效果还不错，唯一可惜的就是完整的代码基本没人开源，不过后来参考GitHub上的一些相关代码也慢慢搭建出了整个框架。</p><p>整个项目思路主要参考的是这两篇论文：戴季峰的<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf" target="_blank" rel="noopener">BoxSup</a>和Max Planck Institute的<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Simple Does It</a>，主要的思路就是先设定几个从bounding box annotations生成segment proposals的方法（主要是opencv中GrabCut），然后利用此label去进行supervised training，最后过一下<a href="https://github.com/lucasb-eyer/pydensecrf" target="_blank" rel="noopener">denseCRF</a>优化一下，让boundary更加丝滑。当然也可以试试递归训练，让performance不错的model去预测生成新的training set中的label，然后进行下一轮的训练。</p><p>因为代码比较庞杂，分块不好展示，完整的代码就直接放在我的<a href="https://github.com/Richardyu114/weakly-segmentation-with-bounding-box" target="_blank" rel="noopener">github</a>上。</p><h3 id="pre-processing"><a href="#pre-processing" class="headerlink" title="pre-processing"></a>pre-processing</h3><p>原始的数据集中有1217张阳性图片，此外这些图片的标注还有47张莫名奇妙多了些20x20的框（可能是标的时候手抖了），因此要先一个个去掉。</p><p><img src="/2019/10/04/实习痰涂片项目总结/1.jpg" alt="看到了吗，左下角和右上角都多了个小框"></p><p>之后，对这些图片进行大致masks的生成，我这里给了三种方法：</p><ul><li><em>Box_segments</em>: 把整个box里面的像素都认为是杆菌（要把box的坐标都转成int，得对上像素）</li><li><em>Sbox_segments</em>:取box里面的80%的矩形框，认为该框里面的像素都是杆菌（同样，坐标都是int类型）</li><li><em>GrabCut_segments</em>: 利用经典的计算机视觉方法<a href="https://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf" target="_blank" rel="noopener">GrabCut</a>来得到杆菌的分割区域，但是该方法一般对图片的里面的单个的大物体比较友好，而杆菌又细又长，同时又包含着染色质，所以利用颜色分布的GrabCut分割出的杆菌要么会大点，要么就没有。大点的我不管，没有的我在这里就直接用<em>Box_segments</em>代替了。</li></ul><p>GrabCut部分代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">def grabcut(img_name):</span><br><span class="line">        masks = [] </span><br><span class="line">        # one image has many object that need to grabcut</span><br><span class="line">        for i, ann_info in enumerate(ANNS[img_name], start=1):</span><br><span class="line">               img = cv.imread((img_dir +img_name).rstrip()+&apos;.jpg&apos;)</span><br><span class="line">               grab_name = ann_info[1]</span><br><span class="line">               xmin = ann_info[3]</span><br><span class="line">               ymin = ann_info[2]</span><br><span class="line">               xmax = ann_info[5]</span><br><span class="line">               ymax = ann_info[4]</span><br><span class="line">               &quot;&quot;&quot;get int box coor&quot;&quot;&quot;</span><br><span class="line">               img_w = img.shape[1]</span><br><span class="line">               img_h = img.shape[0]</span><br><span class="line">               xmin, ymin, xmax, ymax = get_int_coor(xmin, ymin, xmax, ymax, img_w, img_h)           </span><br><span class="line">               box_w = xmax - xmin</span><br><span class="line">               box_h = ymax - ymin</span><br><span class="line">               # cv.grabcut&apos;s para</span><br><span class="line">               mask = np.zeros(img.shape[:2], np.uint8)</span><br><span class="line">               # rect is the tuple</span><br><span class="line">               rect = (xmin, ymin, box_w, box_h)</span><br><span class="line">               bgdModel = np.zeros((1, 65), np.float64)</span><br><span class="line">               fgdModel = np.zeros((1, 65), np.float64)</span><br><span class="line">               #for small bbox:</span><br><span class="line">               if box_w * box_h &lt; MINI_AREA:</span><br><span class="line">                   img_mask = mask[ymin:ymax, xmin:xmax] = 1</span><br><span class="line">                # for big box that area == img.area(one object bbox is just the whole image)</span><br><span class="line">               elif box_w * box_h == img.shape[1] * img.shape[0]:</span><br><span class="line">                      rect = [RECT_SHRINK, RECT_SHRINK, box_w - RECT_SHRINK * 2, box_h - RECT_SHRINK * 2]</span><br><span class="line">                      cv.grabCut(img, mask, rect, bgdModel,fgdModel, ITER_NUM, cv.GC_INIT_WITH_RECT)</span><br><span class="line">                      # astype(&apos;uint8&apos;) keep the image pixel in range[0,255]</span><br><span class="line">                      img_mask =  np.where((mask == 0) | (mask == 2), 0, 1).astype(&apos;uint8&apos;)</span><br><span class="line">                # for normal bbox:</span><br><span class="line">               else:</span><br><span class="line">                       cv.grabCut(img, mask, rect, bgdModel,fgdModel, ITER_NUM, cv.GC_INIT_WITH_RECT)</span><br><span class="line">                       img_mask = np.where((mask == 0) | (mask == 2), 0, 1).astype(&apos;uint8&apos;)</span><br><span class="line">                       # if the grabcut output is just background(it happens in my dataset)</span><br><span class="line">                       if np.sum(img_mask) == 0:</span><br><span class="line">                           img_mask = np.where((mask == 0), 0, 1).astype(&apos;uint8&apos;)</span><br><span class="line">                        # couting IOU</span><br><span class="line">                        # if the grabcut output too small region, it need reset to bbox mask</span><br><span class="line">                       box_mask = np.zeros((img.shape[0], img.shape[1]))</span><br><span class="line">                       box_mask[ymin:ymax, xmin:xmax] = 1</span><br><span class="line">                       sum_area = box_mask + img_mask</span><br><span class="line">                       intersection = np.where((sum_area==2), 1, 0).astype(&apos;uint8&apos;)</span><br><span class="line">                       union = np.where((sum_area==0), 0, 1).astype(&apos;uint8&apos;)</span><br><span class="line">                       IOU = np.sum(intersection) / np.sum(union)</span><br><span class="line">                       if IOU &lt;= IOU_THRESHOLD:</span><br><span class="line">                           img_mask = box_mask</span><br><span class="line">                # for draw mask on the image later           </span><br><span class="line">               img = cv.cvtColor(img, cv.COLOR_BGR2RGB) </span><br><span class="line">               masks.append([img_mask, grab_name, rect])</span><br><span class="line">        </span><br><span class="line">        num_object = i</span><br><span class="line">        &quot;&quot;&quot;for multi-objects intersection and fix the label &quot;&quot;&quot;</span><br><span class="line">        masks.sort(key=lambda mask: np.sum(mask[0]), reverse=True)</span><br><span class="line">        for j in range(num_object):</span><br><span class="line">              for k in range(j+1, num_object):</span><br><span class="line">                      masks[j][0] = masks[j][0] - masks[k][0]</span><br><span class="line">              masks[j][0] = np.where((masks[j][0]==1), 1, 0).astype(&apos;uint8&apos;)</span><br><span class="line">              &quot;&quot;&quot;get class name  id&quot;&quot;&quot;</span><br><span class="line">              grab_name = masks[j][1]</span><br><span class="line">              class_id = grab_name.split(&apos;_&apos;)[-1]</span><br><span class="line">              class_id = int(class_id.split(&apos;.&apos;)[0])</span><br><span class="line"></span><br><span class="line">              #set the numpy value to class_id</span><br><span class="line">              masks[j][0] = np.where((masks[j][0]==1), class_id, 0).astype(&apos;uint8&apos;)</span><br><span class="line">              # save grabcut_inst(one object in a image)</span><br><span class="line">              scipy.misc.toimage(masks[j][0], cmin=0, cmax=255, pal=tbvoc_info.colors_map,</span><br><span class="line">                                                      mode=&apos;P&apos; ).save((grabcut_dir).rstrip()+masks[j][1])</span><br><span class="line">        </span><br><span class="line">        &quot;&quot;&quot;merge masks&quot;&quot;&quot;</span><br><span class="line">        # built array(img.shape size)</span><br><span class="line">        mask_ = np.zeros(img.shape[:2])</span><br><span class="line">        for mask in masks:</span><br><span class="line">                mask_ = mask_ + mask[0]</span><br><span class="line">        # save segmetation_label(every object in a image)</span><br><span class="line">        scipy.misc.toimage(mask_, cmin=0, cmax=255, pal=tbvoc_info.colors_map,</span><br><span class="line">                                                mode=&apos;P&apos;).save((segmentation_label_dir+img_name).rstrip()+&apos;.png&apos;)</span><br></pre></td></tr></table></figure><p>这里面我是用scipy来保存masks的，我用的版本是0.19.0，超过这个版本的scipy就没有toimage()这个函数了，据说PIL有可以替代的函数，但是我看两个的功效好像不一样，就没去折腾了。</p><p><img src="/2019/10/04/实习痰涂片项目总结/2.jpg" alt="原图，带框标注"></p><p><img src="/2019/10/04/实习痰涂片项目总结/3.png" alt="GrabCut生成的segmentation label"></p><p>读取数据部分进行了resize处理，原图尺寸是1632x1224，1224不能被32整除，五次下采样和上采样的时候会出现feature map维度不匹配的错误，因此resize成了1632x1216。这里要注意，原图是利用双线性插值进行resize的，masks图是利用最近邻进行resize的（实际上我是生成好masks后训练时才意识到这个问题，实际上可以在最开始就把dataset的数据resize好，这样masks的误差可能就小点），PIL和cv2里面都有类似的函数。</p><p>数据读取部分代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class TBDataset(Dataset):</span><br><span class="line">      def __init__(self, txt_dir, width, height,  transform=None):</span><br><span class="line">          self.img_names = []</span><br><span class="line">          with open(txt_dir, &apos;r&apos;) as f_txt:</span><br><span class="line">               for img_name in f_txt:</span><br><span class="line">                   self.img_names.append(img_name)</span><br><span class="line">          </span><br><span class="line">          self.transform = transform</span><br><span class="line">          self.txt_dir = txt_dir</span><br><span class="line">          self.width = width</span><br><span class="line">          self.height = height</span><br><span class="line">                   </span><br><span class="line">      def __getitem__(self, index):</span><br><span class="line">          img_name = self.img_names[index]</span><br><span class="line">          img = Image.open(os.path.join(img_dir, img_name).rstrip()+&apos;.jpg&apos;)</span><br><span class="line">          # the resize function like bilinear</span><br><span class="line">          img = img.resize((self.width, self.height), Image.LANCZOS)</span><br><span class="line">          img = np.array(img)</span><br><span class="line">          label = Image.open(os.path.join(label_dir, img_name).rstrip()+&apos;.png&apos;)</span><br><span class="line">          # for consider class_id is not consecutive and just fixed by user</span><br><span class="line">          label = label.resize((self.width, self.height), Image.NEAREST)</span><br><span class="line">          label = np.array(label)</span><br><span class="line">          if self.transform is not None:</span><br><span class="line">             img = self.transform(img)</span><br><span class="line">          #img = torch.FloatTensor(img)</span><br><span class="line">          label = torch.FloatTensor(label)</span><br><span class="line">          return img, label</span><br><span class="line">                             </span><br><span class="line">      def __len__(self):</span><br><span class="line">          return len(self.img_names)</span><br></pre></td></tr></table></figure><h3 id="train-1"><a href="#train-1" class="headerlink" title="train"></a>train</h3><p>训练部分模型用的是FCN和UNet，因为考虑到只有二分类，后面也可以考虑deeplab，UNet++等等。FCN用的是VGG-16 backbone，下采样5次，UNet下采样4次，都是按照论文来的，没做什么改动。模型最后输出的是一个1632x1216的feature map，然后直接过sigmoid激活函数，再和1632x1216的mask图片（读进来的是一个二维0-1矩阵，代表每个像素点的label）进行loss计算，然后BP，更新参数学习。loss也用了交叉熵和focal loss.</p><h3 id="post-processing"><a href="#post-processing" class="headerlink" title="post-processing"></a>post-processing</h3><p>对模型预测出的结果再过一遍denseCRF，优化分割的同时也会去掉一些false-positive</p><p>部分代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">def run_densecrf(img_dir, img_name, masks_pro):</span><br><span class="line">        height = masks_pro.shape[0]</span><br><span class="line">        width = masks_pro.shape[1]</span><br><span class="line"></span><br><span class="line">        # must use cv2.imread()</span><br><span class="line">        # if use PIL.Image.open(), the algorithm will break</span><br><span class="line">        #TODO --need to fix the image problem</span><br><span class="line">        img = cv.imread(os.path.join(img_dir, img_name).rstrip()+&apos;.jpg&apos;)</span><br><span class="line">        img = cv.resize(img, (1632,1216), interpolation = cv.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line">        # expand to [1,H,W]</span><br><span class="line">        masks_pro = np.expand_dims(masks_pro, 0)</span><br><span class="line">        # masks_pro = masks_pro[:, :, np.newaxis]</span><br><span class="line">        # append to array---shape(2,H,W)</span><br><span class="line">        # one depth represents the class 0, the other represents the class 1</span><br><span class="line">        masks_pro = np.append(1-masks_pro, masks_pro, axis=0)</span><br><span class="line">        #[Classes, H, W]</span><br><span class="line">        # U needs to be flat</span><br><span class="line">        U = masks_pro.reshape(2, -1)</span><br><span class="line">        # deepcopy and the order is C-order(from rows to colums)</span><br><span class="line">        U = U.copy(order=&apos;C&apos;)</span><br><span class="line">        # for binary classification, the value after sigmoid may be very small</span><br><span class="line">        U = np.where((U &lt; 1e-12), 1e-12, U)</span><br><span class="line">        d = dcrf.DenseCRF2D(width, height, 2)</span><br><span class="line"></span><br><span class="line">        # make sure the array be c-order which will faster the processing speed</span><br><span class="line">        # reference: https://zhuanlan.zhihu.com/p/59767914</span><br><span class="line">        U = np.ascontiguousarray(U)</span><br><span class="line">        img = np.ascontiguousarray(img)</span><br><span class="line"></span><br><span class="line">        d.setUnaryEnergy(-np.log(U))</span><br><span class="line">        d.addPairwiseGaussian(sxy=3, compat=3)</span><br><span class="line">        d.addPairwiseBilateral(sxy=80, srgb=13, rgbim=img, compat=10)</span><br><span class="line">        Q = d.inference(5)</span><br><span class="line">        # compare each value between two rows by colum</span><br><span class="line">        # and inference each pixel belongs to which class(0 or 1)</span><br><span class="line">        map = np.argmax(Q, axis=0).reshape((height, width))</span><br><span class="line">        proba = np.array(map)</span><br><span class="line"></span><br><span class="line">        return proba</span><br></pre></td></tr></table></figure><p>这里主要用到了二元势pairwise potential，比较每个像素和其他像素的关系，具体原理可以去看看原代码和论文。</p><p>此外，我还顺手进行了下迭代训练。实际上，对于我这个数据集，基本上用GrabCut生成label训练一遍效果就不错了，不过为了看下更新label再训练一轮会不会得到更好的结果，在固定的epoch结束后将训练好得模型设为eval模式，然后预测train set的数据，然后再返回train模式继续训练。需要注意的是，更新label的时候，可能会有漏诊和误诊，我就直接将预测的mask和<em>Box_segments</em>得到的mask相加，只取为2的部分，这样就去掉了假阳性，然后漏诊的部分再用box补回来。</p><p>从实验结果来看，一般我这个是更新3次label（每10个epoch更新一次）就差不多了，再多也没什么提升。总体上来说，这个操作可以提高单张图片同时存在多个杆菌的分割效果，但是提升力度也没什么太令人满意的地方。可能是我的更新姿势不对？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">def update_label(predict_model, device):</span><br><span class="line">       </span><br><span class="line">       &quot;&quot;&quot;load train_pairs.txt info for check the missed diagnosis objects&quot;&quot;&quot;</span><br><span class="line">       #ann_info:[image name, image name_num_ class_id.png, bbox_ymin,</span><br><span class="line">       #                    bbox_xmin,bbox_ymax, bbox_xmax, class_name]</span><br><span class="line">       print(&apos;start to update...&apos;)</span><br><span class="line">       ANNS = &#123;&#125;</span><br><span class="line">       with open(dataset_pairs_dir, &apos;r&apos;) as da_p_txt:</span><br><span class="line">                 for ann_info in da_p_txt:</span><br><span class="line">                        # split the string line, get the list</span><br><span class="line">                        ann_info = ann_info.rstrip().split(&apos;###&apos;)</span><br><span class="line">                        if ann_info[0].rstrip()  not in ANNS:</span><br><span class="line">                            ANNS[ann_info[0].rstrip()] = []</span><br><span class="line">                        ANNS[ann_info[0].rstrip()].append(ann_info)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       predict_model.eval()</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">       # define the same image transformations</span><br><span class="line">       transformations = transforms.Compose([</span><br><span class="line">                                             transforms.ToTensor(),</span><br><span class="line">                                             transforms.Normalize(mean=[0.485, 0.456, 0.406], </span><br><span class="line">                                             std=[0.229, 0.224, 0.225])</span><br><span class="line">                                             ])</span><br><span class="line"></span><br><span class="line">       update_num = 0</span><br><span class="line">       print(&apos;updating progress:&apos;)</span><br><span class="line">       with open(dataset_txt_dir, &apos;r&apos;) as da_txt:</span><br><span class="line">                 # don&apos;t use the code line below</span><br><span class="line">                 # or it will close the file and the whole programm end here (I guess)</span><br><span class="line">                 # I debug here for two hours......</span><br><span class="line">                 #lines = len(da_txt.readlines())</span><br><span class="line">                 for update_name in da_txt:</span><br><span class="line">                         update_num += 1</span><br><span class="line">                         # in RGB [W, H, depth]</span><br><span class="line">                         img = Image.open(os.path.join(img_dir, update_name).rstrip()+&apos;.jpg&apos;)</span><br><span class="line">                         img_w = img.size[0]</span><br><span class="line">                         img_h = img.size[1]</span><br><span class="line">                         img = img.resize((1632, 1216), Image.LANCZOS)</span><br><span class="line">                         input_ = transformations(img).float()</span><br><span class="line">                         # add batch_size dimension</span><br><span class="line">                         #[3, H, W]--&gt;[1, 3, H, W]</span><br><span class="line">                         input_ = input_.unsqueeze_(0)</span><br><span class="line">                         input_ = input_.to(device)</span><br><span class="line">                         pred = predict_model(input_).view([1216, 1632]).data.cpu()</span><br><span class="line">                         #pred.shape[H,W]</span><br><span class="line">                         pred = np.array(pred)</span><br><span class="line">                         &quot;&quot;&quot;crf smooth prediction&quot;&quot;&quot;</span><br><span class="line">                         crf_pred = run_densecrf(img_dir, update_name,  pred)</span><br><span class="line"></span><br><span class="line">                         &quot;&quot;&quot;start to update&quot;&quot;&quot;</span><br><span class="line">                         last_label = Image.open(os.path.join(label_dir, update_name).rstrip()+&apos;.png&apos;)</span><br><span class="line">                         last_label = last_label.resize((1632, 1216), Image.NEAREST)</span><br><span class="line">                         last_label = np.array(last_label)</span><br><span class="line"></span><br><span class="line">                         # predicted label without false-positive segments</span><br><span class="line">                         updated_label = crf_pred + last_label</span><br><span class="line">                         updated_label = np.where((updated_label==2), 1, 0).astype(&apos;uint8&apos;)</span><br><span class="line">                         # predicted label with missed diagnosis </span><br><span class="line">                         # we just use the box segments as missed diagnosis for now</span><br><span class="line">                         info4check = ANNS[update_name.rstrip()]</span><br><span class="line">                         masks_missed = np.zeros((1216, 1632), np.uint8)</span><br><span class="line">                         for box4check in info4check:</span><br><span class="line">                                xmin = box4check[3]</span><br><span class="line">                                ymin = box4check[2]</span><br><span class="line">                                xmax = box4check[5]</span><br><span class="line">                                ymax = box4check[4]</span><br><span class="line">                                xmin, ymin, xmax, ymax = get_int_coor(xmin, ymin, </span><br><span class="line">                                                                       xmax, ymax, img_w, img_h)</span><br><span class="line">                                xmin = int(xmin * 1632 / img_w)</span><br><span class="line">                                xmax = int(xmax * 1632 / img_w)</span><br><span class="line">                                ymin = int(ymin * 1216 / img_h)</span><br><span class="line">                                ymax = int(ymax * 1216 / img_h)</span><br><span class="line">                                if np.sum(updated_label[ymin:ymax, xmin:xmax]) == 0:</span><br><span class="line">                                    masks_missed[ymin:ymax, xmin:xmax] = 1</span><br><span class="line"></span><br><span class="line">                         updated_label = updated_label + masks_missed</span><br><span class="line">                         scipy.misc.toimage(updated_label, cmin=0, cmax=255, pal=colors_map, </span><br><span class="line">                                                            mode=&apos;P&apos;).save(os.path.join(label_dir, </span><br><span class="line">                                                                           update_name).rstrip()+ &apos;.png&apos;)</span><br><span class="line">                         print(&apos;&#123;&#125; / &#123;&#125;&apos;.format(update_num, len(ANNS)), end=&apos;\r&apos;)</span><br></pre></td></tr></table></figure><h3 id="metric"><a href="#metric" class="headerlink" title="metric"></a>metric</h3><p>一般的segmentation论文都是用IoU来进行比较的，但是这个数据集没有segmentation groundtruth，所以我就自己定义了个检测的acc：预测的mask和框有交叉(np.sum(region of box)!=0)，就认为检测出了一个，然后算average acc，通过这个指标和test set上的预测结果来大致衡量哪些方法组合在一起不错。最后总结下来，还是GrabCut+FCN+FL($\alpha=0.75,\gamma=1$)更好些，不过我没加大UNet的深度和通道数，否则的话我猜想可能UNet会占上风。</p><p>篇幅有限，放几个还不错的预测结果：</p><p><img src="/2019/10/04/实习痰涂片项目总结/4.png" alt="GrabCut+FCN+FL"></p><p><img src="/2019/10/04/实习痰涂片项目总结/5.png" alt="GrabCut+FCN+FL"></p><p><img src="/2019/10/04/实习痰涂片项目总结/6.png" alt="GrabCut+UNet+FL，UNet的结果似乎要圆润一些"></p><p><img src="/2019/10/04/实习痰涂片项目总结/7.png" alt="GrabCut+FCN+FL更新3次label，效果。。也就马马虎虎吧"></p><h3 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h3><p>总的来说，最后的弱监督分割还是收获挺多的，尤其是自己的工程能力得到了锻炼，代码组织和书写也得到了一定地提升，最后相关成果也写成论文投了ISBI会议，如果能中的话，还是很舒服的^-^</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;classification&quot;&gt;&lt;a href=&quot;#classification&quot; class=&quot;headerlink&quot; title=&quot;classification&quot;&gt;&lt;/a&gt;classification&lt;/h2&gt;&lt;p&gt;在上篇博客提到，该任务就是将原始数据的每张图片（256x256）进行grid级别的label预测，思路很简单，就是最后卷出的feature map是4x4的，不要过average global pooling layer，直接拉成1x16的向量过sigmoid激活函数即可（label也要变成16个字符，有点类似OCR)。&lt;/p&gt;&lt;h3 id=&quot;dataset&quot;&gt;&lt;a href=&quot;#dataset&quot; class=&quot;headerlink&quot; title=&quot;dataset&quot;&gt;&lt;/a&gt;dataset&lt;/h3&gt;&lt;p&gt;原始数据给的标注是json格式的框标注，但是框不是杆菌的具体位置，而是代表这个grid里面存在杆菌：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;frames&amp;quot;:&amp;#123;&amp;quot;0_grid.png&amp;quot;:[&amp;#123;&amp;quot;x1&amp;quot;:162.42542787286064,&amp;quot;y1&amp;quot;:25.34963325183374,&amp;quot;x2&amp;quot;:170.24938875305622,&amp;quot;y2&amp;quot;:34.11246943765281,&amp;quot;width&amp;quot;:256,&amp;quot;height&amp;quot;:256,&amp;quot;box&amp;quot;:&amp;#123;&amp;quot;x1&amp;quot;:162.42542787286064,&amp;quot;y1&amp;quot;:25.34963325183374,&amp;quot;x2&amp;quot;:170.24938875305622,&amp;quot;y2&amp;quot;:34.11246943765281&amp;#125;,&amp;quot;UID&amp;quot;:&amp;quot;fd548124&amp;quot;,&amp;quot;id&amp;quot;:0,&amp;quot;type&amp;quot;:&amp;quot;Rectangle&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;TB01&amp;quot;],&amp;quot;name&amp;quot;:1&amp;#125;,&amp;#123;&amp;quot;x1&amp;quot;:214.3765281173594,&amp;quot;y1&amp;quot;:24.410757946210268,&amp;quot;x2&amp;quot;:224.07823960880197,&amp;quot;y2&amp;quot;:34.11246943765281,&amp;quot;width&amp;quot;:256,&amp;quot;height&amp;quot;:256,&amp;quot;box&amp;quot;:&amp;#123;&amp;quot;x1&amp;quot;:214.3765281173594,&amp;quot;y1&amp;quot;:24.410757946210268,&amp;quot;x2&amp;quot;:224.07823960880197,&amp;quot;y2&amp;quot;:34.11246943765281&amp;#125;,&amp;quot;UID&amp;quot;:&amp;quot;5318dd81&amp;quot;,&amp;quot;id&amp;quot;:1,&amp;quot;type&amp;quot;:&amp;quot;Rectangle&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;TB01&amp;quot;],&amp;quot;name&amp;quot;:2&amp;#125;,...&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;部分标注内容如上，主要包含了对应的文件夹下有哪些图片，图片上有无杆菌，杆菌的位置在哪个格子（要自己判断），以及一张图片有杆菌的话共有几个（”name”）。&lt;/p&gt;
    
    </summary>
    
      <category term="工作总结" scheme="http://densecollections.top/categories/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
      <category term="CNN" scheme="http://densecollections.top/tags/CNN/"/>
    
      <category term="medical image analysis" scheme="http://densecollections.top/tags/medical-image-analysis/"/>
    
  </entry>
  
  <entry>
    <title>实习见闻及其他</title>
    <link href="http://densecollections.top/2019/10/03/%E5%AE%9E%E4%B9%A0%E8%A7%81%E9%97%BB%E5%8F%8A%E5%85%B6%E4%BB%96/"/>
    <id>http://densecollections.top/2019/10/03/实习见闻及其他/</id>
    <published>2019-10-03T07:13:24.000Z</published>
    <updated>2020-02-03T02:28:47.258Z</updated>
    
    <content type="html"><![CDATA[<p>早有耳闻近年的机器学习和计算机视觉岗位不好找，很多研究生都会借着自己城市和学校的优势，去争取一些公司的实习。我是兜兜转转好几年后才开始正式去学机器学习，也算是半路出家，而且有种49年入国军的赶脚。平日里看着教研室的师兄们即使做着非机器学习算法也在求职的道路上一波三折，心里不免对自己的前途充满着担忧和焦虑。好在导师关系网络庞大，在今年5月丢给了我一个去AI医疗影像公司实习的机会。能够得到锻炼，同时也可以见识见识工业界的研究节奏和方式，这样的运气，我自然不会让其白白溜走。这家公司南京分部的CTO是一位加州海归博士，热爱技术又充满激情，在勉强通过他的面试之后，我得到了三个月的实习机会。</p><a id="more"></a><p>公司的效率和推进速度还是非常快的，刚开始确实不适应，毕竟放羊久了，长时间地坐在电脑前看论文，垒代码，调参数还是有点乏人的。不过我是分配了一个单独的项目，所以三个月来就相当于单干，虽然没有外部的压力，但是没能参与核心项目，与公司大佬们一起讨论，还是心存遗憾（不过自己水平确实不高，也很正常，咱心里还得有点数才行…）。实习的任务主要是对他们提供的痰涂片（sputum smear）数据集进行分类，但是不是一般意义上的单张图片分类。这里我先简单科普下，痰涂片是为了辅助诊断结核病而采集的样本，医生根据采集的病人的痰液里面的结核杆菌（tuberculosis bacillus）数量来判断阳性或阴性，这种手段主要在发展中国家，贫困地区等地方常见（有条件的直接照X光了）。原始数据集的每张图片都是从显微镜采的，而且分成了4x4的grid，我要做的就是分类图片每个grid的label，有点类似one-stage的目标检测。</p><p><img src="/2019/10/03/实习见闻及其他/1.png" alt="痰涂片数据示例"></p><p>然而，数据又脏又少。。。。折腾了两个月实在没弄出满意的结果（好像最后最好才到0.9的acc）。没办法，最后Google找了好几天偶然发现了一个公开的数据集，还带着bounding box的标注！！！于是后面和CTO商讨，干脆上了weakly semantic segmentation（最后10几天的工作量比前两个月的还要多的多。。果然deadline是第一生产力）。好歹最后结果还可以。详细的讲解可以看我下一篇博客。</p><p><img src="/2019/10/03/实习见闻及其他/2.png" alt="weakly segmentation example"></p><p>下面我想重点谈谈自己的收获与感想：</p><ul><li><strong>代码和算法</strong>。只有真正经历了才明白代码能力是多么的重要。从面试一直到解决实际工程问题，或者是自己平时研究，搭网站，作图，实现自己的一个想法…等等等等，都可以发挥巨大的的作用。虽然说做机器学习数学相对重要点，可是进入职场以后，算法和代码能力才是硬实力。对于我这种菜比来说，多练，多参考，多总结是唯一有效的办法</li><li><strong>工业界和学术界</strong>。工业界基本上都是以项目和数据驱动的（可能国内大的学教研室跟其也没什么不同。。），基本上不关心如何创造新方法，新理论，只关心产品如何更好地生成和落地。每天的迭代节奏都很快，各个领域的知识都会涉及（初创公司可能更多）。一般的项目组都是利用现成的SOTA去修改然后适配自己的数据，所以就要求职员得预先懂得很多东西，或者知道哪些方法可以用上，要是从头开始调研的话就太慢了。此外，工业界还有着模块化的管理，比如利用trello这样的软件和早会来安排具体每个人一段时间的任务，交流进展情况，及时解决blocked问题等等。对于涉及到算法的公司，代码的管理，维护，更新是个big problem，从代码托管平台或者服务器的选择，coding的规则制定，review code的指定和审阅标准，要审几轮之后才能merge等都是一些比较琐碎却很实际的问题，是需要慢慢完善的过程。现在想想，那么多学术大牛后面选择投身工业界不是没有道理的，对他们来说研究可以照常进行，资源也更加丰富，而且更有那种creating的青春感，反而学术界里面无拘无束的环境，可能会慢慢消磨他们的激情。处于象牙塔里是很可能被温水煮青蛙的。</li><li><strong>保持学习和交流的习惯</strong>。即使是工作以后，也要保持一种好奇心，要促使自己不断地去进步。学习不仅是让你更加游刃有余于自己的工作，也是为了打开自己的视野，丰富自己的生活。以技术学习为例，公司的研究组一般都会有paper reading这样的活动，每周读一篇典型的论文，轮流安排组员上去讲解，一方面打开工作思路，一方面也是锻炼人的说话和展示能力（费曼学习法）。不要对别人的工作满不在乎，漠不关心，经常主动和别人交流，既交了朋友，也可以扩展自己的领域认知。</li><li><strong>把握工作节奏，保持沟通</strong>。如今996大行其道，加上媒体大肆宣扬，搞得程序猿们人心惶惶。实际上如果是一个项目要到期了，某段时间可能要加点班一起赶出来的话，是可以接受的，但是如果每天强制加班，搞得连自己生活都没了，那我肯定是果断放弃了，除非给我年薪百万。。。因此，在找工作前，首先得平衡好自己的要求，认清一些现状，身体健康和生活质量是放在第一位的，其他的都是为了这两者服务的。有困难，有难题一定要及时反映或者在其恰当的时机吐露出来，不要动不动就轻视自己的生命（可能还没到这种地步），当然这是最差的情况。平时和同级沟通，和上级沟通（反映意见，提出工作思路，建议等）都是必不可少的职场状态。</li></ul><p>总的来说，第一次实习还是挺圆满的，虽然没学到什么大本事，但是起码这段经历让我收获了很多，也更加坚定了自己的选择，对未来的路也看得明晰了些。不得不说，有些事情还是要自己经历过后才能懂得，这些没法感同身受的经验，是怎么也无法理解的。趁着年轻，还是应该多把自己推出舒适区，多去争取一些历练和出走的机会，以此开阔自己的眼界，认识社会和世界。</p><hr><p>好像写到现在也没怎么提及到实习公司的情况。。。想着最后了，还是祝贺下组里的大佬们帮公司拿到了吴恩达<a href="https://stanfordmlgroup.github.io/competitions/chexpert/" target="_blank" rel="noopener">chestXpert 数据集比赛</a>的冠军，而且霸榜了几个月！着实佩服。我也从他们身上学到了不少新知识和新姿势。</p><p>自己厚着脸皮也和他们一起公费嗨皮庆祝了下。</p><p><img src="/2019/10/03/实习见闻及其他/3.jpg" alt="去紫峰大厦吃自助"></p><p><img src="/2019/10/03/实习见闻及其他/4.jpg" alt="大龙虾，可是我吃不出啥道道来。很僵硬"></p><p><img src="/2019/10/03/实习见闻及其他/5.jpg" alt="从紫峰大厦观景台俯瞰玄武湖"></p><p><img src="/2019/10/03/实习见闻及其他/6.jpg" alt="夜景"></p><p>好了，今年的实习告一段落了，明年希望能自己争取到大厂的实习。(‘ o_o ‘)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;早有耳闻近年的机器学习和计算机视觉岗位不好找，很多研究生都会借着自己城市和学校的优势，去争取一些公司的实习。我是兜兜转转好几年后才开始正式去学机器学习，也算是半路出家，而且有种49年入国军的赶脚。平日里看着教研室的师兄们即使做着非机器学习算法也在求职的道路上一波三折，心里不免对自己的前途充满着担忧和焦虑。好在导师关系网络庞大，在今年5月丢给了我一个去AI医疗影像公司实习的机会。能够得到锻炼，同时也可以见识见识工业界的研究节奏和方式，这样的运气，我自然不会让其白白溜走。这家公司南京分部的CTO是一位加州海归博士，热爱技术又充满激情，在勉强通过他的面试之后，我得到了三个月的实习机会。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔杂谈" scheme="http://densecollections.top/categories/%E9%9A%8F%E7%AC%94%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="reflections" scheme="http://densecollections.top/tags/reflections/"/>
    
      <category term="job" scheme="http://densecollections.top/tags/job/"/>
    
      <category term="coding" scheme="http://densecollections.top/tags/coding/"/>
    
  </entry>
  
  <entry>
    <title>往者不谏，来者可追</title>
    <link href="http://densecollections.top/2019/05/11/%E5%BE%80%E8%80%85%E4%B8%8D%E8%B0%8F%EF%BC%8C%E6%9D%A5%E8%80%85%E5%8F%AF%E8%BF%BD/"/>
    <id>http://densecollections.top/2019/05/11/往者不谏，来者可追/</id>
    <published>2019-05-11T07:07:13.000Z</published>
    <updated>2020-02-03T02:28:47.263Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>要想得到某样东西，最可靠的办法先让自己配得上它</p></blockquote><p><img src="/2019/05/11/往者不谏，来者可追/4.jpg" class="full-image"></p><p>前些日子无意间看到了一位A. J. Davison教授的新博士生在知乎上写的一篇<a href="https://www.zhihu.com/question/298181420/answer/619558578" target="_blank" rel="noopener">回答</a>，他在里面谈到了自己科研之路曲折受挫的经历，尤其是屡屡面对着与自己期望不符合的情况，让我感触很深，想起了自己去年准备国外博士申请失败的那段苦痛记忆。可能对于我来讲，自己对读博的认识和准备都还欠缺得很，既没有拿得出手的本领，技能和honor，也没有对未来研究方向的清楚认识，与另一位机器人大牛<a href="https://zhuanlan.zhihu.com/p/41313724" target="_blank" rel="noopener">YY硕</a>读博的初衷和认知相比，实在是汗颜。</p><blockquote><p>所谓人的成长，其实是“不断发现个人独特的经历原来都只是人类普遍经验的一部分”的过程</p><p>​ —- 多丽丝 莱辛</p></blockquote><a id="more"></a><p><img src="/2019/05/11/往者不谏，来者可追/3.jpg" class="full-image"></p><p>自己步入大学也快接近五年了，在这五年来的科研学习生涯上，没有出现一件让我满意的工作，甚至可以说连接触“像样的工作”的机会都没有。回想本科的生活，似乎陷入了一种想要寻求突破发展，却又每每哀叹碰壁于现实，最终困于浑浑噩噩的循环中。虽然高考成绩把我带到了这个所谓的“培优学院”中来，但是实际上鸡肋的培养模式和自己能力不足以及信息缺失等问题，让自己走了很多弯路。从大二上学期进入学院导师的实验室开始，就开始了漫长的专业方向调研和探索学习之旅：先是MEMS器件，再是小型固定翼无人机飞行控制系统，然后转到了飞控数据采集和处理硬件平台，后来又顺着跳到了四旋翼避障，然而结果最终都因为各种原因和变故而终止。现在磕磕碰碰转到了SLAM，从此开始接触计算机视觉，又加上人工智能大热，算是勉强把自己的研究定位在visual SLAM, Robotics和Machine Learning上，即使现在没有任何老师和同专业的师兄师姐指导，但是我心底里知道，这是自己好不容易觉得自己感兴趣的可以坚持做下去的方向，心里也十分珍惜，不愿轻易放弃，纵使单枪匹马很难在这个领域做出东西来。</p><p>心里开始真正坚定了出国留学的想法是在保研后期的时候，那时通过自己的偶然探寻和师兄的推荐，先后联系了两位国外的教授，虽然他们对我的态度都很好，也愿意接收我，但是最终都因为CSC奖学金的问题而以失败告终（甚至连尝试申请CSC的机会都没有）。在这期间，我曾自信满满地以为自己一定能与过去失败的生活说再见，可以在新的教研室，友好的导师的帮助和指导下开启自己目前最感兴趣的课题，然后三四年读完博士，之后再去争取更好的平台继续研究。现在看来，我想得太过于理想化，现实是残酷的，会时不时让人痛苦，“而这种痛苦往往来自于人对于自己无能的愤怒”。在得知CSC奖学金不再对我申请的学校提供名额的那一刻起，就基本上宣判我大半年的准备都是白费的，毕竟自己和意向导师的实验室都缺钱，所以CSC几乎是唯一的funding。当时的我陷入了极大的消极之中，就像看到了希望，看到了黑暗生活中的一丝光亮，想要伸手去触碰，去争取，却又被黑暗中的种种阻碍和枷锁困住，动弹不得。</p><p>后来我安慰自己，人是无法预测自己的命运的，我们可以做的就是不断地让自己变得更好，to be kind。其实我后来想想，这似乎也是天意，让我好好地去审视自己，看清楚自己身上到底出了什么问题，而不是就这么简单地给予我幸运的关照，让我顺顺利利地就能跑到国外读博，逃避现实。因为不论是从科研能力上，心理素质上，理性认知上，以及生活技能上我都还欠火候，还需要不断地修炼，毕竟选择科研这条路不是一句简单的承诺，不仅仅是付出就可以安然走下去的。</p><p>让我欣慰的是，自己地沉沦与消极并没有持续很久，几天之后就慢慢放下了，可能自己心底里也并不是对那些学校和生活特别期待和憧憬，可能自己心底里也还是有个标准的。我一边开始继续自己的研究生工作学习和课题探寻，一边反思自己的生活和精神状态：那些让自己觉得失败的经历到底是如何发生的，为什么我早就认识到了却没有去很好地补救？</p><p>我想，一方面是信息的缺失，身边没有一些志同道合的大佬级别的人之外，另一方面是自己持续不断的，反反复复的自怨自艾，想得太多而又做的太少，眼高手低。前者是没办法选择的，毕竟环境和出生不同，这是客观的因素，我能做的就是学会善用信息时代的网络技术，持续地阅读优质的内容，挖掘有效的有价值的信息，并且主动地结交那些志同道合的前辈；后者是主观因素，是内因，才是自己最应该去改变的地方。因为只有不断地去做，去尝试，机会和惊喜才会眷顾你。努力不一定有收获，但是起码会给你一些选择和机会，同时让人在这个过程中成长。</p><blockquote><p>Work cures everything.</p><p>​ ——马蒂斯</p></blockquote><p>因此，我开始承认自己菜，而且承认自己是真的菜，那些优秀的人不仅在学习和科研上有很高的建树，在其他方面，诸如绘画，音乐，运动，哲学，文学等都有很高的领悟和施展技巧。我开始慢慢认识到自己的不足和平凡，开始努力地在各个方面去提升自己，当然也是自己一贯就感兴趣的地方，同时也想打开自己的眼界，愿意并持续地去尝试新事物，涉猎不同的领域，发展多种爱好。一言以蔽之，就是活在当下，持续工作，持续输入输出，以此提升自己，慢慢地体验生活，了解生活，享受生活。毕竟这才是人活于世的一项基本目的。</p><p>写到这，我想起了英国演员本尼迪克特在节目letters live上<a href="https://www.bilibili.com/video/av6465704/?spm_id_from=333.788.videocard.2" target="_blank" rel="noopener">朗读的一封书信</a>，这是美国先驱艺术家<a href="https://zh.wikipedia.org/wiki/索爾·勒維特" target="_blank" rel="noopener">索尔·勒维特</a>写给他的好友伊娃·黑塞一封信，背景是1965 年，黑塞经历了一段自我怀疑的时期，她的创作遇到瓶颈，她迷茫、沮丧，不知道该怎么办，不知道未来在哪里，她向勒维特倾诉自己遇到的“心灵困境”。几周后，勒维特用以下这件作品回复了她：一封精妙、宝贵的建议信，让她不要再彷徨，而是”stop it and just Do “.</p><p><img src="/2019/05/11/往者不谏，来者可追/sol.jpg" alt="Sol LeWitt"></p><p>信的内容如下：</p><blockquote><p>Dear Eva,</p><p>It will be almost a month since you wrote to me and you have possibly forgotten your state of mind(I doubt it though).</p><p>You seem the same as always, and being you, hate every minute of it.</p><p>Don’t!</p><p>Learn to say Fuck You to the world every once in a while. You have every right to.</p><p><strong>Just stop thinking, worrying, looking over your shoulder wondering, doubting, fearing, hurting, hoping for some easy way out, struggling, grasping, confusing, itching, scratching, mumbling, bumbling, grumbling, humbling, stumbling, rumbling, rambling, numbling, gambling, tumbling, scumbling, scrambling, hitching, hatching, bitching, moaning, groaning, honing, boning, horse-shitting, hair-splitting, nit-picking, piss-trickling, nose-sticking, ass-gouging, eyeball-poking, finger pointing, alleyway-sneaking, long waiting, small stepping, evil-eyeing, back scratching, searching, perching, besmirching, grinding, grinding away at yourself. Stop it and just DO.</strong></p><p>From your description, and from what I know of your previous work and your ability; the work you are doing sounds very good. Drawing-clean-clear but crazy like machines, larger and bolder…real nonsense. That sounds fine, wonderful-real nonsense. Do more, more nonsensical, more crazy, more machines, more breasts, penises, cunts, whatever-make them abound with nonsense. Try and tickle something inside you, your weird humor.</p><p>You belong in the most secret part of you.</p><p><strong>Don’t worry about cool, make your own uncool.Make your own, your own world. If you fear, make it work for you-draw &amp; paint your fear and anxiety. And stop worrying about big, deep things such as to decide on a purpose and way of life, a consistent approach to even some impossible end or even an imagined end.</strong></p><p><strong>You must practice being stupid, dumb, unthinking, empty. Then you will be able to DO.</strong></p><p>I have much confidence in you and even though you are tormenting yourself, the work you do is very good. Try to do some BAD work. The worst you can think of and see what happens, but mainly relax and let everything go to hell.</p><p><strong>You are not responsible for the world-you are only responsible for your work-so DO it. And don’t think that your work has to conform to any preconceived form, idea or flavor. It can be anything you want it to be. But if life would be easier for you if you stopped working-then stop. Don’t punish yourself.</strong></p><p>However, I think it is so deeply engrained in you that it would be better for you to DO. It seems I do understand your attitude somewhat, anyway, because I go through a similar process every now and again myself. I have an Agonizing Reappraisal of my work and change everything as much as possible-and hate everything I’ve done, and try to do something entirely different and better. Maybe that kind of process is necessary to me, pushing me on and on. The feeling that I that I can do better than that shit I just did. Maybe you need your agony to accomplish what you do. And maybe it goads you on to do better.</p><p>But it is very painful I know.</p><p><strong>It would be better if you had the confidence just to do the stuff and not even think about it. Can’t you leave the world and ART alone and also quit fondling your ego. I know that you(or anyone) can only work so much and the rest of the time you are left with your thoughts. But when you work of before your work you have to empty you mind and concentrate on what you are doing. After you do something it is done and that’s that. After a while you can see some are better than others but also you can see what direction you are going.</strong></p><p>I’m sure you know all that.</p><p>You also must know that you don’t have to justify your work-not even to yourself.</p><p>Well, you know I admire your work greatly and can’t understand why you are so bothered by it. But you can see the next ones and I can’t. You also must believe in your ability. I think you do.</p><p>So try the most outrageous things you can-shock yourself. You have at your power the ability to do anything.</p><p>I would like to see your work and will have to content to wait until Aug or Sept. I have seen photos of some of Tom’s new things at Lucy’s. They are impressive-especially the ones with the more rigorous form: the simpler ones. I guess he’ll send some more later on.</p><p>Let me know how the shows are going and that kind of stuff. My work had changed since you left and it is much better. I will be having a show May 4-29 at the Daniels Gallery 17 E 64th St(where Emmerich was), I wish you could be there.</p><p>Much love to you both.</p><p>Sol</p></blockquote><p><img src="/2019/05/11/往者不谏，来者可追/letter1.jpg" alt="letter1"></p><p><img src="/2019/05/11/往者不谏，来者可追/letter2.jpg" alt="letter2"></p><p><img src="/2019/05/11/往者不谏，来者可追/letter3.jpg" alt="letter3"></p><p><img src="/2019/05/11/往者不谏，来者可追/letter4.jpg" alt="letter4"></p><p><img src="/2019/05/11/往者不谏，来者可追/letter5.jpg" alt="letter5"></p><p>实际上，这段朗读视频和信的内容我在本科大二的时候就接触到了，可惜当时并没有对此有很深的认知，看来很多道理和改变还是经历过才会真正懂得和接受。现在重读这封信，突然觉得字字箴言。每个人或多或少都会遇到类似的情况，会消沉，会不自信，会怀疑人生，但是事实如此，且事出有因，我们唯一能做的就是调整自己的状态和方向，持续“做下去”，而不是想太多，束缚自己的手脚，正如Sol在信中说的 :</p><blockquote><p>Don’t worry about cool, make your own uncool.Make your own, your own world. If you fear, make it work for you-draw &amp; paint your fear and anxiety.</p><p>After you do something it is done and that’s that. After a while you can see some are better than others but also you can see what direction you are going.</p></blockquote><p>“现在，我接受自己满意的作品，也接受不满意的作品，接受积极状态，也接受不积极状态，这是正常的生活，但我不会停止去做，在过程中体验其意义和价值。”</p><p>自怨自艾，消沉停滞是没有意义的，起码对那时的自己没有帮助，不管是顺境还是逆境，都得持续做下去，因为只有不断地进行下去，最后的意义才会明了。从某种意义上说这似乎是一种”down-top”的思想，我不知道最后结果如何，或者说意义如何，但是我现在要做这件事。就像我身边很多人跟我说“你有没有觉得这一年你感觉天天都很忙，每天起床干活，晚上上床睡觉，日复一日，可是我回想却发现好像啥也没做成”，包括我也是这样。我还年轻，这个问题我没办法理解透彻，只能姑且认为这些都是一种积累，大部分的积累最后会形成质变，其他的一些则会在以后的某个阶段给予我帮助，提供给我一些机会。</p><p>与“down-top”相反的则是”top-down”，拥有这些能力的估计应该都是比较厉害的人吧，从一开始就知道自己想要做什么，整个人生的大阶段和小阶段都有详细的目标和追求，然后对达到目标的过程又有很清晰的认识，有远见，胸有成竹，因此按照条件需求进行学习提升，后面根据成果和目标之间的差距进行微调，接着进行下一轮的提升。这样的人应当是具有很强的自制力的和自律性的，通常应该属于精英阶层，各方面条件都会不错。然而我只是一个普通人，在认识到自己的局限性后，希望能通过各方面的学习，交流，思考和输出来达到一个让自己满意的境界。“不丢脸是不可能变强的”。</p><p>那么到底什么样的东西是值得做的，什么样的人生是值得过的呢？“just DO”之前，如何知道现在做的或者之后做的事是值得的呢？实际上，我认为我们可能不会知道，正是因为不知道，所以我们之前才可能会陷入怀疑，但是如果喜欢却不继续工作下去是件很可惜的事，就像Yan LeCun在上个世纪坚持做神经网络和你现在决定做神经网络的道理一样，你做是因为现在神经网络很火，工资高，而Yan LeCun那个时候做是因为他认为这是值得做的，是自己喜欢做的，觉得是可以引发革命的，所以即使受到怀疑，也还是坚持做了下去（举个例子，当然事实可能不一定是这样，另外SLAM领域的大牛Andrew. J. Davison也是花了十年的时间才让人们注意到他工作的价值）。所以做与思考（个体和团体）是一个交织的整体，不应该是看别人做什么就做什么。在不知道自己喜欢什么之前，可以先和别人交流，看看大概是什么样子，觉得还不错的情况下可以去试着深入了解下，至少在科研上是如此，我在走了很多弯路之后才发现视觉SLAM是个非常有挑战性和综合性的领域，充满着令人兴奋的点和创新式的解决办法，所以我才愿意花时间深入研究，虽然申请该专业的博士失败了，但并不代表着我对相关领域的学习研究就此结束，至少我明白了，科研是科研，它代表的是自己的一种兴趣和工作的结合，它不应该掺杂太多的功利性和急于求成的心思在里面，不应该老是渴望着被人认可，希望受到其他研究工作者的高评价，这样会扰乱自己的思绪，甚至阻碍新想法的出现，同样也会过得不自由。</p><blockquote><p>自由就是不再寻求认可。</p><p>”你是这样年轻，一切都在开始，亲爱的先生，我要尽我所能请求你，对于你心里一切得疑难要多多忍耐，要去爱这些“问题得本身”，像是爱一间锁闭了的房屋，或是一本用别种文字写成的书。现在你不要去追求那些你还不能得到的答案，因为你还不能在生活里体验到它们，一切都要亲自生活“ ——-里尔克，《给青年人的十封信》</p></blockquote><p>还是从科研的功利性出发，对理工科学生来说，不一定非得以自己的大学专业谋生，虽然这有点站着说话不腰疼的感觉，但是如果觉得工作受限，或者有自己想法的话，不妨放下这种执念，虽然知识是财富，但是知识不等于金钱，这跟自己以后能赚大钱是没有必然的联系的（但是理工类学生的工资好像确实高点，而且学历和工资基本正相关）。这让我想起毕导公众号的一篇文章，叫<a href="https://mp.weixin.qq.com/s/EPXQwRj5YY65IsG1EUx3xQ" target="_blank" rel="noopener">理工科的硬核浪漫</a>，他在里面说我们在科研，学习的过程中，更多的是训练了我们的一种独特的理工科思维，虽然有时候用这种思维看待问题会让人觉得有点蠢萌和直男，但更多的是会让我们发现生活中的闪光的地方，让我们不自觉地对自己的生活认真起来。</p><blockquote><p>因此在这里我想给大家传递一种观点：理工到现在已经成为我的思维方式，也许你觉得理工科男博士这个群体，他们秃头，他们穿衣很差，他们收入很低，有很多的缺点。但事实上，当他们把这种思维用在生活当中之后，<strong>在我们同样平凡无趣的生活当中，理工男也许能看到更多闪光的地方</strong>，看到更多精彩好玩的地方，他还可以把这个分享给你。</p><p>之前有人问过，毕导你是清华大学的博士生，你从清华大学博士生从科研转向去做自媒体这件事，你觉得可惜不可惜？</p><p><strong>我觉得一点不可惜。你在从事这个专业之后，就一条路走到黑，一直从事这一个专业里面的事情，我反而觉得有点可惜。因为你把自己思维局限住了。所以到现在为止，我给自己一点要求是什么呢？我希望自己好好学习理工科知识，把自己的思维学到一个高度，达到一定的高度之后，怀揣着理工科的理想，去到我感兴趣的领域。</strong></p></blockquote><p>写到这里，感觉也快差不多了，抱歉写得比较杂乱，似乎都是在说一些空洞而无味的东西。主要是最近一年自己身上确实发生了一些起起伏伏的事情，积压了太久的情绪和想法在沉淀了一段时间之后被知乎上的那个回答给带了出来，因此就想写点什么了。不过好久没写东西了，突然感觉想完全表达自己的想法变得费劲起来，以后还得多多练习才行。估计一年之后回过头来重新看这篇文章可能会觉得有点傻吧。&gt;_&lt;|||</p><p>最后总结一下：</p><ul><li>不管现在状况如何，先找到自己喜欢的，或者认为值得自己做的，坚持做下去，并且keep working</li><li>停止抱怨和自怨自艾，少说多做，大多数的不如意来自于自身的缺陷</li><li>多阅读，多思考，多交流，尽量少看一些没有营养的东西，多多培养不同的兴趣</li><li>学会“输出”，不能只是不停地看，否则很容易忘记，对那些有用的，有趣的，通过费曼学习法让其在脑子里变得更加深刻</li><li>坚持写作，保持创作的热忱</li><li>主动一点，遇到比较厉害的前辈多多请教，不怕丢脸，学习优秀的人的习惯和思维是一种又快又好的进步方式</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;要想得到某样东西，最可靠的办法先让自己配得上它&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src=&quot;/2019/05/11/往者不谏，来者可追/4.jpg&quot; class=&quot;full-image&quot;&gt;&lt;/p&gt;&lt;p&gt;前些日子无意间看到了一位A. J. Davison教授的新博士生在知乎上写的一篇&lt;a href=&quot;https://www.zhihu.com/question/298181420/answer/619558578&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;回答&lt;/a&gt;，他在里面谈到了自己科研之路曲折受挫的经历，尤其是屡屡面对着与自己期望不符合的情况，让我感触很深，想起了自己去年准备国外博士申请失败的那段苦痛记忆。可能对于我来讲，自己对读博的认识和准备都还欠缺得很，既没有拿得出手的本领，技能和honor，也没有对未来研究方向的清楚认识，与另一位机器人大牛&lt;a href=&quot;https://zhuanlan.zhihu.com/p/41313724&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;YY硕&lt;/a&gt;读博的初衷和认知相比，实在是汗颜。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;所谓人的成长，其实是“不断发现个人独特的经历原来都只是人类普遍经验的一部分”的过程&lt;/p&gt;&lt;p&gt;​ —- 多丽丝 莱辛&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="随笔杂谈" scheme="http://densecollections.top/categories/%E9%9A%8F%E7%AC%94%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="reflections" scheme="http://densecollections.top/tags/reflections/"/>
    
      <category term="thoughts" scheme="http://densecollections.top/tags/thoughts/"/>
    
      <category term="feelings" scheme="http://densecollections.top/tags/feelings/"/>
    
      <category term="emotions" scheme="http://densecollections.top/tags/emotions/"/>
    
  </entry>
  
  <entry>
    <title>FutureMapping by A.J.Davison</title>
    <link href="http://densecollections.top/2019/04/16/FutureMapping-by-A-J-Davison/"/>
    <id>http://densecollections.top/2019/04/16/FutureMapping-by-A-J-Davison/</id>
    <published>2019-04-16T08:24:31.000Z</published>
    <updated>2020-02-03T02:28:47.264Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><hr><p>IML的<a href="https://www.imperial.ac.uk/people/a.davison" target="_blank" rel="noopener">A.J.Davison</a>教授是visual SLAM领域里的奠基人之一，近期抽空看了他在推特上置顶的有关SLAM未来的一些思考的论文： <strong>FutureMapping: The Computational Structure of Spatial AI Systems</strong>。对于做SLAM的同学，感觉这篇论文还是值得一看的，这里有个博主将该论文翻成了<a href="https://blog.csdn.net/cicibabe/article/details/79846466" target="_blank" rel="noopener">中文</a>。</p><p>实际上，通篇读下来我的感受是并没有发现Davison提出了比较吸引人眼球的见解，不过有不少亮点，也让我加深了对SLAM这个东西的理解。虽然他对未来visual SLAM的功能性估计也和综述<strong>Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age</strong> 里描述得差不多，只不过后者说的比较“大而空”，都是一些常见的想法，什么动态啦，语义融合啦，模仿生物视觉啦，地图表征与更新等等，但是在Davison的这篇论文里，他以一个机器人学者的角度出发，试图从硬件和软件这两个方面去思考，未来的需求下机器人应该如何完成视觉任务，硬件应该如何发展去支持算法有效的计算，以及整个系统该有怎样的结构，才能使得机器人更好地在不同的场景下，甚至是大场景中完成不同的任务。</p><p>作为一个入门SLAM不算太久的工科学生来说，虽然不少技术知识还未掌握，但是偶尔看看这样的文章和思考也不是未尝不可，至少能激发我思考什么样的东西是值得做的，哪一个技术是未来需求的？</p><p>不过有一个疑问是一直存在我心中的，大家都谈到了未来的visual SLAM会结合机器学习或者模仿生物视觉机制和大脑存储记忆的机制，这个我是赞同的，<strong>但是对于三维地图而言，是否是有必要去重建的</strong>，除了它们在AR上的一些应用？大家都说想要融合语义标签，物体分类与识别融合到三维图里面，有的人可能还想让地图进行实时更新，容纳动态物体（至少我曾经是这样想的），但是这样的做的目的和意义到底是什么？单纯说目的是为了让机器进一步理解环境是无法让我满足的，所以我也试图在寻找和思考这个问题的答案。遗憾的是，Davison在这篇文章里面也没有提到此类问题，不过也没有夸大三维地图的作用，而是强调图模型的作用，这一点我是赞同的。也就是说，类似人一样，我们利用视觉和计算完成任务时，”graph”这个东西是肯定发挥了很大的作用，但是却没必要“事无巨细”的记下来，我们大多是提取重要的特征，压缩下来，然后进行推断，从而得出各种预测和结论，而且事后也可以在脑海中回忆重建出场景的三维模型，此外，这些压缩信息也会随时间及时进行更迭，经常重访的则会记得牢固一些，调取起来也很快，那些不常去的可能就会进一步压缩或者删除了。</p><p>Davison在论文是这样描述地图表征的利用形式的：</p><blockquote><p>In real-time, the system must maintain and update a world model, with geometric and semantic information, and estimate its position within that model, from primarily or only measurements from its on-board sensors.</p><p>The system should provide a wide variety of taskuseful information about <strong>‘what’ is ‘where’ in the scene</strong>. Ideally, <strong>it will provide a full semantic level model of the identities, positions, shapes and motion of all of the objects and other entities in the surroundings</strong>.</p><p>The representation of the world model will <strong>be close to metric</strong>, at least locally, to enable rapid reasoning about arbitrary predictions and measurements of interest to an AI or IA system.</p><p>It will probably <strong>retain a maximum quality representation of geometry and semantics only in a focused manner</strong>; most obviously for the part of the scene currently observed and relevant for near-future interaction. <strong>The rest of the model will be stored at a hierarchy of residual quality levels, which can be rapidly upgraded when revisited.</strong></p><p>The system will be generally alert, in the sense that every incoming piece of visual data is checked against a forward predictive scene model: for tracking, and for detecting changes in the environment and independent motion. The system will be able to <strong>respond to changes</strong> in its environment.</p></blockquote><p>所以我的意思是，在进行视觉任务时，重建三维地图应该不是必要的，至少在实际任务上目前可能起不到很大的作用，可能需要的是一种更加简洁凝练的图表征模式，这种模式更适合机器去认识环境，去进行编码，解码，计算，存储以及维护，而不是像人一样以为这样看到的是环境的理解方式，毕竟我们看到的是已经经过大脑处理的“人机交互结果”，并不是最核心的表征方式。但是大家为什么现在都比较热衷与做3D视觉，3D重建，我想可能是计算机视觉的一个难题吧，毕竟计算机图形学还是很有魅力的，毕竟未来的应用谁也说不准，只希望最后不要让这些技术让人类迷失在虚拟世界里。。。在这个方面，还得多一些<a href="https://www.technologyreview.com/s/613311/mapping-the-world-in-3d-will-let-us-paint-streets-with-augmented-reality/?utm_campaign=the_download.unpaid.engagement&amp;utm_source=hs_email&amp;utm_medium=email&amp;utm_content=71836962&amp;_hsenc=p2ANqtz-8xmRJCA-lRmkREUy6bMXIyqkH3NbAngJgynjDgBx2V-dGw-HLkRxMi3j1Z2izhsqPrpw6txfcuN5lVt9tU4FFzZ1ggiw&amp;_hsmi=71836962" target="_blank" rel="noopener">产品层面的思考</a>。</p><p>以上只是我的一点不成熟的想法，还需要多去阅读思考和交流。</p><hr><a id="more"></a><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><p>这篇的文章的摘要如下：</p><blockquote><p>We discuss and predict the evolution of Simultaneous Localization and Mapping (SLAM) into a <strong>general geometric and semantic ‘Spatial AI’ perception capability for intelligent embodied devices</strong>. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or consumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We <strong>explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments.</strong></p></blockquote><p>可以看到Davison教授关注的是<strong>general geometric and semantic ‘Spatial AI’ perception capability for intelligent embodied devices</strong>。首先介绍了Spatial AI system的相关概念，<code>the goal of a Spatial AI system is not abstract scene understanding, but continuously to capture the right information, and to build the right representations, to enable real-time interpretation and action.</code> 然后分别从算法层面和硬件层面去探讨，什么样的元素应该具备，什么样的结构是合理的，什么样的计算方式和维护更新方式可能被采用，以及从被动的分析到主动的分析预测的思考（感觉这才是有点智能的味道），最后还批判现在的计算机视觉研究者都热衷于刷点，而不去思考架构本身的问题，这个吐槽很准了，毕竟听说今年CVPR2019刷点，刷速率的文章接受率都显著下降了，我们读者都疲劳了，更何况评委。当然教授最后吐槽的重点是为了思考Benchmark对visual SLAM的意义，因为visual SLAM是一个实践性很强的系统，是为了解决实际机器人问题而生的，因此在实际的实时实验中效果好才是真的好，一味地去比较各个指标没有太大的意义，而且也很难比较，指标又很多，比如最后文末列的：</p><blockquote><p>• Local pose accuracy in newly explored ares (visual odometry drift rate).<br>• Long term metric pose repeatability in well mapped areas.<br>• Tracking robustness percentage.<br>• Relocalisation robustness percentage.<br>• SLAM system latency.<br>• Dense distance prediction accuracy at every pixel.<br>• Object segmentation accuracy.<br>• Object classification accuracy.<br>• AR pixel registration accuracy.<br>• Scene change detection accuracy.<br>• Power usage.<br>• Data movement (bits×millimetres).</p></blockquote><p>文中的具体内容不再一一讲了，这里主要讲几个文章中让我感兴趣的点。</p><h3 id="ML-或者-DL能为Spatial-AI-system做什么"><a href="#ML-或者-DL能为Spatial-AI-system做什么" class="headerlink" title="ML 或者 DL能为Spatial AI system做什么"></a>ML 或者 DL能为Spatial AI system做什么</h3><p>传统机器学习算法和深度学习中的神经网络擅长做分类和回归，它们在对图像的特征学习上有着得天独厚的优势。近些年也有好多工作是利用CNN和RNN，以及unsupervised learning等深度学习的方法来进行位姿估计和深度估计，也取得了不错的效果。不过有些人认为深度学习在已经研究差不多的3D Geometry上并没有什么意义，况且数学模型我们都知道，没必要去及蹭热度利用深度学习来做，相反那些现有的算法无法处理的图像问题，比如鲁棒性好的特征点提取，光照的变化，纹理的单一，场景的识别以及运动模糊等可以尝试利用深度学习隐式地解决。此外，深度学习在object detection，semantic segmentation等都有很好的成果，可以进行应用。</p><blockquote><p>These are learning architectures which use the designer’s knowledge of the structure of the underlying estimation problem to increase what can be gained from training data, and can be seen as hybrids between pure black box learning and hand-built estimation. … Why should a neural network have to use some of its capacity to learn a well understood and modelled concept like 3D geometry? Instead it can focus its learning on the harder to capture factors such as correspondence of surfaces under varying lighting.</p><p>These insights support our first hypothesis that future Spatial AI systems will have recognisable and general map-ping capability which builds a close to metric 3D model. This ‘SLAM’ element may either be embedded within a specially architected neural network, or be more explicitly separated from machine learning in another module which stores and updates map properties using standard estimation methods (as in SemanticFusion for instance). In both cases, there should be an identifiable region of memory which contains a representation of space with some recognisable geometric structure.</p></blockquote><p>个人感觉机器学习以及现在，未来可能出现的一系列理论可能对Spatial AI system帮助最大地可能就是图像理解和环境表征方面了，另外长时间运行带来的认识融合和更新。以及压缩等，可能也会有帮助。有关这个方面的思考目前不是很深，因为我现在还没开始学习机器学习，所以对技术了解不深，但是我感觉这东西是个“万精油”，在SLAM上的应用很大程度上可能归功于设计者怎么用，用在哪里，也就是说怎么设计网络，然后通过什么去学习什么功能，而不是紧紧盯着传统的方法然后去想方设法实现它。</p><h3 id="硬件与云端"><a href="#硬件与云端" class="headerlink" title="硬件与云端"></a>硬件与云端</h3><p>硬件方面Davison教授主要探讨了装载该Spatial AI System的嵌入式硬件应该具有什么样的结构，而且还具有匹配视觉计算特点的算力，同时还得有一定的存储能力。他肯定了分割计算，并行计算，多核心，多线程，神经形态硬件架构的需求，也列出了一些正在研究的例子，具体的内容可以参见文章内容。</p><p>实际上，硬件对算法的促进具有着决定性的作用，从Yann Lecun在2019年的ISSCC上做的<a href="https://pan.baidu.com/s/1lXv0aDSEKXKYQVhJc5X6-A" target="_blank" rel="noopener">报告</a>就可以看出</p><p>，没有良好的硬件支持，算法根本没办法进行实验验证，就很难进步。因此，硬件方面的迫切需求是现在整个智能行业的燃眉之急。</p><p><img src="/2019/04/16/FutureMapping-by-A-J-Davison/yanlecun_lesson1.jpeg" alt="yan lecun lesson1 about hardware"></p><p>另外，Davison教授也肯定了云端的重要性。因为云端相当于一个存储中心，可以存储环境表征这样的信息，而且可以同时对环境分布中的机器人进行通信和数据传输，这对机器人在大场景中，长时间执行任务起到“预热”等辅助性作用。</p><blockquote><p>Finally, when considering the evolution of the computing resources for Spatial AI, we should never forget that, cloud computing resources will continue to expand in capacity and reduce in cost. All future Spatial AI systems will likely be cloud-connected most of the time, and from their point of view the processing and memory resources of the cloud can be assumed to be close to infinite and free. What is not free is communication between an embedded device and the cloud, which can be expensive in power terms, particularly if high bandwidthdata such as video is transmitted. The other important consideration is the time delay, typically of significant fractions of a second, in sending data to the cloud for processing.</p><p>The long term potential for cloud-connected Spatial AI is clearly enormous. The vision of Richard Newcombe, Director of Research Science at Oculus, is that all of these devices should communicate and collaborate to build and maintain shared a ‘machine perception map’ of the whole world. The master map will be stored in the cloud, and individual devices will interact with parts of it as needed. A<br>shared map can be much more complete and detailed than that build by any individual device, due to both sensor coverage and the computing resources which can be put into it. A particularly interesting point is that the Spatial AI work which each individual device needs to do in this setup can in theory be much reduced. Having estimated its location within the global map, it would not need to densely map<br>or semantically label its surrounding if other devices had already done that job and their maps could simply be projected into its field of view. It would only need to be alert for changes, and in turn play its part in returning updates.</p></blockquote><h3 id="Graphs"><a href="#Graphs" class="headerlink" title="Graphs"></a>Graphs</h3><p>Davison在论文的第5节讲了很多有关”Graphs”的东西，我们知道，现在的visual SLAM框架都开始逐渐认同将图优化作为减小估计误差的手段要比滤波器估计的效果好得多，因为”graphs”本身就是视觉的一种表征方式，而且在约束上具有非线性性，能更好地模拟现实情况。</p><p>在SLAM方面，教授主要提出了geometry和local appearance两者是否可以联系起来的观点：</p><blockquote><p>We have not yet discovered a <strong>suitable feature representation which describes both local appearance and geometry in such a way that a relatively sparse feature set can provide a dense scene prediction.</strong> We believe that learned features arising from ongoing geometric deep learning research will provide the path towards this.</p><p>Some very promising recent work which we believe is heading in the right direction Bloesch et al.’s CodeSLAM. This method uses an image-conditioned autoencoder to discover an optimisable code with a small number of parameters which describes the dense depth map at a keyframe. <strong>In SLAM, camera poses and these depth codes can be jointly optimised to estimate dense scene shape which is represented by relatively few parameters.</strong> In this method, the scene geometryis stilllocked to keyframes, but we believe that the next step is to discover learned codes which can efficiently represent both appearance and 3D shape, and to make these the elements of a graph SLAM system.</p></blockquote><p>Davison教授另外一个观点是该实时系统中的”Computation Graph”，并且再次提出了”object-oriented SLAM”的概念。</p><blockquote><p>How can we get back to this ‘object-oriented SLAM’ capability in the much more general sense, where a wide range of object classes of varying style and details could be dealt with? As discussed before, <strong>SLAM maps of the future will probably be represented as multi-scale graphs of learned features which describe geometry, appearance and semantics</strong>. Some of these features will represent immediately recognised whole objects as in SLAM++. <strong>Others will represent generic semantic elements or geometric parts (planes, corners, legs, lids?</strong>) which are part of objects either already known or yet to be discovered. Others may approach surfels or other standard dense geometric elements in representing the geometry and appearance of pieces whose semantic identity is not yet known, or does not need to be known.</p><p><strong>Recognition, and unsupervised learning, will operate on these feature maps to cluster, label and segment them</strong>. The machine learning methods which do this job will themselves improve by selfsupervision during the SLAM process, taking advantage of dense SLAM’s properties as a “correspondence engine”.</p></blockquote><p><img src="/2019/04/16/FutureMapping-by-A-J-Davison/some_elements_of_computation_graph.PNG" alt="some elements of computation graph"></p><p>这个图基本上等于是把系统算法的框架给列出来了，可以看出，核心还是”定位“（camera state）和”建图“（world model）。只不过里面加入了深度学习来提高系统的性能。</p><blockquote><p><strong>Most computation relates to the world model</strong>, which is a persistent, continuously changing and <strong>improving data store</strong> where the system’s generative representation of the important elements of the scene is held; and the input camera data stream. Some of the main computational elements are:</p><p>• Empirical labelling of images to features (e.g. via a CNN).<br>• <strong>Rendering</strong>: getting a dense prediction from the world map to image space.<br>• Tracking: aligning a prediction with new image data, including finding outliers and detecting independent movement.<br>• <strong>Fusion</strong>: fusing updated geometry and labels back into the map.<br>• <strong>Map consolidation: fusing elements into objects, or imposing smoothing, regularisation</strong>.<br>• Relocalisation/loop closure detection: detecting self similarity in the map.<br>• Map consistency optimization, for instance after confirming a loop closure.<br>• <strong>Self-supervised learning of correspondence information from the running system.</strong></p></blockquote><p>这些都是当前比较主流的观点，而且里面涉及的知识体系比较庞大，因此大部分都是先针对一个来展开研究，不过我觉得要想对其进行突破，最大的，也是最有挑战性的问题应该就是世界模型表征问题了，对于机器来讲，这个应当是个非常简洁和高效的表征方式，同时也易于存储，调用，翻译和编码。</p><h3 id="地图的处理，表示，预测和更新"><a href="#地图的处理，表示，预测和更新" class="headerlink" title="地图的处理，表示，预测和更新"></a>地图的处理，表示，预测和更新</h3><p>其实这个部分前面已经提及了不少了，而Davison教授也单独在第6节讲了这个问题，对里面的几个关键问题进行了总结和思考：一个是硬件支持，一个是地图存储，一个是实时回环。</p><p>地图表征方面：</p><blockquote><p>There is a large degreeof choice possible in the representation of a 3D scene, but as explained in Section 5.1.2, we envision maps which <strong>consist of graphs of learned features, which are linked in multi-scale patterns relating to camera motion</strong>. These features must <strong>represent geometry as well as appearance,</strong> such that they can be used to render a dense predicted view of the scene from a novel viewpoint. It may be that they <strong>do not need to represent full photometric appearance</strong>, and that a somewhat abstracted view is sufficient as long as it captures geometric detail.</p></blockquote><p>地图存储与维持方面（更新）：</p><blockquote><p>Within the main processor, a major area will be devoted to storing this map, in a manner which is <strong>distributed around potentially a large number of individual cores which are strongly connected in a topology to mirror the map graph topology</strong>. In SLAM, of course the map is defined and <strong>grown dynamically</strong>, so the graph within the processor must either be able to change dynamically as well, or must be initially defined with a large unused capacity which is filled as SLAM progresses.</p><p>Importantly, a significant portion of the processing associated with large scale SLAM can be built directly into this graph. This is mainly the sort of ‘maintenance’ processing via which the map optimises and refines itself; including:</p><p>• <strong>Feature clustering; object segmentation and identification.</strong><br>• Loop closure detection.<br>• Loop closure optimization.<br>• <strong>Map regularisation (smoothing).</strong><br>• <strong>Unsupervised clustering to discover new semantic categories.</strong></p><p>With time, data and processing, a map which starts off as dense geometry and low level features can be refined towards an efficient object level map. Some of these operations will run with very high parallelism, as each part of the map is refined on its local core(s), while other operations such as loop closure detection and optimisation will require message passing around large parts of the graph. Still, importantly, they can take place in a manner which is internal to the map store itself.</p></blockquote><p>实时回环方面，教授提出了地图的存储与场景识别方面的一些难点，即“翻译”和“融合”之间协作的问题。因为相机的运动会对地图进行实时更新，该模块的重心在于维持，而不是比较数据，因此可能会对场景识别造成一定的影响。教授在这里提出了可以利用节点（node），海马体结构，以及小世界拓扑结构地图等来解决。我想可能是模仿人的记忆功能。</p><blockquote><p>Instead, a possible solution is to <strong>define special interface nodes which sit between the real-time loop block and the map store</strong>. These are nodes focused on <strong>communication</strong>, which are connected to the relevant components of real-time loop processing and then also to various sites in the map graph, and <strong>may have some analogue in the hippocampus of mammal brains</strong>. If the map store is <strong>organised such that it has a ‘small world’ topology</strong>, meaning that any part is joined to any other part by a small number of edge hops, then the interface nodes should be able to access (copy) any relevant map data in a small number of operations and serve them up to the real-time loop.</p><p><strong>Each node in the map store will also have to play some part in this communication procedure, where it will sometimes beused as part of the route for copying map databackwards and forwards.</strong></p></blockquote><h3 id="注意力机制，主动视觉"><a href="#注意力机制，主动视觉" class="headerlink" title="注意力机制，主动视觉"></a>注意力机制，主动视觉</h3><p>这里的主动视觉是指机器人主动移动相机去采集和任务有关的信息，是一种“top-down”的执行方式。</p><blockquote><p>The active vision paradigm advocates using sensing resources, such as the directions that a camera points towards or the processing applied to its feed, <strong>in a way which is controlled depending on the task at hand and prior knowledge available</strong>. <strong>This ‘top-down’ approach contrasts with ‘bottom-up’, blanket processing of all of the data received from a camera.</strong></p></blockquote><p>Davison提到人的视觉机制是“bottom-up”和”top-down“并存的，而且现在的”bottom-up“的图像处理机制也有很不错的发展，而且在处理很多问题上都很有效，因此两者的结合应当也是一种必然，毕竟”top-down“的执行是需要信息和预测来作为先决条件的。主动视觉在系统的实时性上会有很大的帮助，因为减少了信息和数据处理的冗余度，只分析我需要的数据，因此会大大减小对算力的需求。</p><blockquote><p><strong>It is important that when assessing the relative efficiency of bottom-up versus top-down vision, we take into account not just processing operations but also data transfer, and to what extent memory locality and graph-based computation can be achieved by each alternative.</strong> This will make certain possibilities in model-based prediction unattractive, such as searching large global maps or databases. The amazing advances in <strong>CNN-based vision</strong> means that we have to raise our sights when it comes to what we can expect from <strong>pure bottom-up image processing</strong>. But also, <strong>graph processing will surely permit new ways to store and retrieve model data efficiently,</strong> and will favour keeping and updating world models (such as graph SLAM maps) which have data locality.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Davison这篇论文提出的思考和观点还是比较符合现在的主流认知的，而且在技术上，教授也给出了一些比较具体的方案。不过这个目标比较长远，目前其中的小环节可能都还没处理好，而且还需要硬件铺路，因此想要彻底实现难度还是有点大的。总之，这样的系统我估计未来都是模块化的，分布式的，并且是多协作的，以任务为中心的，毕竟现在的AI还没有大的突破，因此想要实现像人类那样的视觉机制还比较困难，得需要很多个学科的大佬共同研究努力才行。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;hr&gt;&lt;p&gt;IML的&lt;a href=&quot;https://www.imperial.ac.uk/people/a.davison&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;A.J.Davison&lt;/a&gt;教授是visual SLAM领域里的奠基人之一，近期抽空看了他在推特上置顶的有关SLAM未来的一些思考的论文： &lt;strong&gt;FutureMapping: The Computational Structure of Spatial AI Systems&lt;/strong&gt;。对于做SLAM的同学，感觉这篇论文还是值得一看的，这里有个博主将该论文翻成了&lt;a href=&quot;https://blog.csdn.net/cicibabe/article/details/79846466&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;中文&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;实际上，通篇读下来我的感受是并没有发现Davison提出了比较吸引人眼球的见解，不过有不少亮点，也让我加深了对SLAM这个东西的理解。虽然他对未来visual SLAM的功能性估计也和综述&lt;strong&gt;Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age&lt;/strong&gt; 里描述得差不多，只不过后者说的比较“大而空”，都是一些常见的想法，什么动态啦，语义融合啦，模仿生物视觉啦，地图表征与更新等等，但是在Davison的这篇论文里，他以一个机器人学者的角度出发，试图从硬件和软件这两个方面去思考，未来的需求下机器人应该如何完成视觉任务，硬件应该如何发展去支持算法有效的计算，以及整个系统该有怎样的结构，才能使得机器人更好地在不同的场景下，甚至是大场景中完成不同的任务。&lt;/p&gt;&lt;p&gt;作为一个入门SLAM不算太久的工科学生来说，虽然不少技术知识还未掌握，但是偶尔看看这样的文章和思考也不是未尝不可，至少能激发我思考什么样的东西是值得做的，哪一个技术是未来需求的？&lt;/p&gt;&lt;p&gt;不过有一个疑问是一直存在我心中的，大家都谈到了未来的visual SLAM会结合机器学习或者模仿生物视觉机制和大脑存储记忆的机制，这个我是赞同的，&lt;strong&gt;但是对于三维地图而言，是否是有必要去重建的&lt;/strong&gt;，除了它们在AR上的一些应用？大家都说想要融合语义标签，物体分类与识别融合到三维图里面，有的人可能还想让地图进行实时更新，容纳动态物体（至少我曾经是这样想的），但是这样的做的目的和意义到底是什么？单纯说目的是为了让机器进一步理解环境是无法让我满足的，所以我也试图在寻找和思考这个问题的答案。遗憾的是，Davison在这篇文章里面也没有提到此类问题，不过也没有夸大三维地图的作用，而是强调图模型的作用，这一点我是赞同的。也就是说，类似人一样，我们利用视觉和计算完成任务时，”graph”这个东西是肯定发挥了很大的作用，但是却没必要“事无巨细”的记下来，我们大多是提取重要的特征，压缩下来，然后进行推断，从而得出各种预测和结论，而且事后也可以在脑海中回忆重建出场景的三维模型，此外，这些压缩信息也会随时间及时进行更迭，经常重访的则会记得牢固一些，调取起来也很快，那些不常去的可能就会进一步压缩或者删除了。&lt;/p&gt;&lt;p&gt;Davison在论文是这样描述地图表征的利用形式的：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;In real-time, the system must maintain and update a world model, with geometric and semantic information, and estimate its position within that model, from primarily or only measurements from its on-board sensors.&lt;/p&gt;&lt;p&gt;The system should provide a wide variety of taskuseful information about &lt;strong&gt;‘what’ is ‘where’ in the scene&lt;/strong&gt;. Ideally, &lt;strong&gt;it will provide a full semantic level model of the identities, positions, shapes and motion of all of the objects and other entities in the surroundings&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;The representation of the world model will &lt;strong&gt;be close to metric&lt;/strong&gt;, at least locally, to enable rapid reasoning about arbitrary predictions and measurements of interest to an AI or IA system.&lt;/p&gt;&lt;p&gt;It will probably &lt;strong&gt;retain a maximum quality representation of geometry and semantics only in a focused manner&lt;/strong&gt;; most obviously for the part of the scene currently observed and relevant for near-future interaction. &lt;strong&gt;The rest of the model will be stored at a hierarchy of residual quality levels, which can be rapidly upgraded when revisited.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The system will be generally alert, in the sense that every incoming piece of visual data is checked against a forward predictive scene model: for tracking, and for detecting changes in the environment and independent motion. The system will be able to &lt;strong&gt;respond to changes&lt;/strong&gt; in its environment.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;所以我的意思是，在进行视觉任务时，重建三维地图应该不是必要的，至少在实际任务上目前可能起不到很大的作用，可能需要的是一种更加简洁凝练的图表征模式，这种模式更适合机器去认识环境，去进行编码，解码，计算，存储以及维护，而不是像人一样以为这样看到的是环境的理解方式，毕竟我们看到的是已经经过大脑处理的“人机交互结果”，并不是最核心的表征方式。但是大家为什么现在都比较热衷与做3D视觉，3D重建，我想可能是计算机视觉的一个难题吧，毕竟计算机图形学还是很有魅力的，毕竟未来的应用谁也说不准，只希望最后不要让这些技术让人类迷失在虚拟世界里。。。在这个方面，还得多一些&lt;a href=&quot;https://www.technologyreview.com/s/613311/mapping-the-world-in-3d-will-let-us-paint-streets-with-augmented-reality/?utm_campaign=the_download.unpaid.engagement&amp;amp;utm_source=hs_email&amp;amp;utm_medium=email&amp;amp;utm_content=71836962&amp;amp;_hsenc=p2ANqtz-8xmRJCA-lRmkREUy6bMXIyqkH3NbAngJgynjDgBx2V-dGw-HLkRxMi3j1Z2izhsqPrpw6txfcuN5lVt9tU4FFzZ1ggiw&amp;amp;_hsmi=71836962&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;产品层面的思考&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;以上只是我的一点不成熟的想法，还需要多去阅读思考和交流。&lt;/p&gt;&lt;hr&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="SLAM" scheme="http://densecollections.top/tags/SLAM/"/>
    
      <category term="mapping" scheme="http://densecollections.top/tags/mapping/"/>
    
      <category term="AI system" scheme="http://densecollections.top/tags/AI-system/"/>
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="hardware" scheme="http://densecollections.top/tags/hardware/"/>
    
  </entry>
  
  <entry>
    <title>visual SLAM by Gaoxiang(3)</title>
    <link href="http://densecollections.top/2019/03/26/visual-SLAM-by-Gaoxiang-3/"/>
    <id>http://densecollections.top/2019/03/26/visual-SLAM-by-Gaoxiang-3/</id>
    <published>2019-03-26T09:14:25.000Z</published>
    <updated>2020-02-03T02:28:47.283Z</updated>
    
    <content type="html"><![CDATA[<p>​</p><p>本次课程主要研究李群和李代数(Lie Group, Lie Algebra)，主要的目的是为了能够相机得旋转和平移进行微调。因为相机的运动估计可能不准确，而无法对旋转矩阵加上微小量之后依然是旋转矩阵（旋转矩阵无法定义加法，如果用四元数，必须是单位四元数，那么也无法定义加法）。李群李代数与后面的优化，流形都会有很大的联系。</p><blockquote><p>在视觉SLAM中，相机的位姿是未知的，而我们需要解决什么样的相机位姿最符合当前观测数据这样的问题。一种典型的方式是把其构建成一个优化问题，求解最优的$R$和$t$，使得误差最小化。</p><p>由于旋转矩阵自身带有约束，即必须正交且行列式为1，因此作为优化变量会引入额外的约束，使得优化变得困难。而通过李群李代数的转换关系，可以顺利求导，把位姿估计变成无约束的优化问题。</p></blockquote><h2 id="群"><a href="#群" class="headerlink" title="群"></a>群</h2><p>群(Group)是一种集合加上一种运算的代数结构，满足封闭性，结合律，<strong>幺元</strong>，逆。其中幺元可以认为是单位元，就是与其他元素作用不改变这个元素，逆是元素和和它的逆进行运算后得到了幺元。</p><p>三维旋转矩阵构成了三维正交群(special orthogonal group)</p><script type="math/tex;mode=display">SO(3) = \left \{ R \in {\mathbb R}^{3 \times 3} | RR^{T} =I, \det (R)=1 \right \}</script><p>三维变换矩阵构成了特殊欧式群(special euclidean group)</p><script type="math/tex;mode=display">SE(3) = \left \{T= \begin{bmatrix}R & t\\O^{T} & 1\end{bmatrix}\in {\mathbb R}^{4 \times 4} |R \in SO(3), t \in {\mathbb R}^{3} \right \}</script><p>旋转矩阵集合与矩阵乘法构成群，变换矩阵集合与矩阵乘法也构成了群，因此称它们为旋转矩阵群和变换矩阵群。</p><p>群结构保证了在群上的运算具有良好的性质。</p><a id="more"></a><h2 id="李群与李代数"><a href="#李群与李代数" class="headerlink" title="李群与李代数"></a>李群与李代数</h2><h3 id="李群"><a href="#李群" class="headerlink" title="李群"></a>李群</h3><p>具有连续（光滑）性质的群；</p><p>既是群也是流形；</p><p>直观上看，一个刚体能够连续地在空间中运动，因此$SO(3)$和$SE(3)$都是李群，然而，它们都没有定义加法，所以很难进行取极限和求导等操作；</p><h3 id="李代数"><a href="#李代数" class="headerlink" title="李代数"></a>李代数</h3><p>与李代数对应的一种结构，位于向量空间（李群单位元处的正切空间）$\mathfrak so(3)$，$\mathfrak se(3)$</p><p>从旋转矩阵引出旋转正交群的李代数：</p><p>对于相机的连续运动，旋转矩阵也随时间变化，则有：</p><script type="math/tex;mode=display">\begin{align*}& R(t)R(t)^{T} = I \\& \dot{R}(t)R(t)^{T}+R(t)\dot{R}(t)^{T} = 0  \quad \quad 对时间求导\\& \dot{R}(t)R(t)^{T} = - (\dot{R}(t)R(t)^{T})^{T} \quad \quad 反对称矩阵\\& 记\dot{R}(t)R(t)^{T} = \phi (t)^{\wedge} \implies \dot{R}(t) = \phi (t)^{\wedge} R(t)\end{align*}</script><p>符号$\wedge$看作是反对称矩阵的符号，在这里是指将向量$\phi$变成了反对称矩阵，这是由叉乘引申而来，在第二讲有提过。反过来符号$\vee$代表反对称矩阵到向量的变换。</p><p>上面的式子表示，对旋转矩阵求导，就是在其左侧乘以一个$\phi (t)$，类似于指数函数的求导。</p><p>下面进行进一步地近似，假设在单位元附近，$t_{0}=0, R(0)=I$，则：</p><script type="math/tex;mode=display">R(t) \approx R(t_{0}) + \dot {R}(t_{0})(t-t_{0}) = I + \phi (t_{0})^{\wedge} (t) \quad \quad 将R在t_{0}进行泰勒展开，并忽略二次及以上高阶项</script><p>进一步假设，在$t_{0}$附近，$\phi$不变，则$\dot {R} (t) = \phi (t_{0}) ^{\wedge} R(t) = \phi _{0} ^{\wedge} R(t)$，再根据初值条件，得出：</p><script type="math/tex;mode=display">R(t) = \exp( \phi _{0} ^{\wedge} t)</script><p>在泰勒展开那一步可以看出，$\phi$反映的是一阶导数的性质，位于旋转正交群的正切空间上（tangent space，切平面上）。</p><p>上述证明提供了一种思路，可能不太严谨。实际上可以证明最后得出的式子在任意时间都适用，且该关系称为指数映射（exponential map），$\phi$称为$SO(3)$对应的李代数$\mathfrak so(3)$。</p><p><strong>李群是高维空间的低维曲面，或者说低维流形，在流形原点附近的切空间上的任意一个点，是李代数，可以通过指数映射映回李群上。李代数描述了李群单位元附近的正切空间的性质。</strong></p><script type="math/tex;mode=display">\begin{align*}& 李代数由一个集合\mathbb V，一个数域\mathbb F，和一个二元运算[,]（李括号，直观上说表示了两个元素的差异）组成。如果满足下面的四条性质，称\\& (\mathbb V, \mathbb F, [,])为一个李代数，记作 \mathfrak g。\\&1. 封闭性 \quad \quad \forall {\bf X, Y}\in \mathbb V, [{\bf X,Y}] \in \mathbb V\\&2. 双线性 \quad \quad \forall {\bf X, Y}\in \mathbb V, a,b \in \mathbb F,有：[a {\bf X} + b {\bf Y}, {\bf Z}] =a[{\bf X,Z}]+b[{\bf Y, Z}],[{\bf Z},a{\bf X}+b{\bf Y}] =a[{\bf Z,X}]+b[{\bf Z,Y}] \\&3. 自反性 \quad \quad \forall {\bf X} \in \mathbb V, [{\bf X,X}]={\bf 0}\\&4. 雅可比等价 \quad \quad \forall {\bf X,Y,Z} \in \mathbb V,[{\bf X},[{\bf Y,Z}]]+[{\bf Z},[{\bf Y,X}]]+[{\bf Y},[{\bf Z,X}]]= {\bf 0}\end{align*}</script><p>李代数$\mathfrak so(3)$可以看成是三维空间向量和叉积运算构成的，$so(3)=\left \{ \phi \in \mathbb R^{3}, \Phi = \phi ^{\wedge} \in \mathbb R^{3 \times 3} \right \}$，其中：</p><script type="math/tex;mode=display">\Phi = \phi ^{\wedge} =\begin{bmatrix}0 & - \phi_{3} & \phi _{2}\\\phi _{3} & 0 & -\phi _{1}\\-\phi _{2} & \phi _{1} & 0\end{bmatrix}\in \mathbb R^{3 \times 3}</script><p>李括号$[\phi_{1},\phi_{2}] = (\Phi_{1} \Phi_{2} - \Phi_{2} \Phi_{1})^{\vee}$,容易验证此李括号满足上述四条性质。</p><p>对于变换矩阵的特殊欧式群$SE(3)$，也有对应的李代数$\mathfrak se(3)$(6维的向量)</p><script type="math/tex;mode=display">\mathfrak se(3) = \left \{\xi = \begin{bmatrix}\rho \\\phi\end{bmatrix}\in \mathbb R^{6}, \rho \in \mathbb R^{3}, \phi \in \mathfrak se(3),\xi ^{\wedge} = \begin{bmatrix}\phi ^{\wedge} & \rho \\{\bf 0}^{T} & 0\end{bmatrix}\in \mathbb R^{4 \times 4}\right \}</script><p>此时还是以用符号$\wedge$来表示向量到矩阵的变换，只不过不再是限制于反对称矩阵。</p><script type="math/tex;mode=display">\begin{align*}& 设变换矩阵g(t)=\begin{bmatrix}R & \alpha \\{\bf 0}^{T} & 1\end{bmatrix} ，则g(t)^{-1} = \begin{bmatrix}R^{T} & -R^{T}\alpha \\{\bf 0}^{T} & 1\end{bmatrix}\\&有\dot{g}(t)g(t)^{-1}=\begin{bmatrix}\dot{R}R^{T} & \alpha- \dot{R}R^{T}\alpha \\{\bf 0}^{T} & 0\end{bmatrix}= \begin{bmatrix}\omega^{\wedge} & v\\{\bf 0}^{T} &0\end{bmatrix}，其中，\omega^{\wedge} \in \mathbb R^{3 \times 3},v \in \mathbb R^{3},记\xi^{\wedge}=\dot{g}(t)g(t)^{-1}\\& (\xi^{\wedge})^{\vee}=\begin{bmatrix}v \\\omega\end{bmatrix} \in \mathbb R^{6}\\&\dot{g}(t)=(\dot{g}(t)g(t)^{-1})g(t)=\xi^{\wedge}g(t),则g(t)=\exp(\xi^{\wedge}),假设g(0)=I\end{align*}</script><p>李括号$[\xi _{1}, \xi _{2}] = (\xi _{1} ^{\wedge} \xi_{2} ^{\wedge} - \xi _{2} ^{\wedge} \xi_{1} ^{\wedge}) ^{\vee}$</p><h2 id="指数映射和对数映射"><a href="#指数映射和对数映射" class="headerlink" title="指数映射和对数映射"></a>指数映射和对数映射</h2><p>指数映射反映了李代数到李群的关系，对于旋转矩阵$R$，有$R=\exp (\phi ^{\wedge})=\sum_{n=0}^{\infty} \frac{1}{n!}(\phi ^{\wedge})^{n}$</p><p>为了研究的方便，先将$\phi$写成旋转向量的形式，即设$\phi = \theta \vec{a}$,其中$\vec{a}$是单位向量，而且具有以下性质</p><script type="math/tex;mode=display">\begin{align*}&1.\vec{a}^{\wedge} \vec{a}^{\wedge} = \vec {a} \vec{a}^{T} - I \\&2.\vec {a}^{\wedge} \vec {a}^{\wedge} \vec {a}^{\wedge} = -\vec {a}^{\wedge}\end{align*}</script><p>下面利用上述性质对$\exp (\phi)^{\wedge}$进行Taylor展开</p><script type="math/tex;mode=display">\begin{align*}\exp (\phi)^{\wedge} & =\exp (\theta \vec{a}) = \sum _{n=0}^{\infty}\frac{1}{n!}(\theta \vec{a}^{\wedge})^{n}\\                     & = I+\theta \vec{a}^{\wedge}+\frac{1}{2!}\theta ^{2} \vec{a}^{\wedge}\vec{a}^{\wedge}+\frac{1}{3!} \theta ^{3} \vec{a}^{\wedge}\vec{a}^{\wedge}\vec{a}^{\wedge}+\frac{1}{4!} \theta ^{4} \vec{a}^{\wedge}\vec{a}^{\wedge}\vec{a}^{\wedge}\vec{a}^{\wedge}+ \cdots\\                     & = \vec{a}\vec{a}^{T}-\vec{a}^{\wedge}\vec{a}^{\wedge}+\theta \vec{a}^{\wedge}+\frac{1}{2!}\theta ^{2}\vec{a}^{\wedge}\vec{a}^{\wedge}-\frac{1}{3!} \theta^{3}\vec{a}^{\wedge}-\frac{1}{4!}\theta ^{4}\vec{a}^{\wedge}\vec{a}^{\wedge}+\cdots \\                     & = \vec{a}\vec{a}^{T}+(\theta -\frac{1}{3!}\theta^{3}+\frac{1}{5!}\theta^{5}-\cdots)\vec{a}^{\wedge}-(1-\frac{1}{2!}\theta^{2}+\frac{1}{4!}\theta^{4}-\cdots)\vec{a}^{\wedge}\vec{a}^{\wedge}\\                     & = \vec{a}^{\wedge}\vec{a}^{\wedge}+I+\sin \theta \vec{a}^{\wedge}-\cos \theta \vec{a}^{\wedge}\vec{a}^{\wedge}\\                     & = (1-\cos \theta)\vec{a}^{\wedge}\vec{a}^{\wedge}+I+\sin \theta \vec{a}^{\wedge}\\                     & = \cos \theta I+(1-\cos \theta)\vec{a}\vec{a}^{T}+\sin \theta \vec{a}^{\wedge}\end{align*}</script><p>这进一步说明了李代数$\mathfrak so(3)$的物理意义确实就是旋转向量。</p><p>反之，给定旋转矩阵亦可以求出对应的李代数，即对数映射$\phi = \ln(R)^{\vee}$。不过实际情况下可以通过旋转矩阵到向量的公式来得出李代数。</p><p>同理，可以得到$\mathfrak se(3)$到$SE(3)$的指数映射（具体推导过程会在作业中展示）</p><script type="math/tex;mode=display">\begin{align*}\exp(\xi^{\wedge}) & = \begin{bmatrix}\sum _{n=0}^{\infty}\frac{1}{n!}(\phi^{\wedge})^{n} & \sum _{n=0}^{\infty} \frac{1}{(n+1)!}(\phi ^{\wedge})^{n} \rho \\{\bf 0}^{T} & 1\end{bmatrix}\\& \triangleq \begin{bmatrix}R & J\rho \\{\bf 0}^{T} & 1\end{bmatrix}\\& = T\end{align*}</script><p>其中$J$为$SE(3)$的雅可比矩阵，$J=\frac{\sin \theta}{\theta} I + (1-\frac{\sin \theta} {\theta}) \vec{a} \vec{a}^{T} + \frac{1-\cos \theta} {\theta} \vec{a}^{\wedge}$</p><p>注意，这里的平移部分与变换矩阵的平移部分不完全相同，可以看出，指数映射会对李代数中的平移部分进行线性变换后才得到了真正的平移部分。</p><p><img src="/2019/03/26/visual-SLAM-by-Gaoxiang-3/lie_group_and_lie_algebra.png" alt="李群李代数的对应关系"></p><h2 id="李代数求导与扰动模型"><a href="#李代数求导与扰动模型" class="headerlink" title="李代数求导与扰动模型"></a>李代数求导与扰动模型</h2><p>前面说过，视觉SLAM应用李群李代数的最初目的就是为了对相机的位姿进行优化。因为相机在观测世界的时候，会不可避免地引入噪声，而我们优化的目的就是会取N个观测，然后对其进行误差最小化，得到一个在这么多N个观测的过程中最优的变换关系。因此，针对优化（一般是最小化问题）问题，我们常用的手段就是求导，此时李代数的功能就体现出来了，因为李代数具有良好的加法运算，可以进行无约束优化问题分析。</p><p>不过问题是，李代数上的加法并不会对应李群上的乘法，即$\exp(\phi_{1}^{\wedge}) \exp(\phi_{2}^{\wedge}) \neq \exp ((\phi_{1}+\phi_{2})^{\wedge})$</p><p>该关系由BCH公式（Baker-Campbell-Hausdorff）给出：</p><script type="math/tex;mode=display">\ln (\exp (A) \exp(B)) =A +B + \frac{1}{2} [A,B] + \frac{1}{12}[A,[A,B]] - \frac{1}{12}[B.[A,B]]+\cdots</script><p>如果其中一个量为小量，则有下列的近似表达关系：</p><script type="math/tex;mode=display">\ln (\exp (\phi_{1}^{\wedge}) \exp (\phi_{2}^{\wedge}))^{\vee} \approx\begin{cases}J_{l} (\phi _{2})^{-1} \phi_{1}+\phi_{2} \quad \quad \text{if  $\phi_{1}$is small}\\J_{r}(\phi_{1})^{-1} \phi_{2} + \phi_{1} \quad \quad \text{if  $\phi_{2}$is small}\end{cases}</script><p>其中：</p><script type="math/tex;mode=display">\begin{align*}& J_{l}=J=\frac{\sin \theta}{\theta} I + (1-\frac{\sin \theta} {\theta}) \vec{a} \vec{a}^{T} + \frac{1-\cos \theta} {\theta} \vec{a}^{\wedge}  \quad左雅可比\\& J_{l}^{-1} = \frac{\theta}{2}\cot \frac{\theta}{2}I+(1-\frac{\theta}{2}\cot \frac{\theta}{2})\vec{a}\vec{a}^{T}-\frac{\theta}{2}\vec{a}^{\wedge}\\& J_{r}(\phi) = J_{l}(-\phi) \quad 右雅可比\end{align*}</script><p>一般来说，利用$T_{cw}$时会进行左乘，利用$T_{wc}$时会进行右乘，以左乘为例，直观的写法是</p><script type="math/tex;mode=display">\begin{align*}& \exp (\Delta \phi ^{\wedge}) \exp(\phi ^{\wedge}) = \exp ((J_{l}(\phi)^{-1}\Delta \phi +\phi)^{\wedge}) \quad \quad 李群上的微小量乘法，李代数上的加法相差雅可比矩阵的逆\\& \exp((\phi + \Delta \phi)^{\wedge})=\exp ((J_{l}\Delta \phi)^{\wedge})\exp(\phi^{\wedge})=\exp(\phi^{\wedge}) \exp((J_{r}\Delta \phi)^{\wedge})\quad \quad 李代数上的微小量加法，李群上要乘上雅可比矩阵\end{align*}</script><p>对于$SE(3)$和$\mathfrak se(3)$，关系要复杂一些（雅可比矩阵较为复杂，是个$6 \times 6$矩阵，但在实际计算中不用到该雅可比）：</p><script type="math/tex;mode=display">\begin{align*}& \exp(\Delta \xi ^{\wedge}) \exp(\xi ^{\wedge}) \approx \exp ((\mathcal J_{l}^{-1}\Delta \xi +\xi)^{\wedge})\\& \exp(\xi ^{\wedge}) \exp (\Delta \xi^{\wedge}) \approx \exp ((\mathcal J_{r}^{-1} \Delta \xi+\xi)^{\wedge})\end{align*}</script><p>有了上述公示后，开始对旋转矩阵进行求导，这里有两种方法，一种是导数定义求导，一种是通过扰动模型进行求导，实际中，扰动模型因为形式更加简单，因此采用的更多，先不严谨地记旋转后的点关于旋转矩阵的导数的求导式为 $\frac{\partial (Rp)}{\partial R}$</p><p>导数模型：</p><script type="math/tex;mode=display">\begin{align*}\frac{\partial(\exp (\phi^{\wedge})p)}{\partial \phi} & = \lim_{\delta \phi \to 0} \frac{\exp((\phi+\delta \phi)^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi} \\                                                      & = \lim_{\delta \phi \to 0} \frac{\exp ((J_{l}\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp (\phi^{\wedge})p}{\delta \phi}\\                                                      & \approx \lim_{\delta \phi \to 0} \frac{(I+(J_{l}\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi} \\                                                      & = \lim_{\delta \phi \to 0} \frac{(J_{l}\delta \phi)^{\wedge}\exp(\phi^{\wedge})p}{\delta \phi}\\                                                      & = \lim_{\delta \phi \to 0} \frac{-(\exp(\phi^{\wedge})p)^{\wedge} J_{l}\delta \phi}{\delta \phi} \\                                                      & = -(Rp)^{\wedge}J_{l}\end{align*}</script><p>扰动模型（左乘微小量）：</p><script type="math/tex;mode=display">\begin{align*}\frac{\partial(Rp)}{\partial R} & = \lim_{\delta \phi \to 0} \frac{\exp((\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi}\\                                & \approx \lim_{\delta \phi \to 0} \frac{(I+(\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi}\\                                & = \lim_{\delta \phi \to 0} \frac{(\delta \phi)^{\wedge}Rp}{\delta \phi} =  -(Rp)^{\wedge}\end{align*}</script><p>同理可得，$SE(3)$上的扰动模型（左乘微小量）为：</p><script type="math/tex;mode=display">\frac{\partial (Tp)}{\partial \delta \xi}=\begin{bmatrix}I & -(Rp+t)^{\wedge}\\{\bf 0}^{T} & {\bf 0}^{T}\end{bmatrix}\triangleq (Tp)^{\odot}</script><h2 id="相似变换"><a href="#相似变换" class="headerlink" title="相似变换"></a>相似变换</h2><p>对于单目视觉，由于存在尺度不确定性，因此不能使用$SE(3)$来表达位姿变化，而是利用相似变换群$Sim(3)$，也就是说要加一个尺度因子$s$，这个尺度因子会同时作用在变换的点$p$上，对其进行缩放，也就是在相机坐标系下进行了一次相似变换，而不是欧式变换，即：$p^{‘} = sRp+t$</p><script type="math/tex;mode=display">Sim(3)=\left\{\left[ \begin{array}{lll}{\boldsymbol{S}=} & {\boldsymbol{s} \boldsymbol{R}} & {\boldsymbol{t}} \\ {\boldsymbol{0}^{T}} & {1}\end{array}\right] \in \mathbb{R}^{4 \times 4}\right\}</script><script type="math/tex;mode=display">\mathfrak sim(3)=\left\{\zeta | \zeta=\left[ \begin{array}{l}{\rho} \\ {\phi} \\ {\sigma}\end{array}\right] \in \mathbb{R}^{7}, \zeta^{\wedge}=\left[ \begin{array}{cc}{\sigma I+\phi^{\wedge}} & {\rho} \\ {0^{T}} & {0}\end{array}\right] \in \mathbb{R}^{4 \times 4}\right\}</script><script type="math/tex;mode=display">\exp \left(\zeta^{\wedge}\right)=\left[ \begin{array}{cc}{e^{\sigma} \exp \left(\phi^{\wedge}\right)} & {J_{s} \rho} \\ {0^{T}} & {1}\end{array}\right]</script><script type="math/tex;mode=display">\begin{aligned} J_{s}=& \frac{e^{\sigma}-1}{\sigma} I+\frac{\sigma e^{\sigma} \sin \theta+\left(1-e^{\sigma} \cos \theta\right) \theta}{\sigma^{2}+\theta^{2}} a^{\wedge} \\ &+\left(\frac{e^{\sigma}-1}{\sigma}-\frac{\left(e^{\sigma} \cos \theta-1\right) \sigma+\left(e^{\sigma} \sin \theta\right) \theta}{\sigma^{2}+\theta^{2}}\right) a^{\wedge} a^{\wedge} \end{aligned}</script><script type="math/tex;mode=display">s=e^{\sigma}, \boldsymbol{R}=\exp \left(\boldsymbol{\phi}^{\wedge}\right), \boldsymbol{t}=\boldsymbol{J}_{s} \boldsymbol{\rho}</script><p>对于$Sim(3)$的求导，利用左扰动模型和BCH近似（这里的BCH近似与$SE(3)$公式不同）。假设点$p$经过相似变换$Sp$后，相对于$S$的导数为：</p><script type="math/tex;mode=display">\frac{\partial S p}{\partial \zeta}=\left[ \begin{array}{ccc}{\boldsymbol{I}} & {-\boldsymbol{q}^{\wedge}} & {\boldsymbol{q}} \\ {\mathbf{0}^{T}} & {\mathbf{0}^{T}} & {0}\end{array}\right]</script><p>其中$q$是$Sp$的前三维向量，最后的形式应该是$4 \times 7$的雅可比矩阵。</p><p>有关相似变换群的更为详细的理解和运用，等后面进行实际应用时再说，毕竟库已经提供好了，而且推导过程也与$SE(3)$类似。</p><h2 id="作业与实践"><a href="#作业与实践" class="headerlink" title="作业与实践"></a>作业与实践</h2><h3 id="群的性质"><a href="#群的性质" class="headerlink" title="群的性质"></a>群的性质</h3><p>群要满足封闭性，结合律，幺元和逆这四个性质。其中，满足前两个性质的叫半群，满足前三个性质的叫有单位元的半群，若满足了上述四个性质，还具有交换律的叫做阿贝尔群。</p><p>对于$\left \{ \mathbb Z, + \right \}$封闭性和结合律显然满足，幺元是0，逆为自身的相反数，因此是群，而且是阿贝尔群。</p><p>对于$\left \{ \mathbb N, +\right \}$，前三个性质都满足，幺元是0，但是除了0之外，其他的元素不存在逆，因此不是群，是有单位元的半群。</p><h3 id="验证向量叉乘的李代数性质"><a href="#验证向量叉乘的李代数性质" class="headerlink" title="验证向量叉乘的李代数性质"></a>验证向量叉乘的李代数性质</h3><p>设${\bf X}=a_{1}\vec{i}+b_{1}\vec{j}+c_{1}\vec{k}, {\bf Y}=a_{2}\vec{i}+b_{2}\vec{j}+c_{2}\vec{k}, {\bf Z}=a_{3}\vec{i}+b_{3}\vec{j}+z_{3}\vec{k}$</p><p>封闭性：</p><script type="math/tex;mode=display">[{\bf X},{\bf Y}] = {\bf X} \times {\bf Y} \in \mathbb R^{3}</script><p>双线性：</p><script type="math/tex;mode=display">\begin{align*}& [a{\bf X}+b{\bf Y},{\bf Z}]=(a{\bf X}+b{\bf Y}) \times {\bf Z}=a{\bf X}\times {\bf Z}+b{\bf Y} \times {\bf Z} = a[{\bf X,Z}]+b[{\bf Y,Z}]\\& [{\bf Z},a{\bf X}+b{\bf Y}]={\bf Z} \times (a{\bf X}+b{\bf Y})={\bf Z}\times a {\bf X}+{\bf Z} \times b{\bf Y}=a[{\bf Z,X}]+b[{\bf Z,Y}]\end{align*}</script><p>自反性：</p><script type="math/tex;mode=display">[{\bf X,X}]={\bf X} \times {\bf X}={\bf 0}</script><p>雅可比等价：</p><script type="math/tex;mode=display">\begin{align*}&[{\bf X},[{\bf Y,Z}]]+[{\bf Y},[{\bf Z,X}]]+[{\bf Z},[{\bf X,Y}]]\\& ={\bf X} \times {\bf Y} \times {\bf Z}+{\bf Y} \times {\bf Z} \times {\bf X}+{\bf Z} \times {\bf X} \times {\bf Y}\\& = {\bf X}^{\wedge} {\bf Z}^{\wedge} {\bf Y}+{\bf Y}^{\wedge} {\bf X}^{\wedge} {\bf Z}+{\bf Z}^{\wedge} {\bf Y}^{\wedge} {\bf X}\\& = \begin{bmatrix}0 & -c_{1} & b_{1}\\c_{1} & 0 & -a_{1}\\-b_{1} & a_{1} &0\end{bmatrix}\begin{bmatrix}0 & -c_{3} & b_{3}\\c_{3} & 0 & -a_{3}\\-b_{3} & a_{3} & 0\end{bmatrix}\begin{bmatrix}a_{2}\\b_{2}\\c_{2}\end{bmatrix}+\begin{bmatrix}0 & -c_{2} & b_{2}\\c_{2} & 0 & -a_{2}\\-b_{2} & a_{2} &0\end{bmatrix}\begin{bmatrix}0 & -c_{1} & b_{1}\\c_{1} & 0 & -a_{1}\\-b_{1} & a_{1} & 0\end{bmatrix}\begin{bmatrix}a_{3}\\b_{3}\\c_{3}\end{bmatrix}+\begin{bmatrix}0 & -c_{3} & b_{3}\\c_{3} & 0 & -a_{3}\\-b_{3} & a_{3} &0\end{bmatrix}\begin{bmatrix}0 & -c_{2} & b_{2}\\c_{2} & 0 & -a_{2}\\-b_{2} & a_{2} & 0\end{bmatrix}\begin{bmatrix}a_{1}\\b_{1}\\c_{1}\end{bmatrix}\\& = \begin{bmatrix}a_{3}(b_{1}b_{2}+c_{1}c_{2})-a_{2}(b_{1}b_{3}+c_{1}c_{3})\\b_{3}(a_{1}a_{2}+c_{1}c_{2})-b_{2}(a_{1}a_{3}+c_{1}c_{3})\\c_{3}(a_{1}a_{2}+b_{1}b_{2})-c_{2}(b_{1}b_{3}+a_{1}a_{3})\end{bmatrix}+\begin{bmatrix}a_{1}(b_{2}b_{3}+c_{2}c_{3})-a_{3}(b_{1}b_{2}+c_{1}c_{2})\\b_{1}(a_{2}a_{3}+c_{2}c_{3})-b_{3}(a_{1}a_{2}+c_{1}c_{2})\\c_{1}(a_{2}a_{3}+b_{2}b_{3})-c_{3}(b_{1}b_{2}+a_{1}a_{2})\end{bmatrix}+\begin{bmatrix}a_{2}(b_{1}b_{3}+c_{1}c_{3})-a_{1}(b_{2}b_{3}+c_{1}c_{3})\\b_{2}(a_{1}a_{3}+c_{1}c_{3})-b_{1}(a_{2}a_{3}+c_{1}c_{3})\\c_{2}(a_{1}a_{3}+b_{1}b_{3})-c_{1}(b_{2}b_{3}+a_{1}a_{3})\end{bmatrix}\\& = \begin{bmatrix}0\\0\\0\end{bmatrix}={\bf 0}\end{align*}</script><p>则$\mathfrak g =(\mathbb R^{3}, \mathbb R, \times)$构成李代数。</p><h3 id="推导-SE-3-的指数映射"><a href="#推导-SE-3-的指数映射" class="headerlink" title="推导$SE(3)$的指数映射"></a>推导$SE(3)$的指数映射</h3><p>推导$SE(3)$指数映射部分左雅可比的形式：</p><script type="math/tex;mode=display">\begin{align*}J & \triangleq \sum_{n=0}^{\infty} \frac{1}{(n+1)!}(\phi ^{\wedge})^{n}=\sum_{n=0}^{\infty} \frac{1}{(n+1)!}(\theta a^{\wedge})^{n}\\  & =I+\frac{1}{2!}\theta a^{\wedge}+\frac{1}{3!} \theta^{2}a^{\wedge}a^{\wedge}-\frac{1}{4!}\theta^{3}a^{\wedge}a^{\wedge}a^{\wedge}+\frac{1}{5!}\theta^{4}a^{\wedge}a^{\wedge}a^{\wedge}a^{\wedge}+\cdots\\  & =aa^{T}-a^{\wedge}a^{\wedge}+\frac{1}{2!}\theta a^{\wedge}+\frac{1}{3!}\theta^{2}a^{\wedge}a^{\wedge}-\frac{1}{4!}\theta^{3}a^{\wedge}-\frac{1}{5!}\theta^{4}a^{\wedge}a^{\wedge}+\cdots \\  & = \frac{1}{\theta}\left \{ aa^{T}\theta-a^{\wedge}a^{\wedge}\theta+a^{\wedge}-a^{\wedge}+\frac{1}{2!}\theta^{2}a^{\wedge}+\frac{1}{3!}\theta^{3}a^{\wedge}a^{\wedge}-\frac{1}{4!}\theta^{4}a^{\wedge}-\frac{1}{5!}\theta^{5}a^{\wedge}a^{\wedge}+\cdots \right \} \\  & = \frac{1}{\theta} \left \{ aa^{T}\theta - a^{\wedge}a^{\wedge}(\theta-\frac{1}{3!}\theta^{3}+\frac{1}{5!}\theta^{5}+\cdots)-a^{\wedge}(1-\frac{1}{2!}\theta^{2}+\frac{1}{4!}\theta^{4}+\cdots)+a^{\wedge}  \right\}\\  & = \frac{1}{\theta} \left \{ aa^{T}\theta - a^{\wedge}a^{\wedge}\sin \theta -a^{\wedge}\cos \theta +a^{\wedge}     \right \}\\  & = \frac{1}{\theta} \left \{ aa^{T}\theta-\sin \theta (aa^{T}-I)+a^{\wedge}(1-\cos \theta)   \right  \} \\  & = \frac{1}{\theta} \left \{  \sin \theta I +(\theta-\sin \theta)aa^{T}+(1-\cos \theta)a^{\wedge}   \right \} \\  & = \frac{\sin \theta}{\theta}I + (1-\frac{\sin \theta}{\theta})aa^{T}+\frac{1-\cos \theta}{\theta}a^{\wedge}\end{align*}</script><p>至于为什么雅可比矩阵是这种形式，即$J=\sum_{n=0}^{\infty} \frac{1}{(n+1)!}(\phi ^{\wedge})^{n}$是从何而来的，这个在<code>state estimation for robotics</code>一书的223-226有讲到，但是写的很抽象，下面给出证明：</p><script type="math/tex;mode=display">\begin{align*}& \xi ^{\wedge} = \begin{bmatrix}\phi ^{\wedge} & \rho \\{\bf 0}^{T} & 0\end{bmatrix}\\& \xi ^{\wedge} \xi^{\wedge}= \begin{bmatrix}\phi ^{\wedge} & \rho \\{\bf 0}^{T} & 0\end{bmatrix}\begin{bmatrix}\phi ^{\wedge} & \rho \\{\bf 0}^{T} & 0\end{bmatrix}=\phi ^{\wedge} \xi^{\wedge}\\& \xi^{\wedge}\xi^{\wedge}\xi^{\wedge}=(\phi ^{\wedge})^{2}\xi^{\wedge}\end{align*}</script><script type="math/tex;mode=display">\begin{align*}\exp(\xi ^{\wedge}) & = \sum_{n=0}^{\infty}\frac{1}{n!}(\xi ^{\wedge})^{n}\\                    & = I+\xi^{\wedge}+\frac{1}{2!}\xi^{\wedge}\xi^{\wedge}+\frac{1}{3!}\xi^{\wedge}\xi^{\wedge}\xi^{\wedge}+\cdots \\                    & =I+\xi^{\wedge}+\frac{1}{2!}\phi^{\wedge}\xi^{\wedge}+\frac{1}{3!}(\phi^{\wedge})^{2}\xi^{\wedge}+\cdots \\                    & = I+\sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n}\xi^{\wedge}\\                    & =                     \begin{bmatrix}                    I+\sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n+1} & \sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n}\rho \\                    {\bf 0}^{T} & 1                    \end{bmatrix}                    \\                    & =                     \begin{bmatrix}                    \sum_{n=0}^{\infty}\frac{1}{n!}(\phi^{\wedge})^{n} & \sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n}\rho \\                    {\bf 0}^{T} & 1                    \end{bmatrix}\end{align*}</script><h3 id="伴随"><a href="#伴随" class="headerlink" title="伴随"></a>伴随</h3><p>先证明$\forall a \in \mathbb R^{3}, Ra^{\wedge}R^{T}=(Ra)^{\wedge}$,高翔提供的网址的证明不严谨，即</p><script type="math/tex;mode=display">(Ra)^{\wedge}v=(Ra)\times v =(Ra) \times (RR^{-1}v)=R(a \times R^{-1}v)=Ra^{\wedge}R^{-1}v</script><script type="math/tex;mode=display">AB=BC,不能推导出A \neq B \quad\text{A,B,C是矩阵}</script><p>下面通过旋转矩阵正交的性质来证明该式成立。</p><script type="math/tex;mode=display">\begin{align*}& 设旋转矩阵R=\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\quad \text{这里的${\bf r}_{1},{\bf r}_{2},{\bf r}_{3}$既可以看作是$1 \times 3$的单位向量，也可以看作是矩阵，因此下面不再进行区分}\\& 对于Rp^{\wedge}R^{T} = (Rp)^{\wedge} \iff p^{\wedge}=R^{T}(Rp)^{\wedge}R，因此转为证明后式\\\end{align*}</script><p>在证明之前，有两个事项需要注意：</p><p>1.${\bf r}_{i}^{T} {\bf r}_{j}={\bf r}_{i} \cdot {\bf r}_{j}$ 前面可以看作矩阵，后面看作向量点乘，${\bf r}$是列向量</p><p>2.因为$R$是旋转矩阵，因此行向量和列向量都是单位向量且两两正交。为了后面叉乘运算的一致性，需要将${\bf r}_{1},{\bf r}_{2},{\bf r}_{3}$分别看成三维正交坐标系的$x,y,z$轴，即：</p><script type="math/tex;mode=display">\begin{cases}{\bf r}_{1} \times {\bf r}_{2} = {\bf r}_{1}^{\wedge}{\bf r}_{2}={\bf r}_{3}\\{\bf r}_{2} \times {\bf r}_{3} = {\bf r}_{2}^{\wedge}{\bf r}_{3}={\bf r}_{1}\\{\bf r}_{3} \times {\bf r}_{1} = {\bf r}_{3}^{\wedge}{\bf r}_{1}={\bf r}_{2}\end{cases}</script><script type="math/tex;mode=display">\begin{align*}p^{\wedge} & = R^{T}(Rp)^{\wedge}R \\& = \begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}(\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\begin{bmatrix}p_{1}\\p_{2}\\p_{3}\end{bmatrix})^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\\& =\begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}({\bf r}_{1}p_{1}+{\bf r}_{2}p_{2}+{\bf r}_{3}p_{3})^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\\& = \begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}({\bf r}_{1}^{\wedge}p_{1}+{\bf r}_{2}^{\wedge}p_{2}+{\bf r}_{3}^{\wedge}p_{3})\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\\& = p_{1}\begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}{\bf r}_{1}^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}+p_{2}\begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}{\bf r}_{2}^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}+p_{3}\begin{bmatrix}{\bf r}_{1}^{T}\\{\bf r}_{2}^{T}\\{\bf r}_{3}^{T}\end{bmatrix}{\bf r}_{3}^{\wedge}\begin{bmatrix}{\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3}\end{bmatrix}\\& = p_{1}\begin{bmatrix}{\bf r}_{1}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{1} & {\bf r}_{1}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{2} & {\bf r}_{1}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{3}\\{\bf r}_{2}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{1} & {\bf r}_{2}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{2} & {\bf r}_{2}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{3}\\{\bf r}_{3}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{1} & {\bf r}_{3}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{2} & {\bf r}_{3}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{3}\\\end{bmatrix}+p_{2}\begin{bmatrix}{\bf r}_{1}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{1} & {\bf r}_{1}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{2} & {\bf r}_{1}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{3}\\{\bf r}_{2}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{1} & {\bf r}_{2}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{2} & {\bf r}_{2}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{3}\\{\bf r}_{3}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{1} & {\bf r}_{3}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{2} & {\bf r}_{3}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{3}\\\end{bmatrix}+p_{3}\begin{bmatrix}{\bf r}_{1}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{1} & {\bf r}_{1}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{2} & {\bf r}_{1}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{3}\\{\bf r}_{2}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{1} & {\bf r}_{2}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{2} & {\bf r}_{2}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{3}\\{\bf r}_{3}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{1} & {\bf r}_{3}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{2} & {\bf r}_{3}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{3}\\\end{bmatrix}\\& = p_{1}\begin{bmatrix}0 & 0 & 0\\0 & 0 & -1\\0 & 1 & 0\end{bmatrix}+p_{2}\begin{bmatrix}0 & 0 & 1\\0 & 0 & 0\\-1 & 0 & 0\end{bmatrix}+p_{3}\begin{bmatrix}0 & -1 & 0\\1 & 0 & 0\\0 & 0 & 0\end{bmatrix}\\& =\begin{bmatrix}0 & -p_{3} & p_{2}\\p_{3} & 0 & -p_{1}\\-p_{2} & p_{1} & 0\end{bmatrix}\end{align*}</script><p>对于$SO(3)$上的伴随的证明只需进行泰勒展开即可，与上面的证明相同。</p><script type="math/tex;mode=display">\begin{align*}& \exp ((Ad(R)P)^{\wedge}) = \exp ((Rp)^{\wedge})=\exp (Rp^{\wedge}R^{T})=\exp (R\theta a^{\wedge}R^{T})=\sum _{n=0}^{\infty}\frac{1}{n!}(R\theta a^{\wedge}R^{T})^{n} \\& 令p=\theta a,a是单位向量，\theta为模长\\& Ra^{\wedge}R^{T} Ra^{\wedge}R^{T} =Ra^{\wedge}a^{\wedge}R^{T}=R(aa^{T}-I)R^{T}=Raa^{T}R^{T}-I\\& Ra^{\wedge}R^{T} Ra^{\wedge}R^{T} Ra^{\wedge}R^{T} =Ra^{\wedge}a^{\wedge}a^{\wedge}R^{T}=-Ra^{\wedge}R^{T}\end{align*}</script><p>泰勒展开后的式子利用正余弦函数表示，结果为：</p><script type="math/tex;mode=display">\begin{align*}\exp(Rp^{\wedge}R^{T}) & = \sum _{n=0}^{\infty}\frac{1}{n!}(\theta R a^{\wedge}R^{T})^{n}\\                       & = (1-\cos \theta)Raa^{T}R^{T}+\cos \theta I+\sin \theta Ra^{\wedge}R^{T}\\                       & =R((1-\cos \theta)aa^{T}+\cos \theta I+\sin \theta a^{\wedge})R^{T}\\                       & =R \exp(p^{\wedge})R^{T}\end{align*}</script><p>对于$SE(3)$的伴随$Ad(T)$有：</p><script type="math/tex;mode=display">\begin{align*}& T\exp (\xi ^{\wedge})T^{-1} = \exp((Ad(T)\xi )^{\wedge})\\& Ad(T)=\begin{bmatrix}R & t^{\wedge}R\\{\bf 0} & R\end{bmatrix}\end{align*}</script><p>$SO(3)$和$SE(3)$的伴随将在后面的位姿图优化中用到。</p><h3 id="轨迹的描绘"><a href="#轨迹的描绘" class="headerlink" title="轨迹的描绘"></a>轨迹的描绘</h3><p>1.$T_{WC}$是相机坐标系到世界坐标系的变换矩阵，其平移部分就是相机的移动距离，我们在解算位姿的时候是计算两帧之间的位姿，因此平移部分连起来就是相机的轨迹，即机器人的轨迹。</p><p>实际上，$T_{WC}$和$T_{CW}$之间只差了一个逆而已，都可以用来表示相机的位姿，但是实践当中使用$T_{CW}$更为常见，不过$T_{WC}$更为直观，因为$p_{W} =T_{WC}p_{C}=Rp_{C}+t_{WC}$对于相机原点来说，$p_{W}$就是在其对应于世界坐标系的点，而且正是$T_{WC}$的平移部分，那么连起来就可以看到相机的平移轨迹。</p><p>2.仿照ORB-SLAM2里的<code>CMakeLists.txt</code>写的CMakeLists.txt</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line">project(trajectorydrawing)</span><br><span class="line"></span><br><span class="line">set(CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br><span class="line"></span><br><span class="line"># check C++11 or C++0x support</span><br><span class="line">include(CheckCCompilerFlag)</span><br><span class="line">include(CheckCXXCompilerFlag)</span><br><span class="line">CHECK_CXX_COMPILER_FLAG(&quot;-std=c++11&quot; COMPILER_SUPPORTS_CXX11)</span><br><span class="line">CHECK_CXX_COMPILER_FLAG(&quot;-std=c++0x&quot; COMPILER_SUPPORTS_CXX0X)</span><br><span class="line">if(COMPILER_SUPPORTS_CXX11)</span><br><span class="line">   set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11&quot;)</span><br><span class="line">   add_definitions(-DCOMPILEDWITHC11)</span><br><span class="line">   message(STATUS &quot;Using flag -std=c++11.&quot;)</span><br><span class="line">elseif(COMPILER_SUPPORTS_CXX0X)</span><br><span class="line">   set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++0x&quot;)</span><br><span class="line">   add_definitions(-DCOMPILEDWITHC0X)</span><br><span class="line">   message(STATUS &quot;Using flag -std=c++0x.&quot;)</span><br><span class="line">else()</span><br><span class="line">   message(FATAL_ERROR &quot;The compiler $&#123;CMAKE_CXX_COMPILER&#125; has no C++11 support. Please use a different C++ compiler.&quot;)</span><br><span class="line">endif()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">find_package(Eigen3 REQUIRED)</span><br><span class="line">find_package(Pangolin REQUIRED)</span><br><span class="line">find_package(Sophus REQUIRED)</span><br><span class="line"></span><br><span class="line">include_directories(</span><br><span class="line">    $&#123;EIGEN3_SOURCE_DIR&#125;</span><br><span class="line">    $&#123;Pangolin_INCLUDE_DIR&#125;</span><br><span class="line">    $&#123;Sophus_INCLUDE_DIR&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">add_executable(trajectorydrawing draw_trajectory.cpp)</span><br><span class="line">target_link_libraries(trajectorydrawing $&#123;EIGEN3_LIBRARIES&#125; $&#123;Pangolin_LIBRARIES&#125; $&#123;Sophus_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure><p>读取数据的代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">// 第一种方法，用fstream的getline分行读取</span><br><span class="line"></span><br><span class="line">    ifstream fin(trajectory_file); //从文件中读取数据</span><br><span class="line">    if(!fin.is_open())&#123;</span><br><span class="line">        cout&lt;&lt;&quot;No &quot;&lt;&lt;trajectory_file&lt;&lt;endl;</span><br><span class="line">        return 0;</span><br><span class="line">    &#125;</span><br><span class="line">    double t,tx,ty,tz,qx,qy,qz,qw;</span><br><span class="line">    string line;</span><br><span class="line">    while(getline(fin,line))</span><br><span class="line">    &#123;</span><br><span class="line">       istringstream record(line); //从string读取数据</span><br><span class="line">       record&gt;&gt;t&gt;&gt;tx&gt;&gt;ty&gt;&gt;tz&gt;&gt;qx&gt;&gt;qy&gt;&gt;qz&gt;&gt;qw;</span><br><span class="line">       Eigen::Vector3d p(tx,ty,tz);</span><br><span class="line">       Eigen::Quaterniond q = Eigen::Quaterniond(qw,qx,qy,qz).normalized();</span><br><span class="line">       Sophus::SE3 SE3_qp(q,p);</span><br><span class="line">       poses.push_back(SE3_qp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //第二种方法</span><br><span class="line"></span><br><span class="line">    // ifstream in(trajectory_file);//创建输入流</span><br><span class="line"></span><br><span class="line">    // if(!in)&#123;</span><br><span class="line">    //     cout&lt;&lt;&quot;open posefile failture!!!&quot;&lt;&lt;endl;</span><br><span class="line">    //     return 0;</span><br><span class="line">    // &#125;</span><br><span class="line"></span><br><span class="line">    // for(int i=0; i&lt;620; i++)&#123;</span><br><span class="line"></span><br><span class="line">    //     double data[8]=&#123;0&#125;;</span><br><span class="line">    //     for(auto&amp; d:data) in&gt;&gt;d;//按行依次去除数组中的值</span><br><span class="line"></span><br><span class="line">    //     Eigen::Quaterniond q(data[7], data[8], data[5], data[6]);</span><br><span class="line">    //     Eigen::Vector3d t(data[1], data[2], data[3]);</span><br><span class="line">    //     Sophus::SE3 SE3(q,t);</span><br><span class="line">    //     poses.push_back(SE3);</span><br><span class="line"></span><br><span class="line">    // &#125;</span><br><span class="line">    // end your code here</span><br></pre></td></tr></table></figure><p>生成的轨迹图如下：</p><p><img src="/2019/03/26/visual-SLAM-by-Gaoxiang-3/trajectory.png" alt="trajectory"></p><h3 id="轨迹的误差"><a href="#轨迹的误差" class="headerlink" title="轨迹的误差"></a>轨迹的误差</h3><p>CMakeLists.txt文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line">project(trajectory_error)</span><br><span class="line"></span><br><span class="line">set(CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br><span class="line"></span><br><span class="line"># check C++11 or C++0x support</span><br><span class="line">include(CheckCCompilerFlag)</span><br><span class="line">include(CheckCXXCompilerFlag)</span><br><span class="line">CHECK_CXX_COMPILER_FLAG(&quot;-std=c++11&quot; COMPILER_SUPPORTS_CXX11)</span><br><span class="line">CHECK_CXX_COMPILER_FLAG(&quot;-std=c++0x&quot; COMPILER_SUPPORTS_CXX0X)</span><br><span class="line">if(COMPILER_SUPPORTS_CXX11)</span><br><span class="line">   set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11&quot;)</span><br><span class="line">   add_definitions(-DCOMPILEDWITHC11)</span><br><span class="line">   message(STATUS &quot;Using flag -std=c++11.&quot;)</span><br><span class="line">elseif(COMPILER_SUPPORTS_CXX0X)</span><br><span class="line">   set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++0x&quot;)</span><br><span class="line">   add_definitions(-DCOMPILEDWITHC0X)</span><br><span class="line">   message(STATUS &quot;Using flag -std=c++0x.&quot;)</span><br><span class="line">else()</span><br><span class="line">   message(FATAL_ERROR &quot;The compiler $&#123;CMAKE_CXX_COMPILER&#125; has no C++11 support. Please use a different C++ compiler.&quot;)</span><br><span class="line">endif()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">find_package(Eigen3 REQUIRED)</span><br><span class="line">find_package(Pangolin REQUIRED)</span><br><span class="line">find_package(Sophus REQUIRED)</span><br><span class="line"></span><br><span class="line">include_directories(</span><br><span class="line">    $&#123;EIGEN3_SOURCE_DIR&#125;</span><br><span class="line">    $&#123;Pangolin_INCLUDE_DIR&#125;</span><br><span class="line">    $&#123;Sophus_INCLUDE_DIR&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">add_executable(trajectory_error trajectory_error.cpp)</span><br><span class="line">target_link_libraries(trajectory_error $&#123;EIGEN3_LIBRARIES&#125; $&#123;Pangolin_LIBRARIES&#125; $&#123;Sophus_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure><p>trajectory_error.cpp文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;sophus/se3.h&gt;</span><br><span class="line">#include &lt;string&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;fstream&gt;</span><br><span class="line">#include &lt;cmath&gt;</span><br><span class="line">#include &lt;pangolin/pangolin.h&gt;</span><br><span class="line">#include &lt;Eigen/Core&gt;</span><br><span class="line">#include &lt;Eigen/Geometry&gt;</span><br><span class="line"> </span><br><span class="line">using namespace std;</span><br><span class="line">using namespace Eigen;</span><br><span class="line"> </span><br><span class="line">void ReadData(string FileName ,vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; &amp;poses);</span><br><span class="line">double ErrorTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g,</span><br><span class="line">        vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e);</span><br><span class="line">void DrawTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g,</span><br><span class="line">        vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e);</span><br><span class="line"> </span><br><span class="line">int main(int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">    string GroundFile = &quot;./groundtruth.txt&quot;;</span><br><span class="line">    string ErrorFile = &quot;./estimated.txt&quot;;</span><br><span class="line">    double trajectory_error_RMSE = 0;</span><br><span class="line">    vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g;</span><br><span class="line">    vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e;</span><br><span class="line"> </span><br><span class="line">    ReadData(GroundFile,poses_g);</span><br><span class="line">    ReadData(ErrorFile,poses_e);</span><br><span class="line">    trajectory_error_RMSE = ErrorTrajectory(poses_g, poses_e);</span><br><span class="line">    cout&lt;&lt;&quot;trajectory_error_RMSE = &quot;&lt;&lt; trajectory_error_RMSE&lt;&lt;endl;</span><br><span class="line">    DrawTrajectory(poses_g,poses_e);</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">/***************************读取文件的数据，并存储到vector类型的pose中**************************************/</span><br><span class="line">void ReadData(string FileName ,vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; &amp;poses)</span><br><span class="line">&#123;</span><br><span class="line">    ifstream fin(FileName);  //从文件中读取数据</span><br><span class="line">   //这句话一定要加上，保证能够正确读取文件。如果没有正确读取，结果显示-nan</span><br><span class="line">    if(!fin.is_open())&#123;</span><br><span class="line">        cout&lt;&lt;&quot;No &quot;&lt;&lt;FileName&lt;&lt;endl;</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line">    double t,tx,ty,tz,qx,qy,qz,qw;</span><br><span class="line">    string line;</span><br><span class="line">    while(getline(fin,line)) </span><br><span class="line">    &#123;</span><br><span class="line">        istringstream record(line);    //从string读取数据</span><br><span class="line">        record &gt;&gt; t &gt;&gt; tx &gt;&gt; ty &gt;&gt; tz &gt;&gt; qx &gt;&gt; qy &gt;&gt; qz &gt;&gt; qw;</span><br><span class="line">        Eigen::Vector3d p(tx, ty, tz);</span><br><span class="line">        Eigen::Quaterniond q = Eigen::Quaterniond(qw, qx, qy, qz).normalized();  //四元数的顺序要注意</span><br><span class="line">        Sophus::SE3 SE3_qp(q, p);</span><br><span class="line">        poses.push_back(SE3_qp);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">/*******************************计算轨迹误差*********************************************/</span><br><span class="line">double ErrorTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g,</span><br><span class="line">        vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e )</span><br><span class="line">&#123;</span><br><span class="line">    double RMSE = 0;</span><br><span class="line">    Matrix&lt;double ,6,1&gt; se3;</span><br><span class="line">    vector&lt;double&gt; error;</span><br><span class="line">    for(int i=0;i&lt;poses_g.size();i++)&#123;</span><br><span class="line">        se3=(poses_g[i].inverse()*poses_e[i]).log();  //这里的se3为向量形式，求log之后是向量形式</span><br><span class="line">        //cout&lt;&lt;se3.transpose()&lt;&lt;endl;</span><br><span class="line">        error.push_back( se3.squaredNorm() );  //二范数</span><br><span class="line">       // cout&lt;&lt;error[i]&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    for(int i=0; i&lt;poses_g.size();i++)&#123;</span><br><span class="line">        RMSE += error[i];</span><br><span class="line">    &#125;</span><br><span class="line">    RMSE /= double(error.size());</span><br><span class="line">    RMSE = sqrt(RMSE);</span><br><span class="line">    return RMSE;</span><br><span class="line">&#125;</span><br><span class="line">/*****************************绘制轨迹*******************************************/</span><br><span class="line">void DrawTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g,</span><br><span class="line">        vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e) &#123;</span><br><span class="line">    if (poses_g.empty() || poses_e.empty()) &#123;</span><br><span class="line">        cerr &lt;&lt; &quot;Trajectory is empty!&quot; &lt;&lt; endl;</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    // create pangolin window and plot the trajectory</span><br><span class="line">    pangolin::CreateWindowAndBind(&quot;Trajectory Viewer&quot;, 1024, 768);  //创建一个窗口</span><br><span class="line">    glEnable(GL_DEPTH_TEST);   //启动深度测试</span><br><span class="line">    glEnable(GL_BLEND);       //启动混合</span><br><span class="line">    glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);//混合函数glBlendFunc( GLenum sfactor , GLenum dfactor );sfactor 源混合因子dfactor 目标混合因子</span><br><span class="line"> </span><br><span class="line">    pangolin::OpenGlRenderState s_cam(</span><br><span class="line">            pangolin::ProjectionMatrix(1024, 768, 500, 500, 512, 389, 0.1, 1000),</span><br><span class="line">            pangolin::ModelViewLookAt(0, -0.1, -1.8, 0, 0, 0, 0.0, -1.0, 0.0) //对应的是gluLookAt,摄像机位置,参考点位置,up vector(上向量)</span><br><span class="line">    );</span><br><span class="line"> </span><br><span class="line">    pangolin::View &amp;d_cam = pangolin::CreateDisplay()</span><br><span class="line">            .SetBounds(0.0, 1.0, pangolin::Attach::Pix(175), 1.0, -1024.0f / 768.0f)</span><br><span class="line">            .SetHandler(new pangolin::Handler3D(s_cam));</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    while (pangolin::ShouldQuit() == false) &#123;</span><br><span class="line">        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</span><br><span class="line"> </span><br><span class="line">        d_cam.Activate(s_cam);</span><br><span class="line">        glClearColor(1.0f, 1.0f, 1.0f, 1.0f);</span><br><span class="line"> </span><br><span class="line">        glLineWidth(2);</span><br><span class="line">        for (size_t i = 0; i &lt; poses_g.size() - 1; i++) &#123;</span><br><span class="line">            glColor3f(1 - (float) i / poses_g.size(), 0.0f, (float) i / poses_g.size());</span><br><span class="line">            glBegin(GL_LINES);</span><br><span class="line">            auto p1 = poses_g[i], p2 = poses_g[i + 1];</span><br><span class="line">            glVertex3d(p1.translation()[0], p1.translation()[1], p1.translation()[2]);</span><br><span class="line">            glVertex3d(p2.translation()[0], p2.translation()[1], p2.translation()[2]);</span><br><span class="line">            glEnd();</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        for (size_t j = 0; j &lt; poses_e.size() - 1; j++) &#123;</span><br><span class="line">            //glColor3f(1 - (float) j / poses_e.size(), 0.0f, (float) j / poses_e.size());</span><br><span class="line">            glColor3f(1.0f, 1.0f, 0.f);//为了区分第二条轨迹，用不同的颜色代替,黄色</span><br><span class="line">            glBegin(GL_LINES);</span><br><span class="line">            auto p1 = poses_e[j], p2 = poses_e[j + 1];</span><br><span class="line">            glVertex3d(p1.translation()[0], p1.translation()[1], p1.translation()[2]);</span><br><span class="line">            glVertex3d(p2.translation()[0], p2.translation()[1], p2.translation()[2]);</span><br><span class="line">            glEnd();</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        pangolin::FinishFrame();</span><br><span class="line">        usleep(5000);   // sleep 5 ms</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/2019/03/26/visual-SLAM-by-Gaoxiang-3/trajectory_error.png" alt="error"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​&lt;/p&gt;&lt;p&gt;本次课程主要研究李群和李代数(Lie Group, Lie Algebra)，主要的目的是为了能够相机得旋转和平移进行微调。因为相机的运动估计可能不准确，而无法对旋转矩阵加上微小量之后依然是旋转矩阵（旋转矩阵无法定义加法，如果用四元数，必须是单位四元数，那么也无法定义加法）。李群李代数与后面的优化，流形都会有很大的联系。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;在视觉SLAM中，相机的位姿是未知的，而我们需要解决什么样的相机位姿最符合当前观测数据这样的问题。一种典型的方式是把其构建成一个优化问题，求解最优的$R$和$t$，使得误差最小化。&lt;/p&gt;&lt;p&gt;由于旋转矩阵自身带有约束，即必须正交且行列式为1，因此作为优化变量会引入额外的约束，使得优化变得困难。而通过李群李代数的转换关系，可以顺利求导，把位姿估计变成无约束的优化问题。&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;群&quot;&gt;&lt;a href=&quot;#群&quot; class=&quot;headerlink&quot; title=&quot;群&quot;&gt;&lt;/a&gt;群&lt;/h2&gt;&lt;p&gt;群(Group)是一种集合加上一种运算的代数结构，满足封闭性，结合律，&lt;strong&gt;幺元&lt;/strong&gt;，逆。其中幺元可以认为是单位元，就是与其他元素作用不改变这个元素，逆是元素和和它的逆进行运算后得到了幺元。&lt;/p&gt;&lt;p&gt;三维旋转矩阵构成了三维正交群(special orthogonal group)&lt;/p&gt;&lt;script type=&quot;math/tex;mode=display&quot;&gt;SO(3) = \left \{ R \in {\mathbb R}^{3 \times 3} | RR^{T} =I, \det (R)=1 \right \}&lt;/script&gt;&lt;p&gt;三维变换矩阵构成了特殊欧式群(special euclidean group)&lt;/p&gt;&lt;script type=&quot;math/tex;mode=display&quot;&gt;SE(3) = 
\left \{
T= 
\begin{bmatrix}
R &amp; t\\
O^{T} &amp; 1
\end{bmatrix}
\in {\mathbb R}^{4 \times 4} |
R \in SO(3), t \in {\mathbb R}^{3} 
\right \}&lt;/script&gt;&lt;p&gt;旋转矩阵集合与矩阵乘法构成群，变换矩阵集合与矩阵乘法也构成了群，因此称它们为旋转矩阵群和变换矩阵群。&lt;/p&gt;&lt;p&gt;群结构保证了在群上的运算具有良好的性质。&lt;/p&gt;
    
    </summary>
    
      <category term="科研记录" scheme="http://densecollections.top/categories/%E7%A7%91%E7%A0%94%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="visual SLAM" scheme="http://densecollections.top/tags/visual-SLAM/"/>
    
      <category term="Linux" scheme="http://densecollections.top/tags/Linux/"/>
    
      <category term="C++11" scheme="http://densecollections.top/tags/C-11/"/>
    
      <category term="Computer vision" scheme="http://densecollections.top/tags/Computer-vision/"/>
    
      <category term="Sophus" scheme="http://densecollections.top/tags/Sophus/"/>
    
  </entry>
  
  <entry>
    <title>visual SLAM by Gaoxiang(2)</title>
    <link href="http://densecollections.top/2019/03/12/visual-SLAM-by-Gaoxiang-2/"/>
    <id>http://densecollections.top/2019/03/12/visual-SLAM-by-Gaoxiang-2/</id>
    <published>2019-03-12T13:04:11.000Z</published>
    <updated>2020-02-03T02:28:47.264Z</updated>
    
    <content type="html"><![CDATA[<p>​</p><p>本次课程主要研究<strong>三维空间刚体运动</strong>，即visual slam的运动方程中的$x_{k}$如何表达。</p><h2 id="点与坐标系"><a href="#点与坐标系" class="headerlink" title="点与坐标系"></a>点与坐标系</h2><p>在2D情况下，物体可以通过两个坐标和一个旋转角进行表达，即$(x,y,\theta)$。</p><p>在3D情况下，物体是6自由度的，包括平移和旋转，每个都得用三个变量表达，可以认为3D情况是包含着3个2D的情况，旋转轴不同。</p><a id="more"></a><p>理清坐标系（参考系），点，向量，向量的坐标，运动变换之间的关系。相机会有相机坐标系，机器人会有机体坐标系（通常是运动的），空间会有世界坐标系（通常是固定的），这都是为了研究问题的方便，通过变换进行运动的表述。</p><p>熟悉向量的相关运算规则，比如，向量的加减法，向量的内积和外积。以及向量和矩阵的关系。</p><script type="math/tex;mode=display">\begin{align*}& 内积： \bf{a} \cdot \bf{b} =\bf{a^{T}} \bf{b}=\sum_{i=1}^{n}=|\bf{a}||\bf{b}| \cos \left< \bf{a}, \bf{b}\right> \\&外积： \bf{a} \times \bf{b}=\begin{bmatrix}\bf{i} & \bf{j} & \bf{k} \\a_{1} & a_{2} & a_{3} \\b_{1} & b_{2} & b_{3}\end{bmatrix}=\begin{bmatrix}a_{2}b_{3}-a_{3}b_{2}\\a_{3}b_{1}-a_{1}b_{3}\\a_{1}b_{2}-a_{2}b_{1}\end{bmatrix}=\begin{bmatrix}0 & -a_{3} & a_{2}\\a_{3} & 0 & -a_{1} \\-a_{2} & a_{1} & 0\end{bmatrix}\bf{b} \triangleq \bf{a} ^{\wedge} \bf{b}\end{align*}</script><p>建立了坐标系之后，如何表示同一个向量在不同坐标系之间的坐标，坐标系之间的变换关系又该如何描述。坐标系之间的变换可以分解成坐标系原点之间之间的平移和坐标轴之间的旋转，那么可以用一个平移向量和旋转矩阵来描述这样的变换。我们知道矩阵可以表述坐标系之间的变换，对于三维空间而言，可以用一个$4 \times 4$的矩阵来描述三维坐标系之间的变换，这也就是平移向量和旋转矩阵的合成矩阵，具体后面会细说。</p><h2 id="旋转矩阵"><a href="#旋转矩阵" class="headerlink" title="旋转矩阵"></a>旋转矩阵</h2><p>一个向量在坐标系进行旋转后不变，因此可以通过此推导出旋转矩阵$R$的表达式（即用变换前后的坐标系的基向量进行表述）。</p><script type="math/tex;mode=display">\begin{bmatrix}a_{1}\\a_{2}\\a_{3}\end{bmatrix}=\begin{bmatrix}e_{1}^{T}e_{1}^{'} & e_{1}^{T}e_{2}^{'} & e_{1}^{T}e_{3}^{'}\\e_{2}^{T}e_{1}^{'} & e_{2}^{T}e_{2}^{'} & e_{2}^{T}e_{3}^{'}\\e_{3}^{T}e_{1}^{'} & e_{3}^{T}e_{2}^{'} & e_{3}^{T}e_{3}^{'}\end{bmatrix}\begin{bmatrix}a_{1}^{'}\\a_{2}^{'}\\a_{3}^{'}\end{bmatrix}\triangleq R {\bf a}^{'}</script><p>旋转矩阵是正交矩阵（$RR^{T}=I$），且行列式为1（$\det (R)=1$）。旋转矩阵属于特殊正交群special orthogonal group，即：</p><script type="math/tex;mode=display">SO(n) = \lbrace R \in \mathbb{R}^{n \times n} | RR^{T}=I, \det(R)=1 \rbrace</script><p>对一个旋转矩阵进行转置就描述了一个相反方向的旋转。</p><p>欧拉旋转定理（Euler’s rotation theorem）：刚体在三维空间里的一般运动，可分解为刚体上方某一点的平移，以及绕经过此点的旋转轴的转动。</p><script type="math/tex;mode=display">{\bf a^{'}}= R \bf{a} + \bf{t}</script><p>齐次形式（homogeneous）来更方便地表达变换，因为加上平移不满足线性性。定义变换矩阵$T$</p><script type="math/tex;mode=display">\begin{bmatrix}a^{'}\\1\end{bmatrix}=\begin{bmatrix}R & \bf{t}\\\bf{ 0^{T} } & 1\end{bmatrix}\begin{bmatrix}a \\1\end{bmatrix}\triangleq T\begin{bmatrix}a\\1\end{bmatrix}</script><p>齐次坐标认为其乘以任意非零常数时仍表达同一个坐标（归一化）。 变换矩阵的集合称为特殊欧式群special Euclidean Group:</p><script type="math/tex;mode=display">SE(3)=\lbrace T=\begin{bmatrix}R & \bf{t}\\\bf{ 0^{T}} & 1\end{bmatrix}\in \mathbb{R}^{4 \times 4} |R \in SO(3), \bf{t} \in \mathbb{R}^{3} \rbrace</script><script type="math/tex;mode=display">T^{-1}=\begin{bmatrix}R^{T} & -R^{T}{\bf t}\\{\bf 0^{T}} & 1\end{bmatrix}</script><p>定义了变化矩阵后，多次变换可以直接对变换矩阵直接相乘即可。</p><h2 id="旋转向量与欧拉角"><a href="#旋转向量与欧拉角" class="headerlink" title="旋转向量与欧拉角"></a>旋转向量与欧拉角</h2><p>一般来说视觉SLAM有旋转矩阵和平移向量就够了，一般也是用这样的方式表达。为了进行拓展，比如在航迹推算和组合导航中，通常用四元数来表述物体姿态。</p><p>旋转矩阵有9个元素但仅表示三个自由度，比较冗余，因此引入旋转向量(rotation vector)，方向为旋转轴，长度为转过的角度，又称为角轴或轴角（angle axis）。</p><p>旋转向量只有三个量，无约束，更加直观，但是旋转轴一般不容易得知，其中旋转向量和旋转矩阵的关系可以通过罗德里格斯公式得出（Rodrigues’ s Formula）：（假设旋转轴为$\bf{n}$，旋转角为$\theta$）</p><script type="math/tex;mode=display">\begin{align*}& R = \cos \theta {\bf I}+(1-\cos \theta){\bf n} {\bf n^{T}}+\sin \theta {\bf n^{\wedge}}\\& \theta = \arccos(\frac{tr(R)-1}{2}) \\& R{\bf n}=\bf{n} \quad特征值为1的特征向量\end{align*}</script><p>欧拉角（Euler Angles）将旋转分解成三个方向上的转动，最常见的是Z-Y-X，即yaw-pitch-roll（偏航-俯仰-横滚），不同领域习惯不同。但是欧拉角会有万向锁问题（<a href="https://krasjet.github.io/quaternion/bonus_gimbal_lock.pdf" target="_blank" rel="noopener">gimbal lock</a>)，会在特定值丢失一个自由度，存在奇异性问题，因此欧拉角不适合插值和迭代。实际上，仅用三个实数表达旋转时，会不可避免地存在奇异性问题。视觉SLAM中一般不用欧拉角表达姿态，主要在人机交互中用。</p><h2 id="四元数（Quaternion）"><a href="#四元数（Quaternion）" class="headerlink" title="四元数（Quaternion）"></a>四元数（Quaternion）</h2><p>2D情况下，可以用单位复数表达旋转，$z=x+iy=\rho e^{i \theta}$，乘$i$代表转转90度。</p><p>3D情况下，类似地，四元数可作为复数地扩充。</p><p>四元数有三个虚部和一个实部，${\bf q}=q_{0}+q_{1}i+q_{2}j+q_{3}k$，虚部之间满足关系（自己和自己运算像复数，自己和别人运算像叉乘）:</p><script type="math/tex;mode=display">\begin{cases}i^{2}=j^{2}=k^{2}=-1 \\ij=k,jk=-k \\jk=i.kj=-i \\ki=j,ik=-j\end{cases}</script><p>单位四元数可以表达旋转，${\bf q}=q_{0}+q_{1}i+q_{2}j+q_{3}k=[s, {\bf v}], s=q_{0}\in \mathbb{R}, {\bf v}=[q_{1}, q_{2}, q_{3}]^{T}\in \mathbb{R}^{3}$，四元数有以下地运算规则：</p><script type="math/tex;mode=display">{\bf q}_{a}+{\bf q}_{b}=[s_{a}\pm s_{b}, {\bf v}_{a}\pm {\bf v}_{b}]\\\begin{align*}{\bf q}_{a}{\bf q}_{b}= &s_{a}s_{b}-x_{a}x_{b}-y_{a}y_{b}-z_{a}z_{b}\\& +(s_{a}x_{b}+x_{a}s_{b}+y_{a}z_{b}-z_{a}y_{b})i\\& +(s_{a}y_{b}-x_{a}z_{b}+y_{a}s_{b}+z_{a}x_{b})j\\& +(s_{a}z_{b}+x_{a}y_{b}-y_{b}x_{a}+z_{a}s_{b})k \end{align*}\\{\bf q}_{a}{\bf q}_{b}= [s_{a}s_{b}-{\bf v}_{a}^{T}{\bf v}_{b}, s_{a}{\bf v}_{b}+s_{b}{\bf v}_{a}+{\bf v}_{a} \times {\bf v}_{b}] \\{\bf q}^{*}=s_{a}-x_{a}i-y_{a}j-z_{a}k=[s_{a}, -{\bf v}_{a}]\\\left\| {\bf q}_{a} \right\|= \sqrt{s_{a}^{2}+x_{a}^{2}+y_{a}^{2}+z_{a}^{2}} \\{\bf q}^{-1}={\bf q}^{*}/ \left\| {\bf q} \right\| ^{2}\\k{\bf q}=[ks, k{\bf v}]\\{\bf q}_{a} \cdot {\bf q}_{b}=s_{a}s_{b}+x_{a}x_{b}i+y_{a}y_{b}j+z_{a}z_{b}k</script><p>四元数到角轴：${\bf q}=[\cos \frac{\theta}{2},n_{x}\sin \frac{\theta}{2}, n_{y}\sin \frac{\theta}{2}, n_{z}\sin \frac{\theta}{2}]^{T}$。</p><p>角轴到四元数：$\theta =2 \arccos q_{0}, [n_{x}, n_{y}, n_{z}]^{T} = [q_{1}, q_{2}, q_{3}]^{T}/ \sin \frac{\theta}{2}$</p><p>三维点$p(x,y,z)$经过一次以${\bf q}$表示的旋转后，得到了$p^{‘}$，${\bf p}=[0,x,y,z]=[0, {\bf v}]$，旋转之后的关系为${\bf p}^{‘}={\bf q} {\bf p} {\bf q}^{-1}$。四元数相比与角轴和欧拉角，形式上更加紧凑，也无奇异性。</p><p><strong>值得注意的是，${\bf q}$和$-{\bf q}$表示同一个旋转。</strong></p><hr><p>学习四元数的一个最直观的问题就是为什么三个变量来描述三维旋转，诸如欧拉角会出现奇异性的情况，而用四个变量的四元数就不会？也就说用高维的东西描述低维的东西更加有效。</p><p>知乎上有相关的<a href="https://www.zhihu.com/question/20962240/answer/33438846" target="_blank" rel="noopener">回答</a>，写得还算不错，比较直观。不过依旧没有解决为什么三个变量描述三维旋转会出现奇异性的现象。</p><blockquote><p>利用四元数来对三维点的旋转进行操作，是通过纯四元数来进行的，即变换后的点可以表示为${\bf qwq^{-1} }$，这里的问题是为什么这种形式。（注意，这里的四元数是单位四元数）</p><p>汉密尔顿定义的性质：</p><p>1.运算产生的结果也要是三维向量<br>2.存在一个元运算，任何三维向量进行元运算的结果就是其本身<br>3.对于任何一个运算，都存在一个逆运算，这两个运算的积是元运算<br>4.运算满足结合律</p><p>其实，四元数有四个变量，完全可以被看作一个四维向量。单位四元数（norm=1）则存在于四维空间的一个球面上。${\bf q}_{a} {\bf q}_{b}$，四元数${\bf q}_{a}$乘以四元数${\bf q}_{b}$其实看作（1）对${\bf q}_{a}$进行${\bf q}_{b}$左旋转，或者（2）对${\bf q}_{b}$进行${\bf q}_{a}$右旋转。所以从始至终，四元数定义的都是四维旋转，而不是三维旋转！任意的四维旋转都可以唯一的拆分为一个左旋转和一个右旋转，表达出来就是${\bf q}_{L}{\bf p}{\bf q}_{R}$。这里，我们对四元数（四维向量）${\bf p}$进行了一个${\bf q}_{L}$左旋转和一个${\bf q}_{R}$右旋转。结果当然是一个四元数，符合性质1。这个运算也同时符合性质2，3，4。</p><p>为了进行三维旋转运算，汉密尔顿首先在四维空间里划出了一块三维空间。汉密尔顿定义了一种纯四元数（pure quaternion），其表达式为${\bf q}_{w}=(0,w_{x},w_{y},w_{z})$。纯四元数第一项为零，它存在于四维空间的三维超平面上，与三维空间中的三维向量一一对应。然后，就有了我们常见的${\bf q} {\bf q}_{w} {\bf q}^{*}$这种左乘单位四元数，右乘其共轭的表达式。这个运算形式是为了限制其运算结果所在的空间。简单的说，当对一个三维向量进行三维旋转后，我们希望得到的是一个三维向量，而不是四维向量。</p><p>这也就解释了为什么四元数对应于角轴的关系式中是$\frac{\theta}{2}$，而不是$\theta$，这是因为${\bf q}$做的就是一个$\frac{\theta}{2}$的旋转，而${\bf q}^{-1}$也做了一个$\frac{\theta }{2}$的旋转。我们进行了两次旋转，而不是一次，这两次旋转的结果是一个旋转角为$\theta$的旋转。</p></blockquote><p>此外，还有一点是，四元数可以用2x2复数矩阵（特殊酉群）和4x4矩阵来描述，但是四元数之间不满足乘法交换律，即${\bf q}_{1} {\bf q}_{2} \ne {\bf q}_{2} {\bf q}_{1}$，但是二维平面里的复数相乘满足乘法交换律，这里我粗略地理解是，二维旋转只有角度，没有轴的概念，只有按照什么顺序旋转多少角度，因此先转$\theta$，再转$\alpha$，与先转$\alpha$，再转$\theta$的结果是一样的，但是四元数相乘不是，每次转都是有个旋转轴的，因此不可交换。</p><p>有关四元数，旋转，群的理解和证明，可以参考这篇<a href="https://krasjet.github.io/quaternion/quaternion.pdf" target="_blank" rel="noopener">文章</a></p><script type="math/tex;mode=display">\begin {align*}{\bf v}^{'} & = {\bf v}_{||}^{'} + {\bf v}_{\bot}^{'}={\bf v}_{||} + {\bf q} {\bf v}_{\bot}\\             & = {\bf p} {\bf p}^{-1} {\bf v}_{\bot} + {\bf p} {\bf p} {\bf v}_{\bot}\\            & = {\bf p} {\bf p}^{*} {\bf v}_{\bot} + {\bf p} {\bf p} {\bf v}_{\bot}\\            & = {\bf p} {\bf v}_{||} {\bf p}^{*} + {\bf p} {\bf v}_{\bot} {\bf p}^{*}\\            & = {\bf p} ({\bf v}_{||}+{\bf v}_{\bot}) {\bf p}^{*}\\            & = {\bf p} {\bf v} {\bf p}^{*}\\            & = {\bf v}_{||} + {\bf p}^{2} {\bf v}_{\bot}\end{align*}\\{\bf q}=[\cos \theta, \sin \theta ({\bf u})], {\bf p}=[\cos \frac{\theta}{2}, \sin \frac{\theta}{2} ({\bf u})],{\bf q}={\bf p}^{2}</script><p><strong>对于平行的分量，变换完全抵消，对于垂直的分量，施加两次变换，这也就是为什么是</strong>$\frac {\theta}{2}$。</p><p><strong>四元数与群：</strong></p><p>单位四元数与 3D 旋转有一个<strong>2对1满射同态关系</strong>，或者说单位四元数<strong>双倍覆盖了3D旋转</strong>。因为这个映射是满射，我们可以说所有的单位四元数都对应着一个 3D 旋转。或者说，一个四维单位超球面（也叫做$\mathbb S^{3}$）上任意一点所对应的四元数（$∥q∥ = 1$）都对应着一个 3D 旋转。</p><p>四元数，旋转矩阵，群，geometric algebra都有着紧密的联系，因此可以先做好相关的功课，这里列出几篇阅读材料：</p><p><a href="https://alistairsavage.ca/mat4144/notes/MAT4144-5158-LieGroups.pdf" target="_blank" rel="noopener">Introduction to Lie Groups</a></p><p><a href="http://www.jaapsuter.com/geometric-algebra.pdf" target="_blank" rel="noopener">Geometric Algebra Primer</a></p><p><a href="https://www.youtube.com/watch?reload=9&amp;v=PNlgMPzj-7Q&amp;list=PLpzmRsG7u_gqaTo_vEseQ7U8KFvtiJY4K" target="_blank" rel="noopener">youtube geometric algebra</a></p><blockquote><p><strong>A Quaternion is a scalar plus a bivector.</strong></p><p>Apart from the fact that quaternions have four components, there is nothing four-dimensional or imaginary about a quaternion. The first component is a scalar, and the other three components form the bivector-plane relative to which the rotation is performed.</p></blockquote><p>这似乎解释了之前我的疑问，为什么要用四个变量来描述三维，就是用得用高维的解释低维的，这里指出了四元数拥有四个变量，但是不是指四维或者虚数，而是通过标量和bivector的组合来描述旋转，几何代数的这种形式统一了不同维度的旋转，因此比较优雅。不过更深层次的东西和理解涉及到比较多的数学知识和空间理解，我暂时还达不到。。。。</p><hr><hr><h2 id="作业与实践"><a href="#作业与实践" class="headerlink" title="作业与实践"></a>作业与实践</h2><h3 id="熟悉Eigen运算"><a href="#熟悉Eigen运算" class="headerlink" title="熟悉Eigen运算"></a>熟悉Eigen运算</h3><p>对于线性方程组$Ax=b$，其中$A$是方阵。</p><p>1).什么条件下，$x$有解且唯一？</p><p>当矩阵$A$可逆的时候，也就是此时矩阵满秩。</p><p>2).高斯消元法原理</p><p>高斯消元法其实就是对系数矩阵作初等行变换，这不改变方程的解，将系数矩阵化成上三角矩阵的形式，然后从下往上依次解出方程组的每个分量解（回代）。如果是针对列主元的高斯消元法，需要加入方程组的右侧值，与系数矩阵组成增广矩阵，进而求解。</p><p>3). QR分解的原理</p><p>任意的$A \in {\bf C}^{n \times n}$都可以进行$QR$分解，即$A=QR$，$Q$为n阶酉矩阵($QQ^<br>{H}=I$)，$R$为n阶上三角矩阵。</p><p>$QR$分解与Gram-Schmidt正交化有关，即将$A$进行Gram-Schmidt正交化，化为$Q$，然后求出R。Q的列向量是A的列空间的标准正交基，R是一个非奇异可逆的上三角矩阵，即将矩阵每个列作为一个基本单元，将其化为正交的基向量与在这个基向量上的投影长度的积。</p><p>4).Cholesky分解的原理</p><p>Cholesky分解其实是矩阵Doolittle分解(三角分解的特例)的特例。三角分解是是将方针分解成同阶的下三角阵和上三角阵，其中上三角阵的主对角线元素全为1则为Doolittle分解。如果矩阵$A$既是方阵又是Hermite正定阵时($A=A^{H}$,且特征值全为正数)，则存在唯一分解$A=LL^{H}$，其中$L$是具有主对角元素为正数的下三角矩阵。</p><p>5).利用$QR$和Cholesky分解法分解随机矩阵$A \in {\bf C}^{100 \times 100}$求解$x$。</p><p>主要思路就是先用定义动态大小的矩阵，之后进行调用相关的函数处理。其中需要注意的是，Cholesky分解需要矩阵为正定阵，因此在矩阵定义上需要进行一些处理。</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">#include &lt;ctime&gt;</span><br><span class="line"></span><br><span class="line">// Eigen部分</span><br><span class="line">#include &lt;Eigen/Core&gt;</span><br><span class="line">//Eigen稠密矩阵的代数运算（逆和特征值等）</span><br><span class="line">#include &lt;Eigen/Dense&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main (int argc, char** argv)</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double,Eigen::Dynamic, Eigen::Dynamic&gt; matrix_dynamic; //Eigen固定大小矩阵最大支持到50</span><br><span class="line">Eigen::Matrix&lt;double,Eigen::Dynamic, Eigen::Dynamic&gt; matrix_A;</span><br><span class="line">Eigen::Matrix&lt;double,Eigen::Dynamic, 1&gt; x;</span><br><span class="line">Eigen::Matrix&lt;double,Eigen::Dynamic,1&gt; v_right;</span><br><span class="line"></span><br><span class="line">matrix_dynamic = Eigen::MatrixXd::Random(100,100); //随机化取值</span><br><span class="line"></span><br><span class="line">matrix_A = matrix_dynamic.transpose()*matrix_dynamic; //cholesky分解需要A为正定矩阵</span><br><span class="line"></span><br><span class="line">v_right = Eigen::MatrixXd::Random(100, 1); //方程右边的值随机取值</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//QR Decomposition</span><br><span class="line">clock_t time_stt = clock();</span><br><span class="line"></span><br><span class="line">x = matrix_A.colPivHouseholderQr().solve(v_right);</span><br><span class="line">cout&lt;&lt;&quot;the time used in QR decomposition is &quot;&lt;&lt; 1000* (clock() - time_stt)/(double) CLOCKS_PER_SEC&lt;&lt;&quot;ms&quot;&lt;&lt; endl;</span><br><span class="line">cout&lt;&lt;x&lt;&lt;endl;</span><br><span class="line"></span><br><span class="line">//Cholesky Decomposition</span><br><span class="line">time_stt = clock();</span><br><span class="line"></span><br><span class="line">x = matrix_A.llt().solve(v_right);</span><br><span class="line">cout&lt;&lt;&quot;the time used in Cholesky decomposition is &quot;&lt;&lt; 1000* (clock() - time_stt)/(double) CLOCKS_PER_SEC&lt;&lt;&quot;ms&quot;&lt;&lt; endl;</span><br><span class="line">cout&lt;&lt;x&lt;&lt;endl;</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CMakeLists.txt文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line"></span><br><span class="line">project(QR_cholesky)</span><br><span class="line"></span><br><span class="line">set(CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br><span class="line">set(CMAKE_CXX_FLAGS &quot;-O3&quot;) #Debug版会使用参数-g；Release版使用-O3 –DNDEBUG</span><br><span class="line"></span><br><span class="line">include_directories(&quot;/usr/include/eigen3&quot;)</span><br><span class="line"></span><br><span class="line">add_executable(QR_cholesky QR_cholesky.cpp)</span><br><span class="line">#eigen3都是头文件，不需要target_link_libraries</span><br></pre></td></tr></table></figure><h3 id="几何运算练习"><a href="#几何运算练习" class="headerlink" title="几何运算练习"></a>几何运算练习</h3><p>基本思想就是$T_{cw} p=p^{‘}$，或者$R p + t=p^{‘}$，注意这里是世界坐标系的点变换到相机坐标系。</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;cmath&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//Eigen几何模块</span><br><span class="line">#include &lt;Eigen/Core&gt;</span><br><span class="line">#include&lt;Eigen/Geometry&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main(int argc, char** argv)</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Quaterniond q1(0.55,0.3,0.2,0.2);</span><br><span class="line">q1 = q1.normalized();</span><br><span class="line"></span><br><span class="line">Eigen::Quaterniond q2(-0.1,0.3,-0.7,0.2);</span><br><span class="line">q2 = q2.normalized();</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; t1;</span><br><span class="line">t1 &lt;&lt; 0.7,1.1,0.2;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; t2;</span><br><span class="line">t2 &lt;&lt; -0.1,0.4,0.8;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; p1;</span><br><span class="line">p1 &lt;&lt; 0.5,-0.1,0.2;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; p2;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix &lt;double, 3,1&gt; p; //世界坐标系的点</span><br><span class="line"></span><br><span class="line">//利用变换矩阵的方法</span><br><span class="line"></span><br><span class="line">// Eigen::Isometry3d Tcw1 = Eigen::Isometry3d::Identity();//变换矩阵1</span><br><span class="line">// Tcw1.rotate(q1);</span><br><span class="line">// Tcw1.pretranslate(t1);</span><br><span class="line"></span><br><span class="line">// Eigen::Isometry3d Tcw2 = Eigen::Isometry3d::Identity();//变换矩阵2</span><br><span class="line">// Tcw2.rotate(q2);</span><br><span class="line">// Tcw2.pretranslate(t2);</span><br><span class="line"></span><br><span class="line">// p = Tcw1.inverse()*p1;</span><br><span class="line">// p2 = Tcw2*p;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//直接利用旋转矩阵和平移向量进行组合运算</span><br><span class="line">Eigen::Matrix&lt;double,3,3&gt; R1_inverse;</span><br><span class="line">R1_inverse = q1.matrix().inverse();</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;double,3,3&gt; R2;</span><br><span class="line">R2 = q2.matrix();</span><br><span class="line"></span><br><span class="line">p2 = R2 * R1_inverse *(p1 - t1) + t2;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cout &lt;&lt; &quot;p2 = &quot; &lt;&lt; p2.transpose() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CMakeLists.txt文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line">set (CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br><span class="line">set (CMAKE_CXX_FLAGS &quot;-O3&quot;)</span><br><span class="line"></span><br><span class="line">project(Geometryusing)</span><br><span class="line">include_directories(&quot;/usr/include/eigen3&quot;)</span><br><span class="line"></span><br><span class="line">add_executable(Geometryusing Geometryusing.cpp)</span><br><span class="line">#eigen3都是头文件，不需要target_link_libraries</span><br></pre></td></tr></table></figure><h3 id="旋转的表达"><a href="#旋转的表达" class="headerlink" title="旋转的表达"></a>旋转的表达</h3><p>1).旋转矩阵的正交性</p><p>坐标系中的某个单位正交基$(e_{1},e_{2},e_{3})$经过旋转变换后变为$(e^{‘}_{1},e^{‘}_{2},e^{‘}_{3})$，对于同一个向量在两个坐标系下的坐标分别为$a(a_{1},a_{2},a_{3})$和$(a^{‘}_{1},a^{‘}_{2},a^{‘}_{3})$，由于向量不会随着坐标系的旋转而发生变化，则有：</p><script type="math/tex;mode=display">\begin{bmatrix}e_{1} & e_{2} & e_{3}\end {bmatrix}\begin{bmatrix}a_{1} \\a_{2} \\a_{3}\end {bmatrix}=\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end {bmatrix}\begin{bmatrix}a^{'}_{1} \\a^{'}_{2} \\a^{'}_{3}\end {bmatrix}</script><p>等式两边同时左乘$[e^{T}_{1} \quad e^{T}_{2} \quad e^{T}_{3}]$，则得出旋转矩阵$R$为</p><script type="math/tex;mode=display">R=\begin{bmatrix}e^{T}_{1}\\e^{T}_{2} \\e^{T}_{3}\end{bmatrix}\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end{bmatrix}</script><p>则有：</p><script type="math/tex;mode=display">R^{T}R=\begin{bmatrix}e^{'}_{1}\\e^{'}_{2}\\e^{'}_{3}\end{bmatrix}\begin{bmatrix}e^{T}_{1} & e^{T}_{2} & e^{T}_{2}\end{bmatrix}\begin{bmatrix}e^{T}_{1}\\e^{T}_{2} \\e^{T}_{3}\end{bmatrix}\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end{bmatrix}=I</script><script type="math/tex;mode=display">\det R=\det(\begin{bmatrix}e^{T}_{1}\\e^{T}_{2} \\e^{T}_{3}\end{bmatrix}\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end{bmatrix})=det(\begin{bmatrix}e^{T}_{1}\\e^{T}_{2} \\e^{T}_{3}\end{bmatrix})det(\begin{bmatrix}e^{'}_{1} & e^{'}_{2} & e^{'}_{3}\end{bmatrix})=1</script><p>2).四元数的维度</p><p>易知$\varepsilon$是三维，$\eta$是一维的。</p><p>3).四元数相关证明(题目应该有错）</p><p>符号$x$和$\wedge$代表着反对称矩阵，代表着向量到反对称矩阵的变换，这是从叉乘引申而来的。即</p><script type="math/tex;mode=display">a^{\times}=(\begin{bmatrix}a_{1}\\a_{2}\\a_{3}\end{bmatrix})^{\times}=\begin{bmatrix}0 & -a_{3} & a_{2}\\a_{3} & 0 &-a_{1}\\-a_{2} & a_{1} & 0\end{bmatrix}</script><p>设$q_{1}=[x_{1},y_{1},z_{1},w_{1}]^{T}$,$q_{2}=[x_{2},y_{2},z_{2},w_{2}]^{T}$,</p><script type="math/tex;mode=display">\begin{align*}q_{1}q_{2} & = w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\\     & + (w_{1}x_{2}+x_{1}w_{2}+y_{1}z_{2}-z_{1}y_{2})i\\     & + (w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}+z_{1}x_{2})j\\     & + (w_{1}z_{2}+x_{1}y_{2}-y_{1}x_{2}+z_{1}w_{2})k\end{align*}</script><script type="math/tex;mode=display">q_{1}^{\bigoplus}q_{2}=\begin{bmatrix} w_{1} & -z_{1} & y_{1} & x_{1}\\ z_{1} & w_{1} & -x_{1} & y_{1}\\ -y_{1} & x_{1} & w_{1} & z_{1}\\ -x_{1} & -y_{1} & -z_{1} & w_{1}\end{bmatrix}\begin{bmatrix}x_{2}\\y_{2}\\z_{2}\\w_{2}\end{bmatrix}=\begin{bmatrix}w_{1}x_{2}-z_{1}y_{2}+y_{1}z_{2}+x_{1}w_{2}\\z_{1}x_{2}+w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}\\-x_{2}y_{1}+x_{1}y_{2}+w_{1}z_{2}+z_{1}w_{2}\\w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\end{bmatrix}\begin{align*} & = w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\\     & + (w_{1}x_{2}+x_{1}w_{2}+y_{1}z_{2}-z_{1}y_{2})i\\     & + (w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}+z_{1}x_{2})j\\     & + (w_{1}z_{2}+x_{1}y_{2}-y_{1}x_{2}+z_{1}w_{2})k\end{align*}=q_{1}q_{2}</script><script type="math/tex;mode=display">q_{2}^{+}q_{1}=\begin{bmatrix} w_{2} & z_{2} & -y_{2} & x_{2}\\ -z_{2} & w_{2} & x_{2} & y_{2}\\ y_{2} & -x_{2} & w_{2} & z_{2}\\ -x_{2} & -y_{2} & -z_{2} & w_{2}\end{bmatrix}\begin{bmatrix}x_{1}\\y_{1}\\z_{1}\\w_{1}\end{bmatrix}=\begin{bmatrix}w_{2}x_{1}+y_{1}z_{2}-y_{2}z_{1}+w_{1}x_{2}\\-x_{1}z_{2}+w_{2}y_{1}+x_{2}z_{1}+y_{2}w_{1}\\x_{1}y_{2}-x_{2}y_{1}+w_{2}z_{1}+w_{1}z_{2}\\w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\end{bmatrix}\begin{align*} & = w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\\     & + (w_{1}x_{2}+x_{1}w_{2}+y_{1}z_{2}-z_{1}y_{2})i\\     & + (w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}+z_{1}x_{2})j\\     & + (w_{1}z_{2}+x_{1}y_{2}-y_{1}x_{2}+z_{1}w_{2})k\end{align*}=q_{1}q_{2}</script><h3 id="罗德里格斯公式证明"><a href="#罗德里格斯公式证明" class="headerlink" title="罗德里格斯公式证明"></a>罗德里格斯公式证明</h3><p><img src="/2019/03/12/visual-SLAM-by-Gaoxiang-2/Rodrigues1.png" alt="旋转分解"></p><p><img src="/2019/03/12/visual-SLAM-by-Gaoxiang-2/Rodrigues2.png" alt="分解的表达"></p><p>如上图所示，向量${\bf v}$绕单位旋转轴${\bf k}$旋转$\theta$角后，变换成了${\bf v}_{rot}$，分别将向量${\bf v}$和${\bf v}_{rot}$沿${\bf k}$平行和垂直的方向分解。</p><script type="math/tex;mode=display">\begin{align*}&{\bf v}={\bf v}_{||}+{\bf v}_{\bot}\\&{\bf v}_{||}=({\bf v} \cdot {\bf k}){\bf k}\\& {\bf v}_{\bot}={\bf v}-{\bf v}_{||}=-{\bf k}\times({\bf k} \times {\bf v})\end{align*}</script><p>根据图像，我们知道，平行于旋转轴的分量没有变化，只有垂直于旋转轴的分量进行了旋转，而且它们的模长是一样的，这就说明${\bf v}_{\bot}$和${\bf v}_{rot \bot}$是在一个圆上，长度相等，而且，${\bf v}_{\bot}$和${\bf k} \times {\bf v}_{\bot}$构成了该圆的两个正交坐标轴，因此${\bf v}_{rot \bot}$的坐标可以用这个坐标轴表示，或者说该向量可以被这两个正交坐标基线性表示，即：</p><script type="math/tex;mode=display">\begin{align*}{\bf v}_{rot \bot}&=\cos \theta{\bf v}_{\bot}+\sin \theta({\bf k}\times {\bf v}_{\bot})\\                  &=\cos \theta{\bf v}_{\bot}+\sin \theta({\bf k} \times {\bf v}-{\bf k} \times {\bf v}_{||})\\                  &=\cos \theta{\bf v}_{\bot}+\sin \theta({\bf k}\times {\bf v})\end{align*}</script><script type="math/tex;mode=display">\begin{align*}{\bf v}_{rot} & = {\bf v}_{rot ||}+{\bf v}_{rot \bot}\\              & = {\bf v}_{||}+\cos \theta {\bf v}_{\bot}+\sin \theta ({\bf k}\times{\bf v})\\              & = {\bf v}_{||}+\cos \theta({\bf v}-{\bf v}_{||}) +\sin \theta ({\bf k}\times{\bf v})\\              & = \cos \theta {\bf v}+(1-\cos \theta){\bf v}_{||}+\sin \theta ({\bf k}\times{\bf v})\\              & = \cos \theta {\bf v}+(1-\cos \theta)({\bf k}\cdot {\bf v}){\bf k}+\sin \theta ({\bf k}\times{\bf v})\end{align*}</script><p>下面将这些向量都看成矩阵的形式，将其运算化为矩阵的运算形式，假设${\bf k}$和${\bf v}$都是列向量的形式，则${\bf k}({\bf k}\cdot {\bf v})=kk^{T}v$，${\bf k}\times {\bf v}=k^{\wedge}v$，因此，得到了旋转矩阵的形式为：</p><script type="math/tex;mode=display">R=\cos \theta I+(1-\cos \theta)kk^{T}+\sin \theta k^{\wedge}</script><p>将$k$换成$n$即得罗德里格斯公式。</p><h3 id="四元数运算性质的验证"><a href="#四元数运算性质的验证" class="headerlink" title="四元数运算性质的验证"></a>四元数运算性质的验证</h3><p>设单位四元数$q=[\varepsilon \quad \eta]$(虚部 实部），点$p=[\zeta \quad 0]$，$q=x_{1}i+y_{1}j+z_{1}k+w_{1}, w_{1}^{2}+x_{1}^{2}+y_{1}^{2}+z_{1}^{2}=1$则：</p><script type="math/tex;mode=display">\begin{align*}p^{'}=qpq^{-1} & = q^{\bigoplus}pq^{-1}\\               & = q^{\bigoplus}q^{-1+}p\\               & = \begin{bmatrix}               \eta I+\varepsilon ^{\times} & \varepsilon \\               -\varepsilon^{T} & \eta               \end{bmatrix}               \begin{bmatrix}               \eta I-(-\varepsilon )^{\times} & -\varepsilon \\               -(-\varepsilon)^{T} & \eta               \end{bmatrix}               \begin{bmatrix}               \zeta\\               0               \end{bmatrix}               \\               & = \begin{bmatrix}               w_{1} & -z_{1} & y_{1} & x_{1} \\               z_{1} & w_{1} & -x_{1} & y_{1} \\               -y_{1} & x_{1} & w_{1} & z_{1}\\               -x_{1} & -y_{1} & -z_{1} & w_{1}               \end{bmatrix}               \begin{bmatrix}               w_{1} & -z_{1} & y_{1} & -x_{1} \\               z_{1} & w_{1} & -x_{1} & -y_{1} \\               -y_{1} & x_{1} & w_{1} & -z_{1}\\               x_{1} & y_{1} & z_{1} & w_{1}               \end{bmatrix}               \begin{bmatrix}               \zeta \\               0               \end{bmatrix}               \\               & = \begin{bmatrix}               R_{3 \times 3} & 0_{3 \times 1}\\               0_{1 \times 3} & 1               \end{bmatrix}               \begin{bmatrix}               \zeta \\               0               \end{bmatrix}\end{align*}</script><p>易知$p^{‘}$的实部为0，也就是上述形式限制了点的变换维度，虽然四元数是四个变量，但是不会把三维点变到四维去。</p><p>计算得出旋转矩阵$R$为:</p><script type="math/tex;mode=display">R=\begin{bmatrix}w_{1}^{2}+x_{1}^{2}-y_{1}^{2}-z_{1}^{2} & 2x_{1}y_{1}-2w_{1}z_{1} & 2x_{1}z_{1}+2y_{1}w_{1}\\2x_{1}y_{1}+2z_{1}w_{1} & w_{1}^{2}+y_{1}^{2}-x_{1}^{2}-z_{1}^{2} & 2y_{1}z_{1}-2x_{1}w_{1}\\2x_{1}z_{1}-2y_{1}w_{1} & 2y_{1}z_{1}+2x_{1}w_{1} & w_{1}^{2}+z_{1}^{2}-x_{1}^{2}y_{1}^{2}\end{bmatrix}</script><h3 id="C-11"><a href="#C-11" class="headerlink" title="C++11"></a><a href="https://blog.csdn.net/sinat_35297665/article/details/80101460" target="_blank" rel="noopener">C++11</a></h3><ul><li>for(atuo&amp; a: avec)</li></ul><p><a href="https://blog.csdn.net/hailong0715/article/details/54172848/" target="_blank" rel="noopener">范围for循环</a>，用a遍历avec中的每个量；基于范围的FOR循环的遍历是只读的遍历，除非将变量变量的类型声明为引用类型。</p><ul><li>for(atuo&amp; a: avec)</li></ul><p>自动类型推导，根据a获得的值，用auto自动推断出a的类型；</p><ul><li><a href="const A&amp;a1,const A&amp;a2"></a>{return a1.index&lt;a2.index;})</li></ul><p>运用了<a href="https://www.cnblogs.com/DswCnblog/p/5629165.html" target="_blank" rel="noopener">lambda表达式</a>。</p><ul><li>begin()</li></ul><p><strong>begin</strong> 返回首元素的地址，<strong>end</strong> 返回尾元素的下一个地址。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​&lt;/p&gt;&lt;p&gt;本次课程主要研究&lt;strong&gt;三维空间刚体运动&lt;/strong&gt;，即visual slam的运动方程中的$x_{k}$如何表达。&lt;/p&gt;&lt;h2 id=&quot;点与坐标系&quot;&gt;&lt;a href=&quot;#点与坐标系&quot; class=&quot;headerlink&quot; title=&quot;点与坐标系&quot;&gt;&lt;/a&gt;点与坐标系&lt;/h2&gt;&lt;p&gt;在2D情况下，物体可以通过两个坐标和一个旋转角进行表达，即$(x,y,\theta)$。&lt;/p&gt;&lt;p&gt;在3D情况下，物体是6自由度的，包括平移和旋转，每个都得用三个变量表达，可以认为3D情况是包含着3个2D的情况，旋转轴不同。&lt;/p&gt;
    
    </summary>
    
      <category term="科研记录" scheme="http://densecollections.top/categories/%E7%A7%91%E7%A0%94%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="visual SLAM" scheme="http://densecollections.top/tags/visual-SLAM/"/>
    
      <category term="Linux" scheme="http://densecollections.top/tags/Linux/"/>
    
      <category term="C++11" scheme="http://densecollections.top/tags/C-11/"/>
    
      <category term="Computer vision" scheme="http://densecollections.top/tags/Computer-vision/"/>
    
      <category term="Eigen" scheme="http://densecollections.top/tags/Eigen/"/>
    
  </entry>
  
  <entry>
    <title>visual SLAM by Gaoxiang(1)</title>
    <link href="http://densecollections.top/2019/03/12/visual-SLAM-by-Gaoxiang-1/"/>
    <id>http://densecollections.top/2019/03/12/visual-SLAM-by-Gaoxiang-1/</id>
    <published>2019-03-12T01:10:21.000Z</published>
    <updated>2020-02-03T02:28:47.262Z</updated>
    
    <content type="html"><![CDATA[<p>​</p><h2 id="视觉SLAM概述"><a href="#视觉SLAM概述" class="headerlink" title="视觉SLAM概述"></a>视觉SLAM概述</h2><ul><li>simultaneous localization and mapping</li><li>仅使用相机进行室内/室外定位（有些情况下GPS会崩，IMU漂移随着时间误差增大）</li><li>机器人在未知环境进行导航—-建图(sparse/semi-dense/dense)</li><li>SLAM问题的本质是对运动主体自身和周围环境空间不确定性的估计（spatial uncertainty）</li><li>为了解决SLAM问题，我们需要状态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，估计状态的均值和不确定性（方差）</li></ul><hr><p>学术上研究视觉SLAM较多，尤其是monocular，但是应用上少了点，尤其是建图的作用目前很浅，而且很多人大部分现在在用深度学习搞3D重建。目前应用的地方有：</p><ul><li>手持设备定位</li><li>自动驾驶定位—比GPS的定位信息要丰富，甚至精度更好(可以达到厘米级)</li><li>AR 增强现实（定位，建图，深度学习结合）</li><li>清洁机器人</li></ul><hr><a id="more"></a><h2 id="学习研究步骤"><a href="#学习研究步骤" class="headerlink" title="学习研究步骤"></a>学习研究步骤</h2><p>第一部分是学习相关的数学知识，构建数学模型</p><ul><li>矩阵</li><li>概率论</li><li>李群李代数</li><li>微分几何</li><li>凸优化</li></ul><p>…</p><p>第二部分是计算机视觉的代码实践</p><ul><li>openCV</li><li>c++</li><li>python</li><li>Linux</li></ul><p>…</p><p>教材：</p><ul><li>Multiple view Geometry in computer vision</li><li>State estimation for robotics</li><li>视觉SLAM十四讲-从理论到实践(<a href="https://github.com/AceCoooool/slambook" target="_blank" rel="noopener">作业代码</a>)</li></ul><h2 id="本教程内容提纲"><a href="#本教程内容提纲" class="headerlink" title="本教程内容提纲"></a>本教程内容提纲</h2><ul><li>概述与预备知识</li><li>三维空间的刚体运动</li><li>李群李代数</li><li>相机模型与非线性优化</li><li>特征点法视觉里程计</li><li>直接法视觉里程计</li><li>后端优化</li><li>回环检测</li></ul><h2 id="视觉SLAM的基本框架和模型"><a href="#视觉SLAM的基本框架和模型" class="headerlink" title="视觉SLAM的基本框架和模型"></a>视觉SLAM的基本框架和模型</h2><p>定位与建图是相互关联的，准确的定位需要精确的地图，精确的地图也来自准确的定位。</p><p>为什么选择视觉传感器: 携带安装更加自由，成本更低，信息丰富，功能不单一，可开发性强，智能化程度高具有挑战性（单目monocular/双目stereo/深度相机RGB-D/鱼眼、全景、event-based相机等），而挑战是普通的相机会丢失世界的距离信息，因此需要从图像中进行恢复（相机运动，相机几何关系，物理测量等）。</p><p>视觉SLAM框架</p><ul><li><p>前端：visual odometry(估计邻帧相机相机运动/feature-based/direct-based)</p></li><li><p>后端：optimization(消除噪声，优化轨迹/最大后验概率估计/滤波器/图优化)</p></li><li><p>回环：loop closing(相机回到之前相同的位置，优化约束，消除累计误差/图像相似性/词袋模型)</p></li><li><p>建图：mapping(导航/路劲规划/人机交互/可视化/通讯/度量地图（稀疏地图/稠密地图）/拓扑地图)</p></li></ul><p><img src="/2019/03/12/visual-SLAM-by-Gaoxiang-1/map categories.png" alt="map categories"></p><p>视觉SLAM的数学模型：</p><script type="math/tex;mode=display">\begin{align*} & 运动方程 x_{k} = f(x_{k-1}, u_{k},w_{k}) \\ & 观测方程 z_{k,j} = h(y_{j}, x_{k}, v_{k,j}) \end{align*}</script><p>———-0——-1———2——-3——-4——-5——-6————-&gt; t</p><p>———$x_{0}$—-$x_{1}$——$x_{2}$——$x_{3}$——$x_{4}$———- &gt;</p><p>​ $\searrow $ $\searrow $ $\searrow $ $\searrow $</p><p>​ $\bullet y_{1}$ $\bullet y_{2}$</p><p>​ $\bullet y_{j}$</p><p>​ $\cdots$</p><p>假设一个机器人在实际场景中运动，$x_{k}$是此时刻k的位置，$x_{k-1}$是上一个时刻k-1的，在这之间有个输入，使得机器人运动，我们设其为$u_{k}$，而同时运动也包含着噪声，机器人的运动也不完全受精确控制，假设噪声为$w_{k}$。 现在，机器人携带了相机，相机会拍到一系列现实场景的标志，我们称其为路标点（landmark），假设此场景各个路标点为$y_{1}, y_{2}, \cdots, y_{j}, \cdots$。容易知道，机器人在每个时刻所在的位置都会观测到这些路标点中的一部分或者全部，假设在$x_{0}$位置看到路标点$y_{1}$，我们记为$z_{0,1}$，看到了路标点$y_{2}$，记为$z_{0,2}$，…以此类推，这会反映在图像像素上，因此称为观测方程，同样观测也会有噪声$v_{k,j}$。</p><p>我们通过一系列运动方程和观测方程，利用已知的$u_{k}, z_{k,j}$来推断出$x_{k}, y_{j}$，也就是知道了机器人的位置，同时了解了环境的信息，因此也就是两大任务：定位和建图。</p><h2 id="本次作业与实践"><a href="#本次作业与实践" class="headerlink" title="本次作业与实践"></a>本次作业与实践</h2><p>首先大致了解下Linux下编写C++源码以及利用cmake编译可执行文件的步骤</p><ul><li>工具g++, cmake, VS code(optional IDE)</li><li>一个利用c++编写的简单工程包含着头文件文件夹incliude，源码文件夹src，cmake编译文件夹build，以及main.cpp和CMakeLists.txt等主要文件，其中CMakeLists.txt文件的语言格式要注意</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line">project(HelloSLAM)</span><br><span class="line"></span><br><span class="line">#指定程序编译的模式</span><br><span class="line">set(CMAKE_BUILD_TYPE Debug)</span><br><span class="line"></span><br><span class="line">#claim the include files&apos; directories</span><br><span class="line">include_directories(&quot;include&quot;)</span><br><span class="line"></span><br><span class="line"># only src files</span><br><span class="line">add_library(libHello src/hello.cpp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">add_executable(sayHello main.cpp)</span><br><span class="line"></span><br><span class="line">target_link_libraries(sayHello libHello)</span><br></pre></td></tr></table></figure><p>利用vs code配合cmake进行<a href="https://zhuanlan.zhihu.com/p/52874931" target="_blank" rel="noopener">配置</a>以进行代码的编写和调试，也可以选择KDEVELOP和其他的IDE。</p><h3 id="熟悉LIinux"><a href="#熟悉LIinux" class="headerlink" title="熟悉LIinux"></a>熟悉LIinux</h3><p>1.如何在 Ubuntu 中安装软件（命令⾏界⾯）？它们通常被安装在什么地⽅？</p><ul><li>apt-get 方式的安装；</li></ul><p>普通安装：<code>sudo apt-get install XXX</code></p><p>修复安装：<code>sudo apt-get -f install XXX</code></p><p>重新安装：<code>sudo apt-get -f reinstall XXX</code></p><ul><li>dpkg方式的安装</li></ul><p><code>sudo dpkg -i package_name.deb</code></p><ul><li>安装的地方</li></ul><p>通常被安装在<code>/usr/bin</code>，<code>/usr/local</code>这个目录下</p><p>系统安装软件一般在<code>/usr/share</code>，可执行的文件在<code>/usr/bin</code>，配置文件可能安装到了<code>/etc</code>下等。文档一般在 <code>/usr/share</code>，可执行文件 <code>/usr/bin</code>，配置文件 <code>/etc</code>，lib文件 <code>/usr/lib。</code></p><p>2.linux 的环境变量是什么？我如何定义新的环境变量？</p><ul><li><a href="http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap08.html" target="_blank" rel="noopener">Environment variables</a> defined in this chapter affect the operation of multiple utilities, functions, and applications.</li></ul><p>linux是一个多用户的操作系统。每个用户登录系统后，都会有一个专用的运行环境。通常每个用户默认的环境都是相同的，这个默认环境实际上就是一组环境变量的定义。用户可以对自己的运行环境进行定制，其方法就是修改相应的系统环境变量。环境变量是一个具有特定名字的对象，它包含了一个或者多个应用程序所将使用到的信息</p><p>常见的环境变量：</p><p>PATH：决定了shell将到哪些目录中寻找命令或程序</p><p>HOME：当前用户主目录</p><p>MAIL：是指当前用户的邮件存放目录。</p><p>SHELL：是指当前用户用的是哪种Shell。</p><p>HISTSIZE：是指保存历史命令记录的条数</p><p>LOGNAME：是指当前用户的登录名。</p><p>HOSTNAME：是指主机的名称，许多应用程序如果要用到主机名的话，通常是从这个环境变量中来取得的。</p><p>LANG/LANGUGE：是和语言相关的环境变量，使用多种语言的用户可以修改此环境变量。</p><p>使用修改.bashrc文件进行环境变量的编辑，只对当前用户有用。</p><p>使用修改/etc/profile文件进行环境变量的编辑，是对所有用户有用。</p><p>关于环境变量命令介绍：</p><p>echo显示某个环境变量值echo$PATH</p><p><strong>export设置一个新的环境变量exportHELLO=”hello”(可以无引号)</strong></p><p>env显示所有环境变量</p><p>set显示本地定义的shell变量</p><p>unset清除环境变量unsetHELLO</p><p>readonly设置只读环境变量readonlyHELLO</p><ul><li>对所有用户生效的永久性变量（系统级）:</li></ul><p>这类变量对系统内的所有用户都生效，所有用户都可以使用这类变量。作用范围是整个系统。</p><p>设置方式： 用vim在/etc/profile文件中添加我们想要的环境变量,用export指令添加环境变量</p><p>当然，这个文件只有在root（超级用户）下才能修改。我们可以在etc目录下使用ls -l查看这个文件的用户及权限</p><p>【注意】：添加完成后新的环境变量不会立即生效，除非你调用source /etc/profile 该文件才会生效。否则只能在下次重进此用户时才能生效。</p><ul><li>对单一用户生效的永久性变量（用户级）:</li></ul><p>只针对当前用户，和上面的一样，只不过不需要在etc下面进行添加，直接在.bash_profile文件最下面用export添加就好了。</p><p>这里 .bashrc和.bash_profile原则上来说设置此类环境变量时在这两个文件任意一个里面添加都是可以的。</p><p>~/.bash_profile是交互式login方式进入bash shell运行。</p><p>~/ .bashrc是交互式non-login方式进入bash shell运行。</p><p>二者设置大致相同。</p><p>就是.bash_profile文件只会在用户登录的时候读取一次</p><p>而.bashrc在每次打开终端进行一次新的会话时都会读取。</p><ul><li>临时有效的环境变量（只对当前shell有效）:</li></ul><p>此类环境变量只对当前的shell有效。当我们退出登录或者关闭终端再重新打开时，这个环境变量就会消失。是临时的。 直接使用export指令添加。</p><p>3.linux 根⽬录下⾯的⽬录结构是什么样的？⾄少说出 3 个⽬录的⽤途。</p><ul><li>可通过终端查看/目录下的文件：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /</span><br><span class="line">ls</span><br></pre></td></tr></table></figure><p><code>/bin</code>用户二进制文件<br>包含二进制可执行文件，系统所有用户可执行文件都在这个文件夹里，例如：ls，cp，ping等。</p><p><code>/sbin</code> 系统二进制文件<br>包含二进制可执行文件，但只能由系统管理员运行，对系统进行维护。</p><p><code>/etc</code>配置文件<br>包含所有程序配置文件，也包含了用于启动/停止单个程序的启动和关闭shell脚本。</p><p><code>/dev</code>设备文件<br>包含终端所有设备，USB或连接到系统的任何设备。例如：<code>/dev/tty1</code>、<code>dev/usbmon0</code></p><p><code>/proc</code>进程信息<br>包含系统进程的相关信息。<br>这是一个虚拟的文件系统，包含有关正在运行的进程的信息。例如：<code>/proc/{pid}</code>目录中包含的与特定pid相关的信息。<br>这是一个虚拟的文件系统，系统资源以文本信息形式存在。例如：<code>/proc/uptime</code></p><p><code>/var</code>变量文件<br>可以找到内容可能增长的文件。<br>这包括 - 系统日志文件<code>/var/log</code>;包和数据库文件<code>/var/lib</code>;电子邮件<code>/var/mail</code>;打印队列<code>/var/spool</code>;锁文件<code>/var/lock</code>;多次重新启动需要的临时文件<code>/var/tmp</code>;</p><p><code>/tmp</code>临时文件<br>包含系统和用户创建的临时文件。<br>当系统重新启动时，这个目录下的文件都将被删除。</p><p><code>/usr</code>用户程序<br>包含二进制文件、库文件、文档和二级程序的源代码。<br><code>/usr/bin</code>中包含用户程序的二进制文件。如果你在<code>/bin</code>中找不到用户二进制文件，到<code>/usr/bin</code>目录看看。例如：at、awk、cc、less、scp。<br><code>/usr/sbin</code>中包含系统管理员的二进制文件。如果你在<code>/sbin</code>中找不到系统二进制文件，到<code>/usr/sbin</code>目录看看。例如：atd、cron、sshd、useradd、userdel。<br><code>/usr/lib</code>中包含了<code>/usr/bin</code>和<code>/usr/sbin</code>用到的库。<br><code>/usr/local</code>中包含了从源安装的用户程序。例如，当你从源安装Apache，它会在<code>/usr/local/apache2</code>中。</p><p><code>/home</code> HOME目录<br>所有用户用来存档他们的个人档案。</p><p><code>/boot</code>引导加载程序文件<br>包含引导加载程序相关的文件。<br>内核的initrd、vmlinux、grub文件位于<code>/boot</code>下。</p><p><code>/lib</code>系统库<br>包含支持位于<code>/bin</code>和<code>/sbin</code>下的二进制文件的库文件.<br>库文件名为 ld或lib.so.*</p><p><code>/opt</code>可选的附加应用程序<br>opt代表opitional；<br>包含从个别厂商的附加应用程序。<br>附加应用程序应该安装在<code>/opt/</code>或者<code>/opt/</code>的子目录下。</p><p><code>/mnt</code>挂载目录<br>临时安装目录，系统管理员可以挂载文件系统。</p><p><code>/media</code> 可移动媒体设备<br>用于挂载可移动设备的临时目录。<br>举例来说，挂载CD-ROM的<code>/media/cdrom</code>，挂载软盘驱动器的<code>/media/floppy</code>;</p><p><code>/srv</code>服务数据<br>srv代表服务。<br>包含服务器特定服务相关的数据。<br>例如，<code>/srv/cvs</code>包含cvs相关的数据。</p><p>4.假设我要给 a.sh 加上可执⾏权限，该输⼊什么命令？</p><p><code>chmod 777 文件名</code> 将文件设置成对拥有者、组成员、其他人可读、可写、可执行。<br><code>chmod a+x 文件名</code>将文件在原来的配置上增加可执行权限。</p><p>5.假设我要将 a.sh ⽂件的所有者改成 xiang:xiang，该输⼊什么命令？</p><p><code>chown xiang:xiang 文件名</code>将文件的所有者改成xiang:xiang</p><h3 id="SLAM文献阅读"><a href="#SLAM文献阅读" class="headerlink" title="SLAM文献阅读"></a>SLAM文献阅读</h3><p>1.SLAM 会在哪些场合中⽤到？⾄少列举三个⽅向。</p><p>机器人定位导航、手持设备定位、增强现实、自动泊车、语义地图重建等</p><p>2.SLAM中定位与建图是什么关系？为什么在定位的同时需要建图？</p><p>定位需要精确的地图，详细的地图需要精确地定位，两者相辅相成，相互依存。重定位和局部建图是slam框架中很重要的两个线程。</p><p>定位：机器人必须<strong>知道自己在环境中位置</strong>；</p><p>建图：机器人必须<strong>记录环境中特征的位置</strong>（如果知道自己的位置）；</p><p>机器人在未知环境中从一个未知位置开始移动,在移动过程中根据位置估计和地图进行自身定位,同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。</p><p>3.SLAM发展历史如何？我们可以将它划分成哪⼏个阶段？</p><p>1985-1990：, Chatila 和Laumond (1985) and Smith et al. (1990)提出以建图和定位同时进行；</p><p><strong>单传感器为外部传感器：</strong></p><p>早期：</p><ul><li>声呐(Tardós et al. 2002; Ribas et al. 2008)；</li></ul><p>后期：</p><ul><li><p>激光雷达(Nüchter etal. 2007; Thrun et al. 2006)；</p></li><li><p>相机(Se et al. 2005; Lemaire et al. 2007; Davison 2003;Bogdan et al. 2009)；</p></li><li><p>GPS(Thrun et al. 2005a)；</p></li></ul><p><strong>多传感器融合</strong></p><p>三个发展阶段：</p><ul><li><p>初始阶段：二十世纪八十年代</p></li><li><p>发展阶段：二十世纪九十年代</p></li><li><p>快速发展阶段</p></li></ul><p>4.列举三篇在SLAM领域的经典⽂献。</p><ul><li><p>Smith, R.C. and P. Cheeseman, On the Representation and Estimation of Spatial Uncertainty. International Journal of Robotics Research, 1986. 5</p></li><li><p>Se, S., D. Lowe and J. Little, Mobile robot localization and mapping with uncertainty using scale­invariant visual landmarks. The international Journal of robotics Research, 2002. 21</p></li><li><p>Mullane, J., et al., A Random­Finite­Set Approach to Bayesian SLAM. IEEE Transactions on Robotics, 2011</p></li></ul><h3 id="CMake练习"><a href="#CMake练习" class="headerlink" title="CMake练习"></a>CMake练习</h3><blockquote><p>1.程序代码由头文件和源文件组成；</p><p>2.带有main函数的源文件编译成可执行程序，其他的编译成库文件；</p><p>3.如果可执行程序想调用库文件中的函数，它需要参考该库提供的头文件，以明白调用的格式。同时要把可执行程序链接到库文件上；</p></blockquote><p>在自己的工程目录里面建立子目录<code>build</code>文件夹和<code>src</code>文件夹和<code>libhello</code>文件夹，hello.cpp,hello.h放入<code>libhello</code>中,useHello.cpp放入<code>src</code>中。同时在工程目录下创建一个顶层的CMakeLists.txt文件，内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">project(sayHello)</span><br><span class="line"></span><br><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line"></span><br><span class="line">#ADD_SUBDIRECTORY(source_dir [binary_dir] [EXCLUDE_FROM_ALL])</span><br><span class="line">#这个指令用于向当前工程添加存放源文件的子目录，并可以指定中间二进制和目标二进制存放的位置</span><br><span class="line">add_subdirectory(src) </span><br><span class="line">#在 build 目录中将出现一个 src 目录，生成的目标代码 hello 将存放在 src 目录中</span><br><span class="line"></span><br><span class="line">add_subdirectory(libhello)</span><br><span class="line"></span><br><span class="line">set(CMAKE_BUILD_TYPE &quot;Release&quot;)</span><br></pre></td></tr></table></figure><p>在<code>src</code>文件夹下新建CMakeLists.txt文件，输入如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line"></span><br><span class="line">#头文件目录</span><br><span class="line">include_directories($&#123;PROJECT_SOURCE_DIR&#125;/libhello)</span><br><span class="line"></span><br><span class="line">set(APP_SRC useHello.cpp)</span><br><span class="line"></span><br><span class="line">#指定最终的目标二进制的位置</span><br><span class="line">#SET(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/bin)</span><br><span class="line">#SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)</span><br><span class="line">set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/bin)</span><br><span class="line"></span><br><span class="line">add_executable(sayHello $&#123;APP_SRC&#125;)</span><br><span class="line"></span><br><span class="line">#为 target 添加需要链接的库</span><br><span class="line">target_link_libraries(sayHello libhello)</span><br></pre></td></tr></table></figure><p>最后在<code>libhello</code>文件夹下新建CMakeLists.txt文件，输入如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8)</span><br><span class="line"></span><br><span class="line">set(LIB_SRC hello.cpp)</span><br><span class="line"></span><br><span class="line">#向 C/C++编译器添加-D 定义</span><br><span class="line">add_definitions(&quot;-DLIBHELLO_BUILD&quot;)</span><br><span class="line"></span><br><span class="line">#添加动态库</span><br><span class="line">add_library(libhello SHARED $&#123;LIB_SRC&#125;)</span><br><span class="line">#设置动态库输出位置</span><br><span class="line">set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)</span><br><span class="line"></span><br><span class="line">#SET_TARGET_PROPERTIES(target1 target2 ...</span><br><span class="line">#PROPERTIES prop1 value1 </span><br><span class="line">#prop2 value2 ...)</span><br><span class="line">#该指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本</span><br><span class="line">set_target_properties(libhello PROPERTIES OUTPUT_NAME &quot;sayhello&quot;)</span><br></pre></td></tr></table></figure><p>接着在<code>build</code>文件夹下<code>cmake ..</code>, <code>make</code>即可，在biuld文件中的子问夹下的bin文件生成可执行文件sayHello;在lib中生成库文件libhello.so共享库文件。</p><p>对于安装路径，可以使用<code>CMAKE_INSTALL_PREFIX</code>命令。</p><h3 id="理解ORBSLAM2框架"><a href="#理解ORBSLAM2框架" class="headerlink" title="理解ORBSLAM2框架"></a>理解ORBSLAM2框架</h3><p>1.<code>git clone https://github.com/raulmur/ORB_SLAM2</code></p><p>2.(a) 6个可执行文件一个库文件 <code>set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${PROJECT_SOURCE_DIR}/lib)</code></p><p>(b)<code>include</code>文件夹包含：对应<code>src</code>中程序的代码函数头文件；<code>src</code>文件夹包含：相应程序的代码函数的c++文件；Examples文件夹包含RGB-D文件夹，Stereo文件夹， Monocular文件夹，具体的内容在总目录下的CMakeLists.txt文件中有写：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># Build examples</span><br><span class="line"></span><br><span class="line">set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/Examples/RGB-D)</span><br><span class="line"></span><br><span class="line">add_executable(rgbd_tum</span><br><span class="line">Examples/RGB-D/rgbd_tum.cc)</span><br><span class="line">target_link_libraries(rgbd_tum $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/Examples/Stereo)</span><br><span class="line"></span><br><span class="line">add_executable(stereo_kitti</span><br><span class="line">Examples/Stereo/stereo_kitti.cc)</span><br><span class="line">target_link_libraries(stereo_kitti $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(stereo_euroc</span><br><span class="line">Examples/Stereo/stereo_euroc.cc)</span><br><span class="line">target_link_libraries(stereo_euroc $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/Examples/Monocular)</span><br><span class="line"></span><br><span class="line">add_executable(mono_tum</span><br><span class="line">Examples/Monocular/mono_tum.cc)</span><br><span class="line">target_link_libraries(mono_tum $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(mono_kitti</span><br><span class="line">Examples/Monocular/mono_kitti.cc)</span><br><span class="line">target_link_libraries(mono_kitti $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(mono_euroc</span><br><span class="line">Examples/Monocular/mono_euroc.cc)</span><br><span class="line">target_link_libraries(mono_euroc $&#123;PROJECT_NAME&#125;)</span><br></pre></td></tr></table></figure><p>(c) OPENCV_LIBS、EIGEN3_LIBS、Pangolin_LIBRARIES、/ORB-SLAM2/Thirdparty/DBoW2/lib/libDBoW2.so、/ORB-SLAM2/Thirdparty/g2o/lib/libg2o.so</p><h3 id="运行ORB-SLAM2"><a href="#运行ORB-SLAM2" class="headerlink" title="运行ORB-SLAM2"></a>运行ORB-SLAM2</h3><p>1.编译安装依赖项，可以按照<a href="https://github.com/raulmur/ORB_SLAM2" target="_blank" rel="noopener">ORB-SLAM2源码</a>的指导进行配置。其他依赖项按照高翔书中所说的安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libopencv-dev libeigen3-dev libqt4-dev qt4-qmake libqglviewer-dev libsuitesparse-dev libcxsparse3.1.2 libcholmod[tab安装]</span><br></pre></td></tr></table></figure><p>其中需要注意Pangolin需要下载手动编译安装，步骤比较简单，不再赘述</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libglew-dev</span><br><span class="line">---</span><br><span class="line">sudo apt-get install libboost-dev libboost-thread-dev libboost-filesystem-dev</span><br><span class="line">---</span><br><span class="line">git clone https://github.com/stevenlovegrove/Pangolin Pangolin</span><br><span class="line">--</span><br><span class="line">cd Pangolin</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make </span><br><span class="line">sudo make install</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>另外一个注意的点就是opencv库，我是手动下载编译安装的，网上的教程很多，主要是编译时注意cmake的设置，但是我在安装的时候无法被python链接，也就是没有生成cv2.so，导致没办法在Python2和python3中<code>import cv2</code>，估计可能是我之前先装了anaconda的原因，导致了opencv编译忽略了Python路径问题。。。尝试了几次，未果，等以后用到了python再解决这个问题（可以直接用<code>sudo pip3 install opencv-python</code>，如果不想麻烦的话，不过以后可能会出现一些问题）</p><p>2.如何将 myslam.cpp或 myvideo.cpp 加⼊到 ORB-SLAM2 ⼯程中？请给出你的 CMakeLists.txt 修改⽅案。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#add myvideo.cpp and myslam.cpp</span><br><span class="line">set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(myvideo myvideo.cpp)</span><br><span class="line">target_link_libraries(myvideo $&#123;PROJECT_NAME&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(myslam myslam.cpp)</span><br><span class="line">target_link_libraries(myslam $&#123;PROJECT_NAME&#125;)</span><br></pre></td></tr></table></figure><p><strong>将myvideo.cpp、myslam.cpp、myvideo.yaml、myslam.yaml以及myvideo.mp4都放到ORB-SLAM2目录下</strong>，再次编译ORB-SLAM2后，就会生成可执行文件，然后终端输入./myvideo和./myslam就可以直接运行了。</p><p><img src="/2019/03/12/visual-SLAM-by-Gaoxiang-1/myvideo.png" alt="myvideo"></p><p>这里有个问题，一开始我是根据CMakeLits.txt中上面的语句来写的，也就是我把myvideo.cpp、myslam.cpp放在了、Examples/Monocular文件夹中，然后进行编译，结果虽然生成了可执行文件，但是程序无法运行，终端要么提示无法读取对应的.yaml的设置，要么出现segmentation fault错误，最后放到ORB-SLAM2总目录下就没问题了。。。目前不知道什么原因，等后面再看看。</p><hr><hr><p>因为ORB-SLAM2开源的版本没有稠密建图功能，因此如果想尝试稠密建图功能，可以参考高翔博士的ORB-SLAM2_modofied，利用PCL工具实时拼接RGB-D图像，效果还可以，不过没有原作者视频中最后的那个效果好，安装步骤在<a href="https://github.com/Richardyu114/ORBSLAM2_with_pointcloud_map-in-Ubuntu-16.04" target="_blank" rel="noopener">这里</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​&lt;/p&gt;&lt;h2 id=&quot;视觉SLAM概述&quot;&gt;&lt;a href=&quot;#视觉SLAM概述&quot; class=&quot;headerlink&quot; title=&quot;视觉SLAM概述&quot;&gt;&lt;/a&gt;视觉SLAM概述&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;simultaneous localization and mapping&lt;/li&gt;&lt;li&gt;仅使用相机进行室内/室外定位（有些情况下GPS会崩，IMU漂移随着时间误差增大）&lt;/li&gt;&lt;li&gt;机器人在未知环境进行导航—-建图(sparse/semi-dense/dense)&lt;/li&gt;&lt;li&gt;SLAM问题的本质是对运动主体自身和周围环境空间不确定性的估计（spatial uncertainty）&lt;/li&gt;&lt;li&gt;为了解决SLAM问题，我们需要状态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，估计状态的均值和不确定性（方差）&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;p&gt;学术上研究视觉SLAM较多，尤其是monocular，但是应用上少了点，尤其是建图的作用目前很浅，而且很多人大部分现在在用深度学习搞3D重建。目前应用的地方有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;手持设备定位&lt;/li&gt;&lt;li&gt;自动驾驶定位—比GPS的定位信息要丰富，甚至精度更好(可以达到厘米级)&lt;/li&gt;&lt;li&gt;AR 增强现实（定位，建图，深度学习结合）&lt;/li&gt;&lt;li&gt;清洁机器人&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;
    
    </summary>
    
      <category term="科研记录" scheme="http://densecollections.top/categories/%E7%A7%91%E7%A0%94%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="visual SLAM" scheme="http://densecollections.top/tags/visual-SLAM/"/>
    
      <category term="Linux" scheme="http://densecollections.top/tags/Linux/"/>
    
      <category term="C++11" scheme="http://densecollections.top/tags/C-11/"/>
    
      <category term="Computer vision" scheme="http://densecollections.top/tags/Computer-vision/"/>
    
      <category term="OpenCV" scheme="http://densecollections.top/tags/OpenCV/"/>
    
  </entry>
  
  <entry>
    <title>Beyond Supervised Learning-A Computer Vision Perspective</title>
    <link href="http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/"/>
    <id>http://densecollections.top/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/</id>
    <published>2019-03-02T12:34:28.000Z</published>
    <updated>2020-02-03T02:28:47.261Z</updated>
    
    <content type="html"><![CDATA[<p>​</p><h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>这篇论文（<a href="https://link.springer.com/article/10.1007/s41745-019-0099-3" target="_blank" rel="noopener">链接</a>）是印度的几位教授写的，以计算机视觉的角度来阐述目前在全监督学习和无监督学习之间的几种训练方式，写得比较简略，但是适合我这种刚入门连名词都不是很清楚的人，可以让我用来梳理整个领域的技术发展脉络和现状。同时该论文也给出了大量的参考文献，也方便进行下一步的研究。实际上，严格地说，这篇文章不能算一篇合格的综述，只能说是那种“扫盲”的阅读材料。</p><p>出现上述技术发展的主要原因是数据集比较庞大，但是完全被标注好的数据却很少，而训练好的网络在面对现实多变的复杂场景时依然会出现问题，因此这两者之间出现了合成数据（synthetic data)，迁移学习(transfer learning)，弱监督学习(weakly supervised learning)，少量学习（Few-shot learning/K-shot learning)以及自监督学习(self-supervised learning)，边界当然就是无监督学习了(unsupervised learning)。其中需要注意，self-supervised learning和unsupervised learning不是一回事，前者仍然是有监督学习的一种，正如<a href="https://www.quora.com/What-is-the-difference-between-self-supervised-and-unsupervised-learning" target="_blank" rel="noopener">Quora</a>中的回答：</p><blockquote><p>Self-supervised, imposes classification by itself, derivative from the input data.</p><p>Unsupervised, does not. At all.</p><p>Meaning, that self-supervised, STILL has a measurement of terms of right contra wrong, as in, terms of classification.</p><p>Unsupervised, does not.</p></blockquote><a id="more"></a><ul><li>而目前，self-supervised learning正是Yann lecun在2019的ISSCC上所说的最新“蛋糕”组成成分。</li></ul><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/new AI cake.jpg" alt="new AI cake"></p><p>不过在阅读之前，首先解决一个问题是：<strong>什么是machine learning中的“label”和“feature”?</strong></p><ul><li><p>简短地说，<strong>feature就是输入，label就是输出</strong>。一个feature就是我们输入网络数据中的一列，比如说，我们要分类猫和鸟，我们输入特征可能包括颜色，大小，重量等，label就是输出的结果，就是图片的动物是猫还是鸟。</p></li><li><p>Quora上的一个<a href="https://www.quora.com/What-distinguishes-a-feature-from-a-label-in-machine-learning" target="_blank" rel="noopener">回答</a>比较系统点，这里直接贴出来：</p></li></ul><blockquote><p>Imagine how a toddler might learn to recognize things in the world. The parent, often sits with her and they read a picture book, with photos of animals. The parent teaches the toddler but pointing to the pictures and <strong>labeling</strong> them: <em>“this is a dog”, “this is a cat”, “this is a tree”, “this is a house”</em>, …</p><p>After she has learned enough examples, the toddler gets a grasp on what are the key features of a dog, vs these of a cat or of a house or of a tree. So when the toddler and her dad walk in the park, and the father points to a thing she has never seen before, and asks her “<em>what’s that?</em>“, she is capable of generalising her learning, and correctly answer “<em>it’s a dog!</em>“. What might have happened behind the scenes is that consciously or subconsciously, the toddler extracted and examined the <strong>features</strong> of the “thing” - four legs, fluffy white hair, sticking tounge, round black eyes, etc - which aligned nicely with her preception of a “dog”, and did not match any of the other entity types she knows.</p><p><strong>So labels were the ground truth categories provided explicitly by her father during learning. And features are what she inferred from these implicitly, and then extracted and examined at “run time” in the park</strong>.</p><p>Similarly, in supervised learning, the NN learns by examples: an experts gives it many training examples, each example explicitly labled with the ground truth answer. The NN tries to predict these labels by modifying the values of the parameters of the NN. When an input comes, different areas of the network becomes active (I.e receive high value), depending on the input. These represent the features of the specific input. The output of the network (e.g classification decision) is a function of which of the parameters are active.</p><p><strong>So the labels are explicitly given by the trainer during the training, and the features is the configuration of the network - which is implicitly learned by the network, as guided by the labels</strong>.</p><p>It’s these features that then define the output of the network for a given input in runtime.<br>Want to learn more or see how these features are represented? See this great article: <a href="https://distill.pub/2017/feature-visualization" target="_blank" rel="noopener">Feature Visualization</a></p></blockquote><p>整个supervised learning到unsupervised learning的发展就是人工标注的工作量太大，因为“label”这个东西需要人去手工注解，而且越精细，越准确就使训练结果越好，但是同时也会带来不适应现实数据变化的问题，鲁棒性不好。另外一方面，未标注的数据，“粗糙”的原始数据却很多，同时也比较容易抓取，所以慢慢地会向unsupervised learning发展。</p><h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>该论文的整个写作框架如下：</p><ul><li><p>Abstract</p></li><li><p>Introduction</p><ul><li>Notations and Definitions</li><li>Success of Supervised Learning</li></ul></li><li><p>Effectiveness of Synthetic Data</p></li><li><p>Domain Adaption and Transfer Learning</p></li><li><p>Weakly Supervised Learning</p><ul><li>Incomplete Supervision</li><li>Inexact Supervision</li><li>Inaccurate Supervision</li></ul></li><li><p>K-Shot Learning</p></li><li><p>Self-Supervised Learning</p></li><li><p>Conclusion and Discussion</p></li></ul><p>从整体上看，文章按照监督强度来写的，然后介绍了一些方法出现的原因，同时提及了一些关键的技术，对于更深的探讨没有做进一步地表述，而是直接给出了参考文献。</p><p>论文的内容：First, we summarize the <strong>relevant techniques that fall between the paradigm of supervised and unsupervised learning</strong>. Second, we take autonomous navigation as a running example to explain and compare different models. Finally, we highlight some shortcomings of current methods and suggest future directions.</p><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><h3 id="Reason"><a href="#Reason" class="headerlink" title="Reason"></a>Reason</h3><ul><li><p>Supervised deep learning-based techniques <strong>require a large amount of human-annotated training data</strong> to learn an adequate model. It is not viable to do so for every domain and task. Particularly, for problems in health care and autonomous navigation, collecting an exhaustive data set is either very expensive or all but impossible. (训练数据集大)</p></li><li><p>Even though supervised methods excel at learning from a large quantity of data, results show that they are particularly <strong>poor in generalizing the learned knowledge to new task or domain</strong>. This is because a majority of learning techniques assume that both the train and test data are sampled from the same distribution. (功能单一性太强)</p></li><li><p>Two bottlenecks of fully supervised deep learning methods—(1) <strong>lack of labeled data in a particular domain</strong>; (2) <strong>unavailability of direct supervision for a particular task in a given domain</strong>.</p></li></ul><h3 id="Categories-of-Methods"><a href="#Categories-of-Methods" class="headerlink" title="Categories of Methods"></a>Categories of Methods</h3><p>1.Data-centric techniques which solve the problem by <strong>generating a large amount of data similar</strong> to the one present in the original data set.</p><ul><li>Data-centric techniques include data augmentation which involves <strong>tweaking the data samples with some pre-defined transformations to increase the overall size of the data set</strong>. Another method is to use techniques borrowed from computer graphics to <strong>generate synthetic data</strong> which is used along with the original data to train the model.</li></ul><p>2.Algorithm-centric techniques which <strong>tweak the learning method to harness the limited data efficiently through various techniques</strong> like on-demand human intervention, exploiting the inherent structure of data, capitalizing on freely available data on the web or solving for an easier but related surrogate task.</p><ul><li>Algorithm-centric techniques try to <strong>relax the need of perfectly labeled data by altering the model requirements to acquire supervision through inexact , inaccurate , and incomplete labels</strong>. Another set of methods exploit the knowledge gained while <strong>learning from a related domain or task</strong> by efficiently transferring it to the test environment.</li></ul><p>3.Hybrid techniques which combine ideas from both the data and algorithm-centric methods.</p><ul><li>Hybrid methods incorporate techniques which focus on improving the performance of the model at both the data and algorithm level. （比如自动驾驶中的城市道路场景理解任务中，先用合成数据进行训练，然后进行domain shift去适应真实的场景理解任务）</li></ul><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/degree of supervision.png" alt="degree of supervision"></p><h3 id="Simple-Math-Definition"><a href="#Simple-Math-Definition" class="headerlink" title="Simple Math Definition"></a>Simple Math Definition</h3><p>假设${\cal X}$和${\cal Y}$分别是输入和标签空间（也就是输出了），在一般的机器学习问题中，我们假设要从数据集中学习N个objects。我们从这些objects提取特征去训练模型，即$X = ({\cal x}_{1}, {\cal x}_{2}, \cdots, {\cal x}_{N})$,$P(X)$是$X$上的边缘概率（marginal probability）。在fully supervised learning中，通常假设有相对应的标签$Y = ({\cal y}_{1}, {\cal y}_{2}, \cdots, {\cal y}_{N})$。</p><p>学习算法就是在假设空间（hypothesis space）${\cal F}$中寻找函数$f: {\cal X} \rightarrow {\cal Y}$，同时在空间${\cal L}$定义了损失函数（loss function)​ $l: {\cal Y} \times {\cal Y} \rightarrow \mathbb {R}^{\geq 0}$来衡量函数的适用性。同时机器学习算法也会最小化误差函数（错误预测）$R$来提升函数的正确性：</p><script type="math/tex;mode=display">R = \frac{1}{N}\sum ^{N} _{n=0}l({\cal y}_{i}, f({\cal x}_{i}))</script><p>对synthetic data来说，输入空间发生了变换，设为${\cal X}_{synth}$，标签空间不变，此外，由于输入特征空间个边缘概率分布都发生了变化，因此利用新的domain${\cal D}_{synth}=\lbrace {\cal X}_{synth}, P({\cal X}_{synth}) \rbrace$来代替原来的real domain${\cal D}=\lbrace {\cal X}, P({\cal X}) \rbrace$。因此，我们也不可能用之前的预测函数$f_{synth}: {\cal X}_{synth} \rightarrow {\cal Y}$ 来构建${\cal X}$到${\cal Y}$的映射。</p><p>迁移学习（transfer learning）就是用于解决domain adaptation(DA)问题的技术，其不仅可以用于不同domain之间，也可以用于不同的task之间。根据input feature space 在source and target input distribution的分布是否相同，即 ${\cal X}_{s}$是否等于${\cal X}_{t}$，DA可以分为homogeneous DA和heterogeneous DA（同质和异质），显然heterogeneous DA问题要更为复杂些。</p><p>通常情况下，训练时，supervised learning认为所有的feature sets ${\cal x}_{i}$会有相应的标签${\cal y}_{i}$与之对应，实际情况是，这些标签在实际场景中可能是 $ {\tt incomplete, inexact, inaccurate}$的，因此可能就要在weakly supervised learning技术框架下训练，比如说针对那些在网络抓取的数据而言。针对incomplete的标签场景而言，定义feature set $X = ({\cal x}_{1}, {\cal x}_{2}, \cdots, {\cal x}_{l}, {\cal x}_{l+1}, \cdots, {\cal x}_{n})$，其中$X_{labeled} = ({\cal x}_{1}, {\cal x}_{2}, \cdots, {\cal x}_{l})$会有对应的标签$Y_{labeled} = ({\cal y}_{1}, {\cal y}_{2}, \cdots, {\cal y}_{l})$供其训练，但是$X_{unlabeled} = ({\cal x}_{l+1} \cdots, {\cal x}_{n})$就没有任何对应的标签了。此外，一些其他的weakly supervised模型包含一些有着多种标签的单个实例或者多个实例共享一个标签（multiple-instace single-label)。这个时候会对feature set中的${\cal x}_{i}$进行打包处理，即${\cal x}_{i,j}, j=1,2,\cdots, m$。</p><p>尽管上述技术框架对应着不同程度的supervision，但是都需要大量的实例instances $X$来训练模型。如果某些class没有足够的instances的话就会使得训练不理想，因此出现了Few-shot learning(少量学习)和Zero-shot learning(ZSL)。</p><p>如果没有了监督信号（supervision signal），可以利用instances的内在结构（inherent structure）去训练模型。假设$X$和$Y$分别是feature set和label set，此时$P(Y|X)$无法得出，也无法确立任务${\cal T} = \lbrace {\cal Y}, P(Y|X) \rbrace$，不过我们可以定义一个proxy task${\cal T}_{proxy} = \lbrace Z, P(Z|X)\rbrace$，label set$Z$可以自己从数据中提取。For computer vision problems, proxy tasks have been defined based on spatial and temporal alignment, color, and motion cues.</p><h3 id="Effectiveness-of-Synthetic-Data（个人感觉还是与RL相关的）"><a href="#Effectiveness-of-Synthetic-Data（个人感觉还是与RL相关的）" class="headerlink" title="Effectiveness of Synthetic Data（个人感觉还是与RL相关的）"></a>Effectiveness of Synthetic Data（个人感觉还是与RL相关的）</h3><p>supervised learning在很多方面都取得了成功，但是存在的问题是需要大量的标注数据进行训练，即使是训练好了，也难以适应现实环境。</p><p>随着计算机图形学的发展，synthetic data十分易得，而且提供了精确的ground truth，同时数据的可操作性可以让其模拟任何真实的环境场景，甚至是真实环境下难以发生的。</p><p>In the visual domain, synthetic data have been used mainly for two purposes: (1) evaluation of the generalizability of the model due to the large variability of synthetic test examples, and (2) aiding the training through data augmentation for tasks where it is difficult to obtain ground truth, e.g., optical flow or depth perception.(测试模型的普适性和增强训练过程)</p><p>此外，还有直接在真实图像中加入synthetic data，比如在KITTI数据集中加入车辆3D模型，以辅助模型训练。</p><p>One drawback of using syntheticdata for training a model is that it gives rise to <a href="https://www.lyrn.ai/2018/12/30/sim2real-using-simulation-to-train-real-life-grasping-robots/" target="_blank" rel="noopener">“sim2real” </a>domain gap. 这个sim2real是与RL相关的概念，我这里找了一个CMU的<a href="http://www.andrew.cmu.edu/course/10-703/slides/Lecture_sim2realmaxentRL.pdf" target="_blank" rel="noopener">PPT</a>，可以看下。Recently, a stream of works in domain randomization claims to generate synthetic data with sufficient variations, such that the <strong>model views real data as just another variation of the synthetic data set.</strong> 将真实的数据视为合成数据的一个变体。</p><p>One of the major challenges in using synthetic data for training is the domain gap between real and synthetic data sets. 而迁移学习它提供了一些解决办法。</p><h3 id="Domain-Adaptation-and-Transfer-Learning"><a href="#Domain-Adaptation-and-Transfer-Learning" class="headerlink" title="Domain Adaptation and Transfer Learning"></a>Domain Adaptation and Transfer Learning</h3><p>A model trained on source domain does not perform well on a target domain with <strong>different distribution.</strong> Domain adaptation(DA) is a technique which addresses this issue by reusing the knowledge gained through the source domain for the target domain.</p><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/conventional methods for DA.PNG" alt="conventional methods for DA"></p><p>DA techniques根据三个不同的标准进行分类，这三个标准（criteria）是：</p><ul><li><p>distance between domains</p></li><li><p>presence of supervision in the source and target domain</p></li><li><p>type of domain divergences</p></li></ul><p>Prevalent literature also classifies DA in supervised, semi-supervised, and unsupervised setting according to the presence of labels in source and target domain.</p><p>Earlier works categorized the domain adaptation problem into homogeneous and heterogeneous settings. 现在，随着深度学习的热度提高，DA开始引入DNN和GAN的思想。DA使用DNN 去learn representations invariant to the domain，Adversarial methods encompass a framework which consists of a label classifier trained adversarially to the domain classifier.</p><h3 id="Weakly-Supervised-Learning"><a href="#Weakly-Supervised-Learning" class="headerlink" title="Weakly Supervised Learning"></a>Weakly Supervised Learning</h3><p>Weakly supervised learning is an umbrella term covering the <strong>predictive models which are trained under incomplete, inexact, or inaccurate labels</strong>. Apart from saving annotation cost and time, weakly supervised methods have proven to be robust to change in the domain during testing.</p><h4 id="Incomplete-Supervision-标签不全"><a href="#Incomplete-Supervision-标签不全" class="headerlink" title="Incomplete Supervision(标签不全)"></a>Incomplete Supervision(标签不全)</h4><p>Weakly supervised techniques pertaining incomplete labels make use of either semi-supervised or active learning methods. The conventional semi-supervised approaches include self-training, co-training, and graph-based methods.</p><h4 id="Inexact-Supervision-标签注解程度，比如一幅图像的bounding-box标注和pixel-level标注"><a href="#Inexact-Supervision-标签注解程度，比如一幅图像的bounding-box标注和pixel-level标注" class="headerlink" title="Inexact Supervision(标签注解程度，比如一幅图像的bounding box标注和pixel-level标注)"></a>Inexact Supervision(标签注解程度，比如一幅图像的bounding box标注和pixel-level标注)</h4><p>Apart from dealing with partially labeled data sets, weakly supervised techniques also help <strong>relax the degree of annotation</strong> needed to solve a structured prediction problem.</p><p>A popular approach to harness inexact labels is to formulate the problem in <strong>multiple-instance learning (MIL) framework</strong>. In MIL, the image is interpreted as a <strong>bag of patches.</strong> If one of the patches within the image contains the object of interest, the image is labeled as a positive instance, otherwise negative. Learning scheme alternates between estimating object appearance model and predicting the patches within positive image.</p><h4 id="Inaccurate-Supervision-策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。-“webly”-supervised-scenario"><a href="#Inaccurate-Supervision-策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。-“webly”-supervised-scenario" class="headerlink" title="Inaccurate Supervision(策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。 “webly” supervised scenario)"></a>Inaccurate Supervision(策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。 “webly” supervised scenario)</h4><p>Broadly, we categorize the techniques into two sets—the first approach resorts to <strong>treating the noisy instances as outliers and discard them during training</strong>. Another stream of methods focus on <strong>building algorithms robust to noise</strong> by devising noise-tolerant loss functions or adding appropriate regularization terms.</p><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/incomplete-inexact-inaccurate supervision.PNG" alt="incomplete-inexact-inaccurate supervision"></p><h3 id="K-Shot-Learning-样本少-Few-shot-learning和Zero-shot-learning"><a href="#K-Shot-Learning-样本少-Few-shot-learning和Zero-shot-learning" class="headerlink" title="K-Shot Learning(样本少-Few-shot learning和Zero-shot learning)"></a>K-Shot Learning(样本少-Few-shot learning和Zero-shot learning)</h3><p>Few-shot learning techniques attempt to adapt the current machine learning methods to perform well under a scenario where only a few training instances are available per class. More recent efforts into a few-shot learning techniques can be broadly categorized into <strong>metric-learning</strong> and <strong>meta-learning-based </strong>methods.</p><ul><li><p>Metric-learning aims to design techniques for<strong>embedding the input instances to a feature space</strong> beneficial to few-shot settings. A common approach is to find a good similarity metric in the new feature space applicable to novel categories.</p></li><li><p>Meta-learning entails a class of approaches which quickly adapt to a new task using only a few data instances and training iterations. To achieve this, the model is <strong>trained on a set of tasks</strong>, such that it transfers the “learning ability” to a novel task. In other words, <strong>meta-learners treat the tasks as training examples</strong>.</p></li></ul><p>Another set of methods for few-shot learning relies on<strong>efficient regularization techniques </strong>to <strong>avoid over-fitting</strong> on the small number ofinstances.</p><p>Literature pertaining to Zero-Shot Learning (ZSL) focuses on finding the representation of a novel category without any instance. <strong>Methods used to address ZSL are distinct from few-shot learning.</strong> A major assumptiontaken in this setting is that<strong> the classes observed by model during training are semantically related to the unseen classes encountered during testing.</strong>This semantic relationship is often captured through class-attributes containing shape, color, pose, etc., of the object which are <strong>either labeled by experts or obtained through knowledge sources </strong>such as Wikipedia, Flickr, etc.</p><p>In ZSL, a joint embedding space is learned during training where <strong>both the visual features and semantic vectors are projected</strong>. During testing on unseen classes, <strong>nearest-neighbor search</strong> is performed in this embedding space to match the projection of visual feature vector against a novel object type.</p><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/comparison of supervised learning with ZSL.PNG" alt="comparison of supervised learning with ZSL"></p><h3 id="Self-supervised-Learning-without-any-external-supervision"><a href="#Self-supervised-Learning-without-any-external-supervision" class="headerlink" title="Self-supervised Learning(without any external supervision)"></a>Self-supervised Learning(without any external supervision)</h3><p>Explicit annotation pertaining to the main task is avoided by<strong>defining an auxiliary task that provides a supervisory signal in self-supervised learning</strong>. The assumption is that successful training of the model on the auxiliary task will inherently make it <strong>learn semantic concepts such as object classes and boundaries</strong>. This makes it possible to share knowledge between two tasks.</p><p>However, unlike transfer learning, it does not require a large amount of annotated data from another domain or task.</p><p>The existing literature pertaining self-supervision relies on <strong>using the spatial and temporal context of an entity for “free” supervision signal.</strong></p><p><img src="/2019/03/02/Beyond-Supervised-Learning-A-Computer-Vision-Perspective/supervised-weakly-supervised-self-supervised learning.PNG" alt="supervised learning/weakly-supervised learning/self-supervised learning"></p><p>上图比较了supervised learning / weakly-supervised learning / self-supervised learning之间的区别，supervised learning需要bounding box作为label进行训练 ；weakly-supervised learning使用图像级别的标题，语义描述嵌入等进行神经网络预训练；self-supervised learning使用pretext task来学习物体的表示方法。</p><h3 id="Conclusion-and-Discussion"><a href="#Conclusion-and-Discussion" class="headerlink" title="Conclusion and Discussion"></a>Conclusion and Discussion</h3><p>The space between fully supervised and unsupervised learning can be qualitatively divided on the basis of the degree of supervision needed to learn the model.</p><ul><li><p>While synthetic data are cost effective and flexible alternative to real-world data sets, the models learned using it still need to be adapted to the real-world setting.</p></li><li><p>Transfer learning techniques address this issue by explicitly aligning different domains through discrepancy-based or adversarial approaches. However, <strong>both of these techniques require “strict” annotation pertaining to the task which hinders the generalization capability of the model.</strong></p></li><li><p>Weakly supervised algorithms <strong>relax the need of exact supervision by making the learning model tolerant of incomplete, inexact, and inaccurate supervision.</strong> This helps the model to harness the huge amount of data available on the web.</p></li><li><p>Even when a particular domain contains an insufficient number of instances, methods in k-shot learning try to build a reasonable model using parameter regularization or meta-learning techniques.</p></li><li><p>Finally, self-supervised techniques <strong>completely eliminate the need of annotation as they define a proxy task for which annotation is implicit within the data instances.</strong></p></li><li><p>Despite their success, recent models weigh heavily on deep neural networks for their performance. Hence they carry both the pros and cons of using these models; <strong>cons being lack of interpretability and outcomes which largely depend on hyperparameters.</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​&lt;/p&gt;&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;这篇论文（&lt;a href=&quot;https://link.springer.com/article/10.1007/s41745-019-0099-3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;链接&lt;/a&gt;）是印度的几位教授写的，以计算机视觉的角度来阐述目前在全监督学习和无监督学习之间的几种训练方式，写得比较简略，但是适合我这种刚入门连名词都不是很清楚的人，可以让我用来梳理整个领域的技术发展脉络和现状。同时该论文也给出了大量的参考文献，也方便进行下一步的研究。实际上，严格地说，这篇文章不能算一篇合格的综述，只能说是那种“扫盲”的阅读材料。&lt;/p&gt;&lt;p&gt;出现上述技术发展的主要原因是数据集比较庞大，但是完全被标注好的数据却很少，而训练好的网络在面对现实多变的复杂场景时依然会出现问题，因此这两者之间出现了合成数据（synthetic data)，迁移学习(transfer learning)，弱监督学习(weakly supervised learning)，少量学习（Few-shot learning/K-shot learning)以及自监督学习(self-supervised learning)，边界当然就是无监督学习了(unsupervised learning)。其中需要注意，self-supervised learning和unsupervised learning不是一回事，前者仍然是有监督学习的一种，正如&lt;a href=&quot;https://www.quora.com/What-is-the-difference-between-self-supervised-and-unsupervised-learning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Quora&lt;/a&gt;中的回答：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Self-supervised, imposes classification by itself, derivative from the input data.&lt;/p&gt;&lt;p&gt;Unsupervised, does not. At all.&lt;/p&gt;&lt;p&gt;Meaning, that self-supervised, STILL has a measurement of terms of right contra wrong, as in, terms of classification.&lt;/p&gt;&lt;p&gt;Unsupervised, does not.&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="http://densecollections.top/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="survey" scheme="http://densecollections.top/tags/survey/"/>
    
      <category term="supervised learning" scheme="http://densecollections.top/tags/supervised-learning/"/>
    
      <category term="domain adaptation" scheme="http://densecollections.top/tags/domain-adaptation/"/>
    
      <category term="deep neural network" scheme="http://densecollections.top/tags/deep-neural-network/"/>
    
  </entry>
  
  <entry>
    <title>A Brief Review of Object Detection and Semantic Segmentation</title>
    <link href="http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/"/>
    <id>http://densecollections.top/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/</id>
    <published>2019-02-27T06:58:38.000Z</published>
    <updated>2020-02-03T02:28:47.260Z</updated>
    
    <content type="html"><![CDATA[<p>​</p><h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>目标检测（object detection）和语义分割（semantic segmentation）是计算机视觉的两个重要研究内容，在人脸检测，视频监控和自动驾驶中都有很多的应用，它们对于机器理解环境也具有一定的作用。在视觉SLAM上，已经有科研工作者将这些研究成果加入到现有的SLAM系统中，以提高系统的鲁棒性，同时促进语义SLAM的发展。因此，需要对这些工作进行一些调研总结和学习理解。由于对深度学习不了解，还未系统地进行学习，所以写的内容可能幼稚了些，后续会再进行更新。</p><p>值得注意的是，目标检测在经典计算机视觉中也已经提出了一些特征描述的方法，在此不涉及，只涉及基于深度学习的目标检测方法，毕竟神经网络在图片特征的学习上比较擅长。</p><p>本博客将基于文末给出的参考文献来进行总结和学习，尽力把这些问题和技术梳理清楚，同时加入一些自己的思考和不成熟的想法。</p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/focus_of_object_detection_based_on_DL.PNG" alt="focus on method based on deep learning"></p><h2 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h2><ol><li>image classification—multi level 图像分类，这个任务指的是给出一张图片，要识别出哪张图片属于哪个类别；</li><li>object detection—what categories and location 物体检测，这个任务除了需要识别出图片属于哪个类别，还需要对相应的物体进行具体位置的定位，我们通常用矩形框来框出这个物体；</li><li>semantic segmentation—pixel wise, but does not distinct everything in one category 语义分割，这个任务是指对图片中的每个 pixel 打上标签，比如这里要给它们打上 person、sheep、dog 的标签，需要进行非常精细的分类；</li><li>instance segmentation—object detection+semantic segmentation 实例分割，可以理解为进行物体检测后，对每个矩形框中的物体进行语义分割，该任务除了需要找到物体的类别和位置之外，还需要分辨出不同物体的 pixel；</li></ol><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/task definition.png" alt="task definition"></p><a id="more"></a><h2 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h2><h3 id="Progress-一步法single-stage-两步法two-stage"><a href="#Progress-一步法single-stage-两步法two-stage" class="headerlink" title="Progress (一步法single-stage, 两步法two stage)"></a>Progress (一步法<font color="DeepSkyBlue">single-stage</font>, 两步法two stage)</h3><p>最近有一篇很好的综述<a href="https://arxiv.org/pdf/1809.02165v2.pdf" target="_blank" rel="noopener">[5]</a>详细地梳理了最近5年来的generic object detection技术发展和比较的综述。</p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/developing_route_of_object_detection.jpeg" alt="object detection developing route"></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/recent progress in object detection.png" alt="progress"></p><p>上图绿色的字体表示的是 Two-stage Detector 的发展历程，当然还有很多其他的方法，这里列出的是一些比较有代表性的方法。</p><ul><li><p>2014 年有一项很重要的工作是 R-CNN，它是将物体检测首次应用于深度学习中的一篇论文，它的主要思路是将物体检测转化为这么一个问题：首先找到一个 region（区域），然后对 region 做分类。之后作者又提出了 Fast R-CNN，它是一个基于 R-CNN 的算法，运算速度显著提高。</p></li><li><p>2015 年，这一群人又提出了 Faster R-CNN，它在速度上相比 Fast R-CNN 有了更大的提高，主要是改进了怎样在 Fast R-CNN 和 R-CNN 中找 region 的过程，Faster R-CNN 也是用深度学习的方法得到一些 region（称之为 proposal），然后再用这些 proposal 来做分类。虽然距离 Faster R-CNN 的提出已经三年多了，但它依旧是使用非常广泛的一种算法。</p></li><li><p>2016 年，代季峰等人提出了 R-FCN，它在 Faster R-CNN 的基础上进行了改进，当时它在性能和速度都有非常大的提高。</p></li><li><p>2017 年有两篇影响力非常大的论文，FPN 和 Mask R-CNN。FPN 也就是 Feature Pyramid Network，何恺明大神的论文，它相当于生成了 feature pyramid，然后再用多个 level 的 feature 来做 prediction。Mask R-CNN 这篇论文获得了 ICCV 2017 的最佳论文，也是何恺明大神的作品，其在 Faster R-CNN 基础上增加了 mask branch，可以用来做实例分割，同时因为有 multi-task learning，因此它对物体框的性能也有很大的提高。（另外，今年年初，2019.1月，何恺明将自己的Mask R-CNN和FPN合在了一起，效果也不错）。</p></li><li><p>2018 年，沿着 Faster R-CNN 这条路线提出的方法有 Cascade R-CNN，它将 cascade 结构用在了 Faster R-CNN 中，同时也解决了一些 training distribution 的一些问题，因此它的性能是比较高的。另外还有两篇比较重要的论文——Relaiton Network 和 SNIP。</p></li></ul><p>上图蓝色字体表示的 Single-stage Detector 的发展历程：2014 年的 MultiBox 是非常早期的工作；2016 年提出了 SSD 和 YOLO，这两篇论文是 Single-stage Detector 中的代表作；2017 年提出了 RetinaNet（当时 Single-stage Detector 中性能最高的方法）和 YOLO v2；2018 年有一个新的思路，提出了 CornerNet，把物体检测看成一堆点的检测，然后将这些点 group 起来。</p><h3 id="General-Pipeline-of-Object-Detection"><a href="#General-Pipeline-of-Object-Detection" class="headerlink" title="General Pipeline of Object Detection"></a>General Pipeline of Object Detection</h3><p>参考文献<a href="https://www.cnblogs.com/skyfsm/p/6806246.html" target="_blank" rel="noopener">[2]</a>很好地解释了深度学习进行object detection的思路是什么。</p><p>如前所述，基于深度学习的Object Detection的方法主要有两条发展脉络，一个是<strong>single-stage detector</strong>，另一个是<strong>two-stage detector</strong>.</p><p><strong>Two-stage detector</strong></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/two-stage detector.png" alt="single-stage detector"></p><ul><li><p>第一部分是 Feature generation：首先图片经过 backbone（分类网络）后，会生成 feature； 之后 feature 或者直接进行 prediction，或者再过一个 neck 进行修改或增强；</p></li><li><p>第二部分是 Region proposal：<strong>这个部分是 Two-stage Detector 的第一个 stage</strong>，其中会有一堆 sliding window anchor（预先定义好大小的框），之后对这些框做 dense 的分类和回归；接着再筛除大部分 negative anchor，留出可能是物体的框，这些框称之为 proposal；</p></li><li><p>第三个部分是 Region recognition：有了 proposal 后，在 feature map 上利用 Rol feature extractor 来提取出每一个 proposal 对应的 feature（Rol feature），最后会经过 task head；</p></li></ul><p><strong>Single-stage detector</strong></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/single-stage detector.png" alt="single-stage detector"></p><ul><li>图片首先经过 feature generation 模块，这里也有 sliding window anchor，但是它们不是用来生成 proposal 的，而是直接用于做最终的 prediction，通过 dense 的分类和回归，能直接生成最终的检测结果。</li></ul><p><strong>一步法和两步法的区别到底在哪里？</strong></p><p>实际上，现在的二步法object detection的框架都是基于Faster R-CNN的，一步法中的比较全面的算法是SSD，这两个在后面我都会进行详细地阐述。</p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/one_two_stage_comparison.PNG" alt="comparison of one stage and two stage"></p><p>对于两步法的基础Faster R-CNN，两个阶段分别在RPN和R-CNN中完成，第一阶段是预设一系列不同大小和比例的anchors，然后将整张图传入CNN提取特征，最后利用PRN对anchors进行分类和回归，得到候选区域（proposals）；第二阶段是利用RolPooling扣取每个候选区域的特征，接着把扣取特征的特征送入后续R-CNN网络，最后对候选区域进一步分类和回归，得到最终的检测结果。</p><p>由上图也可以看出，两步法是相对于一步法多了二阶段的分类，回归和特征，因而精度会更好，但是会使得算法运行的时间加长。也就是说，<strong>一步法也就是类似于只有RPN阶段，而二步法多了R-CNN</strong>，使得分类更加精细，同时再对候选区域进行回归。</p><p><strong>那么一步法是否可以仿照这样的思路或者利用其他的方法达到二步法的精度，却又不损失运行效率？</strong></p><p>在参考文献<a href="https://mp.weixin.qq.com/s/84JG1ZGFKb6Xp3WQFjtxZw" target="_blank" rel="noopener">[3]</a>里面，张士峰博士分享了他们的工作<a href="https://github.com/sfzhang15/RefineDet" target="_blank" rel="noopener">Single-Shot Refinement Neural Network for Object Detection</a>，在SSD中加入一些类似R-CNN作用的模块，来提高检测的精度，同时也保持了原有的运行效率。</p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/architecture_of_refinedet.PNG" alt="RefineDet"></p><p>整个结构包含ARM模块，TCB连接和ODM模块，其中ARM模块和ODM模块分别对应二步法中的RPN和R-CNN的功能，TCB的主要作用是转换ARM特征，融合高层特征，因为两步法中的特征不相同，第二阶段会提取新的特征，因此需要转换和融合。<strong>该方法与二步法的区别在于没有RoIPooling</strong>，因此运行时间不会边长。该方法的精度大概较SSD提升了两个点，速度上会更快些，因为输入图像的分辨率可以是320x320的。具体的检测框架和测试结果可以看<a href="https://arxiv.org/pdf/1711.06897v3.pdf" target="_blank" rel="noopener">原论文</a>。</p><hr><p>借此提下object detection近年来的发展趋势应当不再是该网络调参来刷速度和正确率，而是回归到框架本身的反思和设计上，毕竟现有的算法都是从Faster R-CNN来的，因此是否可以反思该框架的问题，然后对其进行改善，或者直接提出新的检测思路，比如说在前处理阶段的anchors和后处理阶段的NMS都是手动设置的，这个是否可以进行自动调节，也是一个值得研究的点。</p><p>另外，在视觉SLAM和很多其他的应用场景上，输入的都是视频流的形式，而不是单个的图像，这一方面对算法的实时性，硬件的运算速度提出了要求，另一方面也对视频物体检测提出了要求，如何利用帧与帧之间的信息来加速物体检测，也是有待研究的。</p><p>最后一点就是，视觉SLAM的最终梦想是能在现实大环境下进行运行，然后给出环境地图，以此与人进行交互。现实场景很复杂，充满动态的，不确定性的因素，因此这样的功能要想实现还有很长的路要走。不过可以预见的是，视觉SLAM需要结合这些新的，基于学习方法的计算机视觉的研究成果，以帮助机器人来理解环境，提高系统的鲁棒性，同时各个模块和任务之间也将是相互联系相互促进的，多任务联合（multi-task），以此提升系统的容错性 和运行时间。</p><hr><h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p>Faster R-CNN 是 Two-stage Detector 工作的基础，它主要提出了两个东西:</p><ul><li><p><strong>RPN</strong>：即 Region Proposal Network，目前是生成 proposal 的一个标准方式;</p></li><li><p><strong>Traning pipeline</strong>：主要讲的是 Two-stage Detector 应该怎样 train，论文里给出的是一种交替(alternating training)的方法;</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/RPN.png" alt="RPN"></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/alternating training.png" alt="alternating training"></p><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/joint training.png" alt="joint training"></p><p>因为深度学习的发展，大家都不想一步一步来回调参，因此会有了<strong>joint training</strong>这个东西。</p><h3 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h3><ul><li><p>FPN（Feature Pyramid Network）主要也是提出了两项重要的思路：<strong>Top-down pathway 和 Multi-level prediction。</strong></p></li><li><p>下图中的 4 张图代表了基于单个或多个 feature map 来做预测的 4 种不同的套路：</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/four ways of FPN.png" alt="four ways of FPN"></p><ul><li>具体实现如下图所示：</li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/pipeline of FPN.png" alt="pipeline of FPN"></p><h3 id="MASK-R-CNN"><a href="#MASK-R-CNN" class="headerlink" title="MASK R-CNN"></a>MASK R-CNN</h3><p>Mask R-CNN 主要也有两点贡献：<strong>RoIAlign</strong>和<strong>Mask branch</strong></p><ul><li><p>RoIAlign：在 Mask R-CNN 之前，大家用得比较多的是 Rol Pooling，实现过程是：给出一个框，在 feature map 上 pool 出一个固定大小的 feature，比如要 pool 一个 2×2 的 feature，首先把这个框画出 2×2 的格子，每个格子映射到 feature map，看它覆盖了多少个点，之后对这些点做 max pooling，这样就能得出一个 2×2 的 feature。它的劣势是如果框稍微偏一点，得出的 feature 却可能是一样的，存在截断误差。RolAlign 就是为了解决这一问题提出的。Rol Align 并不是直接对框内的点做 max pooling，而是用双线性插值的方式得到 feature。其中还有一步是：在 2×2 的每个框中会选择多个点作为它的代表，这里选择了 4 个点，每个点分别做双线性插值，之后再让这 4 个点做 max/average pooling</p></li><li><p>Mask branch：它是一个非常精细的操作，也有额外的监督信息，对整个框架的性能都有所提高。它的操作过程如下图所示:</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/mask branch.png" alt="mask branch"></p><h3 id="Cascade-R-CNN"><a href="#Cascade-R-CNN" class="headerlink" title="Cascade R-CNN"></a>Cascade R-CNN</h3><ul><li><p>Cascade R-CNN 是目前 Faster R-CNN 这条线中较新的方法。这个方法也提出了两点贡献：一是提出了使用 <strong>cascade architecture</strong>；二是提出了怎样来适应 <strong>training distribution</strong>。</p></li><li><p>Cascade architecture ：这个框架不是特别新的东西，之前也有类似的结构。下图左边是 Faster R-CNN，右边是 Cascade R-CNN。其中 I 代表图像或者图像生成的 feature map，H0 是 RPN，B 是 bounding box regression，C 是 classification。经过 RPN 得到的 proposal 再做 pooling 后，会有分类和回归这两个 prediction。</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/cascade architecture.png" alt="cascade architecture"></p><ul><li>Cascade R-CNN 的结构是，在经过第一次分类和回归之后，会用得到 bounding box 再来做一次 pooling，然后对这些框做下一阶段的分类和回归，这个过程可以重复多次。但如果仅仅使用 Cascade R-CNN 而不做其他改变，Cascade R-CNN 带来的提高是非常有限的。 Cascade R-CNN 提出了一个很好的 motivation，这是它比较有意义的一个地方。它研究了一下采用不同的 IoU 阈值来进行 training 的情况下，Detector 和 Regressor 的性能分布。</li></ul><h3 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h3><ul><li><p>RetinaNet 是 Singe-stage Detector 目前比较重要的一项工作，它可以看做是由 FPN+Focal Loss 组成的，其中 FPN 只是该论文中用到的架构，而 Focal Loss 则是本论文主要提出的工作。</p></li><li><p>RetinaNet 结构如下：</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/RetinaNet.png" alt="RetinaNet"></p><ul><li><p>RetinaNet 的结构 和 SSD 非常类似，只不过它用的是 ResNet，并在 ResNet 上加入了 FPN 的结构，每一层都有两个分支：一个用来做分类；另一个用来做框回归。此外它的每个 head 都比 SSD 和 Faster R-CNN 都要大一点，这样的话，它的参数量比较大，计算速度也比较慢。</p></li><li><p>而 Focal Loss 试图解决的问题是 class imbalance。针对 class imbalance 的问题，Two-stage Detector 一般是通过 proposal、mini-batch sampaling 两种方式来解决的；SSD 是通过 hard negative mining 来解决的；而 RetinaNet 则通过 Focal Loss 来解决该问题。</p></li><li><p>Focal loss 的核心思路是：对于 high confidence 的样本，给一个小的 loss——这是因为正负样本不平衡，或者说是由于 class imbalance 导致了这样的问题：比如说正负样本的比例是 1:1000，虽然负样本的 loss 都很小，但数目非常多，这些负样本的 loss 加起来的话，还是比正样本要多——这样的话，负样本就会主导整个框架。</p></li></ul><h3 id="Relation-Network"><a href="#Relation-Network" class="headerlink" title="Relation Network"></a>Relation Network</h3><ul><li>在 Relation Network 之前的大部分 detectcor，在做 prediction 或 training 的时候通常只考虑到当前这一个框，而 Relation Network 提出还要考虑这个框周围的框，并基于此提出了一个 relation module，可以插在网络的任何位置，相当于是 feature refinement。Relation module 如下图所示：</li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/relation network.png" alt="Relation Network"></p><ul><li>它的核心思想是：当前框的 feature 除了由当前框决定之外，还要考虑当前框和周围框及其它框的关系</li></ul><h3 id="SNIP"><a href="#SNIP" class="headerlink" title="SNIP"></a>SNIP</h3><ul><li><p>SNIP（Scale Normalization for Image Pyramids）另一篇比较有启发性的工作。它提出的问题是：在 train 分类器的时候，究竟是要 scale specific 还是 scale invariant。传统的 detector 通常会选择 scale invariant，但 SNIP 研究了一下之前的方法后，发现之前的训练方式得到的 feature 对 scale 并没不是很 robust，因而提出要尽量减少 scale 的 variance，让训练时的 scale 尽可能地相似。</p></li><li><p>SNIP 结构图如下:</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/SNIP.png" alt="SNIP"></p><h3 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h3><ul><li><p>CornerNet，是 Singe-stage Detector 中比较新的方法，其与其他方法最不一样的地方是：之前的方法会在图像上选出框，再对框做分类的问题；CornerNet 则是在图中找到 pair 的关键点，这个点就代表物体。它的 pipeline 包括两步：第一步是检测到这样的 corner，即 keypoint；第二步是 group corner，就是说怎样将同一个物体的左上顶点和右下顶点框到一起。</p></li><li><p>其算法结构如下：</p></li></ul><p><img src="/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/cornerNet.png" alt="CornerNet"></p><h3 id="mmdetection"><a href="#mmdetection" class="headerlink" title="mmdetection"></a><a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener">mmdetection</a></h3><p>港中文多媒体实验室的开源物体检测框架。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1]. <a href="http://www.mooc.ai/open/course/604" target="_blank" rel="noopener">物体检测算法的近期发展及开源框架介绍-陈恺</a><br>[2]. <a href="https://www.cnblogs.com/skyfsm/p/6806246.html" target="_blank" rel="noopener">基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN</a><br>[3]. <a href="https://mp.weixin.qq.com/s/84JG1ZGFKb6Xp3WQFjtxZw" target="_blank" rel="noopener">基于深度学习的物体检测算法对比探索 - 张士峰</a><br>[4]. <a href="https://blog.csdn.net/Gentleman_Qin/article/details/84421435" target="_blank" rel="noopener">基于深度学习的目标检测算法近5年发展历史（综述）</a><br>[5]. <a href="https://arxiv.org/pdf/1809.02165v2.pdf" target="_blank" rel="noopener">Deep Learning for Generic Object Detection: A Survey</a><br>[6]. <a href="https://zhuanlan.zhihu.com/p/48169867" target="_blank" rel="noopener">目标检测算法中检测框合并策略技术综述</a><br>[7]. <a href="https://mp.weixin.qq.com/s/mu_4kNGZuExxUK2JFTdDFw" target="_blank" rel="noopener">综述 | CVPR2019目标检测方法进展</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​&lt;/p&gt;&lt;h2 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h2&gt;&lt;p&gt;目标检测（object detection）和语义分割（semantic segmentation）是计算机视觉的两个重要研究内容，在人脸检测，视频监控和自动驾驶中都有很多的应用，它们对于机器理解环境也具有一定的作用。在视觉SLAM上，已经有科研工作者将这些研究成果加入到现有的SLAM系统中，以提高系统的鲁棒性，同时促进语义SLAM的发展。因此，需要对这些工作进行一些调研总结和学习理解。由于对深度学习不了解，还未系统地进行学习，所以写的内容可能幼稚了些，后续会再进行更新。&lt;/p&gt;&lt;p&gt;值得注意的是，目标检测在经典计算机视觉中也已经提出了一些特征描述的方法，在此不涉及，只涉及基于深度学习的目标检测方法，毕竟神经网络在图片特征的学习上比较擅长。&lt;/p&gt;&lt;p&gt;本博客将基于文末给出的参考文献来进行总结和学习，尽力把这些问题和技术梳理清楚，同时加入一些自己的思考和不成熟的想法。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/focus_of_object_detection_based_on_DL.PNG&quot; alt=&quot;focus on method based on deep learning&quot;&gt;&lt;/p&gt;&lt;h2 id=&quot;Task-Definition&quot;&gt;&lt;a href=&quot;#Task-Definition&quot; class=&quot;headerlink&quot; title=&quot;Task Definition&quot;&gt;&lt;/a&gt;Task Definition&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;image classification—multi level 图像分类，这个任务指的是给出一张图片，要识别出哪张图片属于哪个类别；&lt;/li&gt;&lt;li&gt;object detection—what categories and location 物体检测，这个任务除了需要识别出图片属于哪个类别，还需要对相应的物体进行具体位置的定位，我们通常用矩形框来框出这个物体；&lt;/li&gt;&lt;li&gt;semantic segmentation—pixel wise, but does not distinct everything in one category 语义分割，这个任务是指对图片中的每个 pixel 打上标签，比如这里要给它们打上 person、sheep、dog 的标签，需要进行非常精细的分类；&lt;/li&gt;&lt;li&gt;instance segmentation—object detection+semantic segmentation 实例分割，可以理解为进行物体检测后，对每个矩形框中的物体进行语义分割，该任务除了需要找到物体的类别和位置之外，还需要分辨出不同物体的 pixel；&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;img src=&quot;/2019/02/27/A-Brief-Review-of-Object-Detection-and-Semantic-Segmentation/task definition.png&quot; alt=&quot;task definition&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="科研记录" scheme="http://densecollections.top/categories/%E7%A7%91%E7%A0%94%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="computer vision" scheme="http://densecollections.top/tags/computer-vision/"/>
    
      <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
      <category term="object detection" scheme="http://densecollections.top/tags/object-detection/"/>
    
      <category term="semantic segmentation" scheme="http://densecollections.top/tags/semantic-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04下安装NVIDIA最新驱动以及深度学习环境配置</title>
    <link href="http://densecollections.top/2019/02/18/Ubuntu16-04%E4%B8%8B%E5%AE%89%E8%A3%85NVIDIA%E6%9C%80%E6%96%B0%E9%A9%B1%E5%8A%A8%E4%BB%A5%E5%8F%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://densecollections.top/2019/02/18/Ubuntu16-04下安装NVIDIA最新驱动以及深度学习环境配置/</id>
    <published>2019-02-18T10:15:39.000Z</published>
    <updated>2020-02-03T02:28:47.257Z</updated>
    
    <content type="html"><![CDATA[<hr><hr><hr><center>2019.3.8日来更，可以不用看以前下面写的内容了，因为不是很完整。。。</center><p>如果按照之前说的通过官网下载的run文件来手动安装驱动，可能出现nvidia-settings打不开的情况，比如我就是输出信息：“ERROR: Unable to load info from any available system”，这样会导致我的电脑识别不了外接显示器。。。</p><p>后来在网上看了一些人的回答，大致的原因是官网的run文件在安装时后会默认更改一些系统配置，因此可能会导致一些错误。。</p><p><strong>解决办法：通过Ubuntu自动的install功能进行安装</strong></p><p>1.先按照这个<a href="https://blog.csdn.net/u014561933/article/details/79958017" target="_blank" rel="noopener">教程</a>卸载驱动，然后禁用好nouveau；</p><p>2.重启后确认nouveau已禁用，关闭图形界面，进入命令行界面；</p><p>3.输入下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install nvidia- #这里选择你要安装的版本，最新的应该也没问题，我就是直接装的410，省得到时候又得重装CUDA和CUDNN</span><br><span class="line">reboot #重启</span><br></pre></td></tr></table></figure><p>4.再次开机后应该就没问题了，nvidia-smi, nvidia-settings都没问题了，外接显示器也正常了。</p><hr><hr><hr><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>本人电脑是战神神舟Z7M-KP5GZ，显卡是GTX1050TI，按照以前利用liveusb的方法安装了Ubuntu16.04双系统，然而第一次安装NVIDIA驱动的时候有些问题，直接是在软件更新里面安装的系统自带驱动，版本384。后来配置深度学习环境需要安装CUDA和CUDNN，由于安装最新版本的CUDA需要相应的驱动版本对应，因此需要对NVIDIA驱动进行更新。</p><p>在安装过程中，遇到了一些问题，主要是：</p><ul><li><p>驱动安装后Ubuntu系统无法进入，卡在启动界面的5个原点；</p></li><li><p>成功进入系统后，分辨率低；</p></li></ul><p>后来根据几篇博客解决了上述问题：</p><ul><li><p><a href="https://blog.csdn.net/wang_ys121/article/details/82881716" target="_blank" rel="noopener">博客1</a></p></li><li><p><a href="https://blog.csdn.net/xunan003/article/details/81665835" target="_blank" rel="noopener">博客2</a></p></li><li><p><a href="https://blog.csdn.net/ezhchai/article/details/78788564" target="_blank" rel="noopener">博客3</a></p></li><li><p><a href="https://blog.csdn.net/yinxingtianxia/article/details/82503388" target="_blank" rel="noopener">博客4</a></p></li></ul><p>后来总结了下，<strong>最主要的问题是一定要禁用nouveau</strong>，可以通过下面的命令看是否禁用，如果不输出信息则是成功禁用。。一定要注意，如果之前装过了NVIDIA驱动，然后卸载重装了一定要记得再次禁用！！！！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="利用run方式安装NVIDIA最新驱动"><a href="#利用run方式安装NVIDIA最新驱动" class="headerlink" title="利用run方式安装NVIDIA最新驱动"></a>利用run方式安装NVIDIA最新驱动</h2><p><strong>1.验证nouveau是否已禁用</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure><p>2.如果没有输出信息，则说明已经禁用，如果没有，编辑文件blacklist.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/modprobe.d/blacklist.conf</span><br></pre></td></tr></table></figure><p>在文件最后部分插入以下两行内容(i插入，esc退出编辑，:wq退出并保存)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br></pre></td></tr></table></figure><p>更新系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-initramfs -u</span><br></pre></td></tr></table></figure><p>重启系统，再看看是不是禁用了nouveau</p><p>3.在<a href="http://www.nvidia.cn/page/home.html" target="_blank" rel="noopener">英伟达官网</a>查找并下载自己对应的驱动(.run文件)</p><p>4.ctrl+alt+F1进入命令行界面，登陆进入，并关闭图形界面</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm stop</span><br></pre></td></tr></table></figure><p>5.卸载原有的驱动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apt-get purge nvidia-*</span><br><span class="line">nvidia-smi</span><br><span class="line">#如果还有信息，输入下面的代码</span><br><span class="line">sudo /usr/bin/nvidia-uninstall</span><br><span class="line">nvidia-smi</span><br><span class="line">#没有驱动信息则成功</span><br></pre></td></tr></table></figure><p>6.进入驱动文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod a+x NVIDIA-Linux-x86_64-xxx.x.run #这里版本号每个人可能不同(xxx.xx代表版本号)</span><br><span class="line">sudo ./NVIDIA-Linux-x86_64-xxx.xx.run -no-x-check -no-nouveau-check -no-opengl-files</span><br></pre></td></tr></table></figure><p><strong>7.安装过程中的选项很重要，如果选错了，不要重启，重新安装就行了</strong></p><p><strong>其中register the kernel module souces with DKMS选择NO，Nvidia’s 32-bit compatibility libraries也选NO，其他的YES或者正常就行</strong>。</p><p>8.检查是否安装完成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">modprobe nvidia #挂载驱动</span><br><span class="line">nvidia-smi</span><br><span class="line">#如果出现驱动信息表示安装成功</span><br><span class="line">sudo service lightdm start #重新进入图形界面</span><br></pre></td></tr></table></figure><p><strong>9.如果重启开机没问题，那么万事大吉，如果进不去，或者卡在启动界面，可以选择recovery mode进入系统，然后修改grub文件，我是依据<a href="https://blog.csdn.net/ezhchai/article/details/78788564" target="_blank" rel="noopener">博客3</a>解决了我的问题，我看了下，应该还是nouveau的问题。</strong></p><p><strong>每个电脑不一样，如果依据此博客还是解决不了，那么根据自己的问题去百度或者Google吧。。</strong></p><p>10.通过recovery mode进入了系统后，重新按照前述步骤卸载重装NVIDIA驱动，这说明了我之前卸载不干净或者卸载之后nouveau又被重新开启了。。。。</p><p>11.之后重启后就能正常进入Ubuntu系统了，这说明应该已经成功了，之后再<strong>重新打开grub文件（<a href="https://blog.csdn.net/ezhchai/article/details/78788564" target="_blank" rel="noopener">博客3</a>），把之前改过的再改回来</strong>，然后重启</p><p>12.重启之后发现分辨率低而且无法在设置中心改，按照<a href="https://blog.csdn.net/yinxingtianxia/article/details/82503388" target="_blank" rel="noopener">博客4</a>的方法结果成功解决了问题，自此，NVIDIA驱动安装完成。</p><h2 id="安装CUDA和CUDNN"><a href="#安装CUDA和CUDNN" class="headerlink" title="安装CUDA和CUDNN"></a>安装CUDA和CUDNN</h2><p>安装好驱动后基本上等于深度学习环境配置进度完成了80%了，之后可以安装相应的CUDA和CUDNN了，安装过程比较简单，在此不再赘述，可以参照以下两篇博客，也可以自己重新定义版本关键词进行搜索。</p><p><a href="https://blog.csdn.net/wanzhen4330/article/details/81699769" target="_blank" rel="noopener">博客6</a></p><p><a href="https://blog.csdn.net/weixin_42279044/article/details/83181686" target="_blank" rel="noopener">博客7</a></p><hr><hr><center>**2018.3.2踩坑来更**</center><hr><hr><p>今天Ubuntu系统提示升级，结果升级了内核kernel，导致升级完成重启后出现ACPI ERROR之类的错误，看了一大推的博客总结后，发现问题是<strong>“升级之后的内核，是不会自动加载你的显卡驱动的，那就需要在这个心内核上手动重新安装NV驱动。”</strong> 所以，等装完了驱动后，最好禁止Ubuntu内核更新或者直接禁止Ubuntu更新。</p><p>所以，升级内核后，需要重新安装NVIDIA驱动。。！！！！！！</p><p><strong>当然重装驱动的最重要步骤就是禁用nouveau,这里我看到了另一篇博客（<a href="https://blog.csdn.net/dihuanlai9093/article/details/79253963/" target="_blank" rel="noopener">博客8</a>）讲得比较详细，基本上只看这一篇博客就可以安装并配置好深度学习环境了，要记得同时修改/etc/modprobe.d/disable-nouveau.conf和/etc/default/grub</strong></p><p>而且要特别注意，装完后要记得回来修改/etc/default/grub，否则修改分辨率后会发现没有作用。我今天安装时做了实验，我是按照<a href="https://blog.csdn.net/ezhchai/article/details/78788564" target="_blank" rel="noopener">博客3</a>对/etc/default/grub进行修改的，也就是加了“nomodeset”，发现只有将它删除，<a href="https://blog.csdn.net/yinxingtianxia/article/details/82503388" target="_blank" rel="noopener">博客4</a>的修改方法才有用，另外注意<a href="https://blog.csdn.net/yinxingtianxia/article/details/82503388" target="_blank" rel="noopener">博客4</a>添加的Modes后面的分辨率要根据自己的电脑调整，否则可能出现登录界面比较小，没接到电脑边。</p><p>英伟达的驱动真是一大堆破事，，看来是时候着手docker了。。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;&lt;hr&gt;&lt;hr&gt;&lt;center&gt;2019.3.8日来更，可以不用看以前下面写的内容了，因为不是很完整。。。&lt;/center&gt;&lt;p&gt;如果按照之前说的通过官网下载的run文件来手动安装驱动，可能出现nvidia-settings打不开的情况，比如我就是输出信息：“ERROR: Unable to load info from any available system”，这样会导致我的电脑识别不了外接显示器。。。&lt;/p&gt;&lt;p&gt;后来在网上看了一些人的回答，大致的原因是官网的run文件在安装时后会默认更改一些系统配置，因此可能会导致一些错误。。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;解决办法：通过Ubuntu自动的install功能进行安装&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;1.先按照这个&lt;a href=&quot;https://blog.csdn.net/u014561933/article/details/79958017&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;教程&lt;/a&gt;卸载驱动，然后禁用好nouveau；&lt;/p&gt;&lt;p&gt;2.重启后确认nouveau已禁用，关闭图形界面，进入命令行界面；&lt;/p&gt;&lt;p&gt;3.输入下面的命令：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo add-apt-repository ppa:graphics-drivers/ppa&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo apt-get update&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo apt-get install nvidia- #这里选择你要安装的版本，最新的应该也没问题，我就是直接装的410，省得到时候又得重装CUDA和CUDNN&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;reboot #重启&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;4.再次开机后应该就没问题了，nvidia-smi, nvidia-settings都没问题了，外接显示器也正常了。&lt;/p&gt;&lt;hr&gt;&lt;hr&gt;&lt;hr&gt;&lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;本人电脑是战神神舟Z7M-KP5GZ，显卡是GTX1050TI，按照以前利用liveusb的方法安装了Ubuntu16.04双系统，然而第一次安装NVIDIA驱动的时候有些问题，直接是在软件更新里面安装的系统自带驱动，版本384。后来配置深度学习环境需要安装CUDA和CUDNN，由于安装最新版本的CUDA需要相应的驱动版本对应，因此需要对NVIDIA驱动进行更新。&lt;/p&gt;&lt;p&gt;在安装过程中，遇到了一些问题，主要是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;驱动安装后Ubuntu系统无法进入，卡在启动界面的5个原点；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;成功进入系统后，分辨率低；&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;后来根据几篇博客解决了上述问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/wang_ys121/article/details/82881716&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客1&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/xunan003/article/details/81665835&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/ezhchai/article/details/78788564&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客3&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yinxingtianxia/article/details/82503388&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客4&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;后来总结了下，&lt;strong&gt;最主要的问题是一定要禁用nouveau&lt;/strong&gt;，可以通过下面的命令看是否禁用，如果不输出信息则是成功禁用。。一定要注意，如果之前装过了NVIDIA驱动，然后卸载重装了一定要记得再次禁用！！！！&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;lsmod | grep nouveau&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="深度学习环境配置" scheme="http://densecollections.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="deep learning" scheme="http://densecollections.top/tags/deep-learning/"/>
    
      <category term="linux" scheme="http://densecollections.top/tags/linux/"/>
    
      <category term="nvidia driver" scheme="http://densecollections.top/tags/nvidia-driver/"/>
    
  </entry>
  
  <entry>
    <title>Linear Algebra of MIT</title>
    <link href="http://densecollections.top/2019/02/14/Linear-Algebra-of-MIT/"/>
    <id>http://densecollections.top/2019/02/14/Linear-Algebra-of-MIT/</id>
    <published>2019-02-14T10:40:57.000Z</published>
    <updated>2020-02-03T02:28:47.282Z</updated>
    
    <content type="html"><![CDATA[<p>​ ​</p><p><strong>这是对MIT 18.06 open course— Linear Algebra—by Gilbert Strang 的课程总结。</strong></p><h2 id="课程视频及主页资料"><a href="#课程视频及主页资料" class="headerlink" title="课程视频及主页资料"></a>课程视频及主页资料</h2><p>由于计算机视觉以及机器学习中对矩阵的要求较高，而且本科阶段的高等代数学得不太行，故借此机会将这个著名的MIT线性代数公开课看了一遍，相关的课程视频和材料放在下面。</p><ul><li><p><a href="https://www.bilibili.com/video/av15463995?p=2" target="_blank" rel="noopener">lectures:</a></p></li><li><p><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/readings/" target="_blank" rel="noopener">readings:</a></p></li><li><p><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/assignments/" target="_blank" rel="noopener">assignments:</a></p></li><li><p><a href="http://math.mit.edu/~gs/linearalgebra/" target="_blank" rel="noopener">exams:</a></p></li><li><p><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/tools/" target="_blank" rel="noopener">tools:</a></p></li></ul><a id="more"></a><h2 id="课程脉络"><a href="#课程脉络" class="headerlink" title="课程脉络"></a>课程脉络</h2><p>Gilbert老爷子这门线代课应用性很强，在课堂上经常结合工程实际来探讨矩阵在其中的应用和联系，比如在对投影，最小二乘法原理以及求解矛盾方程组的三者之间用$A^{T}A$进行联系，这中“惊人”的发现也进一步加深了我对线性代数的通用性认识，所以这门课对搞了一些研究回过头来听的人来说很有启发，有时候没事可以时常翻翻这门课的教材。</p><p>这门课的核心思想就是老爷子的那张著名的”four subspaces”图，如下图所示。整门课分为三个部分：$Ax=b$和四个子空间；最小二乘法，行列式和特征值；正定矩阵以及应用。</p><p><img src="/2019/02/14/Linear-Algebra-of-MIT/four subspaces.png" alt="four subspaces"></p><p>Gilbert先从方程组入手，指出现实生活中的问题抽象成一系列方程，组成方程组，因此矩阵的出现是为了应对消元求解方程组，同时利用系数矩阵来从另一个层面认识这个系统的性质，比如可解性，稳定性等等，在这里，Gilbert将系数矩阵看成是向量的线性组合，从而直接转成空间的角度来看待“解”，这也是该部分的一个重要思想。矩阵也会表示空间，最基本的思想是把矩阵认为是n个列向量组成的，研究这些列向量彼此之间的关系来来研究矩阵代表的空间。求解问题也变成是列向量的线性组合。矩阵的行变换代表着消元法，同时也会引发矩阵的分解，求解Ax=b时转换成矩阵的列空间和零空间来二分，特定解和特殊解。之后引申出四个基本子空间：行空间，零空间，列空间，左零空间，进而指出空间的维数和基。</p><p>第一部分的关键词就是<strong>消元，向量，线性组合和空间</strong>。矩阵，空间，方程组之间是等价的。</p><p>到了第二部分，Gilbert开始引入行列式。他先从正交矩阵讲起，即$A^{T}A=I$，指出了它在投影，最小二乘，求解矛盾方程组之间的作用，并且利用它把这些问题都联系在了一起。其本质的原因可能是所谓的”空间度量，距离最小“，然后系数矩阵$A$恰好用这种方式呈现出来。之后引入行列式就很自然了，因为要求”大小“或者”长度“。Gilbert通过三个基本的性质定义了矩阵的行列式，并且推导出了其他几种性质，然后顺带着给出了行列式的计算公式和求解方程组的作用，及求逆公式，虽然他并不推崇这种死板的大计算量的方法来求解。最后，Gilbert借着行列式给出了矩阵的特征值，特征向量，以及特征值在对角化和其他工程中的应用（行列式的作用就是对角化）。不过比较可惜的一点是，教授并没有深层次探讨矩阵的特征值的意义，仅仅是从特征值公式上说”向量经过矩阵变换后方向不变“，虽然后来的例子又将特征值特征向量与主轴定理结合起来，但是依然没有一种很明晰的感觉。可能因为这是工科课程，重在讲解应用和联系。。希望我后面能在”linear algebra done right”这本书里找到答案。</p><p>最后一部分的主题是正定性，这类似与函数当中的最小值对应条件问题。这一部分我看得比较乱，没看出什么真正的名堂出来，毕竟从内容上我没找出什么联系。我后来问了数学系的同学，他说正定矩阵是为了定义内积空间（又称希尔伯空间，量子力学就是定义在这个上面）。这里面涉及到多元函数最小值判定，酉矩阵，以及奇异值分解，因此还是比较重要的内容。</p><p>实际上呢，教授是从多元线性方程组的角度来讲线性代数和矩阵的，因此并不像数学系那样从映射，集合、环、域，空间，然后到空间结构，代数，多元数组的关系来展开讲。因为是工科课程，所以第一关心的问题是怎么用，后面才会去进一步关注如何产生这种思想的问题，在这里，Gilbert教授关注的是矩阵角度解方程组，诸如矩阵分解、对角化，投影，正交矩阵，正定矩阵都是为了和工程中实际方法相联系和结合，因此可以看作是一种“top-down”的讲授思维。</p><p>对于做SLAM和机器学习来讲，关注更多的好像还是数值矩阵求解，即如何利用数值计算工具，更有效，更准确的分解、求解矩阵，所以，我接下来还得继续看fast.ai的numerical linear algebra课了。。</p><h2 id="阅读材料"><a href="#阅读材料" class="headerlink" title="阅读材料"></a>阅读材料</h2><p>这门课除了Gilbert Strang的那本教材，还有一些其他的资料，我把它们都托管到了我的<a href="https://github.com/Richardyu114/MIT-Linear-Algebra-Learning-Materials" target="_blank" rel="noopener">Github</a>上，有需要的请自取。</p><p>清单如下：</p><ul><li><p>Introduction to Linear Algebra</p></li><li><p>超详细MIT线性代数公开课笔记</p></li><li><p>神奇的矩阵第一季</p></li><li><p>神奇的矩阵第二季</p></li><li><p>线性代数的几何意义</p></li><li><p>Linear Algebra Done Right</p></li></ul><p>另外也有其他几本线性代数教材也不错：</p><ul><li><p><strong>圣经</strong> Linear Algebra-hoffman</p></li><li><p>Linear Algebra and Its Applications-David C. Lay</p></li><li>Introduction to Applied Linear Algebra—Vectors, Matrices, and Least Squares</li></ul><hr><hr><center>看完《线性代数的几何意义》和《神奇的矩阵》来更</center><hr><hr><h2 id="线性代数的几何意义"><a href="#线性代数的几何意义" class="headerlink" title="线性代数的几何意义"></a>线性代数的几何意义</h2><h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><ul><li>代数的功能是进行抽象，为了解决问题的方便，提高效率。线性代数里面的线性主要是指线性空间里面的线性变换（可加性和比例性），通过<strong>线性算子</strong>定义了线性变换，也就是变换满足可加性和比例性，实际上，差分，微分都是一种数学上的算子，代表一种运算关系。</li></ul><h3 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h3><ul><li>行列式的几何意义具有深刻的含义。它是指行列式的行向量或列向量所构成的平行多面体的有向体积。这个有向体积是由许多块更小的有向面积或有向体积的累加。行列式的几何意义是什么呢？<strong>概括说来有两个解释：一个解释是行列式就是行列式中的行或列向量所构成的超平行多面体的 有向面积或有向体积；另一个解释是矩阵 $A$ 的行列式 $detA$ 就是线性变换 $A$ 下的图形面积或体积的伸缩因子。一个给定的行列式，它的行向量顺序也给定了，不能随意改变其顺序。</strong></li><li>一个行列式的整体几何意义是有向线段（一阶行列式）或有向面积（二阶行列式）或有向体积（三阶行列式及以上）。因此，行列式最基本的几何意义是由各个坐标轴上的有向线段所围起来的所有有向面积或有向体积的累加和。这个累加要注意每个面积或体积的方向或符号，方向相同的要加，方向相反的要减，因而，这个累加的和是代数和。</li><li>行列式的乘积项及其逆序数的几何意义实际上是行列式最根本的几何意义，因而可以解释所有的行列式的定义及其性质。</li></ul><h3 id="向量空间"><a href="#向量空间" class="headerlink" title="向量空间"></a>向量空间</h3><ul><li>设$V$是非空的n维向量的集合(n=1,2,3…)，如果$V$中的向量对加法和数乘两种运算封闭，即：</li></ul><script type="math/tex;mode=display">\begin{align*}& 1. 若\overrightarrow{a},\overrightarrow{b} \in V,则\overrightarrow{a}+\overrightarrow{b} \in V \\& 2. \overrightarrow{a} \in V,则k\overrightarrow{a} \in V,k为任意实数\end{align*}</script><p>​ 则称$V$为向量空间。</p><ul><li>向量空间主要有两种：一种是由 V 中的一个向量组张成的空间（比如由特征向量张成的子空间等）。另外一种齐次线性方程组的解集组成的解空间。实际上，线性方程组的解空间也是解向量所张成，这两种空间里都包含有无穷多的向量。</li><li>值得注意的是，<strong>所有的子空间一定要包含零空间在内</strong>。实际上，我们现在讨论的向量，不能称之为自由向量，因为所有的向量的尾巴都被拉到了原点上，或者说，所有向量空间里的向量都是从原点出发的，大家都有一个共同的零空间，这就是为什么所有的子空间一定要包含零空间的原因了。</li><li>为什么要把向量的尾部都拉到原点是为了研究向量的方便，因为这样就可以把向量和空间中的点一一对应起来。空间中一旦建立起了坐标系，点有坐标值，那么我们就用点的坐标表示与点对应的向量，这样向量就有了解析式，就有了向量的坐标表达式，我们就可以方面的使用代数中的矩阵技术进行分析及计算了。如果一个子空间没有通过原点，那么从原点出发的向量必然首尾不顾，造成了向量头在子空间中尾在空间外（因为原点在空间外）。当然，向量的加法和数乘也都跑到子空间外面去了。</li><li>内积的定义要解决空间中不同的基带来不同坐标度量带来向量长度和夹角的变化。可以通过<strong>内积度量矩阵</strong>$P^{T}P$来解决此问题，无论你变了多少次基，每变一次基同时就改变一次度量矩阵，从而来保证内积值的不变性，这样也就保证了向量空间度量的一致性，达到了度量值不随坐标改变而改变的目的。</li></ul><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><ul><li>我们知道，在直角坐标系中，一个有序的实数数组$(a,b)$和$(a,b,c)$分别代表了平面上和空间上的一个点，这就是实数组的几何意义。类似的，在线性空间中如果确定了一个基，线性映射就可以用确定的矩阵来表示，这就是矩阵的几何意义：线性空间上的线性映射。矩阵独立的几何意义表现为对向量的作用结果。</li><li>如果用数组来统一定义标量、向量和矩阵的话就是：<strong>标量是一维向量，向量是标量的数组，矩阵则是向量的数组。</strong></li><li>矩阵与向量乘积比如$Ax$表现为矩阵$A$对一个向量$x$作用的结果。其作用的主要过程是对一个向量进行旋转和缩放的综合过程（即线性变换的过程），一个向量就变换为另外一个向量。一个 m 行 n 列的实矩阵$A_{m \times n}$就是一个$R^{n} \rightarrow R^{m}$上的线性变换，或者说，矩阵$A_{m \times n}$把一个n维空间的 n 维向量变换为一个m维空间的m维向量。</li></ul><ul><li>一个矩阵乘以一个向量，一般将会对向量的几何图形进行旋转和伸缩变化，而旋转矩阵只对向量进行旋转变化而没有伸缩变化，例如二阶旋转矩阵$A$：</li></ul><script type="math/tex;mode=display">A=\begin{pmatrix}\cos \theta & -\sin \theta \\\sin \theta & \cos \theta\end{pmatrix}</script><ul><li>如果将上面的旋转矩阵分别按行向量和列向量来看，随着角度的增大，行向量在顺时针旋转，而列向量在逆时针旋转，“这正是正交矩阵的特征”（然而这一结论的得出还不是很明了）。旋转矩阵为正交矩阵。</li><li>矩阵与矩阵的相乘的几何意义，可以从矩阵与多个向量相乘的几何意义得到，只是多个向量被按照顺序组合成了另一个矩阵。</li></ul><ul><li><strong>实际上，从$R^{n} \rightarrow R^{m}$上的线性变换都可以表述为一个矩阵变换；反过来，一个矩阵变换也必然是一个线性变换。两者具有一一对应的关系。一个矩阵变换也必然是一个线性变换。</strong>这个对应关系笼统地表述如下：<ul><li>线性变换的和对应着矩阵的和；</li><li>线性变换的乘积对应着矩阵的乘积；</li><li>线性变换的数量乘积对应着矩阵的数量乘积；</li><li>线性变换的逆对应着矩阵的逆；</li></ul></li></ul><ul><li>下面的定理给出了如何把一个$R^{2}$空间上的线性变换转换成一个对应的 2 阶矩阵的办法:</li></ul><script type="math/tex;mode=display">定理：设T:R^{2} \rightarrow R^{2}是一个线性变换，那么T的矩阵的列向量为T(e_{1})和T(e_{2})</script><ul><li>由于线性变换与矩阵之间有一一对应（在给定基的前提下），而且保持线性运算关系不变（线性变换的加法和数乘分别对应于在某一个基下的矩阵的加法和数乘），因此，可以用矩阵来研究线性变换，也可以用线性变换来研究矩阵。</li><li>常见的线性变换有初等变换、等价变换、相似变换、合同变换等。我们也常常听到正交变换的名字，但由于正交变换包括平移、旋转和镜像，我们知道平移变换不是线性变换，因此不是所有的正交变换是线性变换。</li><li>一个矩阵就是把第一象限的单位立方体变换到其他象限的多面体，单位立方体由单位基向量张成，多面体由矩阵列向量张成；而列向量是由基向量变换得到的。</li><li><strong>对于n阶方阵，把列看作列向量，则行是每个列向量在列空间各个坐标轴上的投影（坐标），行的数量则是列空间坐标系的维数。</strong></li><li>特征值和特征向量的几何意义：$Ax=\lambda x$<ul><li>方阵乘以一个向量的结果仍然是一个同维向量，矩阵乘法对应了一个变换，把一个向量变成同维数的另一个向量。在这个变换的过程中，向量会发生旋转、伸缩或镜像的变化。矩阵不同，向量变化的结果也会不同。如果矩阵对某一个向量或者某些向量只发生伸缩变换，不对这些向量产生旋转效果，那么这些向量就是这个矩阵的特征向量，伸缩的比例就是特征值；如果伸缩的比例值是负值，原向量的方向改变为反方向，原向量仍然是这个矩阵的特征值。因此，从矩阵的几何意义上看，矩阵$A$的特征向量$\overrightarrow a$就是经过矩阵$A$变换后与自己平行的非零向量，矩阵$A$的特征值$\lambda$就是特征向量$\overrightarrow a$经变换后的伸缩系数（复特征值会使特征向量在复平面进行旋转，但在实轴上仍然是只进行伸缩变换）。</li><li>特征值反映了特征向量在变换时的伸缩倍数。对一个变换而言。特征向量指明变换的方向，而特征值反映的是变换的剧烈程度！我们知道，一个矩阵，只要我们找到合适的坐标，它的全部信息就可以用坐标系和特征值表示。这里蕴含了一个哲理：<strong>我们从不同角度看问题，其难易程度是不一样的！而矩阵对角化为我们提供了一个很好的看问题的角度</strong>。</li><li>矩阵的特征值之和等于矩阵的迹，特征值之积等于矩阵的行列式。这个由二阶矩阵推广而来。</li><li>关于特征值和特征向量，一是注意线性不变量的含义，一个是振动的谱含义。特征向量是线性不变量，特征值是振动的谱。机械振动和电振动有频谱，振动的某个频率具有某个幅度，那么矩阵也有矩阵的谱，矩阵的谱就是矩阵特征值的概念，是矩阵所固有的属性，所有的特征值形成了矩阵的一个频谱，每个特征值是矩阵的一个“谐振频点”。<strong>矩阵的谱分解就是抓住了矩阵的主要矛盾</strong>，因此也是主成分分析（PCA），奇异值分解相关的内容。</li><li>矩阵之所以能形成“频率的谱”，就是因为矩阵在特征向量所指的方向上具有对向量产生恒定的变换作用：周期性地增强或减弱特征向量的作用。进一步地，如果矩阵持续地叠代作用于向量，那么特征向量就会凸现出来。可类比与电路中的振荡器。</li></ul></li></ul><ul><li>几何重数与代数重数：一个特征值的求解因式的次数称为代数重数，特征值的特征子空间的维数称为几何重数。一般来说，特征值的代数重数大于或等于几何重数。</li><li>相似矩阵：或矩阵$A$与矩阵$B$相似，一定存在一个非奇异矩阵$P$（基变换矩阵），有$A=PBP^{-1}$。<strong>核心的几何意义就是相似矩阵$A$和$B$是同一个线性变换在两个不同基下的表示矩阵。</strong>线性变换的相似对角化实质是寻找一个适当的坐标系，使得该变换对这个新的坐标系上的单位向量（或基向量）只做伸缩变换，不做旋转变换。</li><li>不是任何矩阵都可以相似对角化，除非由n个线性无关的特征向量，但是实对称矩阵一定可以对角化。这里的证明涉及到一系列定理：</li></ul><script type="math/tex;mode=display">\begin{align*}& 定理1： 实对称矩阵A的特征值都是实数 \\& 定理2： 实对称矩阵A的不同特征值对应的特征向量一定是相互正交的（正交的向量组一定是线性无关向量组）\\& 定理3： 实对称矩阵A的r重特征根\lambda一定有r个线性无关的特征向量\end{align*}</script><ul><li>雅可比矩阵：雅可比矩阵是线性代数和微积分的纽带，是把非线性问题转换为线性问题的有力工具之一。</li><li>一个函数方程组由n个函数构成，每个函数有n个自变量$x_{1},x_{2},x_{3},\cdots x_{n}$</li></ul><script type="math/tex;mode=display">\begin{cases}y_{1}=f_{1}(x_{1},x_{2},\cdots x_{n}) \\y_{2}=f_{2}(x_{1},x_{2},\cdots x_{n})\\\vdots \\y_{n}=f_{n}(x_{1},x_{2},\dots x_{n})\end{cases}</script><ul><li>一般情况下，该函数方程组不是线性方程组，是多维曲线、曲面类的，通过微积分的思想化曲为直，将上述方程组化为超维切平面。先偏微分再写成矩阵的形式：</li></ul><script type="math/tex;mode=display">\begin{pmatrix}dy_{1}\\dy_{2}\\\vdots\\dy_{n}\end{pmatrix}=\begin{bmatrix}\frac{\partial f_{1}}{\partial x_{1}} & \frac{\partial f_{1}}{\partial x_{2}} & \cdots & \frac{\partial f_{1}}{\partial x_{n}}\\\frac{\partial f_{2}}{\partial x_{1}} & \frac{\partial f_{2}}{\partial x_{2}} & \cdots & \frac{\partial f_{2}}{\partial x_{n}}\\\vdots & \vdots & \cdots & \vdots \\\frac{\partial f_{n}}{\partial x_{1}} & \frac{\partial f_{n}}{\partial x_{2}} & \cdots & \frac{\partial f_{n}}{\partial x_{n}}\end{bmatrix}\begin{pmatrix}dx_{1}\\dx_{2}\\\vdots\\dx_{n}\end{pmatrix}</script><ul><li>其中，中间的方块矩阵就是雅可比矩阵$J$，里面的元素一般不是常数，而是变量或函数，说明雅可比矩阵包含着很多个具体的变换矩阵。</li><li><strong>雅可比矩阵把一个超平面的仿射坐标系变换成了一个超曲面坐标系；雅可比行列式就是曲面坐标系下单位微元和仿射坐标系下单位微元面积的比值。（雅可比矩阵把一个空间里的一个平面坐标系（基）变换成了无数个极小平面坐标系（基）；无数个极小平面就是曲面的切平面；雅可比行列式就是切平面上每个坐标系下极小单位元和原坐标系下极小单位元面积的比值。）</strong></li></ul><ul><li>整个线性代数里矩阵之间有三种典型的关系：矩阵相似（similar）、矩阵等价（equivalent）、矩阵合同（congruent）：</li></ul><script type="math/tex;mode=display">\begin{align*}& (1) A和B等价 \Leftrightarrow 存在可逆矩阵P和Q，使得B=PAQ；\\& (2) A和B相似 \Leftrightarrow 存在可逆矩阵P，使得B=P^{-1}AP;\\& (3) A和B合同 \Leftrightarrow 存在可逆矩阵C，使得B=C^{T}AC.\end{align*}</script><ul><li>矩阵相似，矩阵合同一定说明矩阵等价，但是相似与等价之间没有必然的联系，除非是正交矩阵，则两个等价。它们的几何意义如下：</li><li><strong>两个有限维向量空间之间的同一个线性映射，其在这两个向量空间上的不同基下所对应的矩阵之间的关系就是等价关系。</strong> 就是说它们是同一种类型的子空间，变换的作用就是为了把元素之间相互干扰的矩阵化成能一眼看出维数的简单矩阵。因为$P$和$Q$对应行初等变换和列初等变换的叠加，这里只能反映空间的维数。然而，在一些实际工程中，这些信息对我们了解一个系统已经足够用了。</li><li><strong>一个有限维向量空间上的同一个线性变换（或称线性算子），其在不同基下所对应的矩阵之间的关系是相似关系。</strong>就是同一个线性变换的不同基的描述矩阵。矩阵的相似变换可以把一个比较丑的矩阵变成一个比较美的矩阵，而保证这两个矩阵都是描述了同一个线性变换。至于什么样的矩阵是“美”的，什么样的是“丑”的，我们说对角阵是美的。总而言之，<strong>相似变换是为了简化计算</strong>。</li><li><strong>一个有限维向量空间上的同一个双线性函数或内积，其在两个基下的度量矩阵是相合关系。</strong></li><li>矩阵$A$，$B$可以不是方阵，因为是不同空间之间的线性映射矩阵，而$P$ ，$Q$，$C$必须是方阵且可逆，因为是同一空间里的基过渡矩阵。矩阵既可以看作向量的变换也可以看作是基的变换，两者的表达式中自变向量和因变向量的位置相反。</li><li>任一矩阵都可以通过一系列的初等变换化非对角线上的元素为0，从而成为对角阵，因此任一矩阵都等价于一个对角阵，其对角线上的非零元素的个数正好是原矩阵的秩。然而除了秩不变外，矩阵的其他性质在变换以后就很难反映出来了。</li><li>同构：如果两个线性空间上的映射变换既是单射又是满射，就称这两个向量空间同构。两个向量空间同构，那么就有线性映射使这两个线性空间的向量（或点）一一对应，而且保持线性不变，这时往往将这两个向量空间看作同一个。对于向量空间，同构也是等价关系。所以说，相似变换是同一个向量空间的变换，是两个基上的同一个线性变换。</li><li>相似矩阵描述的是在不同参照系的同一个变换，动作规则是相同的，类似地，合同矩阵描述的是在不同参照系下的同一个内积的度量矩阵。</li><li>由前可知，内积的度量应该是不随着坐标系的选取而改变的，因此定义了度量矩阵这个东西。事实上，内积的推广式子为：</li></ul><script type="math/tex;mode=display">(x,y)=x^{T}Sy, S=P^{T}P</script><ul><li>其中方阵$S$就是度量矩阵，度量矩阵是由基向量所构成的过渡方阵$P$与其转置的乘积得到的。</li><li>因为度量矩阵$S$是由基的过渡矩阵所决定的，那么每更换一次基坐标系就会有一个新的度量矩阵$T$，它们对应着同一个内积，且$S$合同于$T$。</li><li>正交变换是保持任意向量的长度不变或者保持度量不变的线性变换，其是欧式空间中的一类重要的变换。正交变换具有合同与相似变换共同的优点，前者仅适用于对称阵，保持了矩阵的对称性、正定性、秩等性质不变；后者适用于一般方阵，保持了矩阵的秩与特征值不变。因为正交变换的矩阵$P$满足$P^{-1} = P^{T}$，实际上是合同变换与相似变换的一种结合。</li><li>正交变换的主要性质是它不改变几何图形的度量。正交变换对应的矩阵就是正交矩阵，简单地说，一个正交矩阵就是一个具有标准正交列（行）向量组的方阵。</li></ul><h3 id="线性方程组"><a href="#线性方程组" class="headerlink" title="线性方程组"></a>线性方程组</h3><ul><li>四个正交子空间。</li><li>线性方程组的研究，包含了对向量的线性组合和矩阵方程的研究。方程组可以写成矩阵方程和向量组和形式等不同的形式，也反映了研究内容的不同。即线性方程组、向量的线性组合和矩阵及其矩阵方程可作等价研究。</li></ul><h3 id="二次型"><a href="#二次型" class="headerlink" title="二次型"></a>二次型</h3><ul><li>二次型的内容就是研究线性空间里的一个几何图形如何在不同的坐标基下的不同的矩阵表示，合同的矩阵表示的是同一个二次函数的几何图形。</li><li>定义：</li></ul><script type="math/tex;mode=display">\begin{align*}& 含有n个变量x_{1}，x_{2}，\cdots，x_{n}的二次齐次函数：\\& f(x_{1}，x_{2}，\cdots，x_{n})=a_{11}x_{1}^{2}+a_{22}x_{2}^{2}+\cdots+a_{nn}x_{n}^{2}+2a_{12}x_{1}x_{2}+2a_{13}x_{1}x_{3}+\cdots+2a_{(n-1)n}x_{n-1}x_{n}\\& 称为二次型。只含有平方项的二次型称为二次型的标准型：\\& f(x_{1}，x_{2}，\cdots，x_{n})=a_{11}x_{1}^{2}+a_{22}x_{2}^{2}+\cdots+a_{nn}x_{n}^{2}\end{align*}</script><ul><li>二次型可以表示为变元向量和矩阵的乘积：</li></ul><script type="math/tex;mode=display">f(x_{1}，x_{2}，\cdots，x_{n})=(x_{1}，x_{2}，\cdots，x_{n})\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n} \\a_{12} & a_{22} & \cdots & a_{2n} \\\vdots & \vdots & \cdots & \vdots \\a_{1n} & a_{2n} & \cdots & a_{nn}\end{bmatrix}\begin{pmatrix}x_{1} \\x_{2} \\\vdots \\x_{n}\end{pmatrix}=x^{T}Ax</script><ul><li><p>任意给一个二次型，就有唯一确定的对称矩阵，反之，任给一个对称矩阵，也就唯一确定一个二次型，其中，上式中间的方块矩阵称为二次型$f$的矩阵。</p></li><li><p>二次型是向量长度的平方，这个平方值不随坐标基的变化而变化，是个不变量，是绝对性的，而二次型函数的数学表达式却随着坐标基的变化而变化。</p></li><li><p>问题是，线性代数是研究”线性问题“的，比如线性变换、线性空间、线性方程组等，二次型却是非线性的，那为何能用矩阵等线性工具研究？</p></li><li><p>一是二次曲线或曲面本身就具有线性的性质，比如直纹曲面，一个坐标方向是直线，一个坐标方向是二次的，就可以描绘出直纹曲面（比如马鞍面）；</p></li><li><p>二是用向量研究一元多项式，是用多项式的系数构成向量来等价地替代研究，不是直接研究多项式本身，因此化非线性为线性；同样，用二次型的系数构建矩阵来等价地替代研究它，同样化非线性为线性了；</p></li><li><p>三是对二次型的研究利用了双线性函数的概念。二次型本来就是一个对称的双线性函数。</p></li><li><p>双线性函数就是定义了某维线性空间里的双向量的一个运算，运算结果是一个数，这个数属于某个数域。其中一个变元固定时是另一个变元的线性函数，两个向量互为线性，称为双线性。</p></li><li><p>双线性函数的一般定义如下：</p></li></ul><script type="math/tex;mode=display">\begin{align*}& 设P^{n}是数域P上n维列向量构成的线性空间，向量x,y \in P^{n}，再设A是P上的n阶方阵。令\\& f(x,y)=x^{T}Ay \\& 则f(x,y)是P^{n}上的一个双线性函数。\end{align*}</script><ul><li><strong>二次型对角化就是把二次型化简成标准型或者规范型。对角化二次型必须要是矩阵的合同变换，或者说度量矩阵和变换后的度量矩阵必须是合同的，只有这样才能使二次型的函数值保持不变，才能使向量长度保持不变，其他方法比如矩阵的相似对角化不能保证二次型的值不变。因为从几何意义上讲，相似对角化法是使矩阵本身所表示的某种线性变换不变进而对角化，而二次型对角化法是保证矩阵背后所代表二次型的值不变，它们所要求的不变量是不同的。</strong></li></ul><script type="math/tex;mode=display">f(x)=x^{T}Ax=(Cy)^{T}ACy=y^{T}(C^{T}AC)y</script><ul><li>对于一个正交矩阵$Q$，有向量替换关系$x=Qy$使：</li></ul><script type="math/tex;mode=display">f(x)=x^{T}Ax=y^{T}(Q^{T}AQ)y=y^{T}(Q^{-1}AQ)y</script><ul><li><p><strong>正交变换既是相似变换同时又是合同变换。由于正交变换保持变换前后的向量内积不变，从而保持向量的长度与夹角不变，所以正交变换属于刚体变换，是代表空间的一个旋转/镜像变换。利用矩阵乘积分解的方法，这种变换的转轴、转角可以用矩阵的特征参数量化地表示出来。</strong></p></li><li><p>和其他合同变换（又称可逆线性替换）不保持图形的原有形状，如可以把椭圆变成圆的特点相比，正交变换则保持原图形的形状。</p></li><li><p><strong>用正交变换化二次型为标准型的定理也称为主轴定理，这同时也是主轴定理的几何意义。</strong></p></li><li><p>主轴定理：</p></li></ul><script type="math/tex;mode=display">\begin {align*}& 对于任意一个n元二次型：\\& f(x_{1}，x_{2}，\cdots，x_{n})=x^{T}Ax\\& 存在正交变换x=Qy（Q为n元正交矩阵），使得\\& x^{T}Ax=y^{T}(Q^{T}AQ)y=\lambda_{1}y_{1}^{2}+\lambda_{2}y_{2}^{2}+\cdots+\lambda_{n}y_{n}^{2}\end{align*}</script><ul><li>其中，$\lambda_{1}，\lambda_{2}，\cdots，\lambda_{n}$是实对称矩阵$A$的n个特征值；$Q$的n个列向量是对应于特征值$\lambda_{1}，\lambda_{2}，\cdots，\lambda_{n}$的标准正交特征向量。</li></ul><h2 id="神奇的矩阵"><a href="#神奇的矩阵" class="headerlink" title="神奇的矩阵"></a>神奇的矩阵</h2><h3 id="空间"><a href="#空间" class="headerlink" title="空间"></a>空间</h3><ul><li><p>空间是现代数学的基础之一。线形空间其实还是比较初级的，如果在里面定义了范数，就成了赋范线性空间。赋范线性空间满足完备性，就成了巴那赫空间；赋范线性空间中定义角度，就有了内积空间，内积空间再满足完备性，就得到希尔伯特空间，如果空间里装载所有类型的函数，就叫泛函空间。</p></li><li><p><strong>容纳运动是空间的本质</strong>。不管是什么空间，都必须容纳和支持在其中发生的符合规则的运动（变换）。在某种空间中往往会存在一种相对应的变换，比如拓扑空间中有拓扑变换，线性空间中有线性变换，仿射空间中有仿射变换，其实这些变换都只不过是对应空间中允许的运动形式而已。因此只要知道， <strong>“空间”是容纳运动的一个对象集合，而变换则规定了对应空间的运动。</strong></p></li></ul><h3 id="矩阵的运动本质属性"><a href="#矩阵的运动本质属性" class="headerlink" title="矩阵的运动本质属性"></a>矩阵的运动本质属性</h3><ul><li><p>线性空间中的运动，被称为线性变换。也就是说，你从线性空间中的一个点运动到任意的另外一个点，都可以通过一个线性变化来完成。在线性空间中，当你选定一组基之后，不仅可以用一个向量来描述空间中的任何一个对象，而且可以用矩阵来描述该空间中的任何一个运动（变换）。而使某个对象发生对应运动的方法，就是用代表那个运动的矩阵，乘以代表那个对象的向量。<strong>简而言之，在线性空间中选定基之后，向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动。矩阵的本质是运动的描述</strong>。</p></li><li><p>不过，这里“运动”的概念不是微积分中的连续性的运动，而是瞬间发生的变化，是物理当中“跃迁”的一种描述，在这里我们将其称为“变换”，也就是说，<strong>矩阵是线性空间里的变换的描述</strong>。</p></li><li><p><strong>“矩阵是线性空间中的线性变换的一个描述。在一个线性空间中，只要我们选定一组基，那么对于任何一个线性变换，都能够用一个确定的矩阵来加以描述”。</strong></p></li></ul><h3 id="矩阵与方程组"><a href="#矩阵与方程组" class="headerlink" title="矩阵与方程组"></a>矩阵与方程组</h3><ul><li>老生常谈的列空间，秩，交点等问题了。</li></ul><h3 id="矩阵与坐标系"><a href="#矩阵与坐标系" class="headerlink" title="矩阵与坐标系"></a>矩阵与坐标系</h3><ul><li><p>向量左乘矩阵将向量变换成另一个向量，一组基右乘一个方阵将其变换到另一组基，左乘一个方阵就是将一组基下的坐标变换到另一组基下的坐标，也就是说运动是相对的，而这个相对体现在左乘和右乘上。</p></li><li><p>对于矩阵乘法，主要是考察一个矩阵对另一个矩阵所起的变换作用。其作用的矩阵看作是动作矩阵，被作用的矩阵可以看作是由行或列向量构成的几何图形。同样，如果一连串的矩阵相乘，就是多次变换的叠加。而矩阵左乘无非是把一个向量或一组向量（即另一个矩阵）进行伸缩或旋转。 <strong>乘积的效果就是多个伸缩和旋转的叠加</strong>。其中两类特殊的矩阵：旋转矩阵和对角矩阵，分别表示对向量的旋转和伸缩。</p></li></ul><h3 id="矩阵与傅里叶变换"><a href="#矩阵与傅里叶变换" class="headerlink" title="矩阵与傅里叶变换"></a>矩阵与傅里叶变换</h3><p>1.分解与叠加的思想，以及完备性才会完全替代“棱角”的波形。在这里，基不再是向量，而是三角函数。</p><p>2.为什么是选择三角函数？</p><ul><li><p>大自然中很多现象可以抽象成一个<strong>线性时不变系统</strong>来研究，无论你用微分方程还是传递函数或者状态空间描述。线性时不变系统可以这样理解： 输入输出信号满足线性关系，而且系统参数不随时间变换。<strong>对于大自然界的很多系统，一个正弦曲线信号输入后，输出的仍是正弦曲线，只有幅度和相位可能发生变化，但是频率和波的形状仍是一样的。也就是说正弦信号是系统的特征向量。</strong></p></li><li><p>当然，指数信号也是系统的特征向量，表示能量的衰减或积聚。自然界的衰减或者扩散现象大多是指数形式的，或者既有波动又有指数衰减（复指数$e^{\alpha+i \beta}$形式），因此具有特征的基函数就由三角函数变成复指数函数。但是，如果输入是方波、三角波或者其他什么波形，那输出就不一定是什么样子了。所以，除了指数信号和正弦信号以外的其他波形都不是线性系统的特征信号。</p></li><li><p>用正弦曲线来代替原来的曲线而不用方波或三角波或者其他什么函数来表示的原因在于： <strong>正弦信号恰好是很多线性时不变系统的特征向量。于是就有了傅里叶变换。对于更一般的线性时不变系统，复指数信号(表示耗散或衰减)是系统的“特征向量”</strong>。于是就有了拉普拉斯变换。z 变换也是同样的道理，这时$z^{n}$是离散系统的“特征向量”。 这里没有区分特征向量和特征函数的概念，主要想表达二者的思想是相同的，只不过一个是有限维向量，一个是无限维函数。</p></li><li><p>傅里叶级数和傅里叶变换其实就是我们之前讨论的特征值与特征向量的问题。分解信号的方法是无穷的，但分解信号的目的是为了更加简单地处理原来的信号。这样，用正余弦来表示原信号会更加简单，因为正余弦拥有原信号所不具有的性质：正弦曲线保真度。且只有正弦曲线才拥有这样的性质。 <strong>这样做的好处就是知道输入，我们就能很简单乘一个系数写出输出。</strong></p></li></ul><p>3.时域和频域</p><ul><li><p>以时间作为参照来观察动态世界的方法我们称其为时域分析，频域 (frequency domain) 是描述信号在频率方面特性时用到的一种坐标系。用线性代数的语言就是装着正弦函数的空间。频域最重要的性质是：它不是真实的，而是一个数学构造。频域是一个遵循特定规则的数学范畴。正弦波是频域中唯一存在的波形，这是频域中最重要的规则，即正弦波是对频域的描述，因为时域中的任何波形都可用正弦波合成。</p></li><li><p><strong>对于一个信号来说，信号强度随时间的变化规律就是时域特性，信号是由哪些单一频率的信号合成的就是频域特性。</strong></p></li></ul><ul><li>这里的核心就是一种信号可以用另一种信号作为基函数线性表示。而由于现实世界中正弦信号是系统的特征向量，所以我们就用傅里叶变换，将研究的信号在频域展开。总而言之， 不管是傅里叶级数，还是傅里叶变换、拉普拉斯变换、z 变换，本质上都是线性代数里面讲的求特征值和特征向量。</li></ul><p>4.傅里叶级数</p><ul><li><p>Strang老爷子在课上专门讲过通过投影和正交基求出傅里叶级数，正交基中的每个函数都可以看做是一条独立的坐标轴，从几何角度来看，傅里叶级数展开其实只是在做一个动作，那就是把函数“投影”到一系列由三角函数构成的“坐标轴”上。上面的系数则是函数在每条坐标轴上的坐标。注意，在有限维中，内积是点积的形式，而在无限维中则是积分的形式。</p></li><li><p>这组正交基是：$\left\{1, \cos \frac {\pi x}{l}, \sin \frac{\pi x}{l}, \cos \frac{2 \pi x}{l}, \sin \frac {2\pi x}{l}, \cdots \right\}$</p></li></ul><script type="math/tex;mode=display">\begin{align*}& a_{0}=\frac {\left\langle f,1 \right\rangle}{\left \langle 1,1 \right \rangle}=\frac{\int_{-l}^{l}f(x)\,dx}{\int_{-l}^{l}\,dx}=\frac{\int_{-l}^{l}f(x)\,dx}{2l} \\& a_{n}=\frac {\left \langle f, \cos \frac {n \pi x}{l}\right \rangle}{\left \langle \cos \frac{n \pi x}{l}, \cos \frac{n \pi x}{l} \right \rangle} =\frac {\int_{-l}^{l}f(x)\cos \frac{n \pi x}{l} \,dx}{\int_{-l}^{l} \cos^2 \frac{n \pi x}{l}\,dx} = \frac {\int_{-l}^{l}f(x)\cos \frac{n \pi x}{l} \,dx}{l},n \ge 1 \\& b_{n}=\frac {\left \langle f, \sin \frac {n \pi x}{l}\right \rangle}{\left \langle \sin \frac{n \pi x}{l}, \sin \frac{n \pi x}{l} \right \rangle} =\frac {\int_{-l}^{l}f(x)\sin \frac{n \pi x}{l} \,dx}{\int_{-l}^{l} \sin^2 \frac{n \pi x}{l}\,dx} = \frac {\int_{-l}^{l}f(x)\sin \frac{n \pi x}{l} \,dx}{l},n \ge 1 \\\end{align*}</script><ul><li>同理，对于复数形式的傅里叶级数，也可以用几何投影的观点来写出所有的系数。</li></ul><h3 id="矩阵的奇异值分解（SVD）"><a href="#矩阵的奇异值分解（SVD）" class="headerlink" title="矩阵的奇异值分解（SVD）"></a>矩阵的奇异值分解（SVD）</h3><ul><li><p>$M=U \sum V^{T}$</p></li><li><p>任意的矩阵$M$是可以分解成三个矩阵。 $V$表示了原始域的标准正交基，$ U$表示$M$经过变换后的 co-domain 的标准正交基，$\sum$表示了$V$中的向量与$U$中相对应向量之间的关系。</p></li></ul><p><img src="/2019/02/14/Linear-Algebra-of-MIT/SVD.png" alt="SVD"></p><ul><li>事实上，我们可以找到任何矩阵的奇异值分解。如果把矩阵$U$用它的列向量表示出来，可以写成：<script type="math/tex;mode=display">U=(u_{1}, u_{2}, \cdots, u_{n})</script></li><li>其中每一个$u_{i}$称为$M$的左奇异向量。类似地，对于$V$，有：<script type="math/tex;mode=display">V=(v_{1}, v_{2}, \cdots, v_{n})</script></li><li>其中每一个$v_{i}$被称为$M$的右奇异向量。然后设矩阵$\sum$的对角线元素为$\sigma_{i}$并按降序排列，则$M$就可以表示为：<script type="math/tex;mode=display">M=\sigma_{1}u_{1}v_{1}^{T}+\sigma_{2}u_{2}v_{2}^{T}+\cdots+\sigma_{n}u_{n}v_{n}^{T}=\sum_{i=1}^{n}\sigma_{i}u_{i}v_{i}^{T}=\sum_{i=1}^{n}A_{i}</script></li><li><p>其中$A_{i}=\sigma_{i}u_{i}v_{i}^{T}$是一个$m \times n$的矩阵，即把原来的矩阵表示成了n个矩阵的和。</p></li><li><p>因此，可以根据$\sigma_{i}$的大小进行矩阵的近似表达，主成分分析的思想就是来源于此。</p></li><li><p>奇异值分解在推荐系统，图像压缩，潜在语义索引，潜在数据表达，降噪，数据分析等有很重要的应用。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​ ​&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这是对MIT 18.06 open course— Linear Algebra—by Gilbert Strang 的课程总结。&lt;/strong&gt;&lt;/p&gt;&lt;h2 id=&quot;课程视频及主页资料&quot;&gt;&lt;a href=&quot;#课程视频及主页资料&quot; class=&quot;headerlink&quot; title=&quot;课程视频及主页资料&quot;&gt;&lt;/a&gt;课程视频及主页资料&lt;/h2&gt;&lt;p&gt;由于计算机视觉以及机器学习中对矩阵的要求较高，而且本科阶段的高等代数学得不太行，故借此机会将这个著名的MIT线性代数公开课看了一遍，相关的课程视频和材料放在下面。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/av15463995?p=2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;lectures:&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/readings/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;readings:&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/assignments/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;assignments:&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;exams:&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/tools/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;tools:&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="基础数学" scheme="http://densecollections.top/categories/%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="MIT" scheme="http://densecollections.top/tags/MIT/"/>
    
      <category term="mathematics" scheme="http://densecollections.top/tags/mathematics/"/>
    
      <category term="linear algebra" scheme="http://densecollections.top/tags/linear-algebra/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Github 博客搭建</title>
    <link href="http://densecollections.top/2019/02/14/Hexo-Github-%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    <id>http://densecollections.top/2019/02/14/Hexo-Github-博客搭建/</id>
    <published>2019-02-14T09:26:38.000Z</published>
    <updated>2020-02-03T03:40:15.400Z</updated>
    
    <content type="html"><![CDATA[<p><strong>完整搭建过程参考<a href="https://eirunye.github.io/categories/Hexo/" target="_blank" rel="noopener">此系列博客</a>（环境win10）</strong></p><p>我的写作编辑器用的是<a href="https://www.typora.io/" target="_blank" rel="noopener">typora</a></p><h2 id="Next主题配置"><a href="#Next主题配置" class="headerlink" title="Next主题配置"></a>Next主题配置</h2><p>Next主题是本人比较中意的hexo主题，所以在此只记录Next主题的配置过程</p><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>建议安装6.0版本。利用git bash here,输入命令：git clone <a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">https://github.com/theme-next/hexo-theme-next</a> themes/next</p><h2 id="配置与个性化"><a href="#配置与个性化" class="headerlink" title="配置与个性化"></a>配置与个性化</h2><a id="more"></a><p>1.添加作者头像并设置旋转效果</p><p><strong>注意：最新版本的next主题已经添加了头像动画功能，直接在主题的配置文件里面修改，因此头像添加（路径）也是在主题配置文件里面添加，而不是在根目录的配置文件添加</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Sidebar Avatar</span><br><span class="line">avatar: </span><br><span class="line">  # in theme directory(source/images): /images/avatar.gif</span><br><span class="line">  # in site  directory(source/uploads): /uploads/avatar.gif</span><br><span class="line">  # You can also use other linking images.</span><br><span class="line">  url: /uploads/header.jpg     #/images/avatar.gif</span><br><span class="line">  # If true, the avatar would be dispalyed in circle.</span><br><span class="line">  rounded: true</span><br><span class="line">  # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar.</span><br><span class="line">  opacity: 1</span><br><span class="line">  # If true, the avatar would be rotated with the cursor.</span><br><span class="line">  rotated: true</span><br></pre></td></tr></table></figure><p>2.修改标签“#”符号和<a href="https://blog.csdn.net/qq_37497322/article/details/80628713" target="_blank" rel="noopener">插入图片</a></p><p>3.添加社交账号</p><p>如果遇到没有的社交图标，可以在<a href="https://fontawesome.com/icons?from=io" target="_blank" rel="noopener">fontawesome</a>中寻找添加。 ||后面的是图标名，建议不要找最新的。</p><p>4.菜单设置</p><p>本地搜索一旦开启会自动添加”搜索“菜单</p><p>5.添加版权信息。参考该<a href="http://stevenshi.me/2017/05/26/hexo-add-copyright/" target="_blank" rel="noopener">文章</a></p><p>上面的链接是手动添加，现在可以在NEXT主题配置文件中进行修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">creative_commons:</span><br><span class="line">  license: by-nc-sa</span><br><span class="line">  sidebar: true</span><br><span class="line">  post: true</span><br><span class="line">  language:</span><br></pre></td></tr></table></figure><p>6.利用leancloud加入阅读计数功能</p><p>若页面LeanCloud访问统计提示’Counter not initialized! See more at console err msg.’</p><p>参考该<a href="https://leaferx.online/2018/02/11/lc-security/" target="_blank" rel="noopener">博客</a>解决</p><p><strong>问题</strong>：安装hexo-leancloud-counter-security报错：npm WARN babel-eslint@10.0.1 requires a peer of eslint@&gt;= 4.12.1 but none is installed. You must install peer dependencies yourself.</p><p>因此缺什么就安装什么，在这里我是安装：npm install eslint@&gt;= 4.12.1,之后再安装hexo-leancloud-counter-security就成功了。</p><p>另外最后在修正deploy的时候注意tab的控制，不然编译会报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  #type: git</span><br><span class="line">  #repository: git@github.com:x695/thinkee.github.io.git</span><br><span class="line">  #repository: https://github.com/x695/thinkee.github.io.git  </span><br><span class="line">  #branch: master</span><br><span class="line">  - type: git</span><br><span class="line">    #repository: git@e.coding.net:yangshixian/blog.git</span><br><span class="line">    repository: git@git.coding.net:thinkee/blog.coding.me.git</span><br><span class="line">    branch: master</span><br><span class="line">  - type: leancloud_counter_security_sync</span><br></pre></td></tr></table></figure><p>然而，十分尴尬的是，阅读数依然没有显示，后来参考这篇<a href="https://leflacon.github.io/52b56662/" target="_blank" rel="noopener">博客</a>得到了解决</p><p>7.利用valine+leancloud添加评论功能</p><p><a href="https://bigwin.ml/2018/11/29/valine-for-next/" target="_blank" rel="noopener">参考</a></p><p>8.网站背景动画</p><p>如果修改主题配置文件之后不出现动画，建议下载主要的库到/source/lib中</p><p>9.加入本地搜索功能</p><p>10.加入Latex数学公式</p><p>这个除了在主题配置文件修改mathjax: true之外，<strong>还需要在每个post的博客里的头代码里面加入mathjax: true才可以正常显示</strong>。如果嫌每次书写都得添加麻烦，可以直接在根目录的scaffolds文件夹里的post.m文件夹直接加入mathjax: true，之后每次新建都会自动添加了。</p><p>不过再后续使用Latex语句的时候，发现博客上还是会显示源码，后来发现应该是渲染引擎的问题，根据<a href="https://blog.csdn.net/qq_34229391/article/details/82725229" target="_blank" rel="noopener">这篇博客</a>的指导修改后就好了。</p><hr><hr><p>2019.3.5日来更新，写完一篇blog进行generate时候出现了错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Template render error: (unknown path) [Line 62, Column 32]</span><br><span class="line">  expected variable end</span><br><span class="line">    at Object.exports.prettifyError</span><br></pre></td></tr></table></figure><p>出现的原因要么是主题配置文件忘了打空格，要么就是自己写的blog文件里面用的符号与其他hexo配置文件语法有冲突，后来发现原因是我打公式时出现了两个“}}”，前面一个”}”是latex语法，后面是”\}”，目的是为了输出括号“}”，之后参照latex的另一种打法“\lbrace \rbrace”就好了</p><hr><hr><hr><p>2019.4.17日来更</p><p>公式直接用<a href="https://mathpix.com/" target="_blank" rel="noopener">Mathpix</a>进行LaTex命令复制吧，非常方便，也几乎没出现什么错误，还省时间。</p><hr><p>11.博客图片点击放大</p><p>在NEXT主题的配置文件里面有fancybox选项，直接改为true就好了，不过在这之前要下载fancybox包到<code>next/source/lib</code>文件夹里面，即：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd next/source/lib</span><br><span class="line">git clone https://github.com/theme-next/theme-next-fancybox3 fancybox</span><br></pre></td></tr></table></figure><p>12.并排插入两张图片</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;figure class=&quot;half&quot;&gt;</span><br><span class="line">    &lt;img src=&quot;xxx.jpg&quot; width=&quot;400&quot;/&gt;</span><br><span class="line">    &lt;img src=&quot;xxx.jpg&quot; width=&quot;400&quot;/&gt;</span><br><span class="line">&lt;/figure&gt;</span><br></pre></td></tr></table></figure><p>不过这只能在.md文件中实现，博客中还是上下各一张。</p><p>13.NEXT主题<a href="http://theme-next.iissnan.com/tag-plugins.html" target="_blank" rel="noopener">内置标签</a>（包括引用文本居中，图片最大化引用等），更详细的标签见hexo<a href="https://hexo.io/zh-cn/docs/tag-plugins.html" target="_blank" rel="noopener">官网</a>。注意在本地利用这些标签不会出现相应的结果，只有上传到博客上才能看到。</p><p>14.添加emoji表情功能</p><p><a href="https://www.webfx.com/tools/emoji-cheat-sheet/" target="_blank" rel="noopener">emoji大全</a></p><p>关于加入表情的功能，我一共找到了两种方法，一种是基于修改渲染引擎的，然后加入twemoji插件，但是我在之前为了支持mathjax已经更换了渲染引擎，结果导致我在按照<a href="https://chaxiaoniu.oschina.io/2017/07/10/HexoAddEmoji/" target="_blank" rel="noopener">博客1</a>和<a href="https://www.cnblogs.com/fsong/p/5929773.html" target="_blank" rel="noopener">博客2</a>的操作进行时出现了fancybox图片放大和图片内部插入的问题，有时候甚至会影响到html语句，后来倒退删除渲染引擎和插件的时候，想要恢复到原来的状态，结果hexo g可以成功，本地却无法显示不出内容，结果只好重装。。目前原因未知。</p><p>此外，第二种方法是安装<code>hexo-filter-github-emojis</code>插件，可参考<a href="https://novnan.github.io/Hexo/emojis-for-hexo-next/" target="_blank" rel="noopener">博客3</a>和<a href="https://www.biueo.com/2018/01/12/Hexo文章添加emoji表情/" target="_blank" rel="noopener">博客4</a>，但是我看博客中也提到了图片干扰的问题，就没再继续试下去了。。。</p><p>下次看到更好的解决办法再来更新。</p><p>15.加入RSS订阅</p><p>直接在NEXT主题配置文件里面更改，在这之前先在博客站点目录下安装<code>npm install hexo-generator-feed --save</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">feed:</span><br><span class="line">    type: rss2</span><br><span class="line">    path: rss2.xml</span><br><span class="line">    limit: 5</span><br><span class="line">    hub:</span><br><span class="line">    content: &apos;true&apos;</span><br></pre></td></tr></table></figure><p>16.博客被谷歌和百度收录</p><p>参考这个<a href="https://www.cnblogs.com/php-linux/p/8493346.html" target="_blank" rel="noopener">博客</a>的做法</p><p>17.购买自己的阿里云域名并设置</p><p><a href="https://zhuanlan.zhihu.com/p/54575457" target="_blank" rel="noopener">参考</a></p><p>出现问题：<code>leancloud阅读统计和评论系统出现问题</code>，在<code>安全中心</code>更换了域名之后，deploy时出现Error，too much request，得知时免费版的限制导致错误，按照<a href="https://yunhao.space/2018/06/27/hexo-leancloud-plugin-installation-tutor/" target="_blank" rel="noopener">此博客</a>修改并重新配置了一个leancloud Counter应用未能解决问题。<a href="https://www.bingyublog.com/2019/02/23/Hexo添加文章阅读量统计功能/" target="_blank" rel="noopener">此博客</a> 通过加入lean-analytics.swig文件也未能解决问题。</p><p><strong>偶然通过换回valine+leancloud评论系统出现<code>code 403</code>问题进行改正时也顺便解决了这个问题。</strong></p><p>个人GitHub博客的<code>settings</code>页面的<code>Github Pages</code>有个<code>Enforce HTTPS</code>选项，我没勾，所以<a href="http://或者https://都可以访问我的域名，而我的leancloud`安全中心`中的安全域名只加了https://的，在加一个http://就好了。。" target="_blank" rel="noopener">http://或者https://都可以访问我的域名，而我的leancloud`安全中心`中的安全域名只加了https://的，在加一个http://就好了。。</a></p><p>但是deploy时依然会有些莫名其妙的错误，比如<code>ERROR Password must be string</code>，但是既然正常显示了，我就没再管了。。</p><p>18.更换评论Gitalk系统</p><p>由于leancloud经常出现问题，不易维护，所以可以考虑更换评论系统为Gitalk，虽然评论必须登陆GitHub账号，但是可以接受，毕竟现在理工科都有Github账号，此外还加了字数统计和阅读时长等信息。具体操作参见<a href="https://yunhao.space/2018/06/29/hexo-next-function-setting/" target="_blank" rel="noopener">链接</a>。</p><p><strong>需要注意，使用Gitalk评论系统时，每次deploy都需要在博客下登陆自己的账号初始化下</strong></p><p>19.对博客进行代码压缩（加快访问速度）和备份（便于更换设备时进行迁移），另外更改文章链接失败。</p><p>参考：<a href="https://indexmoon.com/articles/1153730074/（其中，.gitignore文件可通过在git" target="_blank" rel="noopener">https://indexmoon.com/articles/1153730074/（其中，.gitignore文件可通过在git</a> bash中利用<code>touch .gitignore</code>命令创建）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;完整搭建过程参考&lt;a href=&quot;https://eirunye.github.io/categories/Hexo/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;此系列博客&lt;/a&gt;（环境win10）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我的写作编辑器用的是&lt;a href=&quot;https://www.typora.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;typora&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;Next主题配置&quot;&gt;&lt;a href=&quot;#Next主题配置&quot; class=&quot;headerlink&quot; title=&quot;Next主题配置&quot;&gt;&lt;/a&gt;Next主题配置&lt;/h2&gt;&lt;p&gt;Next主题是本人比较中意的hexo主题，所以在此只记录Next主题的配置过程&lt;/p&gt;&lt;h2 id=&quot;下载&quot;&gt;&lt;a href=&quot;#下载&quot; class=&quot;headerlink&quot; title=&quot;下载&quot;&gt;&lt;/a&gt;下载&lt;/h2&gt;&lt;p&gt;建议安装6.0版本。利用git bash here,输入命令：git clone &lt;a href=&quot;https://github.com/theme-next/hexo-theme-next&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/theme-next/hexo-theme-next&lt;/a&gt; themes/next&lt;/p&gt;&lt;h2 id=&quot;配置与个性化&quot;&gt;&lt;a href=&quot;#配置与个性化&quot; class=&quot;headerlink&quot; title=&quot;配置与个性化&quot;&gt;&lt;/a&gt;配置与个性化&lt;/h2&gt;
    
    </summary>
    
      <category term="博客搭建" scheme="http://densecollections.top/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="hexo" scheme="http://densecollections.top/tags/hexo/"/>
    
      <category term="leancloud" scheme="http://densecollections.top/tags/leancloud/"/>
    
      <category term="next" scheme="http://densecollections.top/tags/next/"/>
    
  </entry>
  
</feed>
