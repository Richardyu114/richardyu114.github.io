<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[paper reading and coding of RCNN series]]></title>
    <url>%2F2019%2F11%2F28%2Fpaper-reading-and-coding-of-RCNN-series%2F</url>
    <content type="text"><![CDATA[AboutFaster R-CNN是目标检测领域中”two-stage”的代表性方法，其精度高，适应性强，兼具学术和工程价值。整个框架由于吸取了很多先前工作的经验，因此比较庞大，而且细节很多，因此需要认真研读下相关paper和Faster R-CNN的python代码。 在此之前，先贴上一位博主做的“The Modern History of Object Recognition — Infographic”，其中也包括了“one-stage”的方法，不过2017年以后的没再更新了。 R-CNNR-CNN(Rich feature hierarchies for accurate object detection and semantic segmentation)是将深度学习应用于目标检测的开山之作，以前传统的目标检测算法使用滑动窗口法依次判断所有的可能区域，在该文章中，采用selective search方法先预先提取一系列可能是物体的候选区域（foreground），之后将这些候选区域（proposals）整合成固定的大小送到预训练的CNN模型上提取特征然后进行fine-tuning，迁移学习，进而达到比较好的识别检测效果。 selective search three modules: regional proposals CNN feature extract linear-SVMs classify and why —appendix B wrap —how appendix A ablation studies—-why cnn better pre-trained model and fine-tune matters detection errors analysis http://dhoiem.web.engr.illinois.edu/publications/eccv2012_detanalysis_derek.pdf how to do experiments and writing paper,or how to express your ideas and thoughts 实验做的好 各向同性与各向异性图像缩放： https://blog.csdn.net/qq_30122883/article/details/89363900 OHEM SSPNet[Fast R-CNN]代码解析：https://blog.csdn.net/weixin_43872578/article/details/86607742 Faster R-NNanchors的思想 pyramid of feature map/filters/anchors http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/ https://blog.csdn.net/u014365862/article/details/77887230 https://zhuanlan.zhihu.com/p/23006190?refer=xiaoleimlnote https://blog.csdn.net/v_july_v/article/details/80170182 https://zhuanlan.zhihu.com/p/31426458 mAP 用res101是在conv4x之后 机器学习笔记： https://campoo.cc/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0/ 为什么需要anchor： https://zhuanlan.zhihu.com/p/73024408 code architecture1.R-CNN 2.Fast R-CNN 3.Faster R-CNN 4.Spatial Transformer Networks roi pooling, roi align, crop pooling的区别的作用 The crop pooling layer takes the ROI boxes output by the proposal target layer and the convolutional feature maps output by the “head” network and outputs square feature maps http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/ cvpr2019 tutorial： http://feichtenhofer.github.io/cvpr2019-recognition-tutorial/ 数据预处理部分会减去pixel mean value，作为数据标准化的一部分，但是对图像不需要再去除以标准差，是因为平稳性的存在，blog，对RGB图像来说，mean pixel value 是个$3 \times 1$的vector: [R_mean, G_mean, B_mean] head network(pretrained model first few layers)—RPN(region proposal network)—classification network ROI(region o interests) — crop olling—classification bounding box回归的系数并不是单纯意义上的坐标，而且相对的系数，这样会使得图像再进行rescale之后不会使系数发生变化]]></content>
      <categories>
        <category>科研记录</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>computer vision</tag>
        <tag>object detection</tag>
        <tag>semantic segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FutureMapping2]]></title>
    <url>%2F2019%2F11%2F17%2FFutureMapping2%2F</url>
    <content type="text"><![CDATA[About继第一篇FutureMapping之后，视觉SLAM领域内的奠基者Andrew Davison最近又将他的和别人讨论的有关未来空间AI对地图构建，机器人协同定位，以及动态问题等新想法撰写成了新的论文FutureMapping2，置顶在了自己的推特上，表示欢迎大家交流自己的想法。 总的来说，这篇文章干货还算是很多的，主要是着眼于factor graph（因子图）和Gaussian Belief Propagation（GBP，高斯置信传播）对整体，动态机器人建图等问题的潜力和前景，不仅在数学上进行一些tutorial，还给出了三个python demos。由于我对视觉SLAM只是了解整体特点，其中各种计算细节和优化方法并没有认真看过和代码书写过，因此读完这篇充满amazing reflections的文章之后我只能把握到整体的idea和一些实现方法。对于GBP和factor graph 结合构建的数学模型，我将在后续花时间弄懂后再对该blog进行补充。 非常建议对SLAM或者机器视觉领域感兴趣的同行阅读此论文，相信会帮助您开阔思路！ ContentIdea Abstract: We argue the case for Gaussian Belief Propagation (GBP) as a strong algorithmic framework for the distributed, generic and incremental probabilistic estimation we need in Spatial AI as we aim at high performance smart robots and devices which operate within the constraints of real products. Processor hardware is changing rapidly, and GBP has the right character to take advantage of highly distributed processing and storage while estimating global quantities, as well as great flexibility. We present a detailed tutorial on GBP, relating to the standard factor graph formulation used in robotics and computer vision, and give several simulation examples with code which demonstrate its properties. 关键词：分布式，边缘计算，局部估计，GBP，因子图，概率模型 （边缘计算，分布式的，局部计算和存储，整体计算和构建以一种‘graph’的方式展开） 首先，背景是现在的spatial AI系统要试着处理异质的数据，通过不同的估计手段将其转换成一致性的表示方式，但是这受限于现在的处理器性能（实时，持续地处理带来计算负担，存储负担和转换负担）。Andrew认为目前有两种方式可以对此进行提升： One is to focus on scene representation, and to find new parameterisations of world models which allow high quality scene models to be built and maintained much more efficiently. 这是表征方式的问题，这也是学术界都在解决的问题，representation learning. 机器人，计算机如何利用自己的硬件特点来认识和表征周围的世界，这跟人认识世界不一定是一样的。 The other is to look towards the changing landscape in computing and sensing hardware. 另一个就是硬件设计，比如现在视觉SLAM中的“event camera”，利用事件来记录。这里Andrew也推荐了ETH苏黎世联邦理工大学 J. Martel的phD thesis，传送门 硬件问题上，Andrew并没有说太多，大都还是一些常见的设想，在后续的section中，Andrew主要是是针对第一点来说的。 The purest representation of the knowledge in a Spatial AI problem is the factor graph itself, rather than probability distributions derived from it, which will always have to be stored with some approximation. What we are really seeking is an algorithm which implements Spatial AI in a distributed way on a computational resource like a graph processor, by storing the factor graph as the master representation and operating on it in place using local computation and message passing to implement estimation of variables as needed but taking account of global influence. (有关因子图的资料，推荐Factor Graphs for Robot Perception， 关于高斯置信传播，Andrew是根据bishop的PRML一书来的，此外他在文中推荐的是比较老的参考文献Factor Graphs and the Sum-Product Algorithm，我自己也在网上发现了一篇伯希来大学的phD论文 Gaussian Belief Propagation: Theory and Application ，希望对后续理解有所帮助。） Andrew认为可以通过因子图来进行局部估计，进行边缘计算，然后进行信息传递，通过这种方法来构建分布式和全局上的计算，从而达到认知和执行任务的目的。 —-to be continued Mathematics modelsfactor graphGBPExamples]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>computer vision</tag>
        <tag>SLAM</tag>
        <tag>mapping</tag>
        <tag>AI system</tag>
        <tag>hardware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习痰涂片项目总结]]></title>
    <url>%2F2019%2F10%2F04%2F%E5%AE%9E%E4%B9%A0%E7%97%B0%E6%B6%82%E7%89%87%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[classification在上篇博客提到，该任务就是将原始数据的每张图片（256x256）进行grid级别的label预测，思路很简单，就是最后卷出的feature map是4x4的，不要过average global pooling layer，直接拉成1x16的向量过sigmoid激活函数即可（label也要变成16个字符，有点类似OCR)。 dataset原始数据给的标注是json格式的框标注，但是框不是杆菌的具体位置，而是代表这个grid里面存在杆菌： 1&quot;frames&quot;:&#123;&quot;0_grid.png&quot;:[&#123;&quot;x1&quot;:162.42542787286064,&quot;y1&quot;:25.34963325183374,&quot;x2&quot;:170.24938875305622,&quot;y2&quot;:34.11246943765281,&quot;width&quot;:256,&quot;height&quot;:256,&quot;box&quot;:&#123;&quot;x1&quot;:162.42542787286064,&quot;y1&quot;:25.34963325183374,&quot;x2&quot;:170.24938875305622,&quot;y2&quot;:34.11246943765281&#125;,&quot;UID&quot;:&quot;fd548124&quot;,&quot;id&quot;:0,&quot;type&quot;:&quot;Rectangle&quot;,&quot;tags&quot;:[&quot;TB01&quot;],&quot;name&quot;:1&#125;,&#123;&quot;x1&quot;:214.3765281173594,&quot;y1&quot;:24.410757946210268,&quot;x2&quot;:224.07823960880197,&quot;y2&quot;:34.11246943765281,&quot;width&quot;:256,&quot;height&quot;:256,&quot;box&quot;:&#123;&quot;x1&quot;:214.3765281173594,&quot;y1&quot;:24.410757946210268,&quot;x2&quot;:224.07823960880197,&quot;y2&quot;:34.11246943765281&#125;,&quot;UID&quot;:&quot;5318dd81&quot;,&quot;id&quot;:1,&quot;type&quot;:&quot;Rectangle&quot;,&quot;tags&quot;:[&quot;TB01&quot;],&quot;name&quot;:2&#125;,...&#125; 部分标注内容如上，主要包含了对应的文件夹下有哪些图片，图片上有无杆菌，杆菌的位置在哪个格子（要自己判断），以及一张图片有杆菌的话共有几个（”name”）。 首先找出哪些是positive的图片，并且根据坐标位置写出标签: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def find_write_positive_imgs(src_json_path, src_imgs_path, dst_csv_path, dst_imgs_path): data_csv = open(dst_csv_path, &apos;a+&apos;, newline=&apos;&apos;) csv_writer = csv.writer(data_csv) csv_writer.writerow([&quot;ImageName&quot;, &quot;LabelId&quot;]) with open(src_json_path,&apos;r&apos;) as load_json: load_dict = json.load(load_json) img_names = load_dict[&apos;visitedFrames&apos;] for img_name in img_names: #n_name represents the boxes quantities of the img &lt;&quot;name&quot; attribute in .json file&gt; n_name=len(load_dict[&apos;frames&apos;][img_name]) if n_name &gt; 0: src_img_path = os.path.join(src_imgs_path, img_name) img = cv2.imread(src_img_path) H = img.shape[0] W = img.shape[1] dst_img_path = os.path.join(dst_imgs_path, img_name.replace(&apos;.png&apos;, &apos;_22.png&apos;)) cv2.imwrite(dst_img_path, img) labelid = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] for i in range(0,n_name): x1 = load_dict[&apos;frames&apos;][img_name][i][&apos;x1&apos;] y1 = load_dict[&apos;frames&apos;][img_name][i][&apos;y1&apos;] area_h0 = 0 area_w0 = 0 for area_h1 in range(H//4, H+1, H//4): if y1 &gt; area_h0 and y1 &lt; area_h1: row_id = (area_h1 * 4 / H) - 1 for area_w1 in range(W//4, W+1, W//4): if x1 &gt; area_w0 and x1 &lt; area_w1: col_id = (area_w1 * 4 / W) - 1 id = int(col_id + 4 * row_id) labelid[id] = 1 break else: area_w0 = area_w1 break else: area_h0 = area_h1 csv_writer.writerow([img_name.replace(&apos;.png&apos;, &apos;_22.png&apos;), &apos;&apos;.join(str(k) for k in labelid)]) 此外，由于最后找出的positive图片很少（好像只有320张），我又对其进行了数据扩增，先是原始旋转一圈，然后right-left翻转后又旋转了一圈，因此总共扩增到了8倍大小。之后进行一下train-val-test set的划分，一般生成随机数就可以按自己的意愿划分，也有专门的库，具体划分代码就不上了。 另外数据增强方面也考虑过rgb转hsv或者ycrcb的，但是我试了一个样例之后效果不是很好，毕竟这样做的目的就是为了将主要的前景和特征显示出来，奈何我的数据太差了些，不好操作，于是作罢。 准备好数据之后，要对数据进行抽取，我用的是pytorch，直接继承Dataset类就好： 1234567891011121314151617181920212223242526272829class SSDataset(Dataset): def __init__(self, imgs_path, csv_path, img_transform=None, loader=default_loader): with open(csv_path, &apos;r&apos;) as f: #这里一定要按字符串读取，否则前面的0会丢掉 #类似于OCR的labe读取 data_info = pd.read_csv(f, dtype=str) #第一列是image name self.img_list = list(data_info.iloc[:,0]) #第二类是labelid self.label_list = list(data_info.iloc[:,1]) self.img_transform = img_transform #loader用PIL.Image.open() #不要用cv2.imread() #pytorch默认PIL格式 self.loader = loader self.imgs_path = imgs_path def __getitem__(self, index): img_path = os.path.join(self.imgs_path, self.img_list[index]) img = self.loader(img_path) label = self.label_list[index] if self.img_transform is not None: img = self.img_transform(img) return img, label def __len__(self): return len(self.label_list) 但是定义的labelid是str，还需要转成tensor去计算loss: 1234567891011def labelid_switch(labels_str): b_s = len(labels_str) pad_label = [] for i in range(0, b_s): temp_label = [0]* 16 temp_label[:16] = labels_str[i] temp_label = list(map(int, temp_label)) pad_label.append(temp_label) pad_label = torch.Tensor(pad_label) labels_float = pad_label.view(b_s, 16) return labels_float train训练模型是主要用的是resnet和vgg，这部分代码可以直接参考torchvision，然后改改后面的layer就好了。 loss function上我试了binary cross entropy和focal loss（毕竟整体上positive grids还是少于negative grids的），此外我也试了下mixup，就是随机把batch里面的图片两两混合，计算loss的时候按照混合的比例分别计算相加，这也是一种应对过拟合，降低模型复杂度的办法（还有一种类似的方法叫sample pairing，只混合图片，不管label，我也试了，不过实际好像没mixup顶用）： 123456789101112class BFocalLoss(nn.Module): def __init__(self, gamma=1,alpha=0.8): super(BFocalLoss, self).__init__() self.gamma = gamma self.alpha = alpha def forward(self, inputs, targets): p = inputs loss = -self.alpha*(1-p)**self.gamma*(targets*torch.log(p+1e-12))-\ (1-self.alpha)*p**self.gamma*((1-targets)*torch.log(1-p+1e-12)) loss = torch.sum(loss) return loss 1234567891011121314151617def mixup_data(in_img, in_label, alpha=1.0): #alpha in [0.1,0.4] in paper has better gain(for imagenet) #for cifar-10 is 1. if alpha &gt; 0: lam = np.random.beta(alpha, alpha) else: lam = 1 Batch_Size = in_img.size()[0] Index = torch.randperm(Batch_Size) mixed_x = lam * in_img + (1 - lam) * in_img[Index, :] y_a, y_b = in_label, in_label[Index] return mixed_x, y_a, y_b, lam #计算loss loss_mixup = lam * criterion(pred, labels_a) + \ (1 - lam) * criterion(pred, labels_b) 接下来的事就是调参，对比实验，开tensorboard看loss的趋势了。（这里有一个现象，前期有一部分时间loss难以下降，总是在一个范围内波动，我猜想可能是因为数据扩增的原因。） test这部分就是加载模型，一张张图片测试，然后写出预测的csv即可，然后给出grid acc 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#部分代码如下：file_pre = open(PRE_TEST_CSV, &apos;w&apos;, newline=&apos;&apos;)pre_writer = csv.writer(file_pre)pre_writer.writerow([&quot;ImageName&quot;, &quot;LabelId&quot;])with open(SRC_TEST_CSV, &apos;r&apos;) as f_test: test_data = pd.read_csv(f_test, dtype=str) img_name = list(test_data.iloc[:,0]) labelid = list(test_data.iloc[:,1]) test_data_len = len(test_data.index) num_right = 0 positive_num = 0 positive_num_right = 0 for i in range(0,test_data_len): img_path = os.path.join(TEST_DATA_PATH, img_name[i]) img = Image.open(img_path) img_tensor = transformations(img).float() img_tensor = img_tensor.unsqueeze_(0) temp_label = [0]*16 temp_label[:16] = labelid[i] temp_label = list(map(int, temp_label)) for temp in temp_label: if temp &gt; 0: positive_num += 1 label = torch.FloatTensor(temp_label) label = label.view(1, 16) input = Variable(img_tensor) input = input.to(device) pred = net(input).data.cpu() #在CPU中比较 output = pred pred_len = pred.size()[1] out = [] for j in range(0, pred_len): if pred[0][j] &lt; 0.5: output[0][j] = 0 out.append(0) if output[0][j] == label[0][j]: num_right += 1 else: output[0][j] = 1 out.append(1) if output[0][j] == label[0][j]: num_right += 1 positive_num_right += 1 pre_writer.writerow([img_name[i],&apos;&apos;.join(str(k) for k in out)]) print(&apos;test acc is: &apos;, num_right/(test_data_len*16)) print(&apos;postivite acc is: &apos;, positive_num_right, &apos;/&apos;, positive_num) summary实际上这个代码下来，调参还是挺费劲的，尤其是对我这种刚开始搞深度学习，经验还不够的新手来说，着实走了不少弯路。可是数据集实在太差，实在是想不出什么招。。所以硬撑了快两个月（实际上前大半个月我是直接分割grid成单独的图片，然后全部丢进去训练的。。这样搞不仅正负样本差距极大，而且切断了图片的连续性，效果奇差也在意料之中了，基本训不动，即使加了focal loss也没什么卵用）最后最高也才得到90%的acc。 weakly semantic segmentation好歹8月下旬那会找到了一个公开的sputum smear的数据集，还带着框的标注： Makerere University, Uganda homepage paper code 跟CTO交流后，他觉得这数据集质量不错，干脆就提议做弱监督分割，毕竟object detection现在都做烂了，而且开源这数据集的小哥自己也把object detection的acc刷的不错了，所以没必要再调包重复同样的事情了。我当时其实没啥思路，但是觉得应该挺有意思的，于是就接了下来。 后来通过调研发现，原来在自然图像上早就有人做了weakly segmentation(又是我恺明哥那些人…)，而且效果还不错，唯一可惜的就是完整的代码基本没人开源，不过后来参考GitHub上的一些相关代码也慢慢搭建出了整个框架。 整个项目思路主要参考的是这两篇论文：戴季峰的BoxSup和Max Planck Institute的Simple Does It，主要的思路就是先设定几个从bounding box annotations生成segment proposals的方法（主要是opencv中GrabCut），然后利用此label去进行supervised training，最后过一下denseCRF优化一下，让boundary更加丝滑。当然也可以试试递归训练，让performance不错的model去预测生成新的training set中的label，然后进行下一轮的训练。 因为代码比较庞杂，分块不好展示，完整的代码就直接放在我的github上。 pre-processing原始的数据集中有1217张阳性图片，此外这些图片的标注还有47张莫名奇妙多了些20x20的框（可能是标的时候手抖了），因此要先一个个去掉。 之后，对这些图片进行大致masks的生成，我这里给了三种方法： Box_segments: 把整个box里面的像素都认为是杆菌（要把box的坐标都转成int，得对上像素） Sbox_segments:取box里面的80%的矩形框，认为该框里面的像素都是杆菌（同样，坐标都是int类型） GrabCut_segments: 利用经典的计算机视觉方法GrabCut来得到杆菌的分割区域，但是该方法一般对图片的里面的单个的大物体比较友好，而杆菌又细又长，同时又包含着染色质，所以利用颜色分布的GrabCut分割出的杆菌要么会大点，要么就没有。大点的我不管，没有的我在这里就直接用Box_segments代替了。 GrabCut部分代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778def grabcut(img_name): masks = [] # one image has many object that need to grabcut for i, ann_info in enumerate(ANNS[img_name], start=1): img = cv.imread((img_dir +img_name).rstrip()+&apos;.jpg&apos;) grab_name = ann_info[1] xmin = ann_info[3] ymin = ann_info[2] xmax = ann_info[5] ymax = ann_info[4] &quot;&quot;&quot;get int box coor&quot;&quot;&quot; img_w = img.shape[1] img_h = img.shape[0] xmin, ymin, xmax, ymax = get_int_coor(xmin, ymin, xmax, ymax, img_w, img_h) box_w = xmax - xmin box_h = ymax - ymin # cv.grabcut&apos;s para mask = np.zeros(img.shape[:2], np.uint8) # rect is the tuple rect = (xmin, ymin, box_w, box_h) bgdModel = np.zeros((1, 65), np.float64) fgdModel = np.zeros((1, 65), np.float64) #for small bbox: if box_w * box_h &lt; MINI_AREA: img_mask = mask[ymin:ymax, xmin:xmax] = 1 # for big box that area == img.area(one object bbox is just the whole image) elif box_w * box_h == img.shape[1] * img.shape[0]: rect = [RECT_SHRINK, RECT_SHRINK, box_w - RECT_SHRINK * 2, box_h - RECT_SHRINK * 2] cv.grabCut(img, mask, rect, bgdModel,fgdModel, ITER_NUM, cv.GC_INIT_WITH_RECT) # astype(&apos;uint8&apos;) keep the image pixel in range[0,255] img_mask = np.where((mask == 0) | (mask == 2), 0, 1).astype(&apos;uint8&apos;) # for normal bbox: else: cv.grabCut(img, mask, rect, bgdModel,fgdModel, ITER_NUM, cv.GC_INIT_WITH_RECT) img_mask = np.where((mask == 0) | (mask == 2), 0, 1).astype(&apos;uint8&apos;) # if the grabcut output is just background(it happens in my dataset) if np.sum(img_mask) == 0: img_mask = np.where((mask == 0), 0, 1).astype(&apos;uint8&apos;) # couting IOU # if the grabcut output too small region, it need reset to bbox mask box_mask = np.zeros((img.shape[0], img.shape[1])) box_mask[ymin:ymax, xmin:xmax] = 1 sum_area = box_mask + img_mask intersection = np.where((sum_area==2), 1, 0).astype(&apos;uint8&apos;) union = np.where((sum_area==0), 0, 1).astype(&apos;uint8&apos;) IOU = np.sum(intersection) / np.sum(union) if IOU &lt;= IOU_THRESHOLD: img_mask = box_mask # for draw mask on the image later img = cv.cvtColor(img, cv.COLOR_BGR2RGB) masks.append([img_mask, grab_name, rect]) num_object = i &quot;&quot;&quot;for multi-objects intersection and fix the label &quot;&quot;&quot; masks.sort(key=lambda mask: np.sum(mask[0]), reverse=True) for j in range(num_object): for k in range(j+1, num_object): masks[j][0] = masks[j][0] - masks[k][0] masks[j][0] = np.where((masks[j][0]==1), 1, 0).astype(&apos;uint8&apos;) &quot;&quot;&quot;get class name id&quot;&quot;&quot; grab_name = masks[j][1] class_id = grab_name.split(&apos;_&apos;)[-1] class_id = int(class_id.split(&apos;.&apos;)[0]) #set the numpy value to class_id masks[j][0] = np.where((masks[j][0]==1), class_id, 0).astype(&apos;uint8&apos;) # save grabcut_inst(one object in a image) scipy.misc.toimage(masks[j][0], cmin=0, cmax=255, pal=tbvoc_info.colors_map, mode=&apos;P&apos; ).save((grabcut_dir).rstrip()+masks[j][1]) &quot;&quot;&quot;merge masks&quot;&quot;&quot; # built array(img.shape size) mask_ = np.zeros(img.shape[:2]) for mask in masks: mask_ = mask_ + mask[0] # save segmetation_label(every object in a image) scipy.misc.toimage(mask_, cmin=0, cmax=255, pal=tbvoc_info.colors_map, mode=&apos;P&apos;).save((segmentation_label_dir+img_name).rstrip()+&apos;.png&apos;) 这里面我是用scipy来保存masks的，我用的版本是0.19.0，超过这个版本的scipy就没有toimage()这个函数了，据说PIL有可以替代的函数，但是我看两个的功效好像不一样，就没去折腾了。 读取数据部分进行了resize处理，原图尺寸是1632x1224，1224不能被32整除，五次下采样和上采样的时候会出现feature map维度不匹配的错误，因此resize成了1632x1216。这里要注意，原图是利用双线性插值进行resize的，masks图是利用最近邻进行resize的（实际上我是生成好masks后训练时才意识到这个问题，实际上可以在最开始就把dataset的数据resize好，这样masks的误差可能就小点），PIL和cv2里面都有类似的函数。 数据读取部分代码： 123456789101112131415161718192021222324252627282930class TBDataset(Dataset): def __init__(self, txt_dir, width, height, transform=None): self.img_names = [] with open(txt_dir, &apos;r&apos;) as f_txt: for img_name in f_txt: self.img_names.append(img_name) self.transform = transform self.txt_dir = txt_dir self.width = width self.height = height def __getitem__(self, index): img_name = self.img_names[index] img = Image.open(os.path.join(img_dir, img_name).rstrip()+&apos;.jpg&apos;) # the resize function like bilinear img = img.resize((self.width, self.height), Image.LANCZOS) img = np.array(img) label = Image.open(os.path.join(label_dir, img_name).rstrip()+&apos;.png&apos;) # for consider class_id is not consecutive and just fixed by user label = label.resize((self.width, self.height), Image.NEAREST) label = np.array(label) if self.transform is not None: img = self.transform(img) #img = torch.FloatTensor(img) label = torch.FloatTensor(label) return img, label def __len__(self): return len(self.img_names) train训练部分模型用的是FCN和UNet，因为考虑到只有二分类，后面也可以考虑deeplab，UNet++等等。FCN用的是VGG-16 backbone，下采样5次，UNet下采样4次，都是按照论文来的，没做什么改动。模型最后输出的是一个1632x1216的feature map，然后直接过sigmoid激活函数，再和1632x1216的mask图片（读进来的是一个二维0-1矩阵，代表每个像素点的label）进行loss计算，然后BP，更新参数学习。loss也用了交叉熵和focal loss. post-processing对模型预测出的结果再过一遍denseCRF，优化分割的同时也会去掉一些false-positive 部分代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940def run_densecrf(img_dir, img_name, masks_pro): height = masks_pro.shape[0] width = masks_pro.shape[1] # must use cv2.imread() # if use PIL.Image.open(), the algorithm will break #TODO --need to fix the image problem img = cv.imread(os.path.join(img_dir, img_name).rstrip()+&apos;.jpg&apos;) img = cv.resize(img, (1632,1216), interpolation = cv.INTER_LINEAR) # expand to [1,H,W] masks_pro = np.expand_dims(masks_pro, 0) # masks_pro = masks_pro[:, :, np.newaxis] # append to array---shape(2,H,W) # one depth represents the class 0, the other represents the class 1 masks_pro = np.append(1-masks_pro, masks_pro, axis=0) #[Classes, H, W] # U needs to be flat U = masks_pro.reshape(2, -1) # deepcopy and the order is C-order(from rows to colums) U = U.copy(order=&apos;C&apos;) # for binary classification, the value after sigmoid may be very small U = np.where((U &lt; 1e-12), 1e-12, U) d = dcrf.DenseCRF2D(width, height, 2) # make sure the array be c-order which will faster the processing speed # reference: https://zhuanlan.zhihu.com/p/59767914 U = np.ascontiguousarray(U) img = np.ascontiguousarray(img) d.setUnaryEnergy(-np.log(U)) d.addPairwiseGaussian(sxy=3, compat=3) d.addPairwiseBilateral(sxy=80, srgb=13, rgbim=img, compat=10) Q = d.inference(5) # compare each value between two rows by colum # and inference each pixel belongs to which class(0 or 1) map = np.argmax(Q, axis=0).reshape((height, width)) proba = np.array(map) return proba 这里主要用到了二元势pairwise potential，比较每个像素和其他像素的关系，具体原理可以去看看原代码和论文。 此外，我还顺手进行了下迭代训练。实际上，对于我这个数据集，基本上用GrabCut生成label训练一遍效果就不错了，不过为了看下更新label再训练一轮会不会得到更好的结果，在固定的epoch结束后将训练好得模型设为eval模式，然后预测train set的数据，然后再返回train模式继续训练。需要注意的是，更新label的时候，可能会有漏诊和误诊，我就直接将预测的mask和Box_segments得到的mask相加，只取为2的部分，这样就去掉了假阳性，然后漏诊的部分再用box补回来。 从实验结果来看，一般我这个是更新3次label（每10个epoch更新一次）就差不多了，再多也没什么提升。总体上来说，这个操作可以提高单张图片同时存在多个杆菌的分割效果，但是提升力度也没什么太令人满意的地方。可能是我的更新姿势不对？ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def update_label(predict_model, device): &quot;&quot;&quot;load train_pairs.txt info for check the missed diagnosis objects&quot;&quot;&quot; #ann_info:[image name, image name_num_ class_id.png, bbox_ymin, # bbox_xmin,bbox_ymax, bbox_xmax, class_name] print(&apos;start to update...&apos;) ANNS = &#123;&#125; with open(dataset_pairs_dir, &apos;r&apos;) as da_p_txt: for ann_info in da_p_txt: # split the string line, get the list ann_info = ann_info.rstrip().split(&apos;###&apos;) if ann_info[0].rstrip() not in ANNS: ANNS[ann_info[0].rstrip()] = [] ANNS[ann_info[0].rstrip()].append(ann_info) predict_model.eval() # define the same image transformations transformations = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) update_num = 0 print(&apos;updating progress:&apos;) with open(dataset_txt_dir, &apos;r&apos;) as da_txt: # don&apos;t use the code line below # or it will close the file and the whole programm end here (I guess) # I debug here for two hours...... #lines = len(da_txt.readlines()) for update_name in da_txt: update_num += 1 # in RGB [W, H, depth] img = Image.open(os.path.join(img_dir, update_name).rstrip()+&apos;.jpg&apos;) img_w = img.size[0] img_h = img.size[1] img = img.resize((1632, 1216), Image.LANCZOS) input_ = transformations(img).float() # add batch_size dimension #[3, H, W]--&gt;[1, 3, H, W] input_ = input_.unsqueeze_(0) input_ = input_.to(device) pred = predict_model(input_).view([1216, 1632]).data.cpu() #pred.shape[H,W] pred = np.array(pred) &quot;&quot;&quot;crf smooth prediction&quot;&quot;&quot; crf_pred = run_densecrf(img_dir, update_name, pred) &quot;&quot;&quot;start to update&quot;&quot;&quot; last_label = Image.open(os.path.join(label_dir, update_name).rstrip()+&apos;.png&apos;) last_label = last_label.resize((1632, 1216), Image.NEAREST) last_label = np.array(last_label) # predicted label without false-positive segments updated_label = crf_pred + last_label updated_label = np.where((updated_label==2), 1, 0).astype(&apos;uint8&apos;) # predicted label with missed diagnosis # we just use the box segments as missed diagnosis for now info4check = ANNS[update_name.rstrip()] masks_missed = np.zeros((1216, 1632), np.uint8) for box4check in info4check: xmin = box4check[3] ymin = box4check[2] xmax = box4check[5] ymax = box4check[4] xmin, ymin, xmax, ymax = get_int_coor(xmin, ymin, xmax, ymax, img_w, img_h) xmin = int(xmin * 1632 / img_w) xmax = int(xmax * 1632 / img_w) ymin = int(ymin * 1216 / img_h) ymax = int(ymax * 1216 / img_h) if np.sum(updated_label[ymin:ymax, xmin:xmax]) == 0: masks_missed[ymin:ymax, xmin:xmax] = 1 updated_label = updated_label + masks_missed scipy.misc.toimage(updated_label, cmin=0, cmax=255, pal=colors_map, mode=&apos;P&apos;).save(os.path.join(label_dir, update_name).rstrip()+ &apos;.png&apos;) print(&apos;&#123;&#125; / &#123;&#125;&apos;.format(update_num, len(ANNS)), end=&apos;\r&apos;) metric一般的segmentation论文都是用IoU来进行比较的，但是这个数据集没有segmentation groundtruth，所以我就自己定义了个检测的acc：预测的mask和框有交叉(np.sum(region of box)!=0)，就认为检测出了一个，然后算average acc，通过这个指标和test set上的预测结果来大致衡量哪些方法组合在一起不错。最后总结下来，还是GrabCut+FCN+FL($\alpha=0.75,\gamma=1$)更好些，不过我没加大UNet的深度和通道数，否则的话我猜想可能UNet会占上风。 篇幅有限，放几个还不错的预测结果： summary总的来说，最后的弱监督分割还是收获挺多的，尤其是自己的工程能力得到了锻炼，代码组织和书写也得到了一定地提升，最后相关成果也写成论文投了ISBI会议，如果能中的话，还是很舒服的^-^]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>computer vision</tag>
        <tag>CNN</tag>
        <tag>medical image analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习见闻及其他]]></title>
    <url>%2F2019%2F10%2F03%2F%E5%AE%9E%E4%B9%A0%E8%A7%81%E9%97%BB%E5%8F%8A%E5%85%B6%E4%BB%96%2F</url>
    <content type="text"><![CDATA[早有耳闻近年的机器学习和计算机视觉岗位不好找，很多研究生都会借着自己城市和学校的优势，去争取一些公司的实习。我是兜兜转转好几年后才开始正式去学机器学习，也算是半路出家，而且有种49年入国军的赶脚。平日里看着教研室的师兄们即使做着非机器学习算法也在求职的道路上一波三折，心里不免对自己的前途充满着担忧和焦虑。好在导师关系网络庞大，在今年5月丢给了我一个去AI医疗影像公司实习的机会。能够得到锻炼，同时也可以见识见识工业界的研究节奏和方式，这样的运气，我自然不会让其白白溜走。这家公司南京分部的CTO是一位加州海归博士，热爱技术又充满激情，在勉强通过他的面试之后，我得到了三个月的实习机会。 公司的效率和推进速度还是非常快的，刚开始确实不适应，毕竟放羊久了，长时间地坐在电脑前看论文，垒代码，调参数还是有点乏人的。不过我是分配了一个单独的项目，所以三个月来就相当于单干，虽然没有外部的压力，但是没能参与核心项目，与公司大佬们一起讨论，还是心存遗憾（不过自己水平确实不高，也很正常，咱心里还得有点数才行…）。实习的任务主要是对他们提供的痰涂片（sputum smear）数据集进行分类，但是不是一般意义上的单张图片分类。这里我先简单科普下，痰涂片是为了辅助诊断结核病而采集的样本，医生根据采集的病人的痰液里面的结核杆菌（tuberculosis bacillus）数量来判断阳性或阴性，这种手段主要在发展中国家，贫困地区等地方常见（有条件的直接照X光了）。原始数据集的每张图片都是从显微镜采的，而且分成了4x4的grid，我要做的就是分类图片每个grid的label，有点类似one-stage的目标检测。 然而，数据又脏又少。。。。折腾了两个月实在没弄出满意的结果（好像最后最好才到0.9的acc）。没办法，最后Google找了好几天偶然发现了一个公开的数据集，还带着bounding box的标注！！！于是后面和CTO商讨，干脆上了weakly semantic segmentation（最后10几天的工作量比前两个月的还要多的多。。果然deadline是第一生产力）。好歹最后结果还可以。详细的讲解可以看我下一篇博客。 下面我想重点谈谈自己的收获与感想： 代码和算法。只有真正经历了才明白代码能力是多么的重要。从面试一直到解决实际工程问题，或者是自己平时研究，搭网站，作图，实现自己的一个想法…等等等等，都可以发挥巨大的的作用。虽然说做机器学习数学相对重要点，可是进入职场以后，算法和代码能力才是硬实力。对于我这种菜比来说，多练，多参考，多总结是唯一有效的办法 工业界和学术界。工业界基本上都是以项目和数据驱动的（可能国内大的学教研室跟其也没什么不同。。），基本上不关心如何创造新方法，新理论，只关心产品如何更好地生成和落地。每天的迭代节奏都很快，各个领域的知识都会涉及（初创公司可能更多）。一般的项目组都是利用现成的SOTA去修改然后适配自己的数据，所以就要求职员得预先懂得很多东西，或者知道哪些方法可以用上，要是从头开始调研的话就太慢了。此外，工业界还有着模块化的管理，比如利用trello这样的软件和早会来安排具体每个人一段时间的任务，交流进展情况，及时解决blocked问题等等。对于涉及到算法的公司，代码的管理，维护，更新是个big problem，从代码托管平台或者服务器的选择，coding的规则制定，review code的指定和审阅标准，要审几轮之后才能merge等都是一些比较琐碎却很实际的问题，是需要慢慢完善的过程。现在想想，那么多学术大牛后面选择投身工业界不是没有道理的，对他们来说研究可以照常进行，资源也更加丰富，而且更有那种creating的青春感，反而学术界里面无拘无束的环境，可能会慢慢消磨他们的激情。处于象牙塔里是很可能被温水煮青蛙的。 保持学习和交流的习惯。即使是工作以后，也要保持一种好奇心，要促使自己不断地去进步。学习不仅是让你更加游刃有余于自己的工作，也是为了打开自己的视野，丰富自己的生活。以技术学习为例，公司的研究组一般都会有paper reading这样的活动，每周读一篇典型的论文，轮流安排组员上去讲解，一方面打开工作思路，一方面也是锻炼人的说话和展示能力（费曼学习法）。不要对别人的工作满不在乎，漠不关心，经常主动和别人交流，既交了朋友，也可以扩展自己的领域认知。 把握工作节奏，保持沟通。如今996大行其道，加上媒体大肆宣扬，搞得程序猿们人心惶惶。实际上如果是一个项目要到期了，某段时间可能要加点班一起赶出来的话，是可以接受的，但是如果每天强制加班，搞得连自己生活都没了，那我肯定是果断放弃了，除非给我年薪百万。。。因此，在找工作前，首先得平衡好自己的要求，认清一些现状，身体健康和生活质量是放在第一位的，其他的都是为了这两者服务的。有困难，有难题一定要及时反映或者在其恰当的时机吐露出来，不要动不动就轻视自己的生命（可能还没到这种地步），当然这是最差的情况。平时和同级沟通，和上级沟通（反映意见，提出工作思路，建议等）都是必不可少的职场状态。 总的来说，第一次实习还是挺圆满的，虽然没学到什么大本事，但是起码这段经历让我收获了很多，也更加坚定了自己的选择，对未来的路也看得明晰了些。不得不说，有些事情还是要自己经历过后才能懂得，这些没法感同身受的经验，是怎么也无法理解的。趁着年轻，还是应该多把自己推出舒适区，多去争取一些历练和出走的机会，以此开阔自己的眼界，认识社会和世界。 好像写到现在也没怎么提及到实习公司的情况。。。想着最后了，还是祝贺下组里的大佬们帮公司拿到了吴恩达chestXpert 数据集比赛的冠军，而且霸榜了几个月！着实佩服。我也从他们身上学到了不少新知识和新姿势。 自己厚着脸皮也和他们一起公费嗨皮庆祝了下。 好了，今年的实习告一段落了，明年希望能自己争取到大厂的实习。(‘ o_o ‘)]]></content>
      <categories>
        <category>随笔杂谈</category>
      </categories>
      <tags>
        <tag>computer vision</tag>
        <tag>reflections</tag>
        <tag>job</tag>
        <tag>coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[往者不谏，来者可追]]></title>
    <url>%2F2019%2F05%2F11%2F%E5%BE%80%E8%80%85%E4%B8%8D%E8%B0%8F%EF%BC%8C%E6%9D%A5%E8%80%85%E5%8F%AF%E8%BF%BD%2F</url>
    <content type="text"><![CDATA[要想得到某样东西，最可靠的办法先让自己配得上它 前些日子无意间看到了一位A. J. Davison教授的新博士生在知乎上写的一篇回答，他在里面谈到了自己科研之路曲折受挫的经历，尤其是屡屡面对着与自己期望不符合的情况，让我感触很深，想起了自己去年准备国外博士申请失败的那段苦痛记忆。可能对于我来讲，自己对读博的认识和准备都还欠缺得很，既没有拿得出手的本领，技能和honor，也没有对未来研究方向的清楚认识，与另一位机器人大牛YY硕读博的初衷和认知相比，实在是汗颜。 所谓人的成长，其实是“不断发现个人独特的经历原来都只是人类普遍经验的一部分”的过程 ​ —- 多丽丝 莱辛 自己步入大学也快接近五年了，在这五年来的科研学习生涯上，没有出现一件让我满意的工作，甚至可以说连接触“像样的工作”的机会都没有。回想本科的生活，似乎陷入了一种想要寻求突破发展，却又每每哀叹碰壁于现实，最终困于浑浑噩噩的循环中。虽然高考成绩把我带到了这个所谓的“培优学院”中来，但是实际上鸡肋的培养模式和自己能力不足以及信息缺失等问题，让自己走了很多弯路。从大二上学期进入学院导师的实验室开始，就开始了漫长的专业方向调研和探索学习之旅：先是MEMS器件，再是小型固定翼无人机飞行控制系统，然后转到了飞控数据采集和处理硬件平台，后来又顺着跳到了四旋翼避障，然而结果最终都因为各种原因和变故而终止。现在磕磕碰碰转到了SLAM，从此开始接触计算机视觉，又加上人工智能大热，算是勉强把自己的研究定位在visual SLAM, Robotics和Machine Learning上，即使现在没有任何老师和同专业的师兄师姐指导，但是我心底里知道，这是自己好不容易觉得自己感兴趣的可以坚持做下去的方向，心里也十分珍惜，不愿轻易放弃，纵使单枪匹马很难在这个领域做出东西来。 心里开始真正坚定了出国留学的想法是在保研后期的时候，那时通过自己的偶然探寻和师兄的推荐，先后联系了两位国外的教授，虽然他们对我的态度都很好，也愿意接收我，但是最终都因为CSC奖学金的问题而以失败告终（甚至连尝试申请CSC的机会都没有）。在这期间，我曾自信满满地以为自己一定能与过去失败的生活说再见，可以在新的教研室，友好的导师的帮助和指导下开启自己目前最感兴趣的课题，然后三四年读完博士，之后再去争取更好的平台继续研究。现在看来，我想得太过于理想化，现实是残酷的，会时不时让人痛苦，“而这种痛苦往往来自于人对于自己无能的愤怒”。在得知CSC奖学金不再对我申请的学校提供名额的那一刻起，就基本上宣判我大半年的准备都是白费的，毕竟自己和意向导师的实验室都缺钱，所以CSC几乎是唯一的funding。当时的我陷入了极大的消极之中，就像看到了希望，看到了黑暗生活中的一丝光亮，想要伸手去触碰，去争取，却又被黑暗中的种种阻碍和枷锁困住，动弹不得。 后来我安慰自己，人是无法预测自己的命运的，我们可以做的就是不断地让自己变得更好，to be kind。其实我后来想想，这似乎也是天意，让我好好地去审视自己，看清楚自己身上到底出了什么问题，而不是就这么简单地给予我幸运的关照，让我顺顺利利地就能跑到国外读博，逃避现实。因为不论是从科研能力上，心理素质上，理性认知上，以及生活技能上我都还欠火候，还需要不断地修炼，毕竟选择科研这条路不是一句简单的承诺，不仅仅是付出就可以安然走下去的。 让我欣慰的是，自己地沉沦与消极并没有持续很久，几天之后就慢慢放下了，可能自己心底里也并不是对那些学校和生活特别期待和憧憬，可能自己心底里也还是有个标准的。我一边开始继续自己的研究生工作学习和课题探寻，一边反思自己的生活和精神状态：那些让自己觉得失败的经历到底是如何发生的，为什么我早就认识到了却没有去很好地补救？ 我想，一方面是信息的缺失，身边没有一些志同道合的大佬级别的人之外，另一方面是自己持续不断的，反反复复的自怨自艾，想得太多而又做的太少，眼高手低。前者是没办法选择的，毕竟环境和出生不同，这是客观的因素，我能做的就是学会善用信息时代的网络技术，持续地阅读优质的内容，挖掘有效的有价值的信息，并且主动地结交那些志同道合的前辈；后者是主观因素，是内因，才是自己最应该去改变的地方。因为只有不断地去做，去尝试，机会和惊喜才会眷顾你。努力不一定有收获，但是起码会给你一些选择和机会，同时让人在这个过程中成长。 Work cures everything. ​ ——马蒂斯 因此，我开始承认自己菜，而且承认自己是真的菜，那些优秀的人不仅在学习和科研上有很高的建树，在其他方面，诸如绘画，音乐，运动，哲学，文学等都有很高的领悟和施展技巧。我开始慢慢认识到自己的不足和平凡，开始努力地在各个方面去提升自己，当然也是自己一贯就感兴趣的地方，同时也想打开自己的眼界，愿意并持续地去尝试新事物，涉猎不同的领域，发展多种爱好。一言以蔽之，就是活在当下，持续工作，持续输入输出，以此提升自己，慢慢地体验生活，了解生活，享受生活。毕竟这才是人活于世的一项基本目的。 写到这，我想起了英国演员本尼迪克特在节目letters live上朗读的一封书信，这是美国先驱艺术家索尔·勒维特写给他的好友伊娃·黑塞一封信，背景是1965 年，黑塞经历了一段自我怀疑的时期，她的创作遇到瓶颈，她迷茫、沮丧，不知道该怎么办，不知道未来在哪里，她向勒维特倾诉自己遇到的“心灵困境”。几周后，勒维特用以下这件作品回复了她：一封精妙、宝贵的建议信，让她不要再彷徨，而是”stop it and just Do “. 信的内容如下： Dear Eva, It will be almost a month since you wrote to me and you have possibly forgotten your state of mind(I doubt it though). You seem the same as always, and being you, hate every minute of it. Don’t! Learn to say Fuck You to the world every once in a while. You have every right to. Just stop thinking, worrying, looking over your shoulder wondering, doubting, fearing, hurting, hoping for some easy way out, struggling, grasping, confusing, itching, scratching, mumbling, bumbling, grumbling, humbling, stumbling, rumbling, rambling, numbling, gambling, tumbling, scumbling, scrambling, hitching, hatching, bitching, moaning, groaning, honing, boning, horse-shitting, hair-splitting, nit-picking, piss-trickling, nose-sticking, ass-gouging, eyeball-poking, finger pointing, alleyway-sneaking, long waiting, small stepping, evil-eyeing, back scratching, searching, perching, besmirching, grinding, grinding away at yourself. Stop it and just DO. From your description, and from what I know of your previous work and your ability; the work you are doing sounds very good. Drawing-clean-clear but crazy like machines, larger and bolder…real nonsense. That sounds fine, wonderful-real nonsense. Do more, more nonsensical, more crazy, more machines, more breasts, penises, cunts, whatever-make them abound with nonsense. Try and tickle something inside you, your weird humor. You belong in the most secret part of you. Don’t worry about cool, make your own uncool.Make your own, your own world. If you fear, make it work for you-draw &amp; paint your fear and anxiety. And stop worrying about big, deep things such as to decide on a purpose and way of life, a consistent approach to even some impossible end or even an imagined end. You must practice being stupid, dumb, unthinking, empty. Then you will be able to DO. I have much confidence in you and even though you are tormenting yourself, the work you do is very good. Try to do some BAD work. The worst you can think of and see what happens, but mainly relax and let everything go to hell. You are not responsible for the world-you are only responsible for your work-so DO it. And don’t think that your work has to conform to any preconceived form, idea or flavor. It can be anything you want it to be. But if life would be easier for you if you stopped working-then stop. Don’t punish yourself. However, I think it is so deeply engrained in you that it would be better for you to DO. It seems I do understand your attitude somewhat, anyway, because I go through a similar process every now and again myself. I have an Agonizing Reappraisal of my work and change everything as much as possible-and hate everything I’ve done, and try to do something entirely different and better. Maybe that kind of process is necessary to me, pushing me on and on. The feeling that I that I can do better than that shit I just did. Maybe you need your agony to accomplish what you do. And maybe it goads you on to do better. But it is very painful I know. It would be better if you had the confidence just to do the stuff and not even think about it. Can’t you leave the world and ART alone and also quit fondling your ego. I know that you(or anyone) can only work so much and the rest of the time you are left with your thoughts. But when you work of before your work you have to empty you mind and concentrate on what you are doing. After you do something it is done and that’s that. After a while you can see some are better than others but also you can see what direction you are going. I’m sure you know all that. You also must know that you don’t have to justify your work-not even to yourself. Well, you know I admire your work greatly and can’t understand why you are so bothered by it. But you can see the next ones and I can’t. You also must believe in your ability. I think you do. So try the most outrageous things you can-shock yourself. You have at your power the ability to do anything. I would like to see your work and will have to content to wait until Aug or Sept. I have seen photos of some of Tom’s new things at Lucy’s. They are impressive-especially the ones with the more rigorous form: the simpler ones. I guess he’ll send some more later on. Let me know how the shows are going and that kind of stuff. My work had changed since you left and it is much better. I will be having a show May 4-29 at the Daniels Gallery 17 E 64th St(where Emmerich was), I wish you could be there. Much love to you both. Sol 实际上，这段朗读视频和信的内容我在本科大二的时候就接触到了，可惜当时并没有对此有很深的认知，看来很多道理和改变还是经历过才会真正懂得和接受。现在重读这封信，突然觉得字字箴言。每个人或多或少都会遇到类似的情况，会消沉，会不自信，会怀疑人生，但是事实如此，且事出有因，我们唯一能做的就是调整自己的状态和方向，持续“做下去”，而不是想太多，束缚自己的手脚，正如Sol在信中说的 : Don’t worry about cool, make your own uncool.Make your own, your own world. If you fear, make it work for you-draw &amp; paint your fear and anxiety. After you do something it is done and that’s that. After a while you can see some are better than others but also you can see what direction you are going. “现在，我接受自己满意的作品，也接受不满意的作品，接受积极状态，也接受不积极状态，这是正常的生活，但我不会停止去做，在过程中体验其意义和价值。” 自怨自艾，消沉停滞是没有意义的，起码对那时的自己没有帮助，不管是顺境还是逆境，都得持续做下去，因为只有不断地进行下去，最后的意义才会明了。从某种意义上说这似乎是一种”down-top”的思想，我不知道最后结果如何，或者说意义如何，但是我现在要做这件事。就像我身边很多人跟我说“你有没有觉得这一年你感觉天天都很忙，每天起床干活，晚上上床睡觉，日复一日，可是我回想却发现好像啥也没做成”，包括我也是这样。我还年轻，这个问题我没办法理解透彻，只能姑且认为这些都是一种积累，大部分的积累最后会形成质变，其他的一些则会在以后的某个阶段给予我帮助，提供给我一些机会。 与“down-top”相反的则是”top-down”，拥有这些能力的估计应该都是比较厉害的人吧，从一开始就知道自己想要做什么，整个人生的大阶段和小阶段都有详细的目标和追求，然后对达到目标的过程又有很清晰的认识，有远见，胸有成竹，因此按照条件需求进行学习提升，后面根据成果和目标之间的差距进行微调，接着进行下一轮的提升。这样的人应当是具有很强的自制力的和自律性的，通常应该属于精英阶层，各方面条件都会不错。然而我只是一个普通人，在认识到自己的局限性后，希望能通过各方面的学习，交流，思考和输出来达到一个让自己满意的境界。“不丢脸是不可能变强的”。 那么到底什么样的东西是值得做的，什么样的人生是值得过的呢？“just DO”之前，如何知道现在做的或者之后做的事是值得的呢？实际上，我认为我们可能不会知道，正是因为不知道，所以我们之前才可能会陷入怀疑，但是如果喜欢却不继续工作下去是件很可惜的事，就像Yan LeCun在上个世纪坚持做神经网络和你现在决定做神经网络的道理一样，你做是因为现在神经网络很火，工资高，而Yan LeCun那个时候做是因为他认为这是值得做的，是自己喜欢做的，觉得是可以引发革命的，所以即使受到怀疑，也还是坚持做了下去（举个例子，当然事实可能不一定是这样，另外SLAM领域的大牛Andrew. J. Davison也是花了十年的时间才让人们注意到他工作的价值）。所以做与思考（个体和团体）是一个交织的整体，不应该是看别人做什么就做什么。在不知道自己喜欢什么之前，可以先和别人交流，看看大概是什么样子，觉得还不错的情况下可以去试着深入了解下，至少在科研上是如此，我在走了很多弯路之后才发现视觉SLAM是个非常有挑战性和综合性的领域，充满着令人兴奋的点和创新式的解决办法，所以我才愿意花时间深入研究，虽然申请该专业的博士失败了，但并不代表着我对相关领域的学习研究就此结束，至少我明白了，科研是科研，它代表的是自己的一种兴趣和工作的结合，它不应该掺杂太多的功利性和急于求成的心思在里面，不应该老是渴望着被人认可，希望受到其他研究工作者的高评价，这样会扰乱自己的思绪，甚至阻碍新想法的出现，同样也会过得不自由。 自由就是不再寻求认可。 ”你是这样年轻，一切都在开始，亲爱的先生，我要尽我所能请求你，对于你心里一切得疑难要多多忍耐，要去爱这些“问题得本身”，像是爱一间锁闭了的房屋，或是一本用别种文字写成的书。现在你不要去追求那些你还不能得到的答案，因为你还不能在生活里体验到它们，一切都要亲自生活“ ——-里尔克，《给青年人的十封信》 还是从科研的功利性出发，对理工科学生来说，不一定非得以自己的大学专业谋生，虽然这有点站着说话不腰疼的感觉，但是如果觉得工作受限，或者有自己想法的话，不妨放下这种执念，虽然知识是财富，但是知识不等于金钱，这跟自己以后能赚大钱是没有必然的联系的（但是理工类学生的工资好像确实高点，而且学历和工资基本正相关）。这让我想起毕导公众号的一篇文章，叫理工科的硬核浪漫，他在里面说我们在科研，学习的过程中，更多的是训练了我们的一种独特的理工科思维，虽然有时候用这种思维看待问题会让人觉得有点蠢萌和直男，但更多的是会让我们发现生活中的闪光的地方，让我们不自觉地对自己的生活认真起来。 因此在这里我想给大家传递一种观点：理工到现在已经成为我的思维方式，也许你觉得理工科男博士这个群体，他们秃头，他们穿衣很差，他们收入很低，有很多的缺点。但事实上，当他们把这种思维用在生活当中之后，在我们同样平凡无趣的生活当中，理工男也许能看到更多闪光的地方，看到更多精彩好玩的地方，他还可以把这个分享给你。 之前有人问过，毕导你是清华大学的博士生，你从清华大学博士生从科研转向去做自媒体这件事，你觉得可惜不可惜？ 我觉得一点不可惜。你在从事这个专业之后，就一条路走到黑，一直从事这一个专业里面的事情，我反而觉得有点可惜。因为你把自己思维局限住了。所以到现在为止，我给自己一点要求是什么呢？我希望自己好好学习理工科知识，把自己的思维学到一个高度，达到一定的高度之后，怀揣着理工科的理想，去到我感兴趣的领域。 写到这里，感觉也快差不多了，抱歉写得比较杂乱，似乎都是在说一些空洞而无味的东西。主要是最近一年自己身上确实发生了一些起起伏伏的事情，积压了太久的情绪和想法在沉淀了一段时间之后被知乎上的那个回答给带了出来，因此就想写点什么了。不过好久没写东西了，突然感觉想完全表达自己的想法变得费劲起来，以后还得多多练习才行。估计一年之后回过头来重新看这篇文章可能会觉得有点傻吧。&gt;_&lt;||| 最后总结一下： 不管现在状况如何，先找到自己喜欢的，或者认为值得自己做的，坚持做下去，并且keep working 停止抱怨和自怨自艾，少说多做，大多数的不如意来自于自身的缺陷 多阅读，多思考，多交流，尽量少看一些没有营养的东西，多多培养不同的兴趣 学会“输出”，不能只是不停地看，否则很容易忘记，对那些有用的，有趣的，通过费曼学习法让其在脑子里变得更加深刻 坚持写作，保持创作的热忱 主动一点，遇到比较厉害的前辈多多请教，不怕丢脸，学习优秀的人的习惯和思维是一种又快又好的进步方式]]></content>
      <categories>
        <category>随笔杂谈</category>
      </categories>
      <tags>
        <tag>reflections</tag>
        <tag>thoughts</tag>
        <tag>feelings</tag>
        <tag>emotions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FutureMapping by A.J.Davison]]></title>
    <url>%2F2019%2F04%2F16%2FFutureMapping-by-A-J-Davison%2F</url>
    <content type="text"><![CDATA[About IML的A.J.Davison教授是visual SLAM领域里的奠基人之一，近期抽空看了他在推特上置顶的有关SLAM未来的一些思考的论文： FutureMapping: The Computational Structure of Spatial AI Systems。对于做SLAM的同学，感觉这篇论文还是值得一看的，这里有个博主将该论文翻成了中文。 实际上，通篇读下来我的感受是并没有发现Davison提出了比较吸引人眼球的见解，不过有不少亮点，也让我加深了对SLAM这个东西的理解。虽然他对未来visual SLAM的功能性估计也和综述Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age 里描述得差不多，只不过后者说的比较“大而空”，都是一些常见的想法，什么动态啦，语义融合啦，模仿生物视觉啦，地图表征与更新等等，但是在Davison的这篇论文里，他以一个机器人学者的角度出发，试图从硬件和软件这两个方面去思考，未来的需求下机器人应该如何完成视觉任务，硬件应该如何发展去支持算法有效的计算，以及整个系统该有怎样的结构，才能使得机器人更好地在不同的场景下，甚至是大场景中完成不同的任务。 作为一个入门SLAM不算太久的工科学生来说，虽然不少技术知识还未掌握，但是偶尔看看这样的文章和思考也不是未尝不可，至少能激发我思考什么样的东西是值得做的，哪一个技术是未来需求的？ 不过有一个疑问是一直存在我心中的，大家都谈到了未来的visual SLAM会结合机器学习或者模仿生物视觉机制和大脑存储记忆的机制，这个我是赞同的，但是对于三维地图而言，是否是有必要去重建的，除了它们在AR上的一些应用？大家都说想要融合语义标签，物体分类与识别融合到三维图里面，有的人可能还想让地图进行实时更新，容纳动态物体（至少我曾经是这样想的），但是这样的做的目的和意义到底是什么？单纯说目的是为了让机器进一步理解环境是无法让我满足的，所以我也试图在寻找和思考这个问题的答案。遗憾的是，Davison在这篇文章里面也没有提到此类问题，不过也没有夸大三维地图的作用，而是强调图模型的作用，这一点我是赞同的。也就是说，类似人一样，我们利用视觉和计算完成任务时，”graph”这个东西是肯定发挥了很大的作用，但是却没必要“事无巨细”的记下来，我们大多是提取重要的特征，压缩下来，然后进行推断，从而得出各种预测和结论，而且事后也可以在脑海中回忆重建出场景的三维模型，此外，这些压缩信息也会随时间及时进行更迭，经常重访的则会记得牢固一些，调取起来也很快，那些不常去的可能就会进一步压缩或者删除了。 Davison在论文是这样描述地图表征的利用形式的： In real-time, the system must maintain and update a world model, with geometric and semantic information, and estimate its position within that model, from primarily or only measurements from its on-board sensors. The system should provide a wide variety of taskuseful information about ‘what’ is ‘where’ in the scene. Ideally, it will provide a full semantic level model of the identities, positions, shapes and motion of all of the objects and other entities in the surroundings. The representation of the world model will be close to metric, at least locally, to enable rapid reasoning about arbitrary predictions and measurements of interest to an AI or IA system. It will probably retain a maximum quality representation of geometry and semantics only in a focused manner; most obviously for the part of the scene currently observed and relevant for near-future interaction. The rest of the model will be stored at a hierarchy of residual quality levels, which can be rapidly upgraded when revisited. The system will be generally alert, in the sense that every incoming piece of visual data is checked against a forward predictive scene model: for tracking, and for detecting changes in the environment and independent motion. The system will be able to respond to changes in its environment. 所以我的意思是，在进行视觉任务时，重建三维地图应该不是必要的，至少在实际任务上目前可能起不到很大的作用，可能需要的是一种更加简洁凝练的图表征模式，这种模式更适合机器去认识环境，去进行编码，解码，计算，存储以及维护，而不是像人一样以为这样看到的是环境的理解方式，毕竟我们看到的是已经经过大脑处理的“人机交互结果”，并不是最核心的表征方式。但是大家为什么现在都比较热衷与做3D视觉，3D重建，我想可能是计算机视觉的一个难题吧，毕竟计算机图形学还是很有魅力的，毕竟未来的应用谁也说不准，只希望最后不要让这些技术让人类迷失在虚拟世界里。。。在这个方面，还得多一些产品层面的思考。 以上只是我的一点不成熟的想法，还需要多去阅读思考和交流。 Content这篇的文章的摘要如下： We discuss and predict the evolution of Simultaneous Localization and Mapping (SLAM) into a general geometric and semantic ‘Spatial AI’ perception capability for intelligent embodied devices. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or consumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments. 可以看到Davison教授关注的是general geometric and semantic ‘Spatial AI’ perception capability for intelligent embodied devices。首先介绍了Spatial AI system的相关概念，the goal of a Spatial AI system is not abstract scene understanding, but continuously to capture the right information, and to build the right representations, to enable real-time interpretation and action. 然后分别从算法层面和硬件层面去探讨，什么样的元素应该具备，什么样的结构是合理的，什么样的计算方式和维护更新方式可能被采用，以及从被动的分析到主动的分析预测的思考（感觉这才是有点智能的味道），最后还批判现在的计算机视觉研究者都热衷于刷点，而不去思考架构本身的问题，这个吐槽很准了，毕竟听说今年CVPR2019刷点，刷速率的文章接受率都显著下降了，我们读者都疲劳了，更何况评委。当然教授最后吐槽的重点是为了思考Benchmark对visual SLAM的意义，因为visual SLAM是一个实践性很强的系统，是为了解决实际机器人问题而生的，因此在实际的实时实验中效果好才是真的好，一味地去比较各个指标没有太大的意义，而且也很难比较，指标又很多，比如最后文末列的： • Local pose accuracy in newly explored ares (visual odometry drift rate).• Long term metric pose repeatability in well mapped areas.• Tracking robustness percentage.• Relocalisation robustness percentage.• SLAM system latency.• Dense distance prediction accuracy at every pixel.• Object segmentation accuracy.• Object classification accuracy.• AR pixel registration accuracy.• Scene change detection accuracy.• Power usage.• Data movement (bits×millimetres). 文中的具体内容不再一一讲了，这里主要讲几个文章中让我感兴趣的点。 ML 或者 DL能为Spatial AI system做什么传统机器学习算法和深度学习中的神经网络擅长做分类和回归，它们在对图像的特征学习上有着得天独厚的优势。近些年也有好多工作是利用CNN和RNN，以及unsupervised learning等深度学习的方法来进行位姿估计和深度估计，也取得了不错的效果。不过有些人认为深度学习在已经研究差不多的3D Geometry上并没有什么意义，况且数学模型我们都知道，没必要去及蹭热度利用深度学习来做，相反那些现有的算法无法处理的图像问题，比如鲁棒性好的特征点提取，光照的变化，纹理的单一，场景的识别以及运动模糊等可以尝试利用深度学习隐式地解决。此外，深度学习在object detection，semantic segmentation等都有很好的成果，可以进行应用。 These are learning architectures which use the designer’s knowledge of the structure of the underlying estimation problem to increase what can be gained from training data, and can be seen as hybrids between pure black box learning and hand-built estimation. … Why should a neural network have to use some of its capacity to learn a well understood and modelled concept like 3D geometry? Instead it can focus its learning on the harder to capture factors such as correspondence of surfaces under varying lighting. These insights support our first hypothesis that future Spatial AI systems will have recognisable and general map-ping capability which builds a close to metric 3D model. This ‘SLAM’ element may either be embedded within a specially architected neural network, or be more explicitly separated from machine learning in another module which stores and updates map properties using standard estimation methods (as in SemanticFusion for instance). In both cases, there should be an identifiable region of memory which contains a representation of space with some recognisable geometric structure. 个人感觉机器学习以及现在，未来可能出现的一系列理论可能对Spatial AI system帮助最大地可能就是图像理解和环境表征方面了，另外长时间运行带来的认识融合和更新。以及压缩等，可能也会有帮助。有关这个方面的思考目前不是很深，因为我现在还没开始学习机器学习，所以对技术了解不深，但是我感觉这东西是个“万精油”，在SLAM上的应用很大程度上可能归功于设计者怎么用，用在哪里，也就是说怎么设计网络，然后通过什么去学习什么功能，而不是紧紧盯着传统的方法然后去想方设法实现它。 硬件与云端硬件方面Davison教授主要探讨了装载该Spatial AI System的嵌入式硬件应该具有什么样的结构，而且还具有匹配视觉计算特点的算力，同时还得有一定的存储能力。他肯定了分割计算，并行计算，多核心，多线程，神经形态硬件架构的需求，也列出了一些正在研究的例子，具体的内容可以参见文章内容。 实际上，硬件对算法的促进具有着决定性的作用，从Yann Lecun在2019年的ISSCC上做的报告就可以看出 ，没有良好的硬件支持，算法根本没办法进行实验验证，就很难进步。因此，硬件方面的迫切需求是现在整个智能行业的燃眉之急。 另外，Davison教授也肯定了云端的重要性。因为云端相当于一个存储中心，可以存储环境表征这样的信息，而且可以同时对环境分布中的机器人进行通信和数据传输，这对机器人在大场景中，长时间执行任务起到“预热”等辅助性作用。 Finally, when considering the evolution of the computing resources for Spatial AI, we should never forget that, cloud computing resources will continue to expand in capacity and reduce in cost. All future Spatial AI systems will likely be cloud-connected most of the time, and from their point of view the processing and memory resources of the cloud can be assumed to be close to infinite and free. What is not free is communication between an embedded device and the cloud, which can be expensive in power terms, particularly if high bandwidthdata such as video is transmitted. The other important consideration is the time delay, typically of significant fractions of a second, in sending data to the cloud for processing. The long term potential for cloud-connected Spatial AI is clearly enormous. The vision of Richard Newcombe, Director of Research Science at Oculus, is that all of these devices should communicate and collaborate to build and maintain shared a ‘machine perception map’ of the whole world. The master map will be stored in the cloud, and individual devices will interact with parts of it as needed. Ashared map can be much more complete and detailed than that build by any individual device, due to both sensor coverage and the computing resources which can be put into it. A particularly interesting point is that the Spatial AI work which each individual device needs to do in this setup can in theory be much reduced. Having estimated its location within the global map, it would not need to densely mapor semantically label its surrounding if other devices had already done that job and their maps could simply be projected into its field of view. It would only need to be alert for changes, and in turn play its part in returning updates. GraphsDavison在论文的第5节讲了很多有关”Graphs”的东西，我们知道，现在的visual SLAM框架都开始逐渐认同将图优化作为减小估计误差的手段要比滤波器估计的效果好得多，因为”graphs”本身就是视觉的一种表征方式，而且在约束上具有非线性性，能更好地模拟现实情况。 在SLAM方面，教授主要提出了geometry和local appearance两者是否可以联系起来的观点： We have not yet discovered a suitable feature representation which describes both local appearance and geometry in such a way that a relatively sparse feature set can provide a dense scene prediction. We believe that learned features arising from ongoing geometric deep learning research will provide the path towards this. Some very promising recent work which we believe is heading in the right direction Bloesch et al.’s CodeSLAM. This method uses an image-conditioned autoencoder to discover an optimisable code with a small number of parameters which describes the dense depth map at a keyframe. In SLAM, camera poses and these depth codes can be jointly optimised to estimate dense scene shape which is represented by relatively few parameters. In this method, the scene geometryis stilllocked to keyframes, but we believe that the next step is to discover learned codes which can efficiently represent both appearance and 3D shape, and to make these the elements of a graph SLAM system. Davison教授另外一个观点是该实时系统中的”Computation Graph”，并且再次提出了”object-oriented SLAM”的概念。 How can we get back to this ‘object-oriented SLAM’ capability in the much more general sense, where a wide range of object classes of varying style and details could be dealt with? As discussed before, SLAM maps of the future will probably be represented as multi-scale graphs of learned features which describe geometry, appearance and semantics. Some of these features will represent immediately recognised whole objects as in SLAM++. Others will represent generic semantic elements or geometric parts (planes, corners, legs, lids?) which are part of objects either already known or yet to be discovered. Others may approach surfels or other standard dense geometric elements in representing the geometry and appearance of pieces whose semantic identity is not yet known, or does not need to be known. Recognition, and unsupervised learning, will operate on these feature maps to cluster, label and segment them. The machine learning methods which do this job will themselves improve by selfsupervision during the SLAM process, taking advantage of dense SLAM’s properties as a “correspondence engine”. 这个图基本上等于是把系统算法的框架给列出来了，可以看出，核心还是”定位“（camera state）和”建图“（world model）。只不过里面加入了深度学习来提高系统的性能。 Most computation relates to the world model, which is a persistent, continuously changing and improving data store where the system’s generative representation of the important elements of the scene is held; and the input camera data stream. Some of the main computational elements are: • Empirical labelling of images to features (e.g. via a CNN).• Rendering: getting a dense prediction from the world map to image space.• Tracking: aligning a prediction with new image data, including finding outliers and detecting independent movement.• Fusion: fusing updated geometry and labels back into the map.• Map consolidation: fusing elements into objects, or imposing smoothing, regularisation.• Relocalisation/loop closure detection: detecting self similarity in the map.• Map consistency optimization, for instance after confirming a loop closure.• Self-supervised learning of correspondence information from the running system. 这些都是当前比较主流的观点，而且里面涉及的知识体系比较庞大，因此大部分都是先针对一个来展开研究，不过我觉得要想对其进行突破，最大的，也是最有挑战性的问题应该就是世界模型表征问题了，对于机器来讲，这个应当是个非常简洁和高效的表征方式，同时也易于存储，调用，翻译和编码。 地图的处理，表示，预测和更新其实这个部分前面已经提及了不少了，而Davison教授也单独在第6节讲了这个问题，对里面的几个关键问题进行了总结和思考：一个是硬件支持，一个是地图存储，一个是实时回环。 地图表征方面： There is a large degreeof choice possible in the representation of a 3D scene, but as explained in Section 5.1.2, we envision maps which consist of graphs of learned features, which are linked in multi-scale patterns relating to camera motion. These features must represent geometry as well as appearance, such that they can be used to render a dense predicted view of the scene from a novel viewpoint. It may be that they do not need to represent full photometric appearance, and that a somewhat abstracted view is sufficient as long as it captures geometric detail. 地图存储与维持方面（更新）： Within the main processor, a major area will be devoted to storing this map, in a manner which is distributed around potentially a large number of individual cores which are strongly connected in a topology to mirror the map graph topology. In SLAM, of course the map is defined and grown dynamically, so the graph within the processor must either be able to change dynamically as well, or must be initially defined with a large unused capacity which is filled as SLAM progresses. Importantly, a significant portion of the processing associated with large scale SLAM can be built directly into this graph. This is mainly the sort of ‘maintenance’ processing via which the map optimises and refines itself; including: • Feature clustering; object segmentation and identification.• Loop closure detection.• Loop closure optimization.• Map regularisation (smoothing).• Unsupervised clustering to discover new semantic categories. With time, data and processing, a map which starts off as dense geometry and low level features can be refined towards an efficient object level map. Some of these operations will run with very high parallelism, as each part of the map is refined on its local core(s), while other operations such as loop closure detection and optimisation will require message passing around large parts of the graph. Still, importantly, they can take place in a manner which is internal to the map store itself. 实时回环方面，教授提出了地图的存储与场景识别方面的一些难点，即“翻译”和“融合”之间协作的问题。因为相机的运动会对地图进行实时更新，该模块的重心在于维持，而不是比较数据，因此可能会对场景识别造成一定的影响。教授在这里提出了可以利用节点（node），海马体结构，以及小世界拓扑结构地图等来解决。我想可能是模仿人的记忆功能。 Instead, a possible solution is to define special interface nodes which sit between the real-time loop block and the map store. These are nodes focused on communication, which are connected to the relevant components of real-time loop processing and then also to various sites in the map graph, and may have some analogue in the hippocampus of mammal brains. If the map store is organised such that it has a ‘small world’ topology, meaning that any part is joined to any other part by a small number of edge hops, then the interface nodes should be able to access (copy) any relevant map data in a small number of operations and serve them up to the real-time loop. Each node in the map store will also have to play some part in this communication procedure, where it will sometimes beused as part of the route for copying map databackwards and forwards. 注意力机制，主动视觉这里的主动视觉是指机器人主动移动相机去采集和任务有关的信息，是一种“top-down”的执行方式。 The active vision paradigm advocates using sensing resources, such as the directions that a camera points towards or the processing applied to its feed, in a way which is controlled depending on the task at hand and prior knowledge available. This ‘top-down’ approach contrasts with ‘bottom-up’, blanket processing of all of the data received from a camera. Davison提到人的视觉机制是“bottom-up”和”top-down“并存的，而且现在的”bottom-up“的图像处理机制也有很不错的发展，而且在处理很多问题上都很有效，因此两者的结合应当也是一种必然，毕竟”top-down“的执行是需要信息和预测来作为先决条件的。主动视觉在系统的实时性上会有很大的帮助，因为减少了信息和数据处理的冗余度，只分析我需要的数据，因此会大大减小对算力的需求。 It is important that when assessing the relative efficiency of bottom-up versus top-down vision, we take into account not just processing operations but also data transfer, and to what extent memory locality and graph-based computation can be achieved by each alternative. This will make certain possibilities in model-based prediction unattractive, such as searching large global maps or databases. The amazing advances in CNN-based vision means that we have to raise our sights when it comes to what we can expect from pure bottom-up image processing. But also, graph processing will surely permit new ways to store and retrieve model data efficiently, and will favour keeping and updating world models (such as graph SLAM maps) which have data locality. 总结Davison这篇论文提出的思考和观点还是比较符合现在的主流认知的，而且在技术上，教授也给出了一些比较具体的方案。不过这个目标比较长远，目前其中的小环节可能都还没处理好，而且还需要硬件铺路，因此想要彻底实现难度还是有点大的。总之，这样的系统我估计未来都是模块化的，分布式的，并且是多协作的，以任务为中心的，毕竟现在的AI还没有大的突破，因此想要实现像人类那样的视觉机制还比较困难，得需要很多个学科的大佬共同研究努力才行。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>computer vision</tag>
        <tag>SLAM</tag>
        <tag>mapping</tag>
        <tag>AI system</tag>
        <tag>hardware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[visual SLAM by Gaoxiang(3)]]></title>
    <url>%2F2019%2F03%2F26%2Fvisual-SLAM-by-Gaoxiang-3%2F</url>
    <content type="text"><![CDATA[​ 本次课程主要研究李群和李代数(Lie Group, Lie Algebra)，主要的目的是为了能够相机得旋转和平移进行微调。因为相机的运动估计可能不准确，而无法对旋转矩阵加上微小量之后依然是旋转矩阵（旋转矩阵无法定义加法，如果用四元数，必须是单位四元数，那么也无法定义加法）。李群李代数与后面的优化，流形都会有很大的联系。 在视觉SLAM中，相机的位姿是未知的，而我们需要解决什么样的相机位姿最符合当前观测数据这样的问题。一种典型的方式是把其构建成一个优化问题，求解最优的$R$和$t$，使得误差最小化。 由于旋转矩阵自身带有约束，即必须正交且行列式为1，因此作为优化变量会引入额外的约束，使得优化变得困难。而通过李群李代数的转换关系，可以顺利求导，把位姿估计变成无约束的优化问题。 群群(Group)是一种集合加上一种运算的代数结构，满足封闭性，结合律，幺元，逆。其中幺元可以认为是单位元，就是与其他元素作用不改变这个元素，逆是元素和和它的逆进行运算后得到了幺元。 三维旋转矩阵构成了三维正交群(special orthogonal group) SO(3) = \left \{ R \in {\mathbb R}^{3 \times 3} | RR^{T} =I, \det (R)=1 \right \}三维变换矩阵构成了特殊欧式群(special euclidean group) SE(3) = \left \{ T= \begin{bmatrix} R & t\\ O^{T} & 1 \end{bmatrix} \in {\mathbb R}^{4 \times 4} | R \in SO(3), t \in {\mathbb R}^{3} \right \}旋转矩阵集合与矩阵乘法构成群，变换矩阵集合与矩阵乘法也构成了群，因此称它们为旋转矩阵群和变换矩阵群。 群结构保证了在群上的运算具有良好的性质。 李群与李代数李群具有连续（光滑）性质的群； 既是群也是流形； 直观上看，一个刚体能够连续地在空间中运动，因此$SO(3)$和$SE(3)$都是李群，然而，它们都没有定义加法，所以很难进行取极限和求导等操作； 李代数与李代数对应的一种结构，位于向量空间（李群单位元处的正切空间）$\mathfrak so(3)$，$\mathfrak se(3)$ 从旋转矩阵引出旋转正交群的李代数： 对于相机的连续运动，旋转矩阵也随时间变化，则有： \begin{align*} & R(t)R(t)^{T} = I \\ & \dot{R}(t)R(t)^{T}+R(t)\dot{R}(t)^{T} = 0 \quad \quad 对时间求导\\ & \dot{R}(t)R(t)^{T} = - (\dot{R}(t)R(t)^{T})^{T} \quad \quad 反对称矩阵\\ & 记\dot{R}(t)R(t)^{T} = \phi (t)^{\wedge} \implies \dot{R}(t) = \phi (t)^{\wedge} R(t) \end{align*}符号$\wedge$看作是反对称矩阵的符号，在这里是指将向量$\phi$变成了反对称矩阵，这是由叉乘引申而来，在第二讲有提过。反过来符号$\vee$代表反对称矩阵到向量的变换。 上面的式子表示，对旋转矩阵求导，就是在其左侧乘以一个$\phi (t)$，类似于指数函数的求导。 下面进行进一步地近似，假设在单位元附近，$t_{0}=0, R(0)=I$，则： R(t) \approx R(t_{0}) + \dot {R}(t_{0})(t-t_{0}) = I + \phi (t_{0})^{\wedge} (t) \quad \quad 将R在t_{0}进行泰勒展开，并忽略二次及以上高阶项进一步假设，在$t_{0}$附近，$\phi$不变，则$\dot {R} (t) = \phi (t_{0}) ^{\wedge} R(t) = \phi _{0} ^{\wedge} R(t)$，再根据初值条件，得出： R(t) = \exp( \phi _{0} ^{\wedge} t)在泰勒展开那一步可以看出，$\phi$反映的是一阶导数的性质，位于旋转正交群的正切空间上（tangent space，切平面上）。 上述证明提供了一种思路，可能不太严谨。实际上可以证明最后得出的式子在任意时间都适用，且该关系称为指数映射（exponential map），$\phi$称为$SO(3)$对应的李代数$\mathfrak so(3)$。 李群是高维空间的低维曲面，或者说低维流形，在流形原点附近的切空间上的任意一个点，是李代数，可以通过指数映射映回李群上。李代数描述了李群单位元附近的正切空间的性质。 \begin{align*} & 李代数由一个集合\mathbb V，一个数域\mathbb F，和一个二元运算[,]（李括号，直观上说表示了两个元素的差异）组成。如果满足下面的四条性质，称\\ & (\mathbb V, \mathbb F, [,])为一个李代数，记作 \mathfrak g。\\ &1. 封闭性 \quad \quad \forall {\bf X, Y}\in \mathbb V, [{\bf X,Y}] \in \mathbb V\\ &2. 双线性 \quad \quad \forall {\bf X, Y}\in \mathbb V, a,b \in \mathbb F,有： [a {\bf X} + b {\bf Y}, {\bf Z}] =a[{\bf X,Z}]+b[{\bf Y, Z}],[{\bf Z},a{\bf X}+b{\bf Y}] =a[{\bf Z,X}]+b[{\bf Z,Y}] \\ &3. 自反性 \quad \quad \forall {\bf X} \in \mathbb V, [{\bf X,X}]={\bf 0}\\ &4. 雅可比等价 \quad \quad \forall {\bf X,Y,Z} \in \mathbb V,[{\bf X},[{\bf Y,Z}]]+[{\bf Z},[{\bf Y,X}]]+[{\bf Y},[{\bf Z,X}]]= {\bf 0} \end{align*}李代数$\mathfrak so(3)$可以看成是三维空间向量和叉积运算构成的，$so(3)=\left \{ \phi \in \mathbb R^{3}, \Phi = \phi ^{\wedge} \in \mathbb R^{3 \times 3} \right \}$，其中： \Phi = \phi ^{\wedge} = \begin{bmatrix} 0 & - \phi_{3} & \phi _{2}\\ \phi _{3} & 0 & -\phi _{1}\\ -\phi _{2} & \phi _{1} & 0 \end{bmatrix} \in \mathbb R^{3 \times 3}李括号$[\phi_{1},\phi_{2}] = (\Phi_{1} \Phi_{2} - \Phi_{2} \Phi_{1})^{\vee}$,容易验证此李括号满足上述四条性质。 对于变换矩阵的特殊欧式群$SE(3)$，也有对应的李代数$\mathfrak se(3)$(6维的向量) \mathfrak se(3) = \left \{ \xi = \begin{bmatrix} \rho \\ \phi \end{bmatrix} \in \mathbb R^{6}, \rho \in \mathbb R^{3}, \phi \in \mathfrak se(3), \xi ^{\wedge} = \begin{bmatrix} \phi ^{\wedge} & \rho \\ {\bf 0}^{T} & 0 \end{bmatrix} \in \mathbb R^{4 \times 4} \right \}此时还是以用符号$\wedge$来表示向量到矩阵的变换，只不过不再是限制于反对称矩阵。 \begin{align*} & 设变换矩阵g(t)= \begin{bmatrix} R & \alpha \\ {\bf 0}^{T} & 1 \end{bmatrix} ，则g(t)^{-1} = \begin{bmatrix} R^{T} & -R^{T}\alpha \\ {\bf 0}^{T} & 1 \end{bmatrix}\\ &有\dot{g}(t)g(t)^{-1}= \begin{bmatrix} \dot{R}R^{T} & \alpha- \dot{R}R^{T}\alpha \\ {\bf 0}^{T} & 0 \end{bmatrix} = \begin{bmatrix} \omega^{\wedge} & v\\ {\bf 0}^{T} &0 \end{bmatrix} ，其中，\omega^{\wedge} \in \mathbb R^{3 \times 3},v \in \mathbb R^{3},记 \xi^{\wedge}=\dot{g}(t)g(t)^{-1}\\ & (\xi^{\wedge})^{\vee}= \begin{bmatrix} v \\ \omega \end{bmatrix} \in \mathbb R^{6} \\ &\dot{g}(t)=(\dot{g}(t)g(t)^{-1})g(t)=\xi^{\wedge}g(t),则g(t)=\exp(\xi^{\wedge}),假设g(0)=I \end{align*}李括号$[\xi _{1}, \xi _{2}] = (\xi _{1} ^{\wedge} \xi_{2} ^{\wedge} - \xi _{2} ^{\wedge} \xi_{1} ^{\wedge}) ^{\vee}$ 指数映射和对数映射指数映射反映了李代数到李群的关系，对于旋转矩阵$R$，有$R=\exp (\phi ^{\wedge})=\sum_{n=0}^{\infty} \frac{1}{n!}(\phi ^{\wedge})^{n}$ 为了研究的方便，先将$\phi$写成旋转向量的形式，即设$\phi = \theta \vec{a}$,其中$\vec{a}$是单位向量，而且具有以下性质 \begin{align*} &1.\vec{a}^{\wedge} \vec{a}^{\wedge} = \vec {a} \vec{a}^{T} - I \\ &2.\vec {a}^{\wedge} \vec {a}^{\wedge} \vec {a}^{\wedge} = -\vec {a}^{\wedge} \end{align*}下面利用上述性质对$\exp (\phi)^{\wedge}$进行Taylor展开 \begin{align*} \exp (\phi)^{\wedge} & =\exp (\theta \vec{a}) = \sum _{n=0}^{\infty}\frac{1}{n!}(\theta \vec{a}^{\wedge})^{n}\\ & = I+\theta \vec{a}^{\wedge}+\frac{1}{2!}\theta ^{2} \vec{a}^{\wedge}\vec{a}^{\wedge}+\frac{1}{3!} \theta ^{3} \vec{a}^{\wedge}\vec{a}^{\wedge}\vec{a}^{\wedge}+\frac{1}{4!} \theta ^{4} \vec{a}^{\wedge}\vec{a}^{\wedge}\vec{a}^{\wedge}\vec{a}^{\wedge}+ \cdots\\ & = \vec{a}\vec{a}^{T}-\vec{a}^{\wedge}\vec{a}^{\wedge}+\theta \vec{a}^{\wedge}+\frac{1}{2!}\theta ^{2}\vec{a}^{\wedge}\vec{a}^{\wedge}-\frac{1}{3!} \theta^{3}\vec{a}^{\wedge}-\frac{1}{4!}\theta ^{4}\vec{a}^{\wedge}\vec{a}^{\wedge}+\cdots \\ & = \vec{a}\vec{a}^{T}+(\theta -\frac{1}{3!}\theta^{3}+\frac{1}{5!}\theta^{5}-\cdots)\vec{a}^{\wedge}-(1-\frac{1}{2!}\theta^{2}+\frac{1}{4!}\theta^{4}-\cdots)\vec{a}^{\wedge}\vec{a}^{\wedge}\\ & = \vec{a}^{\wedge}\vec{a}^{\wedge}+I+\sin \theta \vec{a}^{\wedge}-\cos \theta \vec{a}^{\wedge}\vec{a}^{\wedge}\\ & = (1-\cos \theta)\vec{a}^{\wedge}\vec{a}^{\wedge}+I+\sin \theta \vec{a}^{\wedge}\\ & = \cos \theta I+(1-\cos \theta)\vec{a}\vec{a}^{T}+\sin \theta \vec{a}^{\wedge} \end{align*}这进一步说明了李代数$\mathfrak so(3)$的物理意义确实就是旋转向量。 反之，给定旋转矩阵亦可以求出对应的李代数，即对数映射$\phi = \ln(R)^{\vee}$。不过实际情况下可以通过旋转矩阵到向量的公式来得出李代数。 同理，可以得到$\mathfrak se(3)$到$SE(3)$的指数映射（具体推导过程会在作业中展示） \begin{align*} \exp(\xi^{\wedge}) & = \begin{bmatrix} \sum _{n=0}^{\infty}\frac{1}{n!}(\phi^{\wedge})^{n} & \sum _{n=0}^{\infty} \frac{1}{(n+1)!}(\phi ^{\wedge})^{n} \rho \\ {\bf 0}^{T} & 1 \end{bmatrix} \\ & \triangleq \begin{bmatrix} R & J\rho \\ {\bf 0}^{T} & 1 \end{bmatrix} \\ & = T \end{align*}其中$J$为$SE(3)$的雅可比矩阵，$J=\frac{\sin \theta}{\theta} I + (1-\frac{\sin \theta} {\theta}) \vec{a} \vec{a}^{T} + \frac{1-\cos \theta} {\theta} \vec{a}^{\wedge}$ 注意，这里的平移部分与变换矩阵的平移部分不完全相同，可以看出，指数映射会对李代数中的平移部分进行线性变换后才得到了真正的平移部分。 李代数求导与扰动模型前面说过，视觉SLAM应用李群李代数的最初目的就是为了对相机的位姿进行优化。因为相机在观测世界的时候，会不可避免地引入噪声，而我们优化的目的就是会取N个观测，然后对其进行误差最小化，得到一个在这么多N个观测的过程中最优的变换关系。因此，针对优化（一般是最小化问题）问题，我们常用的手段就是求导，此时李代数的功能就体现出来了，因为李代数具有良好的加法运算，可以进行无约束优化问题分析。 不过问题是，李代数上的加法并不会对应李群上的乘法，即$\exp(\phi_{1}^{\wedge}) \exp(\phi_{2}^{\wedge}) \neq \exp ((\phi_{1}+\phi_{2})^{\wedge})$ 该关系由BCH公式（Baker-Campbell-Hausdorff）给出： \ln (\exp (A) \exp(B)) = A +B + \frac{1}{2} [A,B] + \frac{1}{12}[A,[A,B]] - \frac{1}{12}[B.[A,B]]+\cdots如果其中一个量为小量，则有下列的近似表达关系： \ln (\exp (\phi_{1}^{\wedge}) \exp (\phi_{2}^{\wedge}))^{\vee} \approx \begin{cases} J_{l} (\phi _{2})^{-1} \phi_{1}+\phi_{2} \quad \quad \text{if $\phi_{1}$is small}\\ J_{r}(\phi_{1})^{-1} \phi_{2} + \phi_{1} \quad \quad \text{if $\phi_{2}$is small} \end{cases}其中： \begin{align*} & J_{l}=J=\frac{\sin \theta}{\theta} I + (1-\frac{\sin \theta} {\theta}) \vec{a} \vec{a}^{T} + \frac{1-\cos \theta} {\theta} \vec{a}^{\wedge} \quad左雅可比\\ & J_{l}^{-1} = \frac{\theta}{2}\cot \frac{\theta}{2}I+(1-\frac{\theta}{2}\cot \frac{\theta}{2})\vec{a}\vec{a}^{T}-\frac{\theta}{2}\vec{a}^{\wedge}\\ & J_{r}(\phi) = J_{l}(-\phi) \quad 右雅可比 \end{align*}一般来说，利用$T_{cw}$时会进行左乘，利用$T_{wc}$时会进行右乘，以左乘为例，直观的写法是 \begin{align*} & \exp (\Delta \phi ^{\wedge}) \exp(\phi ^{\wedge}) = \exp ((J_{l}(\phi)^{-1}\Delta \phi +\phi)^{\wedge}) \quad \quad 李群上的微小量乘法，李代数上的加法相差雅可比矩阵的逆\\ & \exp((\phi + \Delta \phi)^{\wedge})=\exp ((J_{l}\Delta \phi)^{\wedge})\exp(\phi^{\wedge})=\exp(\phi^{\wedge}) \exp((J_{r}\Delta \phi)^{\wedge}) \quad \quad 李代数上的微小量加法，李群上要乘上雅可比矩阵 \end{align*}对于$SE(3)$和$\mathfrak se(3)$，关系要复杂一些（雅可比矩阵较为复杂，是个$6 \times 6$矩阵，但在实际计算中不用到该雅可比）： \begin{align*} & \exp(\Delta \xi ^{\wedge}) \exp(\xi ^{\wedge}) \approx \exp ((\mathcal J_{l}^{-1}\Delta \xi +\xi)^{\wedge})\\ & \exp(\xi ^{\wedge}) \exp (\Delta \xi^{\wedge}) \approx \exp ((\mathcal J_{r}^{-1} \Delta \xi+\xi)^{\wedge}) \end{align*}有了上述公示后，开始对旋转矩阵进行求导，这里有两种方法，一种是导数定义求导，一种是通过扰动模型进行求导，实际中，扰动模型因为形式更加简单，因此采用的更多，先不严谨地记旋转后的点关于旋转矩阵的导数的求导式为 $\frac{\partial (Rp)}{\partial R}$ 导数模型： \begin{align*} \frac{\partial(\exp (\phi^{\wedge})p)}{\partial \phi} & = \lim_{\delta \phi \to 0} \frac{\exp((\phi+\delta \phi)^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi} \\ & = \lim_{\delta \phi \to 0} \frac{\exp ((J_{l}\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp (\phi^{\wedge})p}{\delta \phi}\\ & \approx \lim_{\delta \phi \to 0} \frac{(I+(J_{l}\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi} \\ & = \lim_{\delta \phi \to 0} \frac{(J_{l}\delta \phi)^{\wedge}\exp(\phi^{\wedge})p}{\delta \phi}\\ & = \lim_{\delta \phi \to 0} \frac{-(\exp(\phi^{\wedge})p)^{\wedge} J_{l}\delta \phi}{\delta \phi} \\ & = -(Rp)^{\wedge}J_{l} \end{align*}扰动模型（左乘微小量）： \begin{align*} \frac{\partial(Rp)}{\partial R} & = \lim_{\delta \phi \to 0} \frac{\exp((\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi}\\ & \approx \lim_{\delta \phi \to 0} \frac{(I+(\delta \phi)^{\wedge})\exp(\phi^{\wedge})p-\exp(\phi^{\wedge})p}{\delta \phi}\\ & = \lim_{\delta \phi \to 0} \frac{(\delta \phi)^{\wedge}Rp}{\delta \phi} = -(Rp)^{\wedge} \end{align*}同理可得，$SE(3)$上的扰动模型（左乘微小量）为： \frac{\partial (Tp)}{\partial \delta \xi}= \begin{bmatrix} I & -(Rp+t)^{\wedge}\\ {\bf 0}^{T} & {\bf 0}^{T} \end{bmatrix} \triangleq (Tp)^{\odot}相似变换对于单目视觉，由于存在尺度不确定性，因此不能使用$SE(3)$来表达位姿变化，而是利用相似变换群$Sim(3)$，也就是说要加一个尺度因子$s$，这个尺度因子会同时作用在变换的点$p$上，对其进行缩放，也就是在相机坐标系下进行了一次相似变换，而不是欧式变换，即：$p^{‘} = sRp+t$ Sim(3)=\left\{\left[ \begin{array}{lll}{\boldsymbol{S}=} & {\boldsymbol{s} \boldsymbol{R}} & {\boldsymbol{t}} \\ {\boldsymbol{0}^{T}} & {1}\end{array}\right] \in \mathbb{R}^{4 \times 4}\right\} \mathfrak sim(3)=\left\{\zeta | \zeta=\left[ \begin{array}{l}{\rho} \\ {\phi} \\ {\sigma}\end{array}\right] \in \mathbb{R}^{7}, \zeta^{\wedge}=\left[ \begin{array}{cc}{\sigma I+\phi^{\wedge}} & {\rho} \\ {0^{T}} & {0}\end{array}\right] \in \mathbb{R}^{4 \times 4}\right\} \exp \left(\zeta^{\wedge}\right)=\left[ \begin{array}{cc}{e^{\sigma} \exp \left(\phi^{\wedge}\right)} & {J_{s} \rho} \\ {0^{T}} & {1}\end{array}\right] \begin{aligned} J_{s}=& \frac{e^{\sigma}-1}{\sigma} I+\frac{\sigma e^{\sigma} \sin \theta+\left(1-e^{\sigma} \cos \theta\right) \theta}{\sigma^{2}+\theta^{2}} a^{\wedge} \\ &+\left(\frac{e^{\sigma}-1}{\sigma}-\frac{\left(e^{\sigma} \cos \theta-1\right) \sigma+\left(e^{\sigma} \sin \theta\right) \theta}{\sigma^{2}+\theta^{2}}\right) a^{\wedge} a^{\wedge} \end{aligned} s=e^{\sigma}, \boldsymbol{R}=\exp \left(\boldsymbol{\phi}^{\wedge}\right), \boldsymbol{t}=\boldsymbol{J}_{s} \boldsymbol{\rho}对于$Sim(3)$的求导，利用左扰动模型和BCH近似（这里的BCH近似与$SE(3)$公式不同）。假设点$p$经过相似变换$Sp$后，相对于$S$的导数为： \frac{\partial S p}{\partial \zeta}=\left[ \begin{array}{ccc}{\boldsymbol{I}} & {-\boldsymbol{q}^{\wedge}} & {\boldsymbol{q}} \\ {\mathbf{0}^{T}} & {\mathbf{0}^{T}} & {0}\end{array}\right]其中$q$是$Sp$的前三维向量，最后的形式应该是$4 \times 7$的雅可比矩阵。 有关相似变换群的更为详细的理解和运用，等后面进行实际应用时再说，毕竟库已经提供好了，而且推导过程也与$SE(3)$类似。 作业与实践群的性质群要满足封闭性，结合律，幺元和逆这四个性质。其中，满足前两个性质的叫半群，满足前三个性质的叫有单位元的半群，若满足了上述四个性质，还具有交换律的叫做阿贝尔群。 对于$\left \{ \mathbb Z, + \right \}$封闭性和结合律显然满足，幺元是0，逆为自身的相反数，因此是群，而且是阿贝尔群。 对于$\left \{ \mathbb N, +\right \}$，前三个性质都满足，幺元是0，但是除了0之外，其他的元素不存在逆，因此不是群，是有单位元的半群。 验证向量叉乘的李代数性质设${\bf X}=a_{1}\vec{i}+b_{1}\vec{j}+c_{1}\vec{k}, {\bf Y}=a_{2}\vec{i}+b_{2}\vec{j}+c_{2}\vec{k}, {\bf Z}=a_{3}\vec{i}+b_{3}\vec{j}+z_{3}\vec{k}$ 封闭性： [{\bf X},{\bf Y}] = {\bf X} \times {\bf Y} \in \mathbb R^{3}双线性： \begin{align*} & [a{\bf X}+b{\bf Y},{\bf Z}]=(a{\bf X}+b{\bf Y}) \times {\bf Z}=a{\bf X}\times {\bf Z}+b{\bf Y} \times {\bf Z} = a[{\bf X,Z}]+b[{\bf Y,Z}]\\ & [{\bf Z},a{\bf X}+b{\bf Y}]={\bf Z} \times (a{\bf X}+b{\bf Y})={\bf Z}\times a {\bf X}+{\bf Z} \times b{\bf Y}=a[{\bf Z,X}]+b[{\bf Z,Y}] \end{align*}自反性： [{\bf X,X}]={\bf X} \times {\bf X}={\bf 0}雅可比等价： \begin{align*} &[{\bf X},[{\bf Y,Z}]]+[{\bf Y},[{\bf Z,X}]]+[{\bf Z},[{\bf X,Y}]]\\ & ={\bf X} \times {\bf Y} \times {\bf Z}+{\bf Y} \times {\bf Z} \times {\bf X}+{\bf Z} \times {\bf X} \times {\bf Y}\\ & = {\bf X}^{\wedge} {\bf Z}^{\wedge} {\bf Y}+{\bf Y}^{\wedge} {\bf X}^{\wedge} {\bf Z}+{\bf Z}^{\wedge} {\bf Y}^{\wedge} {\bf X}\\ & = \begin{bmatrix} 0 & -c_{1} & b_{1}\\ c_{1} & 0 & -a_{1}\\ -b_{1} & a_{1} &0 \end{bmatrix} \begin{bmatrix} 0 & -c_{3} & b_{3}\\ c_{3} & 0 & -a_{3}\\ -b_{3} & a_{3} & 0 \end{bmatrix} \begin{bmatrix} a_{2}\\ b_{2}\\ c_{2} \end{bmatrix} + \begin{bmatrix} 0 & -c_{2} & b_{2}\\ c_{2} & 0 & -a_{2}\\ -b_{2} & a_{2} &0 \end{bmatrix} \begin{bmatrix} 0 & -c_{1} & b_{1}\\ c_{1} & 0 & -a_{1}\\ -b_{1} & a_{1} & 0 \end{bmatrix} \begin{bmatrix} a_{3}\\ b_{3}\\ c_{3} \end{bmatrix} + \begin{bmatrix} 0 & -c_{3} & b_{3}\\ c_{3} & 0 & -a_{3}\\ -b_{3} & a_{3} &0 \end{bmatrix} \begin{bmatrix} 0 & -c_{2} & b_{2}\\ c_{2} & 0 & -a_{2}\\ -b_{2} & a_{2} & 0 \end{bmatrix} \begin{bmatrix} a_{1}\\ b_{1}\\ c_{1} \end{bmatrix} \\ & = \begin{bmatrix} a_{3}(b_{1}b_{2}+c_{1}c_{2})-a_{2}(b_{1}b_{3}+c_{1}c_{3})\\ b_{3}(a_{1}a_{2}+c_{1}c_{2})-b_{2}(a_{1}a_{3}+c_{1}c_{3})\\ c_{3}(a_{1}a_{2}+b_{1}b_{2})-c_{2}(b_{1}b_{3}+a_{1}a_{3}) \end{bmatrix} + \begin{bmatrix} a_{1}(b_{2}b_{3}+c_{2}c_{3})-a_{3}(b_{1}b_{2}+c_{1}c_{2})\\ b_{1}(a_{2}a_{3}+c_{2}c_{3})-b_{3}(a_{1}a_{2}+c_{1}c_{2})\\ c_{1}(a_{2}a_{3}+b_{2}b_{3})-c_{3}(b_{1}b_{2}+a_{1}a_{2}) \end{bmatrix} + \begin{bmatrix} a_{2}(b_{1}b_{3}+c_{1}c_{3})-a_{1}(b_{2}b_{3}+c_{1}c_{3})\\ b_{2}(a_{1}a_{3}+c_{1}c_{3})-b_{1}(a_{2}a_{3}+c_{1}c_{3})\\ c_{2}(a_{1}a_{3}+b_{1}b_{3})-c_{1}(b_{2}b_{3}+a_{1}a_{3}) \end{bmatrix} \\ & = \begin{bmatrix} 0\\ 0\\ 0 \end{bmatrix} ={\bf 0} \end{align*}则$\mathfrak g =(\mathbb R^{3}, \mathbb R, \times)$构成李代数。 推导$SE(3)$的指数映射推导$SE(3)$指数映射部分左雅可比的形式： \begin{align*} J & \triangleq \sum_{n=0}^{\infty} \frac{1}{(n+1)!}(\phi ^{\wedge})^{n}=\sum_{n=0}^{\infty} \frac{1}{(n+1)!}(\theta a^{\wedge})^{n}\\ & =I+\frac{1}{2!}\theta a^{\wedge}+\frac{1}{3!} \theta^{2}a^{\wedge}a^{\wedge}-\frac{1}{4!}\theta^{3}a^{\wedge}a^{\wedge}a^{\wedge}+\frac{1}{5!}\theta^{4}a^{\wedge}a^{\wedge}a^{\wedge}a^{\wedge}+\cdots\\ & =aa^{T}-a^{\wedge}a^{\wedge}+\frac{1}{2!}\theta a^{\wedge}+\frac{1}{3!}\theta^{2}a^{\wedge}a^{\wedge}-\frac{1}{4!}\theta^{3}a^{\wedge}-\frac{1}{5!}\theta^{4}a^{\wedge}a^{\wedge}+\cdots \\ & = \frac{1}{\theta}\left \{ aa^{T}\theta-a^{\wedge}a^{\wedge}\theta+a^{\wedge}-a^{\wedge}+\frac{1}{2!}\theta^{2}a^{\wedge}+\frac{1}{3!}\theta^{3}a^{\wedge}a^{\wedge}-\frac{1}{4!}\theta^{4}a^{\wedge}-\frac{1}{5!}\theta^{5}a^{\wedge}a^{\wedge}+\cdots \right \} \\ & = \frac{1}{\theta} \left \{ aa^{T}\theta - a^{\wedge}a^{\wedge}(\theta-\frac{1}{3!}\theta^{3}+\frac{1}{5!}\theta^{5}+\cdots)-a^{\wedge}(1-\frac{1}{2!}\theta^{2}+\frac{1}{4!}\theta^{4}+\cdots)+a^{\wedge} \right\}\\ & = \frac{1}{\theta} \left \{ aa^{T}\theta - a^{\wedge}a^{\wedge}\sin \theta -a^{\wedge}\cos \theta +a^{\wedge} \right \}\\ & = \frac{1}{\theta} \left \{ aa^{T}\theta-\sin \theta (aa^{T}-I)+a^{\wedge}(1-\cos \theta) \right \} \\ & = \frac{1}{\theta} \left \{ \sin \theta I +(\theta-\sin \theta)aa^{T}+(1-\cos \theta)a^{\wedge} \right \} \\ & = \frac{\sin \theta}{\theta}I + (1-\frac{\sin \theta}{\theta})aa^{T}+\frac{1-\cos \theta}{\theta}a^{\wedge} \end{align*}至于为什么雅可比矩阵是这种形式，即$J=\sum_{n=0}^{\infty} \frac{1}{(n+1)!}(\phi ^{\wedge})^{n}$是从何而来的，这个在state estimation for robotics一书的223-226有讲到，但是写的很抽象，下面给出证明： \begin{align*} & \xi ^{\wedge} = \begin{bmatrix} \phi ^{\wedge} & \rho \\ {\bf 0}^{T} & 0 \end{bmatrix} \\ & \xi ^{\wedge} \xi^{\wedge}= \begin{bmatrix} \phi ^{\wedge} & \rho \\ {\bf 0}^{T} & 0 \end{bmatrix} \begin{bmatrix} \phi ^{\wedge} & \rho \\ {\bf 0}^{T} & 0 \end{bmatrix} =\phi ^{\wedge} \xi^{\wedge} \\ & \xi^{\wedge}\xi^{\wedge}\xi^{\wedge}=(\phi ^{\wedge})^{2}\xi^{\wedge} \end{align*} \begin{align*} \exp(\xi ^{\wedge}) & = \sum_{n=0}^{\infty}\frac{1}{n!}(\xi ^{\wedge})^{n}\\ & = I+\xi^{\wedge}+\frac{1}{2!}\xi^{\wedge}\xi^{\wedge}+\frac{1}{3!}\xi^{\wedge}\xi^{\wedge}\xi^{\wedge}+\cdots \\ & =I+\xi^{\wedge}+\frac{1}{2!}\phi^{\wedge}\xi^{\wedge}+\frac{1}{3!}(\phi^{\wedge})^{2}\xi^{\wedge}+\cdots \\ & = I+\sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n}\xi^{\wedge}\\ & = \begin{bmatrix} I+\sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n+1} & \sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n}\rho \\ {\bf 0}^{T} & 1 \end{bmatrix} \\ & = \begin{bmatrix} \sum_{n=0}^{\infty}\frac{1}{n!}(\phi^{\wedge})^{n} & \sum_{n=0}^{\infty}\frac{1}{(n+1)!}(\phi^{\wedge})^{n}\rho \\ {\bf 0}^{T} & 1 \end{bmatrix} \end{align*}伴随先证明$\forall a \in \mathbb R^{3}, Ra^{\wedge}R^{T}=(Ra)^{\wedge}$,高翔提供的网址的证明不严谨，即 (Ra)^{\wedge}v=(Ra)\times v =(Ra) \times (RR^{-1}v)=R(a \times R^{-1}v)=Ra^{\wedge}R^{-1}v AB=BC,不能推导出A \neq B \quad\text{A,B,C是矩阵}下面通过旋转矩阵正交的性质来证明该式成立。 \begin{align*} & 设旋转矩阵R= \begin{bmatrix} {\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3} \end{bmatrix} \quad \text{这里的${\bf r}_{1},{\bf r}_{2},{\bf r}_{3}$既可以看作是$1 \times 3$的单位向量，也可以看作是矩阵，因此下面不再进行区分}\\ & 对于Rp^{\wedge}R^{T} = (Rp)^{\wedge} \iff p^{\wedge}=R^{T}(Rp)^{\wedge}R，因此转为证明后式\\ \end{align*} 在证明之前，有两个事项需要注意： 1.${\bf r}_{i}^{T} {\bf r}_{j}={\bf r}_{i} \cdot {\bf r}_{j}$ 前面可以看作矩阵，后面看作向量点乘，${\bf r}$是列向量 2.因为$R$是旋转矩阵，因此行向量和列向量都是单位向量且两两正交。为了后面叉乘运算的一致性，需要将${\bf r}_{1},{\bf r}_{2},{\bf r}_{3}$分别看成三维正交坐标系的$x,y,z$轴，即： \begin{cases} {\bf r}_{1} \times {\bf r}_{2} = {\bf r}_{1}^{\wedge}{\bf r}_{2}={\bf r}_{3}\\ {\bf r}_{2} \times {\bf r}_{3} = {\bf r}_{2}^{\wedge}{\bf r}_{3}={\bf r}_{1}\\ {\bf r}_{3} \times {\bf r}_{1} = {\bf r}_{3}^{\wedge}{\bf r}_{1}={\bf r}_{2} \end{cases} \begin{align*} p^{\wedge} & = R^{T}(Rp)^{\wedge}R \\ & = \begin{bmatrix} {\bf r}_{1}^{T}\\ {\bf r}_{2}^{T}\\ {\bf r}_{3}^{T} \end{bmatrix} (\begin{bmatrix} {\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3} \end{bmatrix} \begin{bmatrix} p_{1}\\ p_{2}\\ p_{3} \end{bmatrix})^{\wedge} \begin{bmatrix} {\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3} \end{bmatrix} \\ & = \begin{bmatrix} {\bf r}_{1}^{T}\\ {\bf r}_{2}^{T}\\ {\bf r}_{3}^{T} \end{bmatrix} ({\bf r}_{1}p_{1}+{\bf r}_{2}p_{2}+{\bf r}_{3}p_{3})^{\wedge} \begin{bmatrix} {\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3} \end{bmatrix} \\ & = \begin{bmatrix} {\bf r}_{1}^{T}\\ {\bf r}_{2}^{T}\\ {\bf r}_{3}^{T} \end{bmatrix} ({\bf r}_{1}^{\wedge}p_{1}+{\bf r}_{2}^{\wedge}p_{2}+{\bf r}_{3}^{\wedge}p_{3}) \begin{bmatrix} {\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3} \end{bmatrix} \\ & = p_{1} \begin{bmatrix} {\bf r}_{1}^{T}\\ {\bf r}_{2}^{T}\\ {\bf r}_{3}^{T} \end{bmatrix} {\bf r}_{1}^{\wedge} \begin{bmatrix} {\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3} \end{bmatrix} + p_{2} \begin{bmatrix} {\bf r}_{1}^{T}\\ {\bf r}_{2}^{T}\\ {\bf r}_{3}^{T} \end{bmatrix} {\bf r}_{2}^{\wedge} \begin{bmatrix} {\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3} \end{bmatrix} + p_{3} \begin{bmatrix} {\bf r}_{1}^{T}\\ {\bf r}_{2}^{T}\\ {\bf r}_{3}^{T} \end{bmatrix} {\bf r}_{3}^{\wedge} \begin{bmatrix} {\bf r}_{1} & {\bf r}_{2} & {\bf r}_{3} \end{bmatrix} \\ & = p_{1} \begin{bmatrix} {\bf r}_{1}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{1} & {\bf r}_{1}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{2} & {\bf r}_{1}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{3}\\ {\bf r}_{2}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{1} & {\bf r}_{2}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{2} & {\bf r}_{2}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{3}\\ {\bf r}_{3}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{1} & {\bf r}_{3}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{2} & {\bf r}_{3}^{T} {\bf r}_{1}^{\wedge} {\bf r}_{3}\\ \end{bmatrix} + p_{2} \begin{bmatrix} {\bf r}_{1}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{1} & {\bf r}_{1}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{2} & {\bf r}_{1}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{3}\\ {\bf r}_{2}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{1} & {\bf r}_{2}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{2} & {\bf r}_{2}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{3}\\ {\bf r}_{3}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{1} & {\bf r}_{3}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{2} & {\bf r}_{3}^{T} {\bf r}_{2}^{\wedge} {\bf r}_{3}\\ \end{bmatrix} + p_{3} \begin{bmatrix} {\bf r}_{1}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{1} & {\bf r}_{1}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{2} & {\bf r}_{1}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{3}\\ {\bf r}_{2}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{1} & {\bf r}_{2}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{2} & {\bf r}_{2}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{3}\\ {\bf r}_{3}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{1} & {\bf r}_{3}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{2} & {\bf r}_{3}^{T} {\bf r}_{3}^{\wedge} {\bf r}_{3}\\ \end{bmatrix} \\ & = p_{1} \begin{bmatrix} 0 & 0 & 0\\ 0 & 0 & -1\\ 0 & 1 & 0 \end{bmatrix} + p_{2} \begin{bmatrix} 0 & 0 & 1\\ 0 & 0 & 0\\ -1 & 0 & 0 \end{bmatrix} + p_{3} \begin{bmatrix} 0 & -1 & 0\\ 1 & 0 & 0\\ 0 & 0 & 0 \end{bmatrix} \\ & = \begin{bmatrix} 0 & -p_{3} & p_{2}\\ p_{3} & 0 & -p_{1}\\ -p_{2} & p_{1} & 0 \end{bmatrix} \end{align*}对于$SO(3)$上的伴随的证明只需进行泰勒展开即可，与上面的证明相同。 \begin{align*} & \exp ((Ad(R)P)^{\wedge}) = \exp ((Rp)^{\wedge})=\exp (Rp^{\wedge}R^{T})=\exp (R\theta a^{\wedge}R^{T})=\sum _{n=0}^{\infty}\frac{1}{n!}(R\theta a^{\wedge}R^{T})^{n} \\ & 令p=\theta a,a是单位向量，\theta为模长\\ & Ra^{\wedge}R^{T} Ra^{\wedge}R^{T} =Ra^{\wedge}a^{\wedge}R^{T}=R(aa^{T}-I)R^{T}=Raa^{T}R^{T}-I\\ & Ra^{\wedge}R^{T} Ra^{\wedge}R^{T} Ra^{\wedge}R^{T} =Ra^{\wedge}a^{\wedge}a^{\wedge}R^{T}=-Ra^{\wedge}R^{T} \end{align*}泰勒展开后的式子利用正余弦函数表示，结果为： \begin{align*} \exp(Rp^{\wedge}R^{T}) & = \sum _{n=0}^{\infty}\frac{1}{n!}(\theta R a^{\wedge}R^{T})^{n}\\ & = (1-\cos \theta)Raa^{T}R^{T}+\cos \theta I+\sin \theta Ra^{\wedge}R^{T}\\ & =R((1-\cos \theta)aa^{T}+\cos \theta I+\sin \theta a^{\wedge})R^{T}\\ & =R \exp(p^{\wedge})R^{T} \end{align*}对于$SE(3)$的伴随$Ad(T)$有： \begin{align*} & T\exp (\xi ^{\wedge})T^{-1} = \exp((Ad(T)\xi )^{\wedge})\\ & Ad(T)= \begin{bmatrix} R & t^{\wedge}R\\ {\bf 0} & R \end{bmatrix} \end{align*}$SO(3)$和$SE(3)$的伴随将在后面的位姿图优化中用到。 轨迹的描绘1.$T_{WC}$是相机坐标系到世界坐标系的变换矩阵，其平移部分就是相机的移动距离，我们在解算位姿的时候是计算两帧之间的位姿，因此平移部分连起来就是相机的轨迹，即机器人的轨迹。 实际上，$T_{WC}$和$T_{CW}$之间只差了一个逆而已，都可以用来表示相机的位姿，但是实践当中使用$T_{CW}$更为常见，不过$T_{WC}$更为直观，因为$p_{W} =T_{WC}p_{C}=Rp_{C}+t_{WC}$对于相机原点来说，$p_{W}$就是在其对应于世界坐标系的点，而且正是$T_{WC}$的平移部分，那么连起来就可以看到相机的平移轨迹。 2.仿照ORB-SLAM2里的CMakeLists.txt写的CMakeLists.txt 123456789101112131415161718192021222324252627282930313233343536cmake_minimum_required(VERSION 2.8)project(trajectorydrawing)set(CMAKE_BUILD_TYPE &quot;Release&quot;)# check C++11 or C++0x supportinclude(CheckCCompilerFlag)include(CheckCXXCompilerFlag)CHECK_CXX_COMPILER_FLAG(&quot;-std=c++11&quot; COMPILER_SUPPORTS_CXX11)CHECK_CXX_COMPILER_FLAG(&quot;-std=c++0x&quot; COMPILER_SUPPORTS_CXX0X)if(COMPILER_SUPPORTS_CXX11) set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11&quot;) add_definitions(-DCOMPILEDWITHC11) message(STATUS &quot;Using flag -std=c++11.&quot;)elseif(COMPILER_SUPPORTS_CXX0X) set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++0x&quot;) add_definitions(-DCOMPILEDWITHC0X) message(STATUS &quot;Using flag -std=c++0x.&quot;)else() message(FATAL_ERROR &quot;The compiler $&#123;CMAKE_CXX_COMPILER&#125; has no C++11 support. Please use a different C++ compiler.&quot;)endif()find_package(Eigen3 REQUIRED)find_package(Pangolin REQUIRED)find_package(Sophus REQUIRED)include_directories( $&#123;EIGEN3_SOURCE_DIR&#125; $&#123;Pangolin_INCLUDE_DIR&#125; $&#123;Sophus_INCLUDE_DIR&#125;)add_executable(trajectorydrawing draw_trajectory.cpp)target_link_libraries(trajectorydrawing $&#123;EIGEN3_LIBRARIES&#125; $&#123;Pangolin_LIBRARIES&#125; $&#123;Sophus_LIBRARIES&#125;) 读取数据的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940// 第一种方法，用fstream的getline分行读取 ifstream fin(trajectory_file); //从文件中读取数据 if(!fin.is_open())&#123; cout&lt;&lt;&quot;No &quot;&lt;&lt;trajectory_file&lt;&lt;endl; return 0; &#125; double t,tx,ty,tz,qx,qy,qz,qw; string line; while(getline(fin,line)) &#123; istringstream record(line); //从string读取数据 record&gt;&gt;t&gt;&gt;tx&gt;&gt;ty&gt;&gt;tz&gt;&gt;qx&gt;&gt;qy&gt;&gt;qz&gt;&gt;qw; Eigen::Vector3d p(tx,ty,tz); Eigen::Quaterniond q = Eigen::Quaterniond(qw,qx,qy,qz).normalized(); Sophus::SE3 SE3_qp(q,p); poses.push_back(SE3_qp); &#125; //第二种方法 // ifstream in(trajectory_file);//创建输入流 // if(!in)&#123; // cout&lt;&lt;&quot;open posefile failture!!!&quot;&lt;&lt;endl; // return 0; // &#125; // for(int i=0; i&lt;620; i++)&#123; // double data[8]=&#123;0&#125;; // for(auto&amp; d:data) in&gt;&gt;d;//按行依次去除数组中的值 // Eigen::Quaterniond q(data[7], data[8], data[5], data[6]); // Eigen::Vector3d t(data[1], data[2], data[3]); // Sophus::SE3 SE3(q,t); // poses.push_back(SE3); // &#125; // end your code here 生成的轨迹图如下： 轨迹的误差CMakeLists.txt文件内容如下： 123456789101112131415161718192021222324252627282930313233343536cmake_minimum_required(VERSION 2.8)project(trajectory_error)set(CMAKE_BUILD_TYPE &quot;Release&quot;)# check C++11 or C++0x supportinclude(CheckCCompilerFlag)include(CheckCXXCompilerFlag)CHECK_CXX_COMPILER_FLAG(&quot;-std=c++11&quot; COMPILER_SUPPORTS_CXX11)CHECK_CXX_COMPILER_FLAG(&quot;-std=c++0x&quot; COMPILER_SUPPORTS_CXX0X)if(COMPILER_SUPPORTS_CXX11) set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11&quot;) add_definitions(-DCOMPILEDWITHC11) message(STATUS &quot;Using flag -std=c++11.&quot;)elseif(COMPILER_SUPPORTS_CXX0X) set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++0x&quot;) add_definitions(-DCOMPILEDWITHC0X) message(STATUS &quot;Using flag -std=c++0x.&quot;)else() message(FATAL_ERROR &quot;The compiler $&#123;CMAKE_CXX_COMPILER&#125; has no C++11 support. Please use a different C++ compiler.&quot;)endif()find_package(Eigen3 REQUIRED)find_package(Pangolin REQUIRED)find_package(Sophus REQUIRED)include_directories( $&#123;EIGEN3_SOURCE_DIR&#125; $&#123;Pangolin_INCLUDE_DIR&#125; $&#123;Sophus_INCLUDE_DIR&#125;)add_executable(trajectory_error trajectory_error.cpp)target_link_libraries(trajectory_error $&#123;EIGEN3_LIBRARIES&#125; $&#123;Pangolin_LIBRARIES&#125; $&#123;Sophus_LIBRARIES&#125;) trajectory_error.cpp文件内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134#include &lt;sophus/se3.h&gt;#include &lt;string&gt;#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;cmath&gt;#include &lt;pangolin/pangolin.h&gt;#include &lt;Eigen/Core&gt;#include &lt;Eigen/Geometry&gt; using namespace std;using namespace Eigen; void ReadData(string FileName ,vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; &amp;poses);double ErrorTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g, vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e);void DrawTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g, vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e); int main(int argc, char **argv)&#123; string GroundFile = &quot;./groundtruth.txt&quot;; string ErrorFile = &quot;./estimated.txt&quot;; double trajectory_error_RMSE = 0; vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g; vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e; ReadData(GroundFile,poses_g); ReadData(ErrorFile,poses_e); trajectory_error_RMSE = ErrorTrajectory(poses_g, poses_e); cout&lt;&lt;&quot;trajectory_error_RMSE = &quot;&lt;&lt; trajectory_error_RMSE&lt;&lt;endl; DrawTrajectory(poses_g,poses_e); &#125; /***************************读取文件的数据，并存储到vector类型的pose中**************************************/void ReadData(string FileName ,vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; &amp;poses)&#123; ifstream fin(FileName); //从文件中读取数据 //这句话一定要加上，保证能够正确读取文件。如果没有正确读取，结果显示-nan if(!fin.is_open())&#123; cout&lt;&lt;&quot;No &quot;&lt;&lt;FileName&lt;&lt;endl; return; &#125; double t,tx,ty,tz,qx,qy,qz,qw; string line; while(getline(fin,line)) &#123; istringstream record(line); //从string读取数据 record &gt;&gt; t &gt;&gt; tx &gt;&gt; ty &gt;&gt; tz &gt;&gt; qx &gt;&gt; qy &gt;&gt; qz &gt;&gt; qw; Eigen::Vector3d p(tx, ty, tz); Eigen::Quaterniond q = Eigen::Quaterniond(qw, qx, qy, qz).normalized(); //四元数的顺序要注意 Sophus::SE3 SE3_qp(q, p); poses.push_back(SE3_qp); &#125; &#125; /*******************************计算轨迹误差*********************************************/double ErrorTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g, vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e )&#123; double RMSE = 0; Matrix&lt;double ,6,1&gt; se3; vector&lt;double&gt; error; for(int i=0;i&lt;poses_g.size();i++)&#123; se3=(poses_g[i].inverse()*poses_e[i]).log(); //这里的se3为向量形式，求log之后是向量形式 //cout&lt;&lt;se3.transpose()&lt;&lt;endl; error.push_back( se3.squaredNorm() ); //二范数 // cout&lt;&lt;error[i]&lt;&lt;endl; &#125; for(int i=0; i&lt;poses_g.size();i++)&#123; RMSE += error[i]; &#125; RMSE /= double(error.size()); RMSE = sqrt(RMSE); return RMSE;&#125;/*****************************绘制轨迹*******************************************/void DrawTrajectory(vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_g, vector&lt;Sophus::SE3, Eigen::aligned_allocator&lt;Sophus::SE3&gt;&gt; poses_e) &#123; if (poses_g.empty() || poses_e.empty()) &#123; cerr &lt;&lt; &quot;Trajectory is empty!&quot; &lt;&lt; endl; return; &#125; // create pangolin window and plot the trajectory pangolin::CreateWindowAndBind(&quot;Trajectory Viewer&quot;, 1024, 768); //创建一个窗口 glEnable(GL_DEPTH_TEST); //启动深度测试 glEnable(GL_BLEND); //启动混合 glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);//混合函数glBlendFunc( GLenum sfactor , GLenum dfactor );sfactor 源混合因子dfactor 目标混合因子 pangolin::OpenGlRenderState s_cam( pangolin::ProjectionMatrix(1024, 768, 500, 500, 512, 389, 0.1, 1000), pangolin::ModelViewLookAt(0, -0.1, -1.8, 0, 0, 0, 0.0, -1.0, 0.0) //对应的是gluLookAt,摄像机位置,参考点位置,up vector(上向量) ); pangolin::View &amp;d_cam = pangolin::CreateDisplay() .SetBounds(0.0, 1.0, pangolin::Attach::Pix(175), 1.0, -1024.0f / 768.0f) .SetHandler(new pangolin::Handler3D(s_cam)); while (pangolin::ShouldQuit() == false) &#123; glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); d_cam.Activate(s_cam); glClearColor(1.0f, 1.0f, 1.0f, 1.0f); glLineWidth(2); for (size_t i = 0; i &lt; poses_g.size() - 1; i++) &#123; glColor3f(1 - (float) i / poses_g.size(), 0.0f, (float) i / poses_g.size()); glBegin(GL_LINES); auto p1 = poses_g[i], p2 = poses_g[i + 1]; glVertex3d(p1.translation()[0], p1.translation()[1], p1.translation()[2]); glVertex3d(p2.translation()[0], p2.translation()[1], p2.translation()[2]); glEnd(); &#125; for (size_t j = 0; j &lt; poses_e.size() - 1; j++) &#123; //glColor3f(1 - (float) j / poses_e.size(), 0.0f, (float) j / poses_e.size()); glColor3f(1.0f, 1.0f, 0.f);//为了区分第二条轨迹，用不同的颜色代替,黄色 glBegin(GL_LINES); auto p1 = poses_e[j], p2 = poses_e[j + 1]; glVertex3d(p1.translation()[0], p1.translation()[1], p1.translation()[2]); glVertex3d(p2.translation()[0], p2.translation()[1], p2.translation()[2]); glEnd(); &#125; pangolin::FinishFrame(); usleep(5000); // sleep 5 ms &#125; &#125;]]></content>
      <categories>
        <category>科研记录</category>
      </categories>
      <tags>
        <tag>visual SLAM</tag>
        <tag>Linux</tag>
        <tag>C++11</tag>
        <tag>Computer vision</tag>
        <tag>Sophus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[visual SLAM by Gaoxiang(2)]]></title>
    <url>%2F2019%2F03%2F12%2Fvisual-SLAM-by-Gaoxiang-2%2F</url>
    <content type="text"><![CDATA[​ 本次课程主要研究三维空间刚体运动，即visual slam的运动方程中的$x_{k}$如何表达。 点与坐标系在2D情况下，物体可以通过两个坐标和一个旋转角进行表达，即$(x,y,\theta)$。 在3D情况下，物体是6自由度的，包括平移和旋转，每个都得用三个变量表达，可以认为3D情况是包含着3个2D的情况，旋转轴不同。 理清坐标系（参考系），点，向量，向量的坐标，运动变换之间的关系。相机会有相机坐标系，机器人会有机体坐标系（通常是运动的），空间会有世界坐标系（通常是固定的），这都是为了研究问题的方便，通过变换进行运动的表述。 熟悉向量的相关运算规则，比如，向量的加减法，向量的内积和外积。以及向量和矩阵的关系。 \begin{align*} & 内积： \bf{a} \cdot \bf{b} =\bf{a^{T}} \bf{b}=\sum_{i=1}^{n}=|\bf{a}||\bf{b}| \cos \left< \bf{a}, \bf{b}\right> \\ &外积： \bf{a} \times \bf{b}= \begin{bmatrix} \bf{i} & \bf{j} & \bf{k} \\ a_{1} & a_{2} & a_{3} \\ b_{1} & b_{2} & b_{3} \end{bmatrix} = \begin{bmatrix} a_{2}b_{3}-a_{3}b_{2}\\ a_{3}b_{1}-a_{1}b_{3}\\ a_{1}b_{2}-a_{2}b_{1} \end{bmatrix} = \begin{bmatrix} 0 & -a_{3} & a_{2}\\ a_{3} & 0 & -a_{1} \\ -a_{2} & a_{1} & 0 \end{bmatrix} \bf{b} \triangleq \bf{a} ^{\wedge} \bf{b} \end{align*}建立了坐标系之后，如何表示同一个向量在不同坐标系之间的坐标，坐标系之间的变换关系又该如何描述。坐标系之间的变换可以分解成坐标系原点之间之间的平移和坐标轴之间的旋转，那么可以用一个平移向量和旋转矩阵来描述这样的变换。我们知道矩阵可以表述坐标系之间的变换，对于三维空间而言，可以用一个$4 \times 4$的矩阵来描述三维坐标系之间的变换，这也就是平移向量和旋转矩阵的合成矩阵，具体后面会细说。 旋转矩阵一个向量在坐标系进行旋转后不变，因此可以通过此推导出旋转矩阵$R$的表达式（即用变换前后的坐标系的基向量进行表述）。 \begin{bmatrix} a_{1}\\ a_{2}\\ a_{3} \end{bmatrix} = \begin{bmatrix} e_{1}^{T}e_{1}^{'} & e_{1}^{T}e_{2}^{'} & e_{1}^{T}e_{3}^{'}\\ e_{2}^{T}e_{1}^{'} & e_{2}^{T}e_{2}^{'} & e_{2}^{T}e_{3}^{'}\\ e_{3}^{T}e_{1}^{'} & e_{3}^{T}e_{2}^{'} & e_{3}^{T}e_{3}^{'} \end{bmatrix} \begin{bmatrix} a_{1}^{'}\\ a_{2}^{'}\\ a_{3}^{'} \end{bmatrix} \triangleq R {\bf a}^{'}旋转矩阵是正交矩阵（$RR^{T}=I$），且行列式为1（$\det (R)=1$）。旋转矩阵属于特殊正交群special orthogonal group，即： SO(n) = \lbrace R \in \mathbb{R}^{n \times n} | RR^{T}=I, \det(R)=1 \rbrace对一个旋转矩阵进行转置就描述了一个相反方向的旋转。 欧拉旋转定理（Euler’s rotation theorem）：刚体在三维空间里的一般运动，可分解为刚体上方某一点的平移，以及绕经过此点的旋转轴的转动。 {\bf a^{'}}= R \bf{a} + \bf{t} 齐次形式（homogeneous）来更方便地表达变换，因为加上平移不满足线性性。定义变换矩阵$T$ \begin{bmatrix} a^{'}\\ 1 \end{bmatrix} = \begin{bmatrix} R & \bf{t}\\ \bf{ 0^{T} } & 1 \end{bmatrix} \begin{bmatrix} a \\ 1 \end{bmatrix} \triangleq T \begin{bmatrix} a\\ 1 \end{bmatrix}齐次坐标认为其乘以任意非零常数时仍表达同一个坐标（归一化）。 变换矩阵的集合称为特殊欧式群special Euclidean Group: SE(3)=\lbrace T= \begin{bmatrix} R & \bf{t}\\ \bf{ 0^{T}} & 1 \end{bmatrix} \in \mathbb{R}^{4 \times 4} | R \in SO(3), \bf{t} \in \mathbb{R}^{3} \rbrace T^{-1}= \begin{bmatrix} R^{T} & -R^{T}{\bf t}\\ {\bf 0^{T}} & 1 \end{bmatrix}定义了变化矩阵后，多次变换可以直接对变换矩阵直接相乘即可。 旋转向量与欧拉角一般来说视觉SLAM有旋转矩阵和平移向量就够了，一般也是用这样的方式表达。为了进行拓展，比如在航迹推算和组合导航中，通常用四元数来表述物体姿态。 旋转矩阵有9个元素但仅表示三个自由度，比较冗余，因此引入旋转向量(rotation vector)，方向为旋转轴，长度为转过的角度，又称为角轴或轴角（angle axis）。 旋转向量只有三个量，无约束，更加直观，但是旋转轴一般不容易得知，其中旋转向量和旋转矩阵的关系可以通过罗德里格斯公式得出（Rodrigues’ s Formula）：（假设旋转轴为$\bf{n}$，旋转角为$\theta$） \begin{align*} & R = \cos \theta {\bf I}+(1-\cos \theta){\bf n} {\bf n^{T}}+\sin \theta {\bf n^{\wedge}}\\ & \theta = \arccos(\frac{tr(R)-1}{2}) \\ & R{\bf n}=\bf{n} \quad特征值为1的特征向量 \end{align*}欧拉角（Euler Angles）将旋转分解成三个方向上的转动，最常见的是Z-Y-X，即yaw-pitch-roll（偏航-俯仰-横滚），不同领域习惯不同。但是欧拉角会有万向锁问题（gimbal lock)，会在特定值丢失一个自由度，存在奇异性问题，因此欧拉角不适合插值和迭代。实际上，仅用三个实数表达旋转时，会不可避免地存在奇异性问题。视觉SLAM中一般不用欧拉角表达姿态，主要在人机交互中用。 四元数（Quaternion）2D情况下，可以用单位复数表达旋转，$z=x+iy=\rho e^{i \theta}$，乘$i$代表转转90度。 3D情况下，类似地，四元数可作为复数地扩充。 四元数有三个虚部和一个实部，${\bf q}=q_{0}+q_{1}i+q_{2}j+q_{3}k$，虚部之间满足关系（自己和自己运算像复数，自己和别人运算像叉乘）: \begin{cases} i^{2}=j^{2}=k^{2}=-1 \\ ij=k,jk=-k \\ jk=i.kj=-i \\ ki=j,ik=-j \end{cases}单位四元数可以表达旋转，${\bf q}=q_{0}+q_{1}i+q_{2}j+q_{3}k=[s, {\bf v}], s=q_{0}\in \mathbb{R}, {\bf v}=[q_{1}, q_{2}, q_{3}]^{T}\in \mathbb{R}^{3}$，四元数有以下地运算规则： {\bf q}_{a}+{\bf q}_{b}=[s_{a}\pm s_{b}, {\bf v}_{a}\pm {\bf v}_{b}]\\ \begin{align*} {\bf q}_{a}{\bf q}_{b}= &s_{a}s_{b}-x_{a}x_{b}-y_{a}y_{b}-z_{a}z_{b}\\ & +(s_{a}x_{b}+x_{a}s_{b}+y_{a}z_{b}-z_{a}y_{b})i\\ & +(s_{a}y_{b}-x_{a}z_{b}+y_{a}s_{b}+z_{a}x_{b})j\\ & +(s_{a}z_{b}+x_{a}y_{b}-y_{b}x_{a}+z_{a}s_{b})k \end{align*} \\ {\bf q}_{a}{\bf q}_{b}= [s_{a}s_{b}-{\bf v}_{a}^{T}{\bf v}_{b}, s_{a}{\bf v}_{b}+s_{b}{\bf v}_{a}+{\bf v}_{a} \times {\bf v}_{b}] \\ {\bf q}^{*}=s_{a}-x_{a}i-y_{a}j-z_{a}k=[s_{a}, -{\bf v}_{a}]\\ \left\| {\bf q}_{a} \right\|= \sqrt{s_{a}^{2}+x_{a}^{2}+y_{a}^{2}+z_{a}^{2}} \\ {\bf q}^{-1}={\bf q}^{*}/ \left\| {\bf q} \right\| ^{2}\\ k{\bf q}=[ks, k{\bf v}]\\ {\bf q}_{a} \cdot {\bf q}_{b}=s_{a}s_{b}+x_{a}x_{b}i+y_{a}y_{b}j+z_{a}z_{b}k四元数到角轴：${\bf q}=[\cos \frac{\theta}{2},n_{x}\sin \frac{\theta}{2}, n_{y}\sin \frac{\theta}{2}, n_{z}\sin \frac{\theta}{2}]^{T}$。 角轴到四元数：$\theta =2 \arccos q_{0}, [n_{x}, n_{y}, n_{z}]^{T} = [q_{1}, q_{2}, q_{3}]^{T}/ \sin \frac{\theta}{2}$ 三维点$p(x,y,z)$经过一次以${\bf q}$表示的旋转后，得到了$p^{‘}$，${\bf p}=[0,x,y,z]=[0, {\bf v}]$，旋转之后的关系为${\bf p}^{‘}={\bf q} {\bf p} {\bf q}^{-1}$。四元数相比与角轴和欧拉角，形式上更加紧凑，也无奇异性。 值得注意的是，${\bf q}$和$-{\bf q}$表示同一个旋转。 学习四元数的一个最直观的问题就是为什么三个变量来描述三维旋转，诸如欧拉角会出现奇异性的情况，而用四个变量的四元数就不会？也就说用高维的东西描述低维的东西更加有效。 知乎上有相关的回答，写得还算不错，比较直观。不过依旧没有解决为什么三个变量描述三维旋转会出现奇异性的现象。 利用四元数来对三维点的旋转进行操作，是通过纯四元数来进行的，即变换后的点可以表示为${\bf qwq^{-1} }$，这里的问题是为什么这种形式。（注意，这里的四元数是单位四元数） 汉密尔顿定义的性质： 1.运算产生的结果也要是三维向量2.存在一个元运算，任何三维向量进行元运算的结果就是其本身3.对于任何一个运算，都存在一个逆运算，这两个运算的积是元运算4.运算满足结合律 其实，四元数有四个变量，完全可以被看作一个四维向量。单位四元数（norm=1）则存在于四维空间的一个球面上。${\bf q}_{a} {\bf q}_{b}$，四元数${\bf q}_{a}$乘以四元数${\bf q}_{b}$其实看作（1）对${\bf q}_{a}$进行${\bf q}_{b}$左旋转，或者（2）对${\bf q}_{b}$进行${\bf q}_{a}$右旋转。所以从始至终，四元数定义的都是四维旋转，而不是三维旋转！任意的四维旋转都可以唯一的拆分为一个左旋转和一个右旋转，表达出来就是${\bf q}_{L}{\bf p}{\bf q}_{R}$。这里，我们对四元数（四维向量）${\bf p}$进行了一个${\bf q}_{L}$左旋转和一个${\bf q}_{R}$右旋转。结果当然是一个四元数，符合性质1。这个运算也同时符合性质2，3，4。 为了进行三维旋转运算，汉密尔顿首先在四维空间里划出了一块三维空间。汉密尔顿定义了一种纯四元数（pure quaternion），其表达式为${\bf q}_{w}=(0,w_{x},w_{y},w_{z})$。纯四元数第一项为零，它存在于四维空间的三维超平面上，与三维空间中的三维向量一一对应。然后，就有了我们常见的${\bf q} {\bf q}_{w} {\bf q}^{*}$这种左乘单位四元数，右乘其共轭的表达式。这个运算形式是为了限制其运算结果所在的空间。简单的说，当对一个三维向量进行三维旋转后，我们希望得到的是一个三维向量，而不是四维向量。 这也就解释了为什么四元数对应于角轴的关系式中是$\frac{\theta}{2}$，而不是$\theta$，这是因为${\bf q}$做的就是一个$\frac{\theta}{2}$的旋转，而${\bf q}^{-1}$也做了一个$\frac{\theta }{2}$的旋转。我们进行了两次旋转，而不是一次，这两次旋转的结果是一个旋转角为$\theta$的旋转。 此外，还有一点是，四元数可以用2x2复数矩阵（特殊酉群）和4x4矩阵来描述，但是四元数之间不满足乘法交换律，即${\bf q}_{1} {\bf q}_{2} \ne {\bf q}_{2} {\bf q}_{1}$，但是二维平面里的复数相乘满足乘法交换律，这里我粗略地理解是，二维旋转只有角度，没有轴的概念，只有按照什么顺序旋转多少角度，因此先转$\theta$，再转$\alpha$，与先转$\alpha$，再转$\theta$的结果是一样的，但是四元数相乘不是，每次转都是有个旋转轴的，因此不可交换。 有关四元数，旋转，群的理解和证明，可以参考这篇文章 \begin {align*} {\bf v}^{'} & = {\bf v}_{||}^{'} + {\bf v}_{\bot}^{'}={\bf v}_{||} + {\bf q} {\bf v}_{\bot}\\ & = {\bf p} {\bf p}^{-1} {\bf v}_{\bot} + {\bf p} {\bf p} {\bf v}_{\bot}\\ & = {\bf p} {\bf p}^{*} {\bf v}_{\bot} + {\bf p} {\bf p} {\bf v}_{\bot}\\ & = {\bf p} {\bf v}_{||} {\bf p}^{*} + {\bf p} {\bf v}_{\bot} {\bf p}^{*}\\ & = {\bf p} ({\bf v}_{||}+{\bf v}_{\bot}) {\bf p}^{*}\\ & = {\bf p} {\bf v} {\bf p}^{*}\\ & = {\bf v}_{||} + {\bf p}^{2} {\bf v}_{\bot} \end{align*} \\ {\bf q}=[\cos \theta, \sin \theta ({\bf u})], {\bf p}=[\cos \frac{\theta}{2}, \sin \frac{\theta}{2} ({\bf u})],{\bf q}={\bf p}^{2}对于平行的分量，变换完全抵消，对于垂直的分量，施加两次变换，这也就是为什么是$\frac {\theta}{2}$。 四元数与群： 单位四元数与 3D 旋转有一个2对1满射同态关系，或者说单位四元数双倍覆盖了3D旋转。因为这个映射是满射，我们可以说所有的单位四元数都对应着一个 3D 旋转。或者说，一个四维单位超球面（也叫做$\mathbb S^{3}$）上任意一点所对应的四元数（$∥q∥ = 1$）都对应着一个 3D 旋转。 四元数，旋转矩阵，群，geometric algebra都有着紧密的联系，因此可以先做好相关的功课，这里列出几篇阅读材料： Introduction to Lie Groups Geometric Algebra Primer youtube geometric algebra A Quaternion is a scalar plus a bivector. Apart from the fact that quaternions have four components, there is nothing four-dimensional or imaginary about a quaternion. The first component is a scalar, and the other three components form the bivector-plane relative to which the rotation is performed. 这似乎解释了之前我的疑问，为什么要用四个变量来描述三维，就是用得用高维的解释低维的，这里指出了四元数拥有四个变量，但是不是指四维或者虚数，而是通过标量和bivector的组合来描述旋转，几何代数的这种形式统一了不同维度的旋转，因此比较优雅。不过更深层次的东西和理解涉及到比较多的数学知识和空间理解，我暂时还达不到。。。。 作业与实践熟悉Eigen运算对于线性方程组$Ax=b$，其中$A$是方阵。 1).什么条件下，$x$有解且唯一？ 当矩阵$A$可逆的时候，也就是此时矩阵满秩。 2).高斯消元法原理 高斯消元法其实就是对系数矩阵作初等行变换，这不改变方程的解，将系数矩阵化成上三角矩阵的形式，然后从下往上依次解出方程组的每个分量解（回代）。如果是针对列主元的高斯消元法，需要加入方程组的右侧值，与系数矩阵组成增广矩阵，进而求解。 3). QR分解的原理 任意的$A \in {\bf C}^{n \times n}$都可以进行$QR$分解，即$A=QR$，$Q$为n阶酉矩阵($QQ^{H}=I$)，$R$为n阶上三角矩阵。 $QR$分解与Gram-Schmidt正交化有关，即将$A$进行Gram-Schmidt正交化，化为$Q$，然后求出R。Q的列向量是A的列空间的标准正交基，R是一个非奇异可逆的上三角矩阵，即将矩阵每个列作为一个基本单元，将其化为正交的基向量与在这个基向量上的投影长度的积。 4).Cholesky分解的原理 Cholesky分解其实是矩阵Doolittle分解(三角分解的特例)的特例。三角分解是是将方针分解成同阶的下三角阵和上三角阵，其中上三角阵的主对角线元素全为1则为Doolittle分解。如果矩阵$A$既是方阵又是Hermite正定阵时($A=A^{H}$,且特征值全为正数)，则存在唯一分解$A=LL^{H}$，其中$L$是具有主对角元素为正数的下三角矩阵。 5).利用$QR$和Cholesky分解法分解随机矩阵$A \in {\bf C}^{100 \times 100}$求解$x$。 主要思路就是先用定义动态大小的矩阵，之后进行调用相关的函数处理。其中需要注意的是，Cholesky分解需要矩阵为正定阵，因此在矩阵定义上需要进行一些处理。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;using namespace std;#include &lt;ctime&gt;// Eigen部分#include &lt;Eigen/Core&gt;//Eigen稠密矩阵的代数运算（逆和特征值等）#include &lt;Eigen/Dense&gt;int main (int argc, char** argv)&#123;Eigen::Matrix &lt;double,Eigen::Dynamic, Eigen::Dynamic&gt; matrix_dynamic; //Eigen固定大小矩阵最大支持到50Eigen::Matrix&lt;double,Eigen::Dynamic, Eigen::Dynamic&gt; matrix_A;Eigen::Matrix&lt;double,Eigen::Dynamic, 1&gt; x;Eigen::Matrix&lt;double,Eigen::Dynamic,1&gt; v_right;matrix_dynamic = Eigen::MatrixXd::Random(100,100); //随机化取值matrix_A = matrix_dynamic.transpose()*matrix_dynamic; //cholesky分解需要A为正定矩阵v_right = Eigen::MatrixXd::Random(100, 1); //方程右边的值随机取值//QR Decompositionclock_t time_stt = clock();x = matrix_A.colPivHouseholderQr().solve(v_right);cout&lt;&lt;&quot;the time used in QR decomposition is &quot;&lt;&lt; 1000* (clock() - time_stt)/(double) CLOCKS_PER_SEC&lt;&lt;&quot;ms&quot;&lt;&lt; endl;cout&lt;&lt;x&lt;&lt;endl;//Cholesky Decompositiontime_stt = clock();x = matrix_A.llt().solve(v_right);cout&lt;&lt;&quot;the time used in Cholesky decomposition is &quot;&lt;&lt; 1000* (clock() - time_stt)/(double) CLOCKS_PER_SEC&lt;&lt;&quot;ms&quot;&lt;&lt; endl;cout&lt;&lt;x&lt;&lt;endl;return 0;&#125; CMakeLists.txt文件内容如下： 1234567891011cmake_minimum_required(VERSION 2.8)project(QR_cholesky)set(CMAKE_BUILD_TYPE &quot;Release&quot;)set(CMAKE_CXX_FLAGS &quot;-O3&quot;) #Debug版会使用参数-g；Release版使用-O3 –DNDEBUGinclude_directories(&quot;/usr/include/eigen3&quot;)add_executable(QR_cholesky QR_cholesky.cpp)#eigen3都是头文件，不需要target_link_libraries 几何运算练习基本思想就是$T_{cw} p=p^{‘}$，或者$R p + t=p^{‘}$，注意这里是世界坐标系的点变换到相机坐标系。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include&lt;iostream&gt;#include&lt;cmath&gt;using namespace std;//Eigen几何模块#include &lt;Eigen/Core&gt;#include&lt;Eigen/Geometry&gt;int main(int argc, char** argv)&#123;Eigen::Quaterniond q1(0.55,0.3,0.2,0.2);q1 = q1.normalized();Eigen::Quaterniond q2(-0.1,0.3,-0.7,0.2);q2 = q2.normalized();Eigen::Matrix &lt;double, 3,1&gt; t1;t1 &lt;&lt; 0.7,1.1,0.2;Eigen::Matrix &lt;double, 3,1&gt; t2;t2 &lt;&lt; -0.1,0.4,0.8;Eigen::Matrix &lt;double, 3,1&gt; p1;p1 &lt;&lt; 0.5,-0.1,0.2;Eigen::Matrix &lt;double, 3,1&gt; p2;Eigen::Matrix &lt;double, 3,1&gt; p; //世界坐标系的点//利用变换矩阵的方法// Eigen::Isometry3d Tcw1 = Eigen::Isometry3d::Identity();//变换矩阵1// Tcw1.rotate(q1);// Tcw1.pretranslate(t1);// Eigen::Isometry3d Tcw2 = Eigen::Isometry3d::Identity();//变换矩阵2// Tcw2.rotate(q2);// Tcw2.pretranslate(t2);// p = Tcw1.inverse()*p1;// p2 = Tcw2*p;//直接利用旋转矩阵和平移向量进行组合运算Eigen::Matrix&lt;double,3,3&gt; R1_inverse;R1_inverse = q1.matrix().inverse();Eigen::Matrix&lt;double,3,3&gt; R2;R2 = q2.matrix();p2 = R2 * R1_inverse *(p1 - t1) + t2;cout &lt;&lt; &quot;p2 = &quot; &lt;&lt; p2.transpose() &lt;&lt; endl;return 0;&#125; CMakeLists.txt文件内容如下： 123456789cmake_minimum_required(VERSION 2.8)set (CMAKE_BUILD_TYPE &quot;Release&quot;)set (CMAKE_CXX_FLAGS &quot;-O3&quot;)project(Geometryusing)include_directories(&quot;/usr/include/eigen3&quot;)add_executable(Geometryusing Geometryusing.cpp)#eigen3都是头文件，不需要target_link_libraries 旋转的表达1).旋转矩阵的正交性 坐标系中的某个单位正交基$(e_{1},e_{2},e_{3})$经过旋转变换后变为$(e^{‘}_{1},e^{‘}_{2},e^{‘}_{3})$，对于同一个向量在两个坐标系下的坐标分别为$a(a_{1},a_{2},a_{3})$和$(a^{‘}_{1},a^{‘}_{2},a^{‘}_{3})$，由于向量不会随着坐标系的旋转而发生变化，则有： \begin{bmatrix} e_{1} & e_{2} & e_{3} \end {bmatrix} \begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \end {bmatrix} = \begin{bmatrix} e^{'}_{1} & e^{'}_{2} & e^{'}_{3} \end {bmatrix} \begin{bmatrix} a^{'}_{1} \\ a^{'}_{2} \\ a^{'}_{3} \end {bmatrix}等式两边同时左乘$[e^{T}_{1} \quad e^{T}_{2} \quad e^{T}_{3}]$，则得出旋转矩阵$R$为 R= \begin{bmatrix} e^{T}_{1}\\ e^{T}_{2} \\ e^{T}_{3} \end{bmatrix} \begin{bmatrix} e^{'}_{1} & e^{'}_{2} & e^{'}_{3} \end{bmatrix}则有： R^{T}R= \begin{bmatrix} e^{'}_{1}\\ e^{'}_{2}\\ e^{'}_{3} \end{bmatrix} \begin{bmatrix} e^{T}_{1} & e^{T}_{2} & e^{T}_{2} \end{bmatrix} \begin{bmatrix} e^{T}_{1}\\ e^{T}_{2} \\ e^{T}_{3} \end{bmatrix} \begin{bmatrix} e^{'}_{1} & e^{'}_{2} & e^{'}_{3} \end{bmatrix} =I \det R= \det( \begin{bmatrix} e^{T}_{1}\\ e^{T}_{2} \\ e^{T}_{3} \end{bmatrix} \begin{bmatrix} e^{'}_{1} & e^{'}_{2} & e^{'}_{3} \end{bmatrix} ) = det(\begin{bmatrix} e^{T}_{1}\\ e^{T}_{2} \\ e^{T}_{3} \end{bmatrix}) det(\begin{bmatrix} e^{'}_{1} & e^{'}_{2} & e^{'}_{3} \end{bmatrix}) =12).四元数的维度 易知$\varepsilon$是三维，$\eta$是一维的。 3).四元数相关证明(题目应该有错） 符号$x$和$\wedge$代表着反对称矩阵，代表着向量到反对称矩阵的变换，这是从叉乘引申而来的。即 a^{\times}= (\begin{bmatrix} a_{1}\\ a_{2}\\ a_{3} \end{bmatrix})^{\times} = \begin{bmatrix} 0 & -a_{3} & a_{2}\\ a_{3} & 0 &-a_{1}\\ -a_{2} & a_{1} & 0 \end{bmatrix}设$q_{1}=[x_{1},y_{1},z_{1},w_{1}]^{T}$,$q_{2}=[x_{2},y_{2},z_{2},w_{2}]^{T}$, \begin{align*} q_{1}q_{2} & = w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\\ & + (w_{1}x_{2}+x_{1}w_{2}+y_{1}z_{2}-z_{1}y_{2})i\\ & + (w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}+z_{1}x_{2})j\\ & + (w_{1}z_{2}+x_{1}y_{2}-y_{1}x_{2}+z_{1}w_{2})k \end{align*} q_{1}^{\bigoplus}q_{2}= \begin{bmatrix} w_{1} & -z_{1} & y_{1} & x_{1}\\ z_{1} & w_{1} & -x_{1} & y_{1}\\ -y_{1} & x_{1} & w_{1} & z_{1}\\ -x_{1} & -y_{1} & -z_{1} & w_{1} \end{bmatrix} \begin{bmatrix} x_{2}\\ y_{2}\\ z_{2}\\ w_{2} \end{bmatrix} = \begin{bmatrix} w_{1}x_{2}-z_{1}y_{2}+y_{1}z_{2}+x_{1}w_{2}\\ z_{1}x_{2}+w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}\\ -x_{2}y_{1}+x_{1}y_{2}+w_{1}z_{2}+z_{1}w_{2}\\ w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2} \end{bmatrix} \begin{align*} & = w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\\ & + (w_{1}x_{2}+x_{1}w_{2}+y_{1}z_{2}-z_{1}y_{2})i\\ & + (w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}+z_{1}x_{2})j\\ & + (w_{1}z_{2}+x_{1}y_{2}-y_{1}x_{2}+z_{1}w_{2})k \end{align*} =q_{1}q_{2} q_{2}^{+}q_{1}= \begin{bmatrix} w_{2} & z_{2} & -y_{2} & x_{2}\\ -z_{2} & w_{2} & x_{2} & y_{2}\\ y_{2} & -x_{2} & w_{2} & z_{2}\\ -x_{2} & -y_{2} & -z_{2} & w_{2} \end{bmatrix} \begin{bmatrix} x_{1}\\ y_{1}\\ z_{1}\\ w_{1} \end{bmatrix} = \begin{bmatrix} w_{2}x_{1}+y_{1}z_{2}-y_{2}z_{1}+w_{1}x_{2}\\ -x_{1}z_{2}+w_{2}y_{1}+x_{2}z_{1}+y_{2}w_{1}\\ x_{1}y_{2}-x_{2}y_{1}+w_{2}z_{1}+w_{1}z_{2}\\ w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2} \end{bmatrix} \begin{align*} & = w_{1}w_{2}-x_{1}x_{2}-y_{1}y_{2}-z_{1}z_{2}\\ & + (w_{1}x_{2}+x_{1}w_{2}+y_{1}z_{2}-z_{1}y_{2})i\\ & + (w_{1}y_{2}-x_{1}z_{2}+y_{1}w_{2}+z_{1}x_{2})j\\ & + (w_{1}z_{2}+x_{1}y_{2}-y_{1}x_{2}+z_{1}w_{2})k \end{align*} =q_{1}q_{2}罗德里格斯公式证明 如上图所示，向量${\bf v}$绕单位旋转轴${\bf k}$旋转$\theta$角后，变换成了${\bf v}_{rot}$，分别将向量${\bf v}$和${\bf v}_{rot}$沿${\bf k}$平行和垂直的方向分解。 \begin{align*} &{\bf v}={\bf v}_{||}+{\bf v}_{\bot}\\ &{\bf v}_{||}=({\bf v} \cdot {\bf k}){\bf k}\\ & {\bf v}_{\bot}={\bf v}-{\bf v}_{||}=-{\bf k}\times({\bf k} \times {\bf v}) \end{align*}根据图像，我们知道，平行于旋转轴的分量没有变化，只有垂直于旋转轴的分量进行了旋转，而且它们的模长是一样的，这就说明${\bf v}_{\bot}$和${\bf v}_{rot \bot}$是在一个圆上，长度相等，而且，${\bf v}_{\bot}$和${\bf k} \times {\bf v}_{\bot}$构成了该圆的两个正交坐标轴，因此${\bf v}_{rot \bot}$的坐标可以用这个坐标轴表示，或者说该向量可以被这两个正交坐标基线性表示，即： \begin{align*} {\bf v}_{rot \bot}&=\cos \theta{\bf v}_{\bot}+\sin \theta({\bf k}\times {\bf v}_{\bot})\\ &=\cos \theta{\bf v}_{\bot}+\sin \theta({\bf k} \times {\bf v}-{\bf k} \times {\bf v}_{||})\\ &=\cos \theta{\bf v}_{\bot}+\sin \theta({\bf k}\times {\bf v}) \end{align*} \begin{align*} {\bf v}_{rot} & = {\bf v}_{rot ||}+{\bf v}_{rot \bot}\\ & = {\bf v}_{||}+\cos \theta {\bf v}_{\bot}+\sin \theta ({\bf k}\times{\bf v})\\ & = {\bf v}_{||}+\cos \theta({\bf v}-{\bf v}_{||}) +\sin \theta ({\bf k}\times{\bf v})\\ & = \cos \theta {\bf v}+(1-\cos \theta){\bf v}_{||}+\sin \theta ({\bf k}\times{\bf v})\\ & = \cos \theta {\bf v}+(1-\cos \theta)({\bf k}\cdot {\bf v}){\bf k}+\sin \theta ({\bf k}\times{\bf v}) \end{align*}下面将这些向量都看成矩阵的形式，将其运算化为矩阵的运算形式，假设${\bf k}$和${\bf v}$都是列向量的形式，则${\bf k}({\bf k}\cdot {\bf v})=kk^{T}v$，${\bf k}\times {\bf v}=k^{\wedge}v$，因此，得到了旋转矩阵的形式为： R=\cos \theta I+(1-\cos \theta)kk^{T}+\sin \theta k^{\wedge}将$k$换成$n$即得罗德里格斯公式。 四元数运算性质的验证设单位四元数$q=[\varepsilon \quad \eta]$(虚部 实部），点$p=[\zeta \quad 0]$，$q=x_{1}i+y_{1}j+z_{1}k+w_{1}, w_{1}^{2}+x_{1}^{2}+y_{1}^{2}+z_{1}^{2}=1$则： \begin{align*} p^{'}=qpq^{-1} & = q^{\bigoplus}pq^{-1}\\ & = q^{\bigoplus}q^{-1+}p\\ & = \begin{bmatrix} \eta I+\varepsilon ^{\times} & \varepsilon \\ -\varepsilon^{T} & \eta \end{bmatrix} \begin{bmatrix} \eta I-(-\varepsilon )^{\times} & -\varepsilon \\ -(-\varepsilon)^{T} & \eta \end{bmatrix} \begin{bmatrix} \zeta\\ 0 \end{bmatrix} \\ & = \begin{bmatrix} w_{1} & -z_{1} & y_{1} & x_{1} \\ z_{1} & w_{1} & -x_{1} & y_{1} \\ -y_{1} & x_{1} & w_{1} & z_{1}\\ -x_{1} & -y_{1} & -z_{1} & w_{1} \end{bmatrix} \begin{bmatrix} w_{1} & -z_{1} & y_{1} & -x_{1} \\ z_{1} & w_{1} & -x_{1} & -y_{1} \\ -y_{1} & x_{1} & w_{1} & -z_{1}\\ x_{1} & y_{1} & z_{1} & w_{1} \end{bmatrix} \begin{bmatrix} \zeta \\ 0 \end{bmatrix} \\ & = \begin{bmatrix} R_{3 \times 3} & 0_{3 \times 1}\\ 0_{1 \times 3} & 1 \end{bmatrix} \begin{bmatrix} \zeta \\ 0 \end{bmatrix} \end{align*}易知$p^{‘}$的实部为0，也就是上述形式限制了点的变换维度，虽然四元数是四个变量，但是不会把三维点变到四维去。 计算得出旋转矩阵$R$为: R= \begin{bmatrix} w_{1}^{2}+x_{1}^{2}-y_{1}^{2}-z_{1}^{2} & 2x_{1}y_{1}-2w_{1}z_{1} & 2x_{1}z_{1}+2y_{1}w_{1}\\ 2x_{1}y_{1}+2z_{1}w_{1} & w_{1}^{2}+y_{1}^{2}-x_{1}^{2}-z_{1}^{2} & 2y_{1}z_{1}-2x_{1}w_{1}\\ 2x_{1}z_{1}-2y_{1}w_{1} & 2y_{1}z_{1}+2x_{1}w_{1} & w_{1}^{2}+z_{1}^{2}-x_{1}^{2}y_{1}^{2} \end{bmatrix}C++11 for(atuo&amp; a: avec) 范围for循环，用a遍历avec中的每个量；基于范围的FOR循环的遍历是只读的遍历，除非将变量变量的类型声明为引用类型。 for(atuo&amp; a: avec) 自动类型推导，根据a获得的值，用auto自动推断出a的类型； {return a1.index&lt;a2.index;}) 运用了lambda表达式。 begin() begin 返回首元素的地址，end 返回尾元素的下一个地址。]]></content>
      <categories>
        <category>科研记录</category>
      </categories>
      <tags>
        <tag>visual SLAM</tag>
        <tag>Linux</tag>
        <tag>C++11</tag>
        <tag>Computer vision</tag>
        <tag>Eigen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[visual SLAM by Gaoxiang(1)]]></title>
    <url>%2F2019%2F03%2F12%2Fvisual-SLAM-by-Gaoxiang-1%2F</url>
    <content type="text"><![CDATA[​ 视觉SLAM概述 simultaneous localization and mapping 仅使用相机进行室内/室外定位（有些情况下GPS会崩，IMU漂移随着时间误差增大） 机器人在未知环境进行导航—-建图(sparse/semi-dense/dense) SLAM问题的本质是对运动主体自身和周围环境空间不确定性的估计（spatial uncertainty） 为了解决SLAM问题，我们需要状态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，估计状态的均值和不确定性（方差） 学术上研究视觉SLAM较多，尤其是monocular，但是应用上少了点，尤其是建图的作用目前很浅，而且很多人大部分现在在用深度学习搞3D重建。目前应用的地方有： 手持设备定位 自动驾驶定位—比GPS的定位信息要丰富，甚至精度更好(可以达到厘米级) AR 增强现实（定位，建图，深度学习结合） 清洁机器人 学习研究步骤第一部分是学习相关的数学知识，构建数学模型 矩阵 概率论 李群李代数 微分几何 凸优化 … 第二部分是计算机视觉的代码实践 openCV c++ python Linux … 教材： Multiple view Geometry in computer vision State estimation for robotics 视觉SLAM十四讲-从理论到实践(作业代码) 本教程内容提纲 概述与预备知识 三维空间的刚体运动 李群李代数 相机模型与非线性优化 特征点法视觉里程计 直接法视觉里程计 后端优化 回环检测 视觉SLAM的基本框架和模型定位与建图是相互关联的，准确的定位需要精确的地图，精确的地图也来自准确的定位。 为什么选择视觉传感器: 携带安装更加自由，成本更低，信息丰富，功能不单一，可开发性强，智能化程度高具有挑战性（单目monocular/双目stereo/深度相机RGB-D/鱼眼、全景、event-based相机等），而挑战是普通的相机会丢失世界的距离信息，因此需要从图像中进行恢复（相机运动，相机几何关系，物理测量等）。 视觉SLAM框架 前端：visual odometry(估计邻帧相机相机运动/feature-based/direct-based) 后端：optimization(消除噪声，优化轨迹/最大后验概率估计/滤波器/图优化) 回环：loop closing(相机回到之前相同的位置，优化约束，消除累计误差/图像相似性/词袋模型) 建图：mapping(导航/路劲规划/人机交互/可视化/通讯/度量地图（稀疏地图/稠密地图）/拓扑地图) 视觉SLAM的数学模型： \begin{align*} & 运动方程 x_{k} = f(x_{k-1}, u_{k},w_{k}) \\ & 观测方程 z_{k,j} = h(y_{j}, x_{k}, v_{k,j}) \end{align*}———-0——-1———2——-3——-4——-5——-6————-&gt; t ———$x_{0}$—-$x_{1}$——$x_{2}$——$x_{3}$——$x_{4}$———- &gt; ​ $\searrow $ $\searrow $ $\searrow $ $\searrow $ ​ $\bullet y_{1}$ $\bullet y_{2}$ ​ $\bullet y_{j}$ ​ $\cdots$ 假设一个机器人在实际场景中运动，$x_{k}$是此时刻k的位置，$x_{k-1}$是上一个时刻k-1的，在这之间有个输入，使得机器人运动，我们设其为$u_{k}$，而同时运动也包含着噪声，机器人的运动也不完全受精确控制，假设噪声为$w_{k}$。 现在，机器人携带了相机，相机会拍到一系列现实场景的标志，我们称其为路标点（landmark），假设此场景各个路标点为$y_{1}, y_{2}, \cdots, y_{j}, \cdots$。容易知道，机器人在每个时刻所在的位置都会观测到这些路标点中的一部分或者全部，假设在$x_{0}$位置看到路标点$y_{1}$，我们记为$z_{0,1}$，看到了路标点$y_{2}$，记为$z_{0,2}$，…以此类推，这会反映在图像像素上，因此称为观测方程，同样观测也会有噪声$v_{k,j}$。 我们通过一系列运动方程和观测方程，利用已知的$u_{k}, z_{k,j}$来推断出$x_{k}, y_{j}$，也就是知道了机器人的位置，同时了解了环境的信息，因此也就是两大任务：定位和建图。 本次作业与实践首先大致了解下Linux下编写C++源码以及利用cmake编译可执行文件的步骤 工具g++, cmake, VS code(optional IDE) 一个利用c++编写的简单工程包含着头文件文件夹incliude，源码文件夹src，cmake编译文件夹build，以及main.cpp和CMakeLists.txt等主要文件，其中CMakeLists.txt文件的语言格式要注意 12345678910111213141516cmake_minimum_required(VERSION 2.8)project(HelloSLAM)#指定程序编译的模式set(CMAKE_BUILD_TYPE Debug)#claim the include files&apos; directoriesinclude_directories(&quot;include&quot;)# only src filesadd_library(libHello src/hello.cpp)add_executable(sayHello main.cpp)target_link_libraries(sayHello libHello) 利用vs code配合cmake进行配置以进行代码的编写和调试，也可以选择KDEVELOP和其他的IDE。 熟悉LIinux1.如何在 Ubuntu 中安装软件（命令⾏界⾯）？它们通常被安装在什么地⽅？ apt-get 方式的安装； 普通安装：sudo apt-get install XXX 修复安装：sudo apt-get -f install XXX 重新安装：sudo apt-get -f reinstall XXX dpkg方式的安装 sudo dpkg -i package_name.deb 安装的地方 通常被安装在/usr/bin，/usr/local这个目录下 系统安装软件一般在/usr/share，可执行的文件在/usr/bin，配置文件可能安装到了/etc下等。文档一般在 /usr/share，可执行文件 /usr/bin，配置文件 /etc，lib文件 /usr/lib。 2.linux 的环境变量是什么？我如何定义新的环境变量？ Environment variables defined in this chapter affect the operation of multiple utilities, functions, and applications. linux是一个多用户的操作系统。每个用户登录系统后，都会有一个专用的运行环境。通常每个用户默认的环境都是相同的，这个默认环境实际上就是一组环境变量的定义。用户可以对自己的运行环境进行定制，其方法就是修改相应的系统环境变量。环境变量是一个具有特定名字的对象，它包含了一个或者多个应用程序所将使用到的信息 常见的环境变量： PATH：决定了shell将到哪些目录中寻找命令或程序 HOME：当前用户主目录 MAIL：是指当前用户的邮件存放目录。 SHELL：是指当前用户用的是哪种Shell。 HISTSIZE：是指保存历史命令记录的条数 LOGNAME：是指当前用户的登录名。 HOSTNAME：是指主机的名称，许多应用程序如果要用到主机名的话，通常是从这个环境变量中来取得的。 LANG/LANGUGE：是和语言相关的环境变量，使用多种语言的用户可以修改此环境变量。 使用修改.bashrc文件进行环境变量的编辑，只对当前用户有用。 使用修改/etc/profile文件进行环境变量的编辑，是对所有用户有用。 关于环境变量命令介绍： echo显示某个环境变量值echo$PATH export设置一个新的环境变量exportHELLO=”hello”(可以无引号) env显示所有环境变量 set显示本地定义的shell变量 unset清除环境变量unsetHELLO readonly设置只读环境变量readonlyHELLO 对所有用户生效的永久性变量（系统级）: 这类变量对系统内的所有用户都生效，所有用户都可以使用这类变量。作用范围是整个系统。 设置方式： 用vim在/etc/profile文件中添加我们想要的环境变量,用export指令添加环境变量 当然，这个文件只有在root（超级用户）下才能修改。我们可以在etc目录下使用ls -l查看这个文件的用户及权限 【注意】：添加完成后新的环境变量不会立即生效，除非你调用source /etc/profile 该文件才会生效。否则只能在下次重进此用户时才能生效。 对单一用户生效的永久性变量（用户级）: 只针对当前用户，和上面的一样，只不过不需要在etc下面进行添加，直接在.bash_profile文件最下面用export添加就好了。 这里 .bashrc和.bash_profile原则上来说设置此类环境变量时在这两个文件任意一个里面添加都是可以的。 ~/.bash_profile是交互式login方式进入bash shell运行。 ~/ .bashrc是交互式non-login方式进入bash shell运行。 二者设置大致相同。 就是.bash_profile文件只会在用户登录的时候读取一次 而.bashrc在每次打开终端进行一次新的会话时都会读取。 临时有效的环境变量（只对当前shell有效）: 此类环境变量只对当前的shell有效。当我们退出登录或者关闭终端再重新打开时，这个环境变量就会消失。是临时的。 直接使用export指令添加。 3.linux 根⽬录下⾯的⽬录结构是什么样的？⾄少说出 3 个⽬录的⽤途。 可通过终端查看/目录下的文件： 12cd /ls /bin用户二进制文件包含二进制可执行文件，系统所有用户可执行文件都在这个文件夹里，例如：ls，cp，ping等。 /sbin 系统二进制文件包含二进制可执行文件，但只能由系统管理员运行，对系统进行维护。 /etc配置文件包含所有程序配置文件，也包含了用于启动/停止单个程序的启动和关闭shell脚本。 /dev设备文件包含终端所有设备，USB或连接到系统的任何设备。例如：/dev/tty1、dev/usbmon0 /proc进程信息包含系统进程的相关信息。这是一个虚拟的文件系统，包含有关正在运行的进程的信息。例如：/proc/{pid}目录中包含的与特定pid相关的信息。这是一个虚拟的文件系统，系统资源以文本信息形式存在。例如：/proc/uptime /var变量文件可以找到内容可能增长的文件。这包括 - 系统日志文件/var/log;包和数据库文件/var/lib;电子邮件/var/mail;打印队列/var/spool;锁文件/var/lock;多次重新启动需要的临时文件/var/tmp; /tmp临时文件包含系统和用户创建的临时文件。当系统重新启动时，这个目录下的文件都将被删除。 /usr用户程序包含二进制文件、库文件、文档和二级程序的源代码。/usr/bin中包含用户程序的二进制文件。如果你在/bin中找不到用户二进制文件，到/usr/bin目录看看。例如：at、awk、cc、less、scp。/usr/sbin中包含系统管理员的二进制文件。如果你在/sbin中找不到系统二进制文件，到/usr/sbin目录看看。例如：atd、cron、sshd、useradd、userdel。/usr/lib中包含了/usr/bin和/usr/sbin用到的库。/usr/local中包含了从源安装的用户程序。例如，当你从源安装Apache，它会在/usr/local/apache2中。 /home HOME目录所有用户用来存档他们的个人档案。 /boot引导加载程序文件包含引导加载程序相关的文件。内核的initrd、vmlinux、grub文件位于/boot下。 /lib系统库包含支持位于/bin和/sbin下的二进制文件的库文件.库文件名为 ld或lib.so.* /opt可选的附加应用程序opt代表opitional；包含从个别厂商的附加应用程序。附加应用程序应该安装在/opt/或者/opt/的子目录下。 /mnt挂载目录临时安装目录，系统管理员可以挂载文件系统。 /media 可移动媒体设备用于挂载可移动设备的临时目录。举例来说，挂载CD-ROM的/media/cdrom，挂载软盘驱动器的/media/floppy; /srv服务数据srv代表服务。包含服务器特定服务相关的数据。例如，/srv/cvs包含cvs相关的数据。 4.假设我要给 a.sh 加上可执⾏权限，该输⼊什么命令？ chmod 777 文件名 将文件设置成对拥有者、组成员、其他人可读、可写、可执行。chmod a+x 文件名将文件在原来的配置上增加可执行权限。 5.假设我要将 a.sh ⽂件的所有者改成 xiang:xiang，该输⼊什么命令？ chown xiang:xiang 文件名将文件的所有者改成xiang:xiang SLAM文献阅读1.SLAM 会在哪些场合中⽤到？⾄少列举三个⽅向。 机器人定位导航、手持设备定位、增强现实、自动泊车、语义地图重建等 2.SLAM中定位与建图是什么关系？为什么在定位的同时需要建图？ 定位需要精确的地图，详细的地图需要精确地定位，两者相辅相成，相互依存。重定位和局部建图是slam框架中很重要的两个线程。 定位：机器人必须知道自己在环境中位置； 建图：机器人必须记录环境中特征的位置（如果知道自己的位置）； 机器人在未知环境中从一个未知位置开始移动,在移动过程中根据位置估计和地图进行自身定位,同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。 3.SLAM发展历史如何？我们可以将它划分成哪⼏个阶段？ 1985-1990：, Chatila 和Laumond (1985) and Smith et al. (1990)提出以建图和定位同时进行； 单传感器为外部传感器： 早期： 声呐(Tardós et al. 2002; Ribas et al. 2008)； 后期： 激光雷达(Nüchter etal. 2007; Thrun et al. 2006)； 相机(Se et al. 2005; Lemaire et al. 2007; Davison 2003;Bogdan et al. 2009)； GPS(Thrun et al. 2005a)； 多传感器融合 三个发展阶段： 初始阶段：二十世纪八十年代 发展阶段：二十世纪九十年代 快速发展阶段 4.列举三篇在SLAM领域的经典⽂献。 Smith, R.C. and P. Cheeseman, On the Representation and Estimation of Spatial Uncertainty. International Journal of Robotics Research, 1986. 5 Se, S., D. Lowe and J. Little, Mobile robot localization and mapping with uncertainty using scale­invariant visual landmarks. The international Journal of robotics Research, 2002. 21 Mullane, J., et al., A Random­Finite­Set Approach to Bayesian SLAM. IEEE Transactions on Robotics, 2011 CMake练习 1.程序代码由头文件和源文件组成； 2.带有main函数的源文件编译成可执行程序，其他的编译成库文件； 3.如果可执行程序想调用库文件中的函数，它需要参考该库提供的头文件，以明白调用的格式。同时要把可执行程序链接到库文件上； 在自己的工程目录里面建立子目录build文件夹和src文件夹和libhello文件夹，hello.cpp,hello.h放入libhello中,useHello.cpp放入src中。同时在工程目录下创建一个顶层的CMakeLists.txt文件，内容如下： 123456789101112project(sayHello)cmake_minimum_required(VERSION 2.8)#ADD_SUBDIRECTORY(source_dir [binary_dir] [EXCLUDE_FROM_ALL])#这个指令用于向当前工程添加存放源文件的子目录，并可以指定中间二进制和目标二进制存放的位置add_subdirectory(src) #在 build 目录中将出现一个 src 目录，生成的目标代码 hello 将存放在 src 目录中add_subdirectory(libhello)set(CMAKE_BUILD_TYPE &quot;Release&quot;) 在src文件夹下新建CMakeLists.txt文件，输入如下内容： 12345678910111213141516cmake_minimum_required(VERSION 2.8)#头文件目录include_directories($&#123;PROJECT_SOURCE_DIR&#125;/libhello)set(APP_SRC useHello.cpp)#指定最终的目标二进制的位置#SET(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/bin)#SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/bin)add_executable(sayHello $&#123;APP_SRC&#125;)#为 target 添加需要链接的库target_link_libraries(sayHello libhello) 最后在libhello文件夹下新建CMakeLists.txt文件，输入如下内容： 1234567891011121314151617cmake_minimum_required(VERSION 2.8)set(LIB_SRC hello.cpp)#向 C/C++编译器添加-D 定义add_definitions(&quot;-DLIBHELLO_BUILD&quot;)#添加动态库add_library(libhello SHARED $&#123;LIB_SRC&#125;)#设置动态库输出位置set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)#SET_TARGET_PROPERTIES(target1 target2 ...#PROPERTIES prop1 value1 #prop2 value2 ...)#该指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本set_target_properties(libhello PROPERTIES OUTPUT_NAME &quot;sayhello&quot;) 接着在build文件夹下cmake .., make即可，在biuld文件中的子问夹下的bin文件生成可执行文件sayHello;在lib中生成库文件libhello.so共享库文件。 对于安装路径，可以使用CMAKE_INSTALL_PREFIX命令。 理解ORBSLAM2框架1.git clone https://github.com/raulmur/ORB_SLAM2 2.(a) 6个可执行文件一个库文件 set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${PROJECT_SOURCE_DIR}/lib) (b)include文件夹包含：对应src中程序的代码函数头文件；src文件夹包含：相应程序的代码函数的c++文件；Examples文件夹包含RGB-D文件夹，Stereo文件夹， Monocular文件夹，具体的内容在总目录下的CMakeLists.txt文件中有写： 1234567891011121314151617181920212223242526272829303132# Build examplesset(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/Examples/RGB-D)add_executable(rgbd_tumExamples/RGB-D/rgbd_tum.cc)target_link_libraries(rgbd_tum $&#123;PROJECT_NAME&#125;)set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/Examples/Stereo)add_executable(stereo_kittiExamples/Stereo/stereo_kitti.cc)target_link_libraries(stereo_kitti $&#123;PROJECT_NAME&#125;)add_executable(stereo_eurocExamples/Stereo/stereo_euroc.cc)target_link_libraries(stereo_euroc $&#123;PROJECT_NAME&#125;)set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/Examples/Monocular)add_executable(mono_tumExamples/Monocular/mono_tum.cc)target_link_libraries(mono_tum $&#123;PROJECT_NAME&#125;)add_executable(mono_kittiExamples/Monocular/mono_kitti.cc)target_link_libraries(mono_kitti $&#123;PROJECT_NAME&#125;)add_executable(mono_eurocExamples/Monocular/mono_euroc.cc)target_link_libraries(mono_euroc $&#123;PROJECT_NAME&#125;) (c) OPENCV_LIBS、EIGEN3_LIBS、Pangolin_LIBRARIES、/ORB-SLAM2/Thirdparty/DBoW2/lib/libDBoW2.so、/ORB-SLAM2/Thirdparty/g2o/lib/libg2o.so 运行ORB-SLAM21.编译安装依赖项，可以按照ORB-SLAM2源码的指导进行配置。其他依赖项按照高翔书中所说的安装 1sudo apt-get install libopencv-dev libeigen3-dev libqt4-dev qt4-qmake libqglviewer-dev libsuitesparse-dev libcxsparse3.1.2 libcholmod[tab安装] 其中需要注意Pangolin需要下载手动编译安装，步骤比较简单，不再赘述 12345678910111213sudo apt-get install libglew-dev---sudo apt-get install libboost-dev libboost-thread-dev libboost-filesystem-dev---git clone https://github.com/stevenlovegrove/Pangolin Pangolin--cd Pangolinmkdir buildcd buildcmake ..make sudo make install--- 另外一个注意的点就是opencv库，我是手动下载编译安装的，网上的教程很多，主要是编译时注意cmake的设置，但是我在安装的时候无法被python链接，也就是没有生成cv2.so，导致没办法在Python2和python3中import cv2，估计可能是我之前先装了anaconda的原因，导致了opencv编译忽略了Python路径问题。。。尝试了几次，未果，等以后用到了python再解决这个问题（可以直接用sudo pip3 install opencv-python，如果不想麻烦的话，不过以后可能会出现一些问题） 2.如何将 myslam.cpp或 myvideo.cpp 加⼊到 ORB-SLAM2 ⼯程中？请给出你的 CMakeLists.txt 修改⽅案。 12345678#add myvideo.cpp and myslam.cppset(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;)add_executable(myvideo myvideo.cpp)target_link_libraries(myvideo $&#123;PROJECT_NAME&#125;)add_executable(myslam myslam.cpp)target_link_libraries(myslam $&#123;PROJECT_NAME&#125;) 将myvideo.cpp、myslam.cpp、myvideo.yaml、myslam.yaml以及myvideo.mp4都放到ORB-SLAM2目录下，再次编译ORB-SLAM2后，就会生成可执行文件，然后终端输入./myvideo和./myslam就可以直接运行了。 这里有个问题，一开始我是根据CMakeLits.txt中上面的语句来写的，也就是我把myvideo.cpp、myslam.cpp放在了、Examples/Monocular文件夹中，然后进行编译，结果虽然生成了可执行文件，但是程序无法运行，终端要么提示无法读取对应的.yaml的设置，要么出现segmentation fault错误，最后放到ORB-SLAM2总目录下就没问题了。。。目前不知道什么原因，等后面再看看。 因为ORB-SLAM2开源的版本没有稠密建图功能，因此如果想尝试稠密建图功能，可以参考高翔博士的ORB-SLAM2_modofied，利用PCL工具实时拼接RGB-D图像，效果还可以，不过没有原作者视频中最后的那个效果好，安装步骤在这里。]]></content>
      <categories>
        <category>科研记录</category>
      </categories>
      <tags>
        <tag>visual SLAM</tag>
        <tag>Linux</tag>
        <tag>C++11</tag>
        <tag>Computer vision</tag>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beyond Supervised Learning-A Computer Vision Perspective]]></title>
    <url>%2F2019%2F03%2F02%2FBeyond-Supervised-Learning-A-Computer-Vision-Perspective%2F</url>
    <content type="text"><![CDATA[​ About这篇论文（链接）是印度的几位教授写的，以计算机视觉的角度来阐述目前在全监督学习和无监督学习之间的几种训练方式，写得比较简略，但是适合我这种刚入门连名词都不是很清楚的人，可以让我用来梳理整个领域的技术发展脉络和现状。同时该论文也给出了大量的参考文献，也方便进行下一步的研究。实际上，严格地说，这篇文章不能算一篇合格的综述，只能说是那种“扫盲”的阅读材料。 出现上述技术发展的主要原因是数据集比较庞大，但是完全被标注好的数据却很少，而训练好的网络在面对现实多变的复杂场景时依然会出现问题，因此这两者之间出现了合成数据（synthetic data)，迁移学习(transfer learning)，弱监督学习(weakly supervised learning)，少量学习（Few-shot learning/K-shot learning)以及自监督学习(self-supervised learning)，边界当然就是无监督学习了(unsupervised learning)。其中需要注意，self-supervised learning和unsupervised learning不是一回事，前者仍然是有监督学习的一种，正如Quora中的回答： Self-supervised, imposes classification by itself, derivative from the input data. Unsupervised, does not. At all. Meaning, that self-supervised, STILL has a measurement of terms of right contra wrong, as in, terms of classification. Unsupervised, does not. 而目前，self-supervised learning正是Yann lecun在2019的ISSCC上所说的最新“蛋糕”组成成分。 不过在阅读之前，首先解决一个问题是：什么是machine learning中的“label”和“feature”? 简短地说，feature就是输入，label就是输出。一个feature就是我们输入网络数据中的一列，比如说，我们要分类猫和鸟，我们输入特征可能包括颜色，大小，重量等，label就是输出的结果，就是图片的动物是猫还是鸟。 Quora上的一个回答比较系统点，这里直接贴出来： Imagine how a toddler might learn to recognize things in the world. The parent, often sits with her and they read a picture book, with photos of animals. The parent teaches the toddler but pointing to the pictures and labeling them: “this is a dog”, “this is a cat”, “this is a tree”, “this is a house”, … After she has learned enough examples, the toddler gets a grasp on what are the key features of a dog, vs these of a cat or of a house or of a tree. So when the toddler and her dad walk in the park, and the father points to a thing she has never seen before, and asks her “what’s that?“, she is capable of generalising her learning, and correctly answer “it’s a dog!“. What might have happened behind the scenes is that consciously or subconsciously, the toddler extracted and examined the features of the “thing” - four legs, fluffy white hair, sticking tounge, round black eyes, etc - which aligned nicely with her preception of a “dog”, and did not match any of the other entity types she knows. So labels were the ground truth categories provided explicitly by her father during learning. And features are what she inferred from these implicitly, and then extracted and examined at “run time” in the park. Similarly, in supervised learning, the NN learns by examples: an experts gives it many training examples, each example explicitly labled with the ground truth answer. The NN tries to predict these labels by modifying the values of the parameters of the NN. When an input comes, different areas of the network becomes active (I.e receive high value), depending on the input. These represent the features of the specific input. The output of the network (e.g classification decision) is a function of which of the parameters are active. So the labels are explicitly given by the trainer during the training, and the features is the configuration of the network - which is implicitly learned by the network, as guided by the labels. It’s these features that then define the output of the network for a given input in runtime.Want to learn more or see how these features are represented? See this great article: Feature Visualization 整个supervised learning到unsupervised learning的发展就是人工标注的工作量太大，因为“label”这个东西需要人去手工注解，而且越精细，越准确就使训练结果越好，但是同时也会带来不适应现实数据变化的问题，鲁棒性不好。另外一方面，未标注的数据，“粗糙”的原始数据却很多，同时也比较容易抓取，所以慢慢地会向unsupervised learning发展。 Structure该论文的整个写作框架如下： Abstract Introduction Notations and Definitions Success of Supervised Learning Effectiveness of Synthetic Data Domain Adaption and Transfer Learning Weakly Supervised Learning Incomplete Supervision Inexact Supervision Inaccurate Supervision K-Shot Learning Self-Supervised Learning Conclusion and Discussion 从整体上看，文章按照监督强度来写的，然后介绍了一些方法出现的原因，同时提及了一些关键的技术，对于更深的探讨没有做进一步地表述，而是直接给出了参考文献。 论文的内容：First, we summarize the relevant techniques that fall between the paradigm of supervised and unsupervised learning. Second, we take autonomous navigation as a running example to explain and compare different models. Finally, we highlight some shortcomings of current methods and suggest future directions. ContentReason Supervised deep learning-based techniques require a large amount of human-annotated training data to learn an adequate model. It is not viable to do so for every domain and task. Particularly, for problems in health care and autonomous navigation, collecting an exhaustive data set is either very expensive or all but impossible. (训练数据集大) Even though supervised methods excel at learning from a large quantity of data, results show that they are particularly poor in generalizing the learned knowledge to new task or domain. This is because a majority of learning techniques assume that both the train and test data are sampled from the same distribution. (功能单一性太强) Two bottlenecks of fully supervised deep learning methods—(1) lack of labeled data in a particular domain; (2) unavailability of direct supervision for a particular task in a given domain. Categories of Methods1.Data-centric techniques which solve the problem by generating a large amount of data similar to the one present in the original data set. Data-centric techniques include data augmentation which involves tweaking the data samples with some pre-defined transformations to increase the overall size of the data set. Another method is to use techniques borrowed from computer graphics to generate synthetic data which is used along with the original data to train the model. 2.Algorithm-centric techniques which tweak the learning method to harness the limited data efficiently through various techniques like on-demand human intervention, exploiting the inherent structure of data, capitalizing on freely available data on the web or solving for an easier but related surrogate task. Algorithm-centric techniques try to relax the need of perfectly labeled data by altering the model requirements to acquire supervision through inexact , inaccurate , and incomplete labels. Another set of methods exploit the knowledge gained while learning from a related domain or task by efficiently transferring it to the test environment. 3.Hybrid techniques which combine ideas from both the data and algorithm-centric methods. Hybrid methods incorporate techniques which focus on improving the performance of the model at both the data and algorithm level. （比如自动驾驶中的城市道路场景理解任务中，先用合成数据进行训练，然后进行domain shift去适应真实的场景理解任务） Simple Math Definition假设${\cal X}$和${\cal Y}$分别是输入和标签空间（也就是输出了），在一般的机器学习问题中，我们假设要从数据集中学习N个objects。我们从这些objects提取特征去训练模型，即$X = ({\cal x}_{1}, {\cal x}_{2}, \cdots, {\cal x}_{N})$,$P(X)$是$X$上的边缘概率（marginal probability）。在fully supervised learning中，通常假设有相对应的标签$Y = ({\cal y}_{1}, {\cal y}_{2}, \cdots, {\cal y}_{N})$。 学习算法就是在假设空间（hypothesis space）${\cal F}$中寻找函数$f: {\cal X} \rightarrow {\cal Y}$，同时在空间${\cal L}$定义了损失函数（loss function)​ $l: {\cal Y} \times {\cal Y} \rightarrow \mathbb {R}^{\geq 0}$来衡量函数的适用性。同时机器学习算法也会最小化误差函数（错误预测）$R$来提升函数的正确性： R = \frac{1}{N}\sum ^{N} _{n=0}l({\cal y}_{i}, f({\cal x}_{i}))对synthetic data来说，输入空间发生了变换，设为${\cal X}_{synth}$，标签空间不变，此外，由于输入特征空间个边缘概率分布都发生了变化，因此利用新的domain${\cal D}_{synth}=\lbrace {\cal X}_{synth}, P({\cal X}_{synth}) \rbrace$来代替原来的real domain${\cal D}=\lbrace {\cal X}, P({\cal X}) \rbrace$。因此，我们也不可能用之前的预测函数$f_{synth}: {\cal X}_{synth} \rightarrow {\cal Y}$ 来构建${\cal X}$到${\cal Y}$的映射。 迁移学习（transfer learning）就是用于解决domain adaptation(DA)问题的技术，其不仅可以用于不同domain之间，也可以用于不同的task之间。根据input feature space 在source and target input distribution的分布是否相同，即 ${\cal X}_{s}$是否等于${\cal X}_{t}$，DA可以分为homogeneous DA和heterogeneous DA（同质和异质），显然heterogeneous DA问题要更为复杂些。 通常情况下，训练时，supervised learning认为所有的feature sets ${\cal x}_{i}$会有相应的标签${\cal y}_{i}$与之对应，实际情况是，这些标签在实际场景中可能是 $ {\tt incomplete, inexact, inaccurate}$的，因此可能就要在weakly supervised learning技术框架下训练，比如说针对那些在网络抓取的数据而言。针对incomplete的标签场景而言，定义feature set $X = ({\cal x}_{1}, {\cal x}_{2}, \cdots, {\cal x}_{l}, {\cal x}_{l+1}, \cdots, {\cal x}_{n})$，其中$X_{labeled} = ({\cal x}_{1}, {\cal x}_{2}, \cdots, {\cal x}_{l})$会有对应的标签$Y_{labeled} = ({\cal y}_{1}, {\cal y}_{2}, \cdots, {\cal y}_{l})$供其训练，但是$X_{unlabeled} = ({\cal x}_{l+1} \cdots, {\cal x}_{n})$就没有任何对应的标签了。此外，一些其他的weakly supervised模型包含一些有着多种标签的单个实例或者多个实例共享一个标签（multiple-instace single-label)。这个时候会对feature set中的${\cal x}_{i}$进行打包处理，即${\cal x}_{i,j}, j=1,2,\cdots, m$。 尽管上述技术框架对应着不同程度的supervision，但是都需要大量的实例instances $X$来训练模型。如果某些class没有足够的instances的话就会使得训练不理想，因此出现了Few-shot learning(少量学习)和Zero-shot learning(ZSL)。 如果没有了监督信号（supervision signal），可以利用instances的内在结构（inherent structure）去训练模型。假设$X$和$Y$分别是feature set和label set，此时$P(Y|X)$无法得出，也无法确立任务${\cal T} = \lbrace {\cal Y}, P(Y|X) \rbrace$，不过我们可以定义一个proxy task${\cal T}_{proxy} = \lbrace Z, P(Z|X)\rbrace$，label set$Z$可以自己从数据中提取。For computer vision problems, proxy tasks have been defined based on spatial and temporal alignment, color, and motion cues. Effectiveness of Synthetic Data（个人感觉还是与RL相关的）supervised learning在很多方面都取得了成功，但是存在的问题是需要大量的标注数据进行训练，即使是训练好了，也难以适应现实环境。 随着计算机图形学的发展，synthetic data十分易得，而且提供了精确的ground truth，同时数据的可操作性可以让其模拟任何真实的环境场景，甚至是真实环境下难以发生的。 In the visual domain, synthetic data have been used mainly for two purposes: (1) evaluation of the generalizability of the model due to the large variability of synthetic test examples, and (2) aiding the training through data augmentation for tasks where it is difficult to obtain ground truth, e.g., optical flow or depth perception.(测试模型的普适性和增强训练过程) 此外，还有直接在真实图像中加入synthetic data，比如在KITTI数据集中加入车辆3D模型，以辅助模型训练。 One drawback of using syntheticdata for training a model is that it gives rise to “sim2real” domain gap. 这个sim2real是与RL相关的概念，我这里找了一个CMU的PPT，可以看下。Recently, a stream of works in domain randomization claims to generate synthetic data with sufficient variations, such that the model views real data as just another variation of the synthetic data set. 将真实的数据视为合成数据的一个变体。 One of the major challenges in using synthetic data for training is the domain gap between real and synthetic data sets. 而迁移学习它提供了一些解决办法。 Domain Adaptation and Transfer LearningA model trained on source domain does not perform well on a target domain with different distribution. Domain adaptation(DA) is a technique which addresses this issue by reusing the knowledge gained through the source domain for the target domain. DA techniques根据三个不同的标准进行分类，这三个标准（criteria）是： distance between domains presence of supervision in the source and target domain type of domain divergences Prevalent literature also classifies DA in supervised, semi-supervised, and unsupervised setting according to the presence of labels in source and target domain. Earlier works categorized the domain adaptation problem into homogeneous and heterogeneous settings. 现在，随着深度学习的热度提高，DA开始引入DNN和GAN的思想。DA使用DNN 去learn representations invariant to the domain，Adversarial methods encompass a framework which consists of a label classifier trained adversarially to the domain classifier. Weakly Supervised LearningWeakly supervised learning is an umbrella term covering the predictive models which are trained under incomplete, inexact, or inaccurate labels. Apart from saving annotation cost and time, weakly supervised methods have proven to be robust to change in the domain during testing. Incomplete Supervision(标签不全)Weakly supervised techniques pertaining incomplete labels make use of either semi-supervised or active learning methods. The conventional semi-supervised approaches include self-training, co-training, and graph-based methods. Inexact Supervision(标签注解程度，比如一幅图像的bounding box标注和pixel-level标注)Apart from dealing with partially labeled data sets, weakly supervised techniques also help relax the degree of annotation needed to solve a structured prediction problem. A popular approach to harness inexact labels is to formulate the problem in multiple-instance learning (MIL) framework. In MIL, the image is interpreted as a bag of patches. If one of the patches within the image contains the object of interest, the image is labeled as a positive instance, otherwise negative. Learning scheme alternates between estimating object appearance model and predicting the patches within positive image. Inaccurate Supervision(策划大的数据集给模型训练成本昂贵，同时效果也不一定很好。如果采用从网站抓取数据的方式来给模型训练，可能比较实际，但是这些原始的数据集存在噪声，给训练算法提出了挑战。 “webly” supervised scenario)Broadly, we categorize the techniques into two sets—the first approach resorts to treating the noisy instances as outliers and discard them during training. Another stream of methods focus on building algorithms robust to noise by devising noise-tolerant loss functions or adding appropriate regularization terms. K-Shot Learning(样本少-Few-shot learning和Zero-shot learning)Few-shot learning techniques attempt to adapt the current machine learning methods to perform well under a scenario where only a few training instances are available per class. More recent efforts into a few-shot learning techniques can be broadly categorized into metric-learning and meta-learning-based methods. Metric-learning aims to design techniques forembedding the input instances to a feature space beneficial to few-shot settings. A common approach is to find a good similarity metric in the new feature space applicable to novel categories. Meta-learning entails a class of approaches which quickly adapt to a new task using only a few data instances and training iterations. To achieve this, the model is trained on a set of tasks, such that it transfers the “learning ability” to a novel task. In other words, meta-learners treat the tasks as training examples. Another set of methods for few-shot learning relies onefficient regularization techniques to avoid over-fitting on the small number ofinstances. Literature pertaining to Zero-Shot Learning (ZSL) focuses on finding the representation of a novel category without any instance. Methods used to address ZSL are distinct from few-shot learning. A major assumptiontaken in this setting is that the classes observed by model during training are semantically related to the unseen classes encountered during testing.This semantic relationship is often captured through class-attributes containing shape, color, pose, etc., of the object which are either labeled by experts or obtained through knowledge sources such as Wikipedia, Flickr, etc. In ZSL, a joint embedding space is learned during training where both the visual features and semantic vectors are projected. During testing on unseen classes, nearest-neighbor search is performed in this embedding space to match the projection of visual feature vector against a novel object type. Self-supervised Learning(without any external supervision)Explicit annotation pertaining to the main task is avoided bydefining an auxiliary task that provides a supervisory signal in self-supervised learning. The assumption is that successful training of the model on the auxiliary task will inherently make it learn semantic concepts such as object classes and boundaries. This makes it possible to share knowledge between two tasks. However, unlike transfer learning, it does not require a large amount of annotated data from another domain or task. The existing literature pertaining self-supervision relies on using the spatial and temporal context of an entity for “free” supervision signal. 上图比较了supervised learning / weakly-supervised learning / self-supervised learning之间的区别，supervised learning需要bounding box作为label进行训练 ；weakly-supervised learning使用图像级别的标题，语义描述嵌入等进行神经网络预训练；self-supervised learning使用pretext task来学习物体的表示方法。 Conclusion and DiscussionThe space between fully supervised and unsupervised learning can be qualitatively divided on the basis of the degree of supervision needed to learn the model. While synthetic data are cost effective and flexible alternative to real-world data sets, the models learned using it still need to be adapted to the real-world setting. Transfer learning techniques address this issue by explicitly aligning different domains through discrepancy-based or adversarial approaches. However, both of these techniques require “strict” annotation pertaining to the task which hinders the generalization capability of the model. Weakly supervised algorithms relax the need of exact supervision by making the learning model tolerant of incomplete, inexact, and inaccurate supervision. This helps the model to harness the huge amount of data available on the web. Even when a particular domain contains an insufficient number of instances, methods in k-shot learning try to build a reasonable model using parameter regularization or meta-learning techniques. Finally, self-supervised techniques completely eliminate the need of annotation as they define a proxy task for which annotation is implicit within the data instances. Despite their success, recent models weigh heavily on deep neural networks for their performance. Hence they carry both the pros and cons of using these models; cons being lack of interpretability and outcomes which largely depend on hyperparameters.]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>survey</tag>
        <tag>supervised learning</tag>
        <tag>domain adaptation</tag>
        <tag>deep neural network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Brief Review of Object Detection and Semantic Segmentation]]></title>
    <url>%2F2019%2F02%2F27%2FA-Brief-Review-of-Object-Detection-and-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[​ About目标检测（object detection）和语义分割（semantic segmentation）是计算机视觉的两个重要研究内容，在人脸检测，视频监控和自动驾驶中都有很多的应用，它们对于机器理解环境也具有一定的作用。在视觉SLAM上，已经有科研工作者将这些研究成果加入到现有的SLAM系统中，以提高系统的鲁棒性，同时促进语义SLAM的发展。因此，需要对这些工作进行一些调研总结和学习理解。由于对深度学习不了解，还未系统地进行学习，所以写的内容可能幼稚了些，后续会再进行更新。 值得注意的是，目标检测在经典计算机视觉中也已经提出了一些特征描述的方法，在此不涉及，只涉及基于深度学习的目标检测方法，毕竟神经网络在图片特征的学习上比较擅长。 本博客将基于文末给出的参考文献来进行总结和学习，尽力把这些问题和技术梳理清楚，同时加入一些自己的思考和不成熟的想法。 Task Definition image classification—multi level 图像分类，这个任务指的是给出一张图片，要识别出哪张图片属于哪个类别； object detection—what categories and location 物体检测，这个任务除了需要识别出图片属于哪个类别，还需要对相应的物体进行具体位置的定位，我们通常用矩形框来框出这个物体； semantic segmentation—pixel wise, but does not distinct everything in one category 语义分割，这个任务是指对图片中的每个 pixel 打上标签，比如这里要给它们打上 person、sheep、dog 的标签，需要进行非常精细的分类； instance segmentation—object detection+semantic segmentation 实例分割，可以理解为进行物体检测后，对每个矩形框中的物体进行语义分割，该任务除了需要找到物体的类别和位置之外，还需要分辨出不同物体的 pixel； Object DetectionProgress (一步法single-stage, 两步法two stage)最近有一篇很好的综述[5]详细地梳理了最近5年来的generic object detection技术发展和比较的综述。 上图绿色的字体表示的是 Two-stage Detector 的发展历程，当然还有很多其他的方法，这里列出的是一些比较有代表性的方法。 2014 年有一项很重要的工作是 R-CNN，它是将物体检测首次应用于深度学习中的一篇论文，它的主要思路是将物体检测转化为这么一个问题：首先找到一个 region（区域），然后对 region 做分类。之后作者又提出了 Fast R-CNN，它是一个基于 R-CNN 的算法，运算速度显著提高。 2015 年，这一群人又提出了 Faster R-CNN，它在速度上相比 Fast R-CNN 有了更大的提高，主要是改进了怎样在 Fast R-CNN 和 R-CNN 中找 region 的过程，Faster R-CNN 也是用深度学习的方法得到一些 region（称之为 proposal），然后再用这些 proposal 来做分类。虽然距离 Faster R-CNN 的提出已经三年多了，但它依旧是使用非常广泛的一种算法。 2016 年，代季峰等人提出了 R-FCN，它在 Faster R-CNN 的基础上进行了改进，当时它在性能和速度都有非常大的提高。 2017 年有两篇影响力非常大的论文，FPN 和 Mask R-CNN。FPN 也就是 Feature Pyramid Network，何恺明大神的论文，它相当于生成了 feature pyramid，然后再用多个 level 的 feature 来做 prediction。Mask R-CNN 这篇论文获得了 ICCV 2017 的最佳论文，也是何恺明大神的作品，其在 Faster R-CNN 基础上增加了 mask branch，可以用来做实例分割，同时因为有 multi-task learning，因此它对物体框的性能也有很大的提高。（另外，今年年初，2019.1月，何恺明将自己的Mask R-CNN和FPN合在了一起，效果也不错）。 2018 年，沿着 Faster R-CNN 这条路线提出的方法有 Cascade R-CNN，它将 cascade 结构用在了 Faster R-CNN 中，同时也解决了一些 training distribution 的一些问题，因此它的性能是比较高的。另外还有两篇比较重要的论文——Relaiton Network 和 SNIP。 上图蓝色字体表示的 Single-stage Detector 的发展历程：2014 年的 MultiBox 是非常早期的工作；2016 年提出了 SSD 和 YOLO，这两篇论文是 Single-stage Detector 中的代表作；2017 年提出了 RetinaNet（当时 Single-stage Detector 中性能最高的方法）和 YOLO v2；2018 年有一个新的思路，提出了 CornerNet，把物体检测看成一堆点的检测，然后将这些点 group 起来。 General Pipeline of Object Detection参考文献[2]很好地解释了深度学习进行object detection的思路是什么。 如前所述，基于深度学习的Object Detection的方法主要有两条发展脉络，一个是single-stage detector，另一个是two-stage detector. Two-stage detector 第一部分是 Feature generation：首先图片经过 backbone（分类网络）后，会生成 feature； 之后 feature 或者直接进行 prediction，或者再过一个 neck 进行修改或增强； 第二部分是 Region proposal：这个部分是 Two-stage Detector 的第一个 stage，其中会有一堆 sliding window anchor（预先定义好大小的框），之后对这些框做 dense 的分类和回归；接着再筛除大部分 negative anchor，留出可能是物体的框，这些框称之为 proposal； 第三个部分是 Region recognition：有了 proposal 后，在 feature map 上利用 Rol feature extractor 来提取出每一个 proposal 对应的 feature（Rol feature），最后会经过 task head； Single-stage detector 图片首先经过 feature generation 模块，这里也有 sliding window anchor，但是它们不是用来生成 proposal 的，而是直接用于做最终的 prediction，通过 dense 的分类和回归，能直接生成最终的检测结果。 一步法和两步法的区别到底在哪里？ 实际上，现在的二步法object detection的框架都是基于Faster R-CNN的，一步法中的比较全面的算法是SSD，这两个在后面我都会进行详细地阐述。 对于两步法的基础Faster R-CNN，两个阶段分别在RPN和R-CNN中完成，第一阶段是预设一系列不同大小和比例的anchors，然后将整张图传入CNN提取特征，最后利用PRN对anchors进行分类和回归，得到候选区域（proposals）；第二阶段是利用RolPooling扣取每个候选区域的特征，接着把扣取特征的特征送入后续R-CNN网络，最后对候选区域进一步分类和回归，得到最终的检测结果。 由上图也可以看出，两步法是相对于一步法多了二阶段的分类，回归和特征，因而精度会更好，但是会使得算法运行的时间加长。也就是说，一步法也就是类似于只有RPN阶段，而二步法多了R-CNN，使得分类更加精细，同时再对候选区域进行回归。 那么一步法是否可以仿照这样的思路或者利用其他的方法达到二步法的精度，却又不损失运行效率？ 在参考文献[3]里面，张士峰博士分享了他们的工作Single-Shot Refinement Neural Network for Object Detection，在SSD中加入一些类似R-CNN作用的模块，来提高检测的精度，同时也保持了原有的运行效率。 整个结构包含ARM模块，TCB连接和ODM模块，其中ARM模块和ODM模块分别对应二步法中的RPN和R-CNN的功能，TCB的主要作用是转换ARM特征，融合高层特征，因为两步法中的特征不相同，第二阶段会提取新的特征，因此需要转换和融合。该方法与二步法的区别在于没有RoIPooling，因此运行时间不会边长。该方法的精度大概较SSD提升了两个点，速度上会更快些，因为输入图像的分辨率可以是320x320的。具体的检测框架和测试结果可以看原论文。 借此提下object detection近年来的发展趋势应当不再是该网络调参来刷速度和正确率，而是回归到框架本身的反思和设计上，毕竟现有的算法都是从Faster R-CNN来的，因此是否可以反思该框架的问题，然后对其进行改善，或者直接提出新的检测思路，比如说在前处理阶段的anchors和后处理阶段的NMS都是手动设置的，这个是否可以进行自动调节，也是一个值得研究的点。 另外，在视觉SLAM和很多其他的应用场景上，输入的都是视频流的形式，而不是单个的图像，这一方面对算法的实时性，硬件的运算速度提出了要求，另一方面也对视频物体检测提出了要求，如何利用帧与帧之间的信息来加速物体检测，也是有待研究的。 最后一点就是，视觉SLAM的最终梦想是能在现实大环境下进行运行，然后给出环境地图，以此与人进行交互。现实场景很复杂，充满动态的，不确定性的因素，因此这样的功能要想实现还有很长的路要走。不过可以预见的是，视觉SLAM需要结合这些新的，基于学习方法的计算机视觉的研究成果，以帮助机器人来理解环境，提高系统的鲁棒性，同时各个模块和任务之间也将是相互联系相互促进的，多任务联合（multi-task），以此提升系统的容错性 和运行时间。 Faster R-CNNFaster R-CNN 是 Two-stage Detector 工作的基础，它主要提出了两个东西: RPN：即 Region Proposal Network，目前是生成 proposal 的一个标准方式; Traning pipeline：主要讲的是 Two-stage Detector 应该怎样 train，论文里给出的是一种交替(alternating training)的方法; 因为深度学习的发展，大家都不想一步一步来回调参，因此会有了joint training这个东西。 FPN FPN（Feature Pyramid Network）主要也是提出了两项重要的思路：Top-down pathway 和 Multi-level prediction。 下图中的 4 张图代表了基于单个或多个 feature map 来做预测的 4 种不同的套路： 具体实现如下图所示： MASK R-CNNMask R-CNN 主要也有两点贡献：RoIAlign和Mask branch RoIAlign：在 Mask R-CNN 之前，大家用得比较多的是 Rol Pooling，实现过程是：给出一个框，在 feature map 上 pool 出一个固定大小的 feature，比如要 pool 一个 2×2 的 feature，首先把这个框画出 2×2 的格子，每个格子映射到 feature map，看它覆盖了多少个点，之后对这些点做 max pooling，这样就能得出一个 2×2 的 feature。它的劣势是如果框稍微偏一点，得出的 feature 却可能是一样的，存在截断误差。RolAlign 就是为了解决这一问题提出的。Rol Align 并不是直接对框内的点做 max pooling，而是用双线性插值的方式得到 feature。其中还有一步是：在 2×2 的每个框中会选择多个点作为它的代表，这里选择了 4 个点，每个点分别做双线性插值，之后再让这 4 个点做 max/average pooling Mask branch：它是一个非常精细的操作，也有额外的监督信息，对整个框架的性能都有所提高。它的操作过程如下图所示: Cascade R-CNN Cascade R-CNN 是目前 Faster R-CNN 这条线中较新的方法。这个方法也提出了两点贡献：一是提出了使用 cascade architecture；二是提出了怎样来适应 training distribution。 Cascade architecture ：这个框架不是特别新的东西，之前也有类似的结构。下图左边是 Faster R-CNN，右边是 Cascade R-CNN。其中 I 代表图像或者图像生成的 feature map，H0 是 RPN，B 是 bounding box regression，C 是 classification。经过 RPN 得到的 proposal 再做 pooling 后，会有分类和回归这两个 prediction。 Cascade R-CNN 的结构是，在经过第一次分类和回归之后，会用得到 bounding box 再来做一次 pooling，然后对这些框做下一阶段的分类和回归，这个过程可以重复多次。但如果仅仅使用 Cascade R-CNN 而不做其他改变，Cascade R-CNN 带来的提高是非常有限的。 Cascade R-CNN 提出了一个很好的 motivation，这是它比较有意义的一个地方。它研究了一下采用不同的 IoU 阈值来进行 training 的情况下，Detector 和 Regressor 的性能分布。 RetinaNet RetinaNet 是 Singe-stage Detector 目前比较重要的一项工作，它可以看做是由 FPN+Focal Loss 组成的，其中 FPN 只是该论文中用到的架构，而 Focal Loss 则是本论文主要提出的工作。 RetinaNet 结构如下： RetinaNet 的结构 和 SSD 非常类似，只不过它用的是 ResNet，并在 ResNet 上加入了 FPN 的结构，每一层都有两个分支：一个用来做分类；另一个用来做框回归。此外它的每个 head 都比 SSD 和 Faster R-CNN 都要大一点，这样的话，它的参数量比较大，计算速度也比较慢。 而 Focal Loss 试图解决的问题是 class imbalance。针对 class imbalance 的问题，Two-stage Detector 一般是通过 proposal、mini-batch sampaling 两种方式来解决的；SSD 是通过 hard negative mining 来解决的；而 RetinaNet 则通过 Focal Loss 来解决该问题。 Focal loss 的核心思路是：对于 high confidence 的样本，给一个小的 loss——这是因为正负样本不平衡，或者说是由于 class imbalance 导致了这样的问题：比如说正负样本的比例是 1:1000，虽然负样本的 loss 都很小，但数目非常多，这些负样本的 loss 加起来的话，还是比正样本要多——这样的话，负样本就会主导整个框架。 Relation Network 在 Relation Network 之前的大部分 detectcor，在做 prediction 或 training 的时候通常只考虑到当前这一个框，而 Relation Network 提出还要考虑这个框周围的框，并基于此提出了一个 relation module，可以插在网络的任何位置，相当于是 feature refinement。Relation module 如下图所示： 它的核心思想是：当前框的 feature 除了由当前框决定之外，还要考虑当前框和周围框及其它框的关系 SNIP SNIP（Scale Normalization for Image Pyramids）另一篇比较有启发性的工作。它提出的问题是：在 train 分类器的时候，究竟是要 scale specific 还是 scale invariant。传统的 detector 通常会选择 scale invariant，但 SNIP 研究了一下之前的方法后，发现之前的训练方式得到的 feature 对 scale 并没不是很 robust，因而提出要尽量减少 scale 的 variance，让训练时的 scale 尽可能地相似。 SNIP 结构图如下: CornerNet CornerNet，是 Singe-stage Detector 中比较新的方法，其与其他方法最不一样的地方是：之前的方法会在图像上选出框，再对框做分类的问题；CornerNet 则是在图中找到 pair 的关键点，这个点就代表物体。它的 pipeline 包括两步：第一步是检测到这样的 corner，即 keypoint；第二步是 group corner，就是说怎样将同一个物体的左上顶点和右下顶点框到一起。 其算法结构如下： mmdetection港中文多媒体实验室的开源物体检测框架。 References[1]. 物体检测算法的近期发展及开源框架介绍-陈恺[2]. 基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN[3]. 基于深度学习的物体检测算法对比探索 - 张士峰[4]. 基于深度学习的目标检测算法近5年发展历史（综述）[5]. Deep Learning for Generic Object Detection: A Survey[6]. 目标检测算法中检测框合并策略技术综述[7]. 综述 | CVPR2019目标检测方法进展]]></content>
      <categories>
        <category>科研记录</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>computer vision</tag>
        <tag>object detection</tag>
        <tag>semantic segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04下安装NVIDIA最新驱动以及深度学习环境配置]]></title>
    <url>%2F2019%2F02%2F18%2FUbuntu16-04%E4%B8%8B%E5%AE%89%E8%A3%85NVIDIA%E6%9C%80%E6%96%B0%E9%A9%B1%E5%8A%A8%E4%BB%A5%E5%8F%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[2019.3.8日来更，可以不用看以前下面写的内容了，因为不是很完整。。。 如果按照之前说的通过官网下载的run文件来手动安装驱动，可能出现nvidia-settings打不开的情况，比如我就是输出信息：“ERROR: Unable to load info from any available system”，这样会导致我的电脑识别不了外接显示器。。。 后来在网上看了一些人的回答，大致的原因是官网的run文件在安装时后会默认更改一些系统配置，因此可能会导致一些错误。。 解决办法：通过Ubuntu自动的install功能进行安装 1.先按照这个教程卸载驱动，然后禁用好nouveau； 2.重启后确认nouveau已禁用，关闭图形界面，进入命令行界面； 3.输入下面的命令： 1234sudo add-apt-repository ppa:graphics-drivers/ppasudo apt-get updatesudo apt-get install nvidia- #这里选择你要安装的版本，最新的应该也没问题，我就是直接装的410，省得到时候又得重装CUDA和CUDNNreboot #重启 4.再次开机后应该就没问题了，nvidia-smi, nvidia-settings都没问题了，外接显示器也正常了。 写在前面本人电脑是战神神舟Z7M-KP5GZ，显卡是GTX1050TI，按照以前利用liveusb的方法安装了Ubuntu16.04双系统，然而第一次安装NVIDIA驱动的时候有些问题，直接是在软件更新里面安装的系统自带驱动，版本384。后来配置深度学习环境需要安装CUDA和CUDNN，由于安装最新版本的CUDA需要相应的驱动版本对应，因此需要对NVIDIA驱动进行更新。 在安装过程中，遇到了一些问题，主要是： 驱动安装后Ubuntu系统无法进入，卡在启动界面的5个原点； 成功进入系统后，分辨率低； 后来根据几篇博客解决了上述问题： 博客1 博客2 博客3 博客4 后来总结了下，最主要的问题是一定要禁用nouveau，可以通过下面的命令看是否禁用，如果不输出信息则是成功禁用。。一定要注意，如果之前装过了NVIDIA驱动，然后卸载重装了一定要记得再次禁用！！！！ 1lsmod | grep nouveau 利用run方式安装NVIDIA最新驱动1.验证nouveau是否已禁用 1lsmod | grep nouveau 2.如果没有输出信息，则说明已经禁用，如果没有，编辑文件blacklist.conf 1sudo vim /etc/modprobe.d/blacklist.conf 在文件最后部分插入以下两行内容(i插入，esc退出编辑，:wq退出并保存) 12blacklist nouveauoptions nouveau modeset=0 更新系统 1sudo update-initramfs -u 重启系统，再看看是不是禁用了nouveau 3.在英伟达官网查找并下载自己对应的驱动(.run文件) 4.ctrl+alt+F1进入命令行界面，登陆进入，并关闭图形界面 1sudo service lightdm stop 5.卸载原有的驱动 123456apt-get purge nvidia-*nvidia-smi#如果还有信息，输入下面的代码sudo /usr/bin/nvidia-uninstallnvidia-smi#没有驱动信息则成功 6.进入驱动文件夹 12sudo chmod a+x NVIDIA-Linux-x86_64-xxx.x.run #这里版本号每个人可能不同(xxx.xx代表版本号)sudo ./NVIDIA-Linux-x86_64-xxx.xx.run -no-x-check -no-nouveau-check -no-opengl-files 7.安装过程中的选项很重要，如果选错了，不要重启，重新安装就行了 其中register the kernel module souces with DKMS选择NO，Nvidia’s 32-bit compatibility libraries也选NO，其他的YES或者正常就行。 8.检查是否安装完成 1234modprobe nvidia #挂载驱动nvidia-smi#如果出现驱动信息表示安装成功sudo service lightdm start #重新进入图形界面 9.如果重启开机没问题，那么万事大吉，如果进不去，或者卡在启动界面，可以选择recovery mode进入系统，然后修改grub文件，我是依据博客3解决了我的问题，我看了下，应该还是nouveau的问题。 每个电脑不一样，如果依据此博客还是解决不了，那么根据自己的问题去百度或者Google吧。。 10.通过recovery mode进入了系统后，重新按照前述步骤卸载重装NVIDIA驱动，这说明了我之前卸载不干净或者卸载之后nouveau又被重新开启了。。。。 11.之后重启后就能正常进入Ubuntu系统了，这说明应该已经成功了，之后再重新打开grub文件（博客3），把之前改过的再改回来，然后重启 12.重启之后发现分辨率低而且无法在设置中心改，按照博客4的方法结果成功解决了问题，自此，NVIDIA驱动安装完成。 安装CUDA和CUDNN安装好驱动后基本上等于深度学习环境配置进度完成了80%了，之后可以安装相应的CUDA和CUDNN了，安装过程比较简单，在此不再赘述，可以参照以下两篇博客，也可以自己重新定义版本关键词进行搜索。 博客6 博客7 **2018.3.2踩坑来更** 今天Ubuntu系统提示升级，结果升级了内核kernel，导致升级完成重启后出现ACPI ERROR之类的错误，看了一大推的博客总结后，发现问题是“升级之后的内核，是不会自动加载你的显卡驱动的，那就需要在这个心内核上手动重新安装NV驱动。” 所以，等装完了驱动后，最好禁止Ubuntu内核更新或者直接禁止Ubuntu更新。 所以，升级内核后，需要重新安装NVIDIA驱动。。！！！！！！ 当然重装驱动的最重要步骤就是禁用nouveau,这里我看到了另一篇博客（博客8）讲得比较详细，基本上只看这一篇博客就可以安装并配置好深度学习环境了，要记得同时修改/etc/modprobe.d/disable-nouveau.conf和/etc/default/grub 而且要特别注意，装完后要记得回来修改/etc/default/grub，否则修改分辨率后会发现没有作用。我今天安装时做了实验，我是按照博客3对/etc/default/grub进行修改的，也就是加了“nomodeset”，发现只有将它删除，博客4的修改方法才有用，另外注意博客4添加的Modes后面的分辨率要根据自己的电脑调整，否则可能出现登录界面比较小，没接到电脑边。 英伟达的驱动真是一大堆破事，，看来是时候着手docker了。。]]></content>
      <categories>
        <category>深度学习环境配置</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>linux</tag>
        <tag>nvidia driver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra of MIT]]></title>
    <url>%2F2019%2F02%2F14%2FLinear-Algebra-of-MIT%2F</url>
    <content type="text"><![CDATA[​ ​ 这是对MIT 18.06 open course— Linear Algebra—by Gilbert Strang 的课程总结。 课程视频及主页资料由于计算机视觉以及机器学习中对矩阵的要求较高，而且本科阶段的高等代数学得不太行，故借此机会将这个著名的MIT线性代数公开课看了一遍，相关的课程视频和材料放在下面。 lectures: readings: assignments: exams: tools: 课程脉络Gilbert老爷子这门线代课应用性很强，在课堂上经常结合工程实际来探讨矩阵在其中的应用和联系，比如在对投影，最小二乘法原理以及求解矛盾方程组的三者之间用$A^{T}A$进行联系，这中“惊人”的发现也进一步加深了我对线性代数的通用性认识，所以这门课对搞了一些研究回过头来听的人来说很有启发，有时候没事可以时常翻翻这门课的教材。 这门课的核心思想就是老爷子的那张著名的”four subspaces”图，如下图所示。整门课分为三个部分：$Ax=b$和四个子空间；最小二乘法，行列式和特征值；正定矩阵以及应用。 Gilbert先从方程组入手，指出现实生活中的问题抽象成一系列方程，组成方程组，因此矩阵的出现是为了应对消元求解方程组，同时利用系数矩阵来从另一个层面认识这个系统的性质，比如可解性，稳定性等等，在这里，Gilbert将系数矩阵看成是向量的线性组合，从而直接转成空间的角度来看待“解”，这也是该部分的一个重要思想。矩阵也会表示空间，最基本的思想是把矩阵认为是n个列向量组成的，研究这些列向量彼此之间的关系来来研究矩阵代表的空间。求解问题也变成是列向量的线性组合。矩阵的行变换代表着消元法，同时也会引发矩阵的分解，求解Ax=b时转换成矩阵的列空间和零空间来二分，特定解和特殊解。之后引申出四个基本子空间：行空间，零空间，列空间，左零空间，进而指出空间的维数和基。 第一部分的关键词就是消元，向量，线性组合和空间。矩阵，空间，方程组之间是等价的。 到了第二部分，Gilbert开始引入行列式。他先从正交矩阵讲起，即$A^{T}A=I$，指出了它在投影，最小二乘，求解矛盾方程组之间的作用，并且利用它把这些问题都联系在了一起。其本质的原因可能是所谓的”空间度量，距离最小“，然后系数矩阵$A$恰好用这种方式呈现出来。之后引入行列式就很自然了，因为要求”大小“或者”长度“。Gilbert通过三个基本的性质定义了矩阵的行列式，并且推导出了其他几种性质，然后顺带着给出了行列式的计算公式和求解方程组的作用，及求逆公式，虽然他并不推崇这种死板的大计算量的方法来求解。最后，Gilbert借着行列式给出了矩阵的特征值，特征向量，以及特征值在对角化和其他工程中的应用（行列式的作用就是对角化）。不过比较可惜的一点是，教授并没有深层次探讨矩阵的特征值的意义，仅仅是从特征值公式上说”向量经过矩阵变换后方向不变“，虽然后来的例子又将特征值特征向量与主轴定理结合起来，但是依然没有一种很明晰的感觉。可能因为这是工科课程，重在讲解应用和联系。。希望我后面能在”linear algebra done right”这本书里找到答案。 最后一部分的主题是正定性，这类似与函数当中的最小值对应条件问题。这一部分我看得比较乱，没看出什么真正的名堂出来，毕竟从内容上我没找出什么联系。我后来问了数学系的同学，他说正定矩阵是为了定义内积空间（又称希尔伯空间，量子力学就是定义在这个上面）。这里面涉及到多元函数最小值判定，酉矩阵，以及奇异值分解，因此还是比较重要的内容。 实际上呢，教授是从多元线性方程组的角度来讲线性代数和矩阵的，因此并不像数学系那样从映射，集合、环、域，空间，然后到空间结构，代数，多元数组的关系来展开讲。因为是工科课程，所以第一关心的问题是怎么用，后面才会去进一步关注如何产生这种思想的问题，在这里，Gilbert教授关注的是矩阵角度解方程组，诸如矩阵分解、对角化，投影，正交矩阵，正定矩阵都是为了和工程中实际方法相联系和结合，因此可以看作是一种“top-down”的讲授思维。 对于做SLAM和机器学习来讲，关注更多的好像还是数值矩阵求解，即如何利用数值计算工具，更有效，更准确的分解、求解矩阵，所以，我接下来还得继续看fast.ai的numerical linear algebra课了。。 阅读材料这门课除了Gilbert Strang的那本教材，还有一些其他的资料，我把它们都托管到了我的Github上，有需要的请自取。 清单如下： Introduction to Linear Algebra 超详细MIT线性代数公开课笔记 神奇的矩阵第一季 神奇的矩阵第二季 线性代数的几何意义 Linear Algebra Done Right 另外也有其他几本线性代数教材也不错： 圣经 Linear Algebra-hoffman Linear Algebra and Its Applications-David C. Lay Introduction to Applied Linear Algebra—Vectors, Matrices, and Least Squares 看完《线性代数的几何意义》和《神奇的矩阵》来更 线性代数的几何意义线性代数 代数的功能是进行抽象，为了解决问题的方便，提高效率。线性代数里面的线性主要是指线性空间里面的线性变换（可加性和比例性），通过线性算子定义了线性变换，也就是变换满足可加性和比例性，实际上，差分，微分都是一种数学上的算子，代表一种运算关系。 行列式 行列式的几何意义具有深刻的含义。它是指行列式的行向量或列向量所构成的平行多面体的有向体积。这个有向体积是由许多块更小的有向面积或有向体积的累加。行列式的几何意义是什么呢？概括说来有两个解释：一个解释是行列式就是行列式中的行或列向量所构成的超平行多面体的 有向面积或有向体积；另一个解释是矩阵 $A$ 的行列式 $detA$ 就是线性变换 $A$ 下的图形面积或体积的伸缩因子。一个给定的行列式，它的行向量顺序也给定了，不能随意改变其顺序。 一个行列式的整体几何意义是有向线段（一阶行列式）或有向面积（二阶行列式）或有向体积（三阶行列式及以上）。因此，行列式最基本的几何意义是由各个坐标轴上的有向线段所围起来的所有有向面积或有向体积的累加和。这个累加要注意每个面积或体积的方向或符号，方向相同的要加，方向相反的要减，因而，这个累加的和是代数和。 行列式的乘积项及其逆序数的几何意义实际上是行列式最根本的几何意义，因而可以解释所有的行列式的定义及其性质。 向量空间 设$V$是非空的n维向量的集合(n=1,2,3…)，如果$V$中的向量对加法和数乘两种运算封闭，即： \begin{align*} & 1. 若\overrightarrow{a},\overrightarrow{b} \in V,则\overrightarrow{a}+\overrightarrow{b} \in V \\ & 2. \overrightarrow{a} \in V,则k\overrightarrow{a} \in V,k为任意实数 \end{align*}​ 则称$V$为向量空间。 向量空间主要有两种：一种是由 V 中的一个向量组张成的空间（比如由特征向量张成的子空间等）。另外一种齐次线性方程组的解集组成的解空间。实际上，线性方程组的解空间也是解向量所张成，这两种空间里都包含有无穷多的向量。 值得注意的是，所有的子空间一定要包含零空间在内。实际上，我们现在讨论的向量，不能称之为自由向量，因为所有的向量的尾巴都被拉到了原点上，或者说，所有向量空间里的向量都是从原点出发的，大家都有一个共同的零空间，这就是为什么所有的子空间一定要包含零空间的原因了。 为什么要把向量的尾部都拉到原点是为了研究向量的方便，因为这样就可以把向量和空间中的点一一对应起来。空间中一旦建立起了坐标系，点有坐标值，那么我们就用点的坐标表示与点对应的向量，这样向量就有了解析式，就有了向量的坐标表达式，我们就可以方面的使用代数中的矩阵技术进行分析及计算了。如果一个子空间没有通过原点，那么从原点出发的向量必然首尾不顾，造成了向量头在子空间中尾在空间外（因为原点在空间外）。当然，向量的加法和数乘也都跑到子空间外面去了。 内积的定义要解决空间中不同的基带来不同坐标度量带来向量长度和夹角的变化。可以通过内积度量矩阵$P^{T}P$来解决此问题，无论你变了多少次基，每变一次基同时就改变一次度量矩阵，从而来保证内积值的不变性，这样也就保证了向量空间度量的一致性，达到了度量值不随坐标改变而改变的目的。 矩阵 我们知道，在直角坐标系中，一个有序的实数数组$(a,b)$和$(a,b,c)$分别代表了平面上和空间上的一个点，这就是实数组的几何意义。类似的，在线性空间中如果确定了一个基，线性映射就可以用确定的矩阵来表示，这就是矩阵的几何意义：线性空间上的线性映射。矩阵独立的几何意义表现为对向量的作用结果。 如果用数组来统一定义标量、向量和矩阵的话就是：标量是一维向量，向量是标量的数组，矩阵则是向量的数组。 矩阵与向量乘积比如$Ax$表现为矩阵$A$对一个向量$x$作用的结果。其作用的主要过程是对一个向量进行旋转和缩放的综合过程（即线性变换的过程），一个向量就变换为另外一个向量。一个 m 行 n 列的实矩阵$A_{m \times n}$就是一个$R^{n} \rightarrow R^{m}$上的线性变换，或者说，矩阵$A_{m \times n}$把一个n维空间的 n 维向量变换为一个m维空间的m维向量。 一个矩阵乘以一个向量，一般将会对向量的几何图形进行旋转和伸缩变化，而旋转矩阵只对向量进行旋转变化而没有伸缩变化，例如二阶旋转矩阵$A$： A= \begin{pmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{pmatrix} 如果将上面的旋转矩阵分别按行向量和列向量来看，随着角度的增大，行向量在顺时针旋转，而列向量在逆时针旋转，“这正是正交矩阵的特征”（然而这一结论的得出还不是很明了）。旋转矩阵为正交矩阵。 矩阵与矩阵的相乘的几何意义，可以从矩阵与多个向量相乘的几何意义得到，只是多个向量被按照顺序组合成了另一个矩阵。 实际上，从$R^{n} \rightarrow R^{m}$上的线性变换都可以表述为一个矩阵变换；反过来，一个矩阵变换也必然是一个线性变换。两者具有一一对应的关系。一个矩阵变换也必然是一个线性变换。这个对应关系笼统地表述如下： 线性变换的和对应着矩阵的和； 线性变换的乘积对应着矩阵的乘积； 线性变换的数量乘积对应着矩阵的数量乘积； 线性变换的逆对应着矩阵的逆； 下面的定理给出了如何把一个$R^{2}$空间上的线性变换转换成一个对应的 2 阶矩阵的办法: 定理：设T:R^{2} \rightarrow R^{2}是一个线性变换，那么T的矩阵的列向量为T(e_{1})和T(e_{2}) 由于线性变换与矩阵之间有一一对应（在给定基的前提下），而且保持线性运算关系不变（线性变换的加法和数乘分别对应于在某一个基下的矩阵的加法和数乘），因此，可以用矩阵来研究线性变换，也可以用线性变换来研究矩阵。 常见的线性变换有初等变换、等价变换、相似变换、合同变换等。我们也常常听到正交变换的名字，但由于正交变换包括平移、旋转和镜像，我们知道平移变换不是线性变换，因此不是所有的正交变换是线性变换。 一个矩阵就是把第一象限的单位立方体变换到其他象限的多面体，单位立方体由单位基向量张成，多面体由矩阵列向量张成；而列向量是由基向量变换得到的。 对于n阶方阵，把列看作列向量，则行是每个列向量在列空间各个坐标轴上的投影（坐标），行的数量则是列空间坐标系的维数。 特征值和特征向量的几何意义：$Ax=\lambda x$ 方阵乘以一个向量的结果仍然是一个同维向量，矩阵乘法对应了一个变换，把一个向量变成同维数的另一个向量。在这个变换的过程中，向量会发生旋转、伸缩或镜像的变化。矩阵不同，向量变化的结果也会不同。如果矩阵对某一个向量或者某些向量只发生伸缩变换，不对这些向量产生旋转效果，那么这些向量就是这个矩阵的特征向量，伸缩的比例就是特征值；如果伸缩的比例值是负值，原向量的方向改变为反方向，原向量仍然是这个矩阵的特征值。因此，从矩阵的几何意义上看，矩阵$A$的特征向量$\overrightarrow a$就是经过矩阵$A$变换后与自己平行的非零向量，矩阵$A$的特征值$\lambda$就是特征向量$\overrightarrow a$经变换后的伸缩系数（复特征值会使特征向量在复平面进行旋转，但在实轴上仍然是只进行伸缩变换）。 特征值反映了特征向量在变换时的伸缩倍数。对一个变换而言。特征向量指明变换的方向，而特征值反映的是变换的剧烈程度！我们知道，一个矩阵，只要我们找到合适的坐标，它的全部信息就可以用坐标系和特征值表示。这里蕴含了一个哲理：我们从不同角度看问题，其难易程度是不一样的！而矩阵对角化为我们提供了一个很好的看问题的角度。 矩阵的特征值之和等于矩阵的迹，特征值之积等于矩阵的行列式。这个由二阶矩阵推广而来。 关于特征值和特征向量，一是注意线性不变量的含义，一个是振动的谱含义。特征向量是线性不变量，特征值是振动的谱。机械振动和电振动有频谱，振动的某个频率具有某个幅度，那么矩阵也有矩阵的谱，矩阵的谱就是矩阵特征值的概念，是矩阵所固有的属性，所有的特征值形成了矩阵的一个频谱，每个特征值是矩阵的一个“谐振频点”。矩阵的谱分解就是抓住了矩阵的主要矛盾，因此也是主成分分析（PCA），奇异值分解相关的内容。 矩阵之所以能形成“频率的谱”，就是因为矩阵在特征向量所指的方向上具有对向量产生恒定的变换作用：周期性地增强或减弱特征向量的作用。进一步地，如果矩阵持续地叠代作用于向量，那么特征向量就会凸现出来。可类比与电路中的振荡器。 几何重数与代数重数：一个特征值的求解因式的次数称为代数重数，特征值的特征子空间的维数称为几何重数。一般来说，特征值的代数重数大于或等于几何重数。 相似矩阵：或矩阵$A$与矩阵$B$相似，一定存在一个非奇异矩阵$P$（基变换矩阵），有$A=PBP^{-1}$。核心的几何意义就是相似矩阵$A$和$B$是同一个线性变换在两个不同基下的表示矩阵。线性变换的相似对角化实质是寻找一个适当的坐标系，使得该变换对这个新的坐标系上的单位向量（或基向量）只做伸缩变换，不做旋转变换。 不是任何矩阵都可以相似对角化，除非由n个线性无关的特征向量，但是实对称矩阵一定可以对角化。这里的证明涉及到一系列定理： \begin{align*} & 定理1： 实对称矩阵A的特征值都是实数 \\ & 定理2： 实对称矩阵A的不同特征值对应的特征向量一定是相互正交的（正交的向量组一定是线性无关向量组）\\ & 定理3： 实对称矩阵A的r重特征根\lambda一定有r个线性无关的特征向量 \end{align*} 雅可比矩阵：雅可比矩阵是线性代数和微积分的纽带，是把非线性问题转换为线性问题的有力工具之一。 一个函数方程组由n个函数构成，每个函数有n个自变量$x_{1},x_{2},x_{3},\cdots x_{n}$ \begin{cases} y_{1}=f_{1}(x_{1},x_{2},\cdots x_{n}) \\ y_{2}=f_{2}(x_{1},x_{2},\cdots x_{n})\\ \vdots \\ y_{n}=f_{n}(x_{1},x_{2},\dots x_{n}) \end{cases} 一般情况下，该函数方程组不是线性方程组，是多维曲线、曲面类的，通过微积分的思想化曲为直，将上述方程组化为超维切平面。先偏微分再写成矩阵的形式： \begin{pmatrix} dy_{1}\\ dy_{2}\\ \vdots\\ dy_{n} \end{pmatrix} = \begin{bmatrix} \frac{\partial f_{1}}{\partial x_{1}} & \frac{\partial f_{1}}{\partial x_{2}} & \cdots & \frac{\partial f_{1}}{\partial x_{n}}\\ \frac{\partial f_{2}}{\partial x_{1}} & \frac{\partial f_{2}}{\partial x_{2}} & \cdots & \frac{\partial f_{2}}{\partial x_{n}}\\ \vdots & \vdots & \cdots & \vdots \\ \frac{\partial f_{n}}{\partial x_{1}} & \frac{\partial f_{n}}{\partial x_{2}} & \cdots & \frac{\partial f_{n}}{\partial x_{n}} \end{bmatrix} \begin{pmatrix} dx_{1}\\ dx_{2}\\ \vdots\\ dx_{n} \end{pmatrix} 其中，中间的方块矩阵就是雅可比矩阵$J$，里面的元素一般不是常数，而是变量或函数，说明雅可比矩阵包含着很多个具体的变换矩阵。 雅可比矩阵把一个超平面的仿射坐标系变换成了一个超曲面坐标系；雅可比行列式就是曲面坐标系下单位微元和仿射坐标系下单位微元面积的比值。（雅可比矩阵把一个空间里的一个平面坐标系（基）变换成了无数个极小平面坐标系（基）；无数个极小平面就是曲面的切平面；雅可比行列式就是切平面上每个坐标系下极小单位元和原坐标系下极小单位元面积的比值。） 整个线性代数里矩阵之间有三种典型的关系：矩阵相似（similar）、矩阵等价（equivalent）、矩阵合同（congruent）： \begin{align*} & (1) A和B等价 \Leftrightarrow 存在可逆矩阵P和Q，使得B=PAQ；\\ & (2) A和B相似 \Leftrightarrow 存在可逆矩阵P，使得B=P^{-1}AP;\\ & (3) A和B合同 \Leftrightarrow 存在可逆矩阵C，使得B=C^{T}AC. \end{align*} 矩阵相似，矩阵合同一定说明矩阵等价，但是相似与等价之间没有必然的联系，除非是正交矩阵，则两个等价。它们的几何意义如下： 两个有限维向量空间之间的同一个线性映射，其在这两个向量空间上的不同基下所对应的矩阵之间的关系就是等价关系。 就是说它们是同一种类型的子空间，变换的作用就是为了把元素之间相互干扰的矩阵化成能一眼看出维数的简单矩阵。因为$P$和$Q$对应行初等变换和列初等变换的叠加，这里只能反映空间的维数。然而，在一些实际工程中，这些信息对我们了解一个系统已经足够用了。 一个有限维向量空间上的同一个线性变换（或称线性算子），其在不同基下所对应的矩阵之间的关系是相似关系。就是同一个线性变换的不同基的描述矩阵。矩阵的相似变换可以把一个比较丑的矩阵变成一个比较美的矩阵，而保证这两个矩阵都是描述了同一个线性变换。至于什么样的矩阵是“美”的，什么样的是“丑”的，我们说对角阵是美的。总而言之，相似变换是为了简化计算。 一个有限维向量空间上的同一个双线性函数或内积，其在两个基下的度量矩阵是相合关系。 矩阵$A$，$B$可以不是方阵，因为是不同空间之间的线性映射矩阵，而$P$ ，$Q$，$C$必须是方阵且可逆，因为是同一空间里的基过渡矩阵。矩阵既可以看作向量的变换也可以看作是基的变换，两者的表达式中自变向量和因变向量的位置相反。 任一矩阵都可以通过一系列的初等变换化非对角线上的元素为0，从而成为对角阵，因此任一矩阵都等价于一个对角阵，其对角线上的非零元素的个数正好是原矩阵的秩。然而除了秩不变外，矩阵的其他性质在变换以后就很难反映出来了。 同构：如果两个线性空间上的映射变换既是单射又是满射，就称这两个向量空间同构。两个向量空间同构，那么就有线性映射使这两个线性空间的向量（或点）一一对应，而且保持线性不变，这时往往将这两个向量空间看作同一个。对于向量空间，同构也是等价关系。所以说，相似变换是同一个向量空间的变换，是两个基上的同一个线性变换。 相似矩阵描述的是在不同参照系的同一个变换，动作规则是相同的，类似地，合同矩阵描述的是在不同参照系下的同一个内积的度量矩阵。 由前可知，内积的度量应该是不随着坐标系的选取而改变的，因此定义了度量矩阵这个东西。事实上，内积的推广式子为： (x,y)=x^{T}Sy, S=P^{T}P 其中方阵$S$就是度量矩阵，度量矩阵是由基向量所构成的过渡方阵$P$与其转置的乘积得到的。 因为度量矩阵$S$是由基的过渡矩阵所决定的，那么每更换一次基坐标系就会有一个新的度量矩阵$T$，它们对应着同一个内积，且$S$合同于$T$。 正交变换是保持任意向量的长度不变或者保持度量不变的线性变换，其是欧式空间中的一类重要的变换。正交变换具有合同与相似变换共同的优点，前者仅适用于对称阵，保持了矩阵的对称性、正定性、秩等性质不变；后者适用于一般方阵，保持了矩阵的秩与特征值不变。因为正交变换的矩阵$P$满足$P^{-1} = P^{T}$，实际上是合同变换与相似变换的一种结合。 正交变换的主要性质是它不改变几何图形的度量。正交变换对应的矩阵就是正交矩阵，简单地说，一个正交矩阵就是一个具有标准正交列（行）向量组的方阵。 线性方程组 四个正交子空间。 线性方程组的研究，包含了对向量的线性组合和矩阵方程的研究。方程组可以写成矩阵方程和向量组和形式等不同的形式，也反映了研究内容的不同。即线性方程组、向量的线性组合和矩阵及其矩阵方程可作等价研究。 二次型 二次型的内容就是研究线性空间里的一个几何图形如何在不同的坐标基下的不同的矩阵表示，合同的矩阵表示的是同一个二次函数的几何图形。 定义： \begin{align*} & 含有n个变量x_{1}，x_{2}，\cdots，x_{n}的二次齐次函数：\\ & f(x_{1}，x_{2}，\cdots，x_{n})=a_{11}x_{1}^{2}+a_{22}x_{2}^{2}+\cdots+a_{nn}x_{n}^{2}+2a_{12}x_{1}x_{2}+2a_{13}x_{1}x_{3}+\cdots+2a_{(n-1)n}x_{n-1}x_{n}\\ & 称为二次型。只含有平方项的二次型称为二次型的标准型：\\ & f(x_{1}，x_{2}，\cdots，x_{n})=a_{11}x_{1}^{2}+a_{22}x_{2}^{2}+\cdots+a_{nn}x_{n}^{2} \end{align*} 二次型可以表示为变元向量和矩阵的乘积： f(x_{1}，x_{2}，\cdots，x_{n})= (x_{1}，x_{2}，\cdots，x_{n}) \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{12} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \cdots & \vdots \\ a_{1n} & a_{2n} & \cdots & a_{nn} \end{bmatrix} \begin{pmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{pmatrix} =x^{T}Ax 任意给一个二次型，就有唯一确定的对称矩阵，反之，任给一个对称矩阵，也就唯一确定一个二次型，其中，上式中间的方块矩阵称为二次型$f$的矩阵。 二次型是向量长度的平方，这个平方值不随坐标基的变化而变化，是个不变量，是绝对性的，而二次型函数的数学表达式却随着坐标基的变化而变化。 问题是，线性代数是研究”线性问题“的，比如线性变换、线性空间、线性方程组等，二次型却是非线性的，那为何能用矩阵等线性工具研究？ 一是二次曲线或曲面本身就具有线性的性质，比如直纹曲面，一个坐标方向是直线，一个坐标方向是二次的，就可以描绘出直纹曲面（比如马鞍面）； 二是用向量研究一元多项式，是用多项式的系数构成向量来等价地替代研究，不是直接研究多项式本身，因此化非线性为线性；同样，用二次型的系数构建矩阵来等价地替代研究它，同样化非线性为线性了； 三是对二次型的研究利用了双线性函数的概念。二次型本来就是一个对称的双线性函数。 双线性函数就是定义了某维线性空间里的双向量的一个运算，运算结果是一个数，这个数属于某个数域。其中一个变元固定时是另一个变元的线性函数，两个向量互为线性，称为双线性。 双线性函数的一般定义如下： \begin{align*} & 设P^{n}是数域P上n维列向量构成的线性空间，向量x,y \in P^{n}，再设A是P上的n阶方阵。令\\ & f(x,y)=x^{T}Ay \\ & 则f(x,y)是P^{n}上的一个双线性函数。 \end{align*} 二次型对角化就是把二次型化简成标准型或者规范型。对角化二次型必须要是矩阵的合同变换，或者说度量矩阵和变换后的度量矩阵必须是合同的，只有这样才能使二次型的函数值保持不变，才能使向量长度保持不变，其他方法比如矩阵的相似对角化不能保证二次型的值不变。因为从几何意义上讲，相似对角化法是使矩阵本身所表示的某种线性变换不变进而对角化，而二次型对角化法是保证矩阵背后所代表二次型的值不变，它们所要求的不变量是不同的。 f(x)=x^{T}Ax=(Cy)^{T}ACy=y^{T}(C^{T}AC)y 对于一个正交矩阵$Q$，有向量替换关系$x=Qy$使： f(x)=x^{T}Ax=y^{T}(Q^{T}AQ)y=y^{T}(Q^ {-1}AQ)y 正交变换既是相似变换同时又是合同变换。由于正交变换保持变换前后的向量内积不变，从而保持向量的长度与夹角不变，所以正交变换属于刚体变换，是代表空间的一个旋转/镜像变换。利用矩阵乘积分解的方法，这种变换的转轴、转角可以用矩阵的特征参数量化地表示出来。 和其他合同变换（又称可逆线性替换）不保持图形的原有形状，如可以把椭圆变成圆的特点相比，正交变换则保持原图形的形状。 用正交变换化二次型为标准型的定理也称为主轴定理，这同时也是主轴定理的几何意义。 主轴定理： \begin {align*} & 对于任意一个n元二次型：\\ & f(x_{1}，x_{2}，\cdots，x_{n})=x^{T}Ax\\ & 存在正交变换x=Qy（Q为n元正交矩阵），使得\\ & x^{T}Ax=y^{T}(Q^{T}AQ)y=\lambda_{1}y_{1}^{2}+\lambda_{2}y_{2}^{2}+\cdots+\lambda_{n}y_{n}^{2} \end{align*} 其中，$\lambda_{1}，\lambda_{2}，\cdots，\lambda_{n}$是实对称矩阵$A$的n个特征值；$Q$的n个列向量是对应于特征值$\lambda_{1}，\lambda_{2}，\cdots，\lambda_{n}$的标准正交特征向量。 神奇的矩阵空间 空间是现代数学的基础之一。线形空间其实还是比较初级的，如果在里面定义了范数，就成了赋范线性空间。赋范线性空间满足完备性，就成了巴那赫空间；赋范线性空间中定义角度，就有了内积空间，内积空间再满足完备性，就得到希尔伯特空间，如果空间里装载所有类型的函数，就叫泛函空间。 容纳运动是空间的本质。不管是什么空间，都必须容纳和支持在其中发生的符合规则的运动（变换）。在某种空间中往往会存在一种相对应的变换，比如拓扑空间中有拓扑变换，线性空间中有线性变换，仿射空间中有仿射变换，其实这些变换都只不过是对应空间中允许的运动形式而已。因此只要知道， “空间”是容纳运动的一个对象集合，而变换则规定了对应空间的运动。 矩阵的运动本质属性 线性空间中的运动，被称为线性变换。也就是说，你从线性空间中的一个点运动到任意的另外一个点，都可以通过一个线性变化来完成。在线性空间中，当你选定一组基之后，不仅可以用一个向量来描述空间中的任何一个对象，而且可以用矩阵来描述该空间中的任何一个运动（变换）。而使某个对象发生对应运动的方法，就是用代表那个运动的矩阵，乘以代表那个对象的向量。简而言之，在线性空间中选定基之后，向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动。矩阵的本质是运动的描述。 不过，这里“运动”的概念不是微积分中的连续性的运动，而是瞬间发生的变化，是物理当中“跃迁”的一种描述，在这里我们将其称为“变换”，也就是说，矩阵是线性空间里的变换的描述。 “矩阵是线性空间中的线性变换的一个描述。在一个线性空间中，只要我们选定一组基，那么对于任何一个线性变换，都能够用一个确定的矩阵来加以描述”。 矩阵与方程组 老生常谈的列空间，秩，交点等问题了。 矩阵与坐标系 向量左乘矩阵将向量变换成另一个向量，一组基右乘一个方阵将其变换到另一组基，左乘一个方阵就是将一组基下的坐标变换到另一组基下的坐标，也就是说运动是相对的，而这个相对体现在左乘和右乘上。 对于矩阵乘法，主要是考察一个矩阵对另一个矩阵所起的变换作用。其作用的矩阵看作是动作矩阵，被作用的矩阵可以看作是由行或列向量构成的几何图形。同样，如果一连串的矩阵相乘，就是多次变换的叠加。而矩阵左乘无非是把一个向量或一组向量（即另一个矩阵）进行伸缩或旋转。 乘积的效果就是多个伸缩和旋转的叠加。其中两类特殊的矩阵：旋转矩阵和对角矩阵，分别表示对向量的旋转和伸缩。 矩阵与傅里叶变换1.分解与叠加的思想，以及完备性才会完全替代“棱角”的波形。在这里，基不再是向量，而是三角函数。 2.为什么是选择三角函数？ 大自然中很多现象可以抽象成一个线性时不变系统来研究，无论你用微分方程还是传递函数或者状态空间描述。线性时不变系统可以这样理解： 输入输出信号满足线性关系，而且系统参数不随时间变换。对于大自然界的很多系统，一个正弦曲线信号输入后，输出的仍是正弦曲线，只有幅度和相位可能发生变化，但是频率和波的形状仍是一样的。也就是说正弦信号是系统的特征向量。 当然，指数信号也是系统的特征向量，表示能量的衰减或积聚。自然界的衰减或者扩散现象大多是指数形式的，或者既有波动又有指数衰减（复指数$e^{\alpha+i \beta}$形式），因此具有特征的基函数就由三角函数变成复指数函数。但是，如果输入是方波、三角波或者其他什么波形，那输出就不一定是什么样子了。所以，除了指数信号和正弦信号以外的其他波形都不是线性系统的特征信号。 用正弦曲线来代替原来的曲线而不用方波或三角波或者其他什么函数来表示的原因在于： 正弦信号恰好是很多线性时不变系统的特征向量。于是就有了傅里叶变换。对于更一般的线性时不变系统，复指数信号(表示耗散或衰减)是系统的“特征向量”。于是就有了拉普拉斯变换。z 变换也是同样的道理，这时$z^{n}$是离散系统的“特征向量”。 这里没有区分特征向量和特征函数的概念，主要想表达二者的思想是相同的，只不过一个是有限维向量，一个是无限维函数。 傅里叶级数和傅里叶变换其实就是我们之前讨论的特征值与特征向量的问题。分解信号的方法是无穷的，但分解信号的目的是为了更加简单地处理原来的信号。这样，用正余弦来表示原信号会更加简单，因为正余弦拥有原信号所不具有的性质：正弦曲线保真度。且只有正弦曲线才拥有这样的性质。 这样做的好处就是知道输入，我们就能很简单乘一个系数写出输出。 3.时域和频域 以时间作为参照来观察动态世界的方法我们称其为时域分析，频域 (frequency domain) 是描述信号在频率方面特性时用到的一种坐标系。用线性代数的语言就是装着正弦函数的空间。频域最重要的性质是：它不是真实的，而是一个数学构造。频域是一个遵循特定规则的数学范畴。正弦波是频域中唯一存在的波形，这是频域中最重要的规则，即正弦波是对频域的描述，因为时域中的任何波形都可用正弦波合成。 对于一个信号来说，信号强度随时间的变化规律就是时域特性，信号是由哪些单一频率的信号合成的就是频域特性。 这里的核心就是一种信号可以用另一种信号作为基函数线性表示。而由于现实世界中正弦信号是系统的特征向量，所以我们就用傅里叶变换，将研究的信号在频域展开。总而言之， 不管是傅里叶级数，还是傅里叶变换、拉普拉斯变换、z 变换，本质上都是线性代数里面讲的求特征值和特征向量。 4.傅里叶级数 Strang老爷子在课上专门讲过通过投影和正交基求出傅里叶级数，正交基中的每个函数都可以看做是一条独立的坐标轴，从几何角度来看，傅里叶级数展开其实只是在做一个动作，那就是把函数“投影”到一系列由三角函数构成的“坐标轴”上。上面的系数则是函数在每条坐标轴上的坐标。注意，在有限维中，内积是点积的形式，而在无限维中则是积分的形式。 这组正交基是：$\left\{1, \cos \frac {\pi x}{l}, \sin \frac{\pi x}{l}, \cos \frac{2 \pi x}{l}, \sin \frac {2\pi x}{l}, \cdots \right\}$ \begin{align*} & a_{0}=\frac {\left\langle f,1 \right\rangle}{\left \langle 1,1 \right \rangle}=\frac{\int_{-l}^{l}f(x)\,dx}{\int_{-l}^{l}\,dx} =\frac{\int_{-l}^{l}f(x)\,dx}{2l} \\ & a_{n}=\frac {\left \langle f, \cos \frac {n \pi x}{l}\right \rangle}{\left \langle \cos \frac{n \pi x}{l}, \cos \frac{n \pi x}{l} \right \rangle} = \frac {\int_{-l}^{l}f(x)\cos \frac{n \pi x}{l} \,dx}{\int_{-l}^{l} \cos^2 \frac{n \pi x}{l}\,dx} = \frac {\int_{-l}^{l}f(x)\cos \frac{n \pi x}{l} \,dx}{l},n \ge 1 \\ & b_{n}=\frac {\left \langle f, \sin \frac {n \pi x}{l}\right \rangle}{\left \langle \sin \frac{n \pi x}{l}, \sin \frac{n \pi x}{l} \right \rangle} = \frac {\int_{-l}^{l}f(x)\sin \frac{n \pi x}{l} \,dx}{\int_{-l}^{l} \sin^2 \frac{n \pi x}{l}\,dx} = \frac {\int_{-l}^{l}f(x)\sin \frac{n \pi x}{l} \,dx}{l},n \ge 1 \\ \end{align*} 同理，对于复数形式的傅里叶级数，也可以用几何投影的观点来写出所有的系数。 矩阵的奇异值分解（SVD） $M=U \sum V^{T}$ 任意的矩阵$M$是可以分解成三个矩阵。 $V$表示了原始域的标准正交基，$ U$表示$M$经过变换后的 co-domain 的标准正交基，$\sum$表示了$V$中的向量与$U$中相对应向量之间的关系。 事实上，我们可以找到任何矩阵的奇异值分解。如果把矩阵$U$用它的列向量表示出来，可以写成： U=(u_{1}, u_{2}, \cdots, u_{n}) 其中每一个$u_{i}$称为$M$的左奇异向量。类似地，对于$V$，有： V=(v_{1}, v_{2}, \cdots, v_{n}) 其中每一个$v_{i}$被称为$M$的右奇异向量。然后设矩阵$\sum$的对角线元素为$\sigma_{i}$并按降序排列，则$M$就可以表示为： M=\sigma_{1}u_{1}v_{1}^{T}+\sigma_{2}u_{2}v_{2}^{T}+\cdots+\sigma_{n}u_{n}v_{n}^{T} =\sum_{i=1}^{n}\sigma_{i}u_{i}v_{i}^{T}=\sum_{i=1}^{n}A_{i} 其中$A_{i}=\sigma_{i}u_{i}v_{i}^{T}$是一个$m \times n$的矩阵，即把原来的矩阵表示成了n个矩阵的和。 因此，可以根据$\sigma_{i}$的大小进行矩阵的近似表达，主成分分析的思想就是来源于此。 奇异值分解在推荐系统，图像压缩，潜在语义索引，潜在数据表达，降噪，数据分析等有很重要的应用。]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
      <tags>
        <tag>mathematics</tag>
        <tag>linear algebra</tag>
        <tag>MIT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github 博客搭建]]></title>
    <url>%2F2019%2F02%2F14%2FHexo-Github-%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[完整搭建过程参考此系列博客（环境win10） 我的写作编辑器用的是typora Next主题配置Next主题是本人比较中意的hexo主题，所以在此只记录Next主题的配置过程 下载建议安装6.0版本。利用git bash here,输入命令：git clone https://github.com/theme-next/hexo-theme-next themes/next 配置与个性化 1.添加作者头像并设置旋转效果 注意：最新版本的next主题已经添加了头像动画功能，直接在主题的配置文件里面修改，因此头像添加（路径）也是在主题配置文件里面添加，而不是在根目录的配置文件添加 123456789101112# Sidebar Avataravatar: # in theme directory(source/images): /images/avatar.gif # in site directory(source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /uploads/header.jpg #/images/avatar.gif # If true, the avatar would be dispalyed in circle. rounded: true # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: true 2.修改标签“#”符号和插入图片 3.添加社交账号 如果遇到没有的社交图标，可以在fontawesome中寻找添加。 ||后面的是图标名，建议不要找最新的。 4.菜单设置 本地搜索一旦开启会自动添加”搜索“菜单 5.添加版权信息。参考该文章 上面的链接是手动添加，现在可以在NEXT主题配置文件中进行修改 12345creative_commons: license: by-nc-sa sidebar: true post: true language: 6.利用leancloud加入阅读计数功能 若页面LeanCloud访问统计提示’Counter not initialized! See more at console err msg.’ 参考该博客解决 问题：安装hexo-leancloud-counter-security报错：npm WARN babel-eslint@10.0.1 requires a peer of eslint@&gt;= 4.12.1 but none is installed. You must install peer dependencies yourself. 因此缺什么就安装什么，在这里我是安装：npm install eslint@&gt;= 4.12.1,之后再安装hexo-leancloud-counter-security就成功了。 另外最后在修正deploy的时候注意tab的控制，不然编译会报错 12345678910deploy: #type: git #repository: git@github.com:x695/thinkee.github.io.git #repository: https://github.com/x695/thinkee.github.io.git #branch: master - type: git #repository: git@e.coding.net:yangshixian/blog.git repository: git@git.coding.net:thinkee/blog.coding.me.git branch: master - type: leancloud_counter_security_sync 然而，十分尴尬的是，阅读数依然没有显示，后来参考这篇博客得到了解决 7.利用valine+leancloud添加评论功能 8.网站背景动画 如果修改主题配置文件之后不出现动画，建议下载主要的库到/source/lib中 9.加入本地搜索功能 10.加入Latex数学公式 这个除了在主题配置文件修改mathjax: true之外，还需要在每个post的博客里的头代码里面加入mathjax: true才可以正常显示。如果嫌每次书写都得添加麻烦，可以直接在根目录的scaffolds文件夹里的post.m文件夹直接加入mathjax: true，之后每次新建都会自动添加了。 不过再后续使用Latex语句的时候，发现博客上还是会显示源码，后来发现应该是渲染引擎的问题，根据这篇博客的指导修改后就好了。 2019.3.5日来更新，写完一篇blog进行generate时候出现了错误： 123Template render error: (unknown path) [Line 62, Column 32] expected variable end at Object.exports.prettifyError 出现的原因要么是主题配置文件忘了打空格，要么就是自己写的blog文件里面用的符号与其他hexo配置文件语法有冲突，后来发现原因是我打公式时出现了两个“}}”，前面一个”}”是latex语法，后面是”\}”，目的是为了输出括号“}”，之后参照latex的另一种打法“\lbrace \rbrace”就好了 2019.4.17日来更 公式直接用Mathpix进行LaTex命令复制吧吧，非常方便，也几乎没出现什么错误，还省时间。 11.博客图片点击放大 在NEXT主题的配置文件里面有fancybox选项，直接改为true就好了，不过在这之前要下载fancybox包到next/source/lib文件夹里面，即： 12cd next/source/libgit clone https://github.com/theme-next/theme-next-fancybox3 fancybox 12.并排插入两张图片 1234&lt;figure class=&quot;half&quot;&gt; &lt;img src=&quot;xxx.jpg&quot; width=&quot;400&quot;/&gt; &lt;img src=&quot;xxx.jpg&quot; width=&quot;400&quot;/&gt;&lt;/figure&gt; 不过这只能在.md文件中实现，博客中还是上下各一张。 13.NEXT主题内置标签（包括引用文本居中，图片最大化引用等），更详细的标签见hexo官网。注意在本地利用这些标签不会出现相应的结果，只有上传到博客上才能看到。 14.添加emoji表情功能 emoji大全 关于加入表情的功能，我一共找到了两种方法，一种是基于修改渲染引擎的，然后加入twemoji插件，但是我在之前为了支持mathjax已经更换了渲染引擎，结果导致我在按照博客1和博客2的操作进行时出现了fancybox图片放大和图片内部插入的问题，有时候甚至会影响到html语句，后来倒退删除渲染引擎和插件的时候，想要恢复到原来的状态，结果hexo g可以成功，本地却无法显示不出内容，结果只好重装。。目前原因未知。 此外，第二种方法是安装hexo-filter-github-emojis插件，可参考博客3和博客4，但是我看博客中也提到了图片干扰的问题，就没再继续试下去了。。。 下次看到更好的解决办法再来更新。 15.加入RSS订阅 直接在NEXT主题配置文件里面更改，在这之前先在博客站点目录下安装npm install hexo-generator-feed --save 123456feed: type: rss2 path: rss2.xml limit: 5 hub: content: &apos;true&apos; 16.博客被谷歌和百度收录 参考这个博客的做法 17.购买自己的阿里云域名并设置 参考]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>leancloud</tag>
        <tag>next</tag>
      </tags>
  </entry>
</search>
