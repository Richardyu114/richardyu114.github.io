<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2"><meta name="google-site-verification" content="true"><meta name="baidu-site-verification" content="true"><link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1"><link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.1.1",sidebar:{position:"left",display:"post",offset:12,onmobile:!1,dimmer:!1},back2top:!0,back2top_sidebar:!1,fancybox:!0,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="baidu-site-verification" content="0bqk4mbBLD"><meta name="google-site-verification" content="FvMbHVgnH7FO0GFRzTmOevjSaFwvzIQZv94LdufMMPY"><meta name="description" content="Abouthomepage;2017 lecture video;中文笔记课程相关时间安排，课件，参考资料，作业详见homepage链接，以最新版为主；官方的assignments和notes做得非常好，强烈推荐学习和反复观看；时间原因，学习19版的时候恰好2020版春季课程将要开始，这一版换了team，里面加了不少新的research介绍，尤其是非监督，自监督之类的，因此会加入一下2020版的内"><meta name="keywords" content="computer vision,public course,deep learning,Standford"><meta property="og:type" content="article"><meta property="og:title" content="Stanford CS231n 笔记"><meta property="og:url" content="http://densecollections.top/posts/notesofCS231N/index.html"><meta property="og:site_name" content="自拙集"><meta property="og:description" content="Abouthomepage;2017 lecture video;中文笔记课程相关时间安排，课件，参考资料，作业详见homepage链接，以最新版为主；官方的assignments和notes做得非常好，强烈推荐学习和反复观看；时间原因，学习19版的时候恰好2020版春季课程将要开始，这一版换了team，里面加了不少新的research介绍，尤其是非监督，自监督之类的，因此会加入一下2020版的内"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/summary.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/3_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/3_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/4_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/4_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/4_3.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/dynamic_static_graph.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/advice_for_frameworks.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/6_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/6_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/6_3.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/7_5.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/7_6.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/8_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/7_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/7_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/7_3.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/7_4.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/8_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/8_4.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/8_5.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/8_3.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_3.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_4.jpg"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_5.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_6.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_7.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_8.jpg"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_9.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_10.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_11.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_12.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_13.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_14.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/9_15.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/10_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/10_4.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/10_5.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/10_6.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/10_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/10_3.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/10_7.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/10_8.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/10_9.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_3.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_4.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_5.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_6.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_7.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_8.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_9.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_10.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/11_11.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/12_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/12_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/13_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/13_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/17_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/18_1.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/18_2.JPG"><meta property="og:image" content="http://densecollections.top/posts/notesofCS231N/18_3.JPG"><meta property="og:updated_time" content="2020-07-08T13:07:33.259Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Stanford CS231n 笔记"><meta name="twitter:description" content="Abouthomepage;2017 lecture video;中文笔记课程相关时间安排，课件，参考资料，作业详见homepage链接，以最新版为主；官方的assignments和notes做得非常好，强烈推荐学习和反复观看；时间原因，学习19版的时候恰好2020版春季课程将要开始，这一版换了team，里面加了不少新的research介绍，尤其是非监督，自监督之类的，因此会加入一下2020版的内"><meta name="twitter:image" content="http://densecollections.top/posts/notesofCS231N/summary.JPG"><link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml"><link rel="canonical" href="http://densecollections.top/posts/notesofCS231N/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>Stanford CS231n 笔记 | 自拙集</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">自拙集</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Work cures everything</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-paperstation"><a href="/PaperStation/" rel="section"><i class="menu-item-icon fa fa-fw fa-edit"></i><br>PaperStation</a></li><li class="menu-item menu-item-mindwandering"><a href="/MindWandering/" rel="section"><i class="menu-item-icon fa fa-fw fa-paper-plane"></i><br>MindWandering</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://densecollections.top/posts/notesofCS231N/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Richard YU"><meta itemprop="description" content="Today everything exists to end in a photograph"><meta itemprop="image" content="/uploads/header.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="自拙集"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Stanford CS231n 笔记</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-02-29 12:44:28" itemprop="dateCreated datePublished" datetime="2020-02-29T12:44:28+08:00">2020-02-29</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-07-08 21:07:33" itemprop="dateModified" datetime="2020-07-08T21:07:33+08:00">2020-07-08</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/课程记录/" itemprop="url" rel="index"><span itemprop="name">课程记录</span></a></span> </span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">评论数：</span> <a href="/posts/notesofCS231N/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/notesofCS231N/" itemprop="commentCount"></span> </a></span><span id="/posts/notesofCS231N/" class="leancloud_visitors" data-flag-title="Stanford CS231n 笔记"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">26k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">24 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><ul><li><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">homepage</a>;</li><li><a href="https://www.bilibili.com/video/av13260183?from=search&amp;seid=2308745029556209710" target="_blank" rel="noopener">2017 lecture video</a>;</li><li><a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">中文笔记</a></li><li>课程相关时间安排，课件，参考资料，作业详见homepage链接，以最新版为主；</li><li>官方的assignments和notes做得非常好，强烈推荐学习和反复观看；</li><li>时间原因，学习19版的时候恰好2020版春季课程将要开始，这一版换了team，里面加了不少新的research介绍，尤其是非监督，自监督之类的，因此会加入一下2020版的内容；</li><li>个人比较喜欢Justin Johnson的讲课风格和深度，因此配合他在UMICH (University of Michigan) 的 <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/" target="_blank" rel="noopener">EECS498: Deep Learning for Computer Vision</a>课一起看，可以相互补充（和CS231N有很大重叠， 但也些不同）；</li></ul><p><img src="/posts/notesofCS231N/summary.JPG" alt="提前把最后一节课的总结放出来。整个课程将按照这个fundamentals-CNN-applications来讲"></p><h2 id="Lecture1-Introduction"><a href="#Lecture1-Introduction" class="headerlink" title="Lecture1. Introduction"></a>Lecture1. Introduction</h2><p>Related courses in Stanford:</p><ul><li><p>CS131: Computer Vision: Foundations and Applications</p></li><li><p>CS231a: Computer Vision, from 3D Reconstruction to Recognition</p></li><li><p>CS 224n: Natural Language Processing with Deep Learning</p></li><li><p>CS 230: Deep Learning</p></li><li><p><strong>CS231n: Convolutional Neural Networks for Visual Recognition</strong></p></li><li><p>Andrew Ng的CS229虽然不在列表中，但个人觉得也值得一看，毕竟也属于经典ML课程。</p></li></ul><p>A Brief history of Computer Vision and CS 231n:</p><ul><li>从史前生物产生视觉开始，到后面的人类对于视觉的研究：相机，生物学研究，以及陆续地对于计算机视觉/机器视觉的系统研究，在社会各个领域结合产生的特定任务的建模设计等工作，这里再一次提到了David Marr的《vision》一书对整个计算机视觉领域的奠基于推动作用。</li><li>介绍卷积神经网络CNN对计算视觉的推动作用，简介了CNN的历史以及在各个视觉有关问题上的强大作用。</li><li>推荐课本教材：《Deep Learning》 by Goodfellow, Bengio, and Courville.</li></ul><a id="more"></a><h2 id="Lecture2-Image-Classification"><a href="#Lecture2-Image-Classification" class="headerlink" title="Lecture2. Image Classification"></a>Lecture2. Image Classification</h2><p>material: 主讲人<a href="http://cs.stanford.edu/people/jcjohns/" target="_blank" rel="noopener">Justin Johnson</a>编写的<a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="noopener">python numpy tutorial</a>，个人觉得是个很不错的教程。</p><p>官方的<a href="http://cs231n.github.io/classification/" target="_blank" rel="noopener">notes1—image classification</a>和<a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="noopener">notes2-linear classification</a>是对本次课的一个总结，补充和拓展（其中第二个笔记大部分是下次课的内容，放在这里我猜可能是预习吧），可以说写得十分详细了，而且常温常新。</p><ul><li>首先阐述对于计算机而言，为什么分类图片很难（光照，视点，类似纹理背景等）；</li><li>介绍了一些传统方法对此的努力，但是这些方法效果不好，具有单一性，鲁棒性和generalization都很差；</li><li>引入data-drive approcah, 也就是收集数据，训练，预测三段式方法；</li><li>介绍Nearest Neighbours分类器，并阐释不同K值和distance metric选取的不同performance（L1和L2的选取与坐标轴旋转对数据引起的改变）；</li><li>介绍交叉验证的作用，以及说明K-NN的维度诅咒（处理大数据吃力）和 predict速度慢的缺陷（需要一个个和训练集图片计算distance）；</li><li>引入线性分类器，权重矩阵<strong>W</strong>，偏置矩阵<strong>b</strong>，并在第二个官网的notes中详细地介绍了SVM和Softmax两种loss衡量方式；</li></ul><p><a href="https://www.jianshu.com/p/43318a3dc715" target="_blank" rel="noopener">关于交叉熵，相对熵以及为什么选择交叉熵作为损失函数</a>。</p><p>官网推荐的一篇阅读review <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank" rel="noopener">A Few Useful Things to Konw About Machine Learnig</a>，写于2012年，但是仍不过时，可以一读。文中写到的12个”folk wisdom”:</p><ul><li>Learning = representation+evaluation+optimization;</li><li>It’s generalization that counts;</li><li>Data alone is not alone;</li><li>Overfitting has many faces;</li><li>Intuition falis in high dimensions;</li><li>Theoretical guarantees are not what they seem;</li><li>Feature engineering is the key;</li><li>More data beats a cleverer algorithm;</li><li>Learn many models, not just one;</li><li>Simplicity does not imply accuracy;</li><li>Representable does not imply learnable;</li><li>Gneralization does not imply causation;</li></ul><h2 id="Lecture3-Loss-Functions-and-Optimization"><a href="#Lecture3-Loss-Functions-and-Optimization" class="headerlink" title="Lecture3. Loss Functions and Optimization"></a>Lecture3. Loss Functions and Optimization</h2><p>官方笔记：<a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="noopener">linear classification</a>, <a href="http://cs231n.github.io/optimization-1/" target="_blank" rel="noopener">optimization</a>有更为详细的课程内容解释和总结;</p><p>其中linear classification笔记中有个交互式<a href="http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/" target="_blank" rel="noopener">demo</a>，主要用来体会SVM和Softmax来训练线性分类器的过程，包括调节正则项，学习率等超参数；</p><ul><li>上节课引入linear classification，现在需要loss function定量分类器对分类结果与training data的GT之间的差异，需要一些optimization方式去有效寻找模型参数可以最小化loss；</li><li>介绍两种loss衡量metric: Multiclass SVM和Softmax，并认为两种方式在训练分类器时差异不会很大，SVM是为了拉大正确的score与非正确score之间的差值到设定的margin，所以分的越差惩罚越重，分的越好基本上不再去多费精力，而Softmax则不断将正确的probability拉的更高，错误的拉的更低；</li><li><p>由于满足loss function的权重矩阵<strong>W</strong>有无数个（针对linear classification），之间可成正比例关系，因此需要引入正则项让模型自己适应数据，得到generalization比较高的模型参数；</p></li><li><p>针对优化参数方法，通过随机搜索过渡到梯度下降gradient descent，在官方笔记中更加仔细地介绍了针对实际工程应用的梯度下降法（数值梯度，分析梯度等），满足计算性能之间的trade-off</p></li><li><p>最后针对图像特征，介绍了color histogram, HoG (Histogram of Oriented Gradients), Bag of Words等方法。将feature映射到更高维的空间，使分类问题变得容易，这也是CNN的一大作用，从而引入下一节主题；</p></li></ul><p><img src="/posts/notesofCS231N/3_1.JPG" alt="特征映射"></p><p>在这里插一下sigmoid, softmax和cross entropy之间的联系，我们一般说的分类是单标签多分类，也就是说一张图片只会有一个标签，标签与标签之间是互斥的，而softmax计算的类别概率是类别互斥的，因为和为1，而针对多标签分类，也就是说一张图片可能有多个标签存在，标签之间不是非此即彼的，类别不互斥，此时用sigmoid激活函数，因为sigmoid是互相独立的计算，只判断此神经元是否属于该位置对应的类别，所以在计算多分类交叉熵时是每个神经元按照二分类交叉熵算然后求和平均。参考<a href="https://www.zhihu.com/question/341500352/answer/795497527" target="_blank" rel="noopener">blog1</a>, <a href="https://blog.csdn.net/tsyccnh/article/details/79163834" target="_blank" rel="noopener">blog2</a>.</p><p>梯度下降算法理解：</p><p>假设一个模型拟合表达式为：$h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$，其中$\theta_{i}, x_{i}$一般都是矩阵，对于一个输入，我们希望计算的值与真实函数$y$尽可能得接近，直接照着真实函数分布去“依葫芦画瓢”比较困难，所以来最小化他们之间得残差来搜索拟合函数的参数，假设我们用平方损失函数：$J(\theta)=\frac{1}{2} \sum \limits _{i=1}^{m}(h_{\theta}(x)-y)^{2}$，现在我们对每一个超参数$\theta_{j}$求偏导，得到关于它得梯度（函数上升最快得方向），由于我们是想找最小值，所以要反方向走（实际可以在损失函数前加个符号，转为找最大值）：</p><script type="math/tex;mode=display">\begin{array}{c}
\frac{\partial}{\partial \theta_{j}} J(\theta)=\frac{\partial}{\partial \theta_{j}} \frac{1}{2}(h _{\theta}(x)-y)^{2}=(h _{\theta}(x)-y) \cdot \frac{\partial}{\partial \theta_{j}}(h _{\theta}(x)-y) \\
=(h _{\theta}(x)-y) \cdot \frac{\partial}{\partial \theta_{j}}\left(\sum_{i=0}^{n} \theta_{i} x_{i}-y\right)=(h _{\theta}(x)-y) x_{j}
\end{array}</script><p>根据梯度，加上步长learning rate, 对参数$\theta_{j}$进行更新：$\theta_{j}:=\theta_{j}-\alpha(h _{\theta}(x)-y) x_{j}$</p><p>通过不断地迭代，使函数往最小值逼近。在深度学习实际应用时，训练的数据样本可能很大，一次迭代要计算所有的样本会非常耗时，所以利用SGD方法，抽取一些数据作为batch，认为可以近似代表整体样本的分布，每次换不同的样本去学习，也可以得到近似的解（但这也只是工程近似）。深度学习中的模型拟合函数是非常庞大而又复杂的，结合BP加梯度下降可以使函数收敛到局部最小值。</p><p><img src="/posts/notesofCS231N/3_2.JPG" alt="模型优化过程"></p><h2 id="Lecture4-Introduction-to-Neural-Networks"><a href="#Lecture4-Introduction-to-Neural-Networks" class="headerlink" title="Lecture4. Introduction to Neural Networks"></a>Lecture4. Introduction to Neural Networks</h2><p>这一节官方指定阅读的笔记为：<a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="noopener">optimization-2-bp</a>, <a href="http://cs231n.stanford.edu/handouts/linear-backprop.pdf" target="_blank" rel="noopener">linear-classifier-bp</a>; 此外还给了一些推荐的阅读材料，以供选择，其中包括一份<a href="http://cs231n.stanford.edu/handouts/derivatives.pdf" target="_blank" rel="noopener">向量，矩阵，BP的导数推导示意笔记</a>，Yan leCun在1998年写的名为“Efficient BackProp”的<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">论文</a>，两篇解释BP的博客（<a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">1</a>, <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">2</a>）和MIT <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/" target="_blank" rel="noopener">Artificial Intelligence公开课</a>中有关BP讲解的课程视频，此外还有一篇ML中automatic differentiation的<a href="https://arxiv.org/abs/1502.05767" target="_blank" rel="noopener">survey</a>. 个人感觉如果对微积分和线性代数比较熟悉的话，看一下官方指定阅读的两篇笔记就差不多了，主要是为了深入理解BP，有时间的话也强烈推荐那篇推荐阅读的博客2，是<a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Micahel Nielsen</a>写的《nndl》教材中的一节，在入门DL的时候我就是看的这本书，十分经典耐读。BP的使用大多是对矩阵或者说tensor张量进行的，如果对矩阵求导不熟悉，或者想系统的学习巩固下，可以移步<a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术-上</a>和<a href="https://zhuanlan.zhihu.com/p/24863977" target="_blank" rel="noopener">矩阵求导术-下</a>，上-篇写的是标量对矩阵的求导方法，下-篇写的是矩阵对矩阵的求导方法，都是通过标量上全微分表达式来进行推广和计算的：</p><p>“<strong>导数与微分的联系是计算的枢纽</strong>，标量对矩阵的导数与微分的联系是$d f=\operatorname{tr}\left(\nabla_{X}^{T} f d X\right)$，先对$f$求微分，再使用迹技巧可求得导数，特别地，标量对向量的导数与微分的联系是$d f=\nabla_{x}^{T} f d \boldsymbol{x}$；矩阵对矩阵的导数与微分的联系是$\operatorname{vec}(d F)=\frac{\partial F^{T}}{\partial X} \operatorname{vec}(d X)$，先对$F$求微分，再使用向量化的技巧可求得导数，特别地，向量对向量的导数与微分的联系是$d \boldsymbol{f}=\frac{\partial \boldsymbol{f}^{T}}{\partial \boldsymbol{x}} d \boldsymbol{x}$。”（其中，$x,\boldsymbol {x}, X$分表代表标量，向量和矩阵）</p><p>我想对矩阵的求导可能是本节课的一个小重点以及后面进行DL理论学习的基石。</p><p><img src="/posts/notesofCS231N/4_1.JPG" alt="2020版的第四讲对BP做了更为细致的讲解"></p><p><img src="/posts/notesofCS231N/4_2.JPG" alt="实际计算不会存储jacobian矩阵，而且隐式计算"></p><ul><li><p>该课延续上一讲提出的linear classifier,介绍了神经元的概念，历史和应用，引入fully-connected network；</p></li><li><p>指出如果层数加多，再加上激活函数，损失函数等，网络所代表的拟合函数表达式将非常庞大而复杂，采用梯度下降的方式训练时，推出分析表达式非常不现实和实际（如果想要零时改变一个函数就要重新推算）；</p></li><li><p>由于我们不关心函数梯度具体表达式，我们只是想通过计算梯度值来更新参数，所以采用computation graph的方式来逆推local gradient，forward + backpropagation形成一个完美体系（upstream gradient, local gradient）；</p></li></ul><p>在此课之前我推导过全连接和卷积神经网络的BP，就是当成链式法则”chain rules“来对待，而这次课给我了一个数学公式之外一个更加形象化理解，也即是”computation graph“和”门电路“：</p><blockquote><p>Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.</p></blockquote><p>而网络通过一系列”add gate”, “mul gate”, “max gate”等”gate”一步步垒成复杂的函数，而且部分组织可以组合成一个特殊的”gate”，也可以被分解成其他简易的”gate”，方便我们进行梯度计算（注意variable分流forward之后bp要加起来+=）。通过简单的example可以了解到”gate” input和output之间的梯度分配关系，非常形象生动。</p><p><img src="/posts/notesofCS231N/4_3.JPG" alt></p><h2 id="Lecture-5-Convolutional-Neural-Networks"><a href="#Lecture-5-Convolutional-Neural-Networks" class="headerlink" title="Lecture 5. Convolutional Neural Networks"></a>Lecture 5. Convolutional Neural Networks</h2><p>该节课的<a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">官方笔记</a>对卷积神经网络CNN来龙去脉和计算都介绍得很详细，同时也对每个层（Conv layer, pooling layer, fc layer）都做了全面的回顾和介绍。</p><p>另有一篇<a href="https://www.matongxue.com/madocs/32.html" target="_blank" rel="noopener">博客</a>解释卷积神经网络中”卷积“一词和信号与系统中得卷积操作的联系，知乎也有个理解卷积的<a href="https://www.zhihu.com/question/22298352" target="_blank" rel="noopener">回答</a>，看完之后可以加深对卷积核的理解。</p><ul><li>首先介绍感知机，神经元和CNN的历史；</li><li>简介一个卷积神经网络的结构以及直觉上层级学习的内容和特征；</li><li>大部分时间讲解卷积的计算过程；</li></ul><p><a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="noopener">Convolution arithmetic</a></p><h2 id="Assignment-1"><a href="#Assignment-1" class="headerlink" title="Assignment 1"></a>Assignment 1</h2><p><a href="http://cs231n.github.io/assignments2019/assignment1/" target="_blank" rel="noopener">assignment page</a>;</p><p>reference：</p><ul><li><p><a href="https://github.com/Wangxb06/CS231n" target="_blank" rel="noopener">github-code</a>;</p></li><li><p>参考培训机构深度之眼的cs231n-camp的文档</p></li></ul><p>具体过程和结果参见这个<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments" target="_blank" rel="noopener">repository</a></p><p>注意：一开始运行<code>knn.ipynb</code>文件时可能显示<code>scipy.misc.imread</code>缺失，这是因为新版的<code>scipy</code>包去掉了<code>imread</code>这个module，解决方法是可以在<code>data_utils.py</code>中的<code>from scipy.misc import imread</code>改为<code>from imageio import imread</code>。</p><h2 id="Lecture-6-Deep-Learning-Hardware-and-Software"><a href="#Lecture-6-Deep-Learning-Hardware-and-Software" class="headerlink" title="Lecture 6. Deep Learning Hardware and Software"></a>Lecture 6. Deep Learning Hardware and Software</h2><p>讲义中包含的官方制作的两大深度学习框架（Pytorch, TensorFlow）的tutorial，我把它们直接下载下来放在<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments" target="_blank" rel="noopener">GitHub</a>便于观看。</p><p>2017版中该课在train neural network之后，19年改为之前，由于两年间框架格局也在发生改变，所以以最新版<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture06.pdf" target="_blank" rel="noopener">slides</a>为主（17版主推TensorFlow，也花了一大半时间介绍TF，19版偏向于PyTorch）。</p><p>结合前面的tutorial一起看，可以当作非常好的入门学习材料。</p><p><img src="/posts/notesofCS231N/dynamic_static_graph.JPG" alt="动态计算图和静态计算图"></p><p><img src="/posts/notesofCS231N/advice_for_frameworks.JPG" alt></p><p><img src="/posts/notesofCS231N/6_1.JPG" alt="利用ONNX制作静态pytorch模型"></p><p><img src="/posts/notesofCS231N/6_2.JPG" alt="利用TorchScript制作pytorch静态模型"></p><p><img src="/posts/notesofCS231N/6_3.JPG" alt="分布式利用pytorch训练数据，以及使用第三方库Horovod进行分布式训练：https://github.com/horovod/horovod/blob/master/docs/pytorch.rst"></p><h2 id="Lecture-7—8-Training-Neural-Networks-I-amp-II"><a href="#Lecture-7—8-Training-Neural-Networks-I-amp-II" class="headerlink" title="Lecture 7—8. Training Neural Networks I &amp; II"></a>Lecture 7—8. Training Neural Networks I &amp; II</h2><p>这两节课硬货非常非常的多，很多东西都值得深究。课程讲的不是很清楚，因为内容多，所以需要线下仔细看看笔记。笔记主要分为三个部分：</p><p>1.<a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="noopener">neural fc network model</a>: 介绍来自脑启发的神经元neuron，常用的激活函数（sigmoid, tanh, ReLU, Leaky ReLU, Maxout, ELU, SELU），全连接神经网络的一些结构问题，比如是否可以拟合任意连续函数，多少层足够，多大足够，以及表征能力和前向计算等。</p><p>Sigmoid在训练时除了会有梯度消失的情况外还有输出不是0均值（zero-centered）的情况：这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。以 $f=sigmoid(Wx+b)$为例， 假设输入均为正数（或负数），那么对$W$的导数（sigmoid的导数总是正的）总是正数（或负数），这样在反向传播过程中要么都往正方向更新，要么都往负方向更新，导致有一种捆绑效果，使得收敛缓慢。</p><blockquote><p>“<em>What neuron type should I use?</em>” Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.</p><p>larger networks will always work better than smaller networks, but their higher model capacity must be appropriately addressed with stronger regularization (such as higher weight decay), or they might overfit. We will see more forms of regularization (especially dropout) in later sections.</p></blockquote><hr><p>2020.6更新：</p><p>最近Yolo v4使用了<strong>Mish</strong>激活函数，2020年新版CS231N也加入了Swish激活函数，后续可以实验看是否高于ReLU，但是可能要注意权重初始化的问题。</p><p>Swish: $f(x) = x * sigmoid(\beta x)$</p><p>Mish: $f(x)=x*tanh(ln(1+e^{x}))$</p><p>以及更多激活函数对比见<a href="https://zhuanlan.zhihu.com/p/139696588" target="_blank" rel="noopener">此</a>。</p><p><img src="/posts/notesofCS231N/7_5.JPG" alt="激活函数选择建议"></p><p>2.<a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener">one time setup</a>: 介绍数据预处理（先减去mean image，或者是mean RGB vector中心化，然后除以std进行normalization；进行PCA降维数据，进行decorrelated或者利用whitening得到各向同性的Gaussian blob；一般利用CNN训练时不需要进行太复杂的数据预处理），<strong>权重初始化</strong>（非常重要和经验化的一点，不要以零对权重进行初始化，而是随机高斯分布采样初始化，“Xavier” initialization, 并且进行<code>w = np.random.randn(n) * sqrt(1/n)</code>对方差进行校准，为的是让输出的方差与输入的相同，后来何恺明的研究中表明，采用ReLU激活函数，初始化为<code>w = np.random.randn(n) * sqrt(2.0/n)</code>见论文”<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>“，在conv中n是$kernel _size^{2}*input _channels$），batch normalization（直接作为一层插入FC/CNN与激活函数层之间，把batch数据进行标准高斯分布化，当然后面还有一个超参的逆向高斯化，主要是为了让网络自己去判断这哪种对自己适合是一种启发式处理），正则化（L1，L2正则化，一般L2更好些；Max norm constraints设置权重边界；Dropout进行组合训练或者是认为进行数据增强）和损失函数（classification, regression (最好不要硬来，因为没有规律，试着往分类转化), structured prediction）等问题。</p><p><img src="/posts/notesofCS231N/7_6.JPG" alt></p><p><img src="/posts/notesofCS231N/8_1.JPG" alt="网络权重初始化问题"></p><p>权重初始化中还有一个transfer learning技巧，利用ImageNet的预训练模型，再根据自己的数据进行层的选择性finetune. 不过目前的研究来看，在object detection领域，自训练似乎比预训练更好（数据够多，卡够多的情况下，数据少还是老老实实用预训练吧），参照恺明大神的<a href="https://arxiv.org/abs/1811.08883" target="_blank" rel="noopener">Rethinking ImageNet Pre-training</a>。</p><p>Dropout: Forces the network to have a redundant representation; Prevents co-adaptation of features</p><p><a href="https://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">对Dropout起防过拟合作用的解释—组合派和噪声派</a>；<a href="https://blog.csdn.net/fu6543210/article/details/84450890" target="_blank" rel="noopener">对Inverted Dropout中神经元进行Dropout之后为何需要对余下工作的神经元进行rescale的一个解释—乘再除确保下一层输出期望不变</a></p><p>（除以p是为了保证训练和测试时输入给下一层的期望是一样的，那些失活的神经元损失的值会通过其他神经元补回来，这样就等于加强了某些神经元的鲁棒性，如果不除的话等于是随机挑选神经元训练，这样可能达不到预期的增强效果。）</p><p>Data Augmentation: translation, rotation, stretching（拉伸）, shearing（修剪）, lens distortions（镜头畸变）, color jitter （颜色抖动）等等。</p><p>Batch Normalization（类似于”白化“，实现数据独立同分布，但是BN并没有实现独立同分布，仅仅是压缩再变换）: 在前向传播时，分两种情况进行讨论：如果是在train过程，就使用当前batch的数据统计均值和标准差，并按照第二章所述公式对Wx+b进行归一化，之后再乘上gamma，加上beta得到Batch Normalization层的输出；如果在进行test过程，则使用记录下的均值和标准差，还有之前训练好的gamma和beta计算得到结果（作业中记录的值也是采用了指数权重滑动平均的方法）。</p><p>此外，CNN使用时叫spatial batchnorm，主要是考虑到卷积神经网络的参数共享特性。（<a href="https://www.zhihu.com/question/269658514" target="_blank" rel="noopener">知乎回答</a>)</p><p>之前大家对BN的理解就是为了解决ICS (internal covariate shift)，让每一层输入的分布不再变化莫测（网络需要不停调整参数适应数据分布变化），同时也要加上线性变换层保留一些模型需要的原始信息，也让输出的数据值落在激活函数的敏感区域，也可作为正则化的手段（进一步解读见：<a href="https://zhuanlan.zhihu.com/p/55852062" target="_blank" rel="noopener">1</a>, <a href="https://www.cnblogs.com/skyfsm/p/8453498.html" target="_blank" rel="noopener">2</a>, <a href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/" target="_blank" rel="noopener">3</a>）。</p><p>但是MIT 发表在NeurIPS 2018上的研究”<strong>How Does Batch Normalization Help Optimization</strong>“发现二者并无关系。研究者证明 BatchNorm 以一种基础的方式影响着网络的训练：它使相关优化问题的解空间更平滑了。这确保梯度更具预测性（见机器之心的<a href="https://www.jiqizhixin.com/articles/2018-11-13-8" target="_blank" rel="noopener">翻译</a>）。</p><p>另外，最近，2020 DeepMind刚出的论文”<strong>Batch Normalization Biases Deep Residual Networks Towards Shallow Paths</strong>“研究了BN为何可以提升深度残差网络的训练深度：”<strong>在初始化阶段，批归一化使用与网络深度的平方根成比例的归一化因子来缩小与跳跃连接相关的残差分支的大小</strong>。这可以确保在训练初期，深度归一化残差网络计算的函数由具备表现良好的梯度的浅路径（shallow path）主导“（见机器之心的<a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650783208&amp;idx=5&amp;sn=5ac1cbc22336a9e949e42a49310e5106&amp;chksm=871a7996b06df080454fe765caa7440b015329d4d67d3c70d5a2c807f928f8ec4151ac1ec075&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1585028895021&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=da21d41c58faea5bd0311609648073cf421257a6d02c4441b3868a2b447633270e7de10abb2aa824a08b5f711c6c34e277a648a52dd60752d3155ec6d6270b2dc5745360dae6f2aea673abbca5e3864c&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10&amp;version=62080079&amp;lang=zh_CN&amp;exportkey=AyJIB1yGzg%2B5xqUfbzluzjc%3D&amp;pass_ticket=IUc1Fn13evk5P8L18SlGS8pMmpxl8gKIhcPljl38AnLtjlv3XajT2eK9gnLXe6OU" target="_blank" rel="noopener">报导</a>）。</p><p><img src="/posts/notesofCS231N/7_1.JPG" alt="BatchNorm，CNN中常用"></p><p><img src="/posts/notesofCS231N/7_2.JPG" alt="LayerNorm，RNN中常用（因为RNN的输入样本不一定都是同样长度的序列）"></p><p><img src="/posts/notesofCS231N/7_3.JPG" alt="InstanceNorm，后面的style transfer会用到"></p><p><img src="/posts/notesofCS231N/7_4.JPG" alt="GroupNorm受HOG特征启发，有些特征是相互联系的，针对目标检测这样batch_size很小的情况做了优化，结合LN和IN"></p><p>出了上述几种变体外，还有个结合版本的Switchable Normalization，给IN, BN, LN分配权重，让网络自己去学习哪种归一化方式适合（<a href="https://blog.csdn.net/liuxiao214/article/details/81037416" target="_blank" rel="noopener">总结1</a>，<a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="noopener">总结2</a>，在NLP中也UCB也通过观测BN的异常情况提出了最新的PN，见<a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247486486&amp;idx=1&amp;sn=3901b342dab93dae889a744887cce454&amp;chksm=970c24c0a07badd676bcaf24833265eddede3fefc468c38031cf54c19f3d384233b1baf41375&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1587291237878&amp;sharer_shareid=4ed82b8c86f7bdeb368019cfe429ee62&amp;key=bdd3db2c3f0f72f7f2db9c82510b3febc816e0a31a5eec50b93b6b3bf0119676f36421d454c5468967fe63a0f4e502b7215e6264ee0cf4d8a2c1fb594d7b81f30102c4b35c6f73a07e6e38a5676d59c4&amp;ascene=1&amp;uin=Mjg0MTMzNDQzMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6209005f&amp;lang=zh_CN&amp;exportkey=A4N3AlFSdyWYx%2FUgnxpPAk8%3D&amp;pass_ticket=gmEgMHc7Z0mPJZbQjqSRRs0scY2%2F8o3EWf%2FiXl5R0OEoJMA1VU0QqZve4JWTLBSa" target="_blank" rel="noopener">此</a>）。</p><p>在防止模型过拟合方面，正则化方法除了对大权重进行惩罚外，还有引入“random noise”的思路，具体有以下几种（如下图所示）。2019年 CS231N 跟进潮流，新加了Cutout和Mixup方法，其中MixUp方法本人在custom dataset上用过，对小数量数据确实有用。</p><p><img src="/posts/notesofCS231N/8_2.JPG" alt="对模型进行正则化的一些方法"></p><blockquote><ul><li>The recommended preprocessing is to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature</li><li>Initialize the weights by drawing them from a gaussian distribution with standard deviation of sqrt(2/n), where n is the number of inputs to the neuron. E.g. in numpy: <code>w = np.random.randn(n) * sqrt(2.0/n)</code>.</li><li>Use L2 regularization and dropout (the inverted version)</li><li>Use batch normalization</li><li>We discussed different tasks you might want to perform in practice, and the most common loss functions for each task</li></ul></blockquote><p>3.<a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">training dynamics &amp; evaluation</a>: 梯度检查，一些网络学习的注意事项，参数更新，超参搜索，模型ensemble等。这里的重点在于optimizer优化器，从一开始的普通SGD到带<a href="http://zh.gluon.ai/chapter_optimization/momentum.html" target="_blank" rel="noopener">动量的SGD</a>，然后是一些自适应学习率的优化器，比如目前最好用的Adam等。</p><p>Momentum updateing: <a href="https://blog.csdn.net/dawningblue/article/details/89487418" target="_blank" rel="noopener">关于“动量”更新参数的动力学解释—“动量”应当是“阻尼系数”，点的质量和运动时间看成“1”，然后推导关于位置的更新</a>；</p><p>Nesterov Momentum: <a href="https://blog.csdn.net/dawningblue/article/details/89487480" target="_blank" rel="noopener">利用趋势点希望可以跳出“坑”的困境</a>；</p><p>Second order optimization: quasi-Newton methods, <a href="https://blog.csdn.net/Im_Chenxi/article/details/80546742" target="_blank" rel="noopener">数学公式</a>，<a href="https://zh.wikipedia.org/wiki/黑塞矩陣" target="_blank" rel="noopener">Hessian矩阵</a>，二阶方法是利用函数的曲率，也就是导数的导数来进行参数更新，速度会更快，但是Hessian阵难以计算，带数学推导的知乎推荐论文<a href="https://arxiv.org/abs/1401.7020" target="_blank" rel="noopener">A Stochastic Quasi-Newton Method for Large-Scale Optimization</a>. 2020.2.20，Google brain放出一篇二阶优化算法训练Transformer的论文<a href="https://arxiv.org/abs/2002.09018" target="_blank" rel="noopener">Second Order Optimization Made Practical</a>. 关于非线性优化方法的一些<a href="https://www.cnblogs.com/maybe2030/p/4751804.html" target="_blank" rel="noopener">区别</a>：一阶法（梯度下降，最速下降），二阶（牛顿法，拟牛顿法），共轭梯度等。</p><blockquote><p><strong>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）</strong></p><p><strong>根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</strong></p></blockquote><p><a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">Adam</a>优化算法: 其实就是momentum + RMSProp，减缓mini-batch训练中的抖动，同时自适应更新学习率，对超参的选择比较鲁棒。相关解释：<a href="https://blog.csdn.net/liuyuemaicha/article/details/52497512" target="_blank" rel="noopener">矩估计</a>，<a href="https://zhuanlan.zhihu.com/p/32335746" target="_blank" rel="noopener">指数加权移动平均与初始时刻阶段偏差修正</a>，<a href="https://moodle2.cs.huji.ac.il/nu15/pluginfile.php/316969/mod_resource/content/1/adam_pres.pdf" target="_blank" rel="noopener">a review ppt</a>.</p><blockquote><p>The two recommended updates to use are either SGD+Nesterov Momentum or Adam.（有时候带动量的SGD法可能比Adam训练出的效果更好，但是调参的难度可能大点）</p></blockquote><p>优化算法总结推荐阅读Juliuszh的知乎专栏：<a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">1</a>，<a href="https://zhuanlan.zhihu.com/p/32262540" target="_blank" rel="noopener">2</a>，<a href="https://zhuanlan.zhihu.com/p/32338983" target="_blank" rel="noopener">3</a></p><p>另有一篇大牛Sebastian Ruder写的<a href="https://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a>，最近一次更新在2020.3.20，加入了新的优化器，博客中也提供了arXiv版本。</p><p>超参Hyperparameters主要包括网络架构，学习率以及其deacy 机制，参数更新机制，和正则化regularization选择及其强度。面对这些超参，在实验时可以通过一下步骤来进行：首先检查模型初始化是的loss，一般权重对每一类都是零认知的，所以会给出平均猜测；然后拿出小部分样本进行实验，进行初始化，给出一些学习率然后对其训练，从中挑出靠谱的；接着再组合学习率和deacy参数，看看哪些不错；然后对上面最好的进行长时间训练，一言以蔽之就是先在数据中的小范围中进行预演。此外也要注意learning rate中的deacy参数选择和加入时间，否则可能使学习不充分。训练刚开始可以采取<a href="https://blog.csdn.net/sinat_36618660/article/details/99650804" target="_blank" rel="noopener">warm up</a>或者grad warm up处理，这个主要是针对大数据，大网络中权重的初始平稳问题，详见<a href="https://www.zhihu.com/question/338066667" target="_blank" rel="noopener">知乎回答</a>。</p><p><img src="/posts/notesofCS231N/8_4.JPG" alt="学习率下降处理"></p><p><img src="/posts/notesofCS231N/8_5.JPG" alt="warmup"></p><p><img src="/posts/notesofCS231N/8_3.JPG" alt="超参选择方法"></p><blockquote><p>To train a Neural Network:</p><ul><li>Gradient check your implementation with a small batch of data and be aware of the pitfalls.</li><li>As a sanity check, make sure your initial loss is reasonable, and that you can achieve 100% training accuracy on a very small portion of the data</li><li>During training, monitor the loss, the training/validation accuracy, and if you’re feeling fancier, the magnitude of updates in relation to parameter values (it should be ~1e-3), and when dealing with ConvNets, the first-layer weights.</li><li>The two recommended updates to use are either SGD+Nesterov Momentum or Adam.</li><li>Decay your learning rate over the period of the training. For example, halve the learning rate after a fixed number of epochs, or whenever the validation accuracy tops off.</li><li>Search for good hyperparameters with random search (not grid search). Stage your search from coarse (wide hyperparameter ranges, training only for 1-5 epochs), to fine (narrower rangers, training for many more epochs)</li><li>Form model ensembles for extra performance</li></ul></blockquote><h2 id="Lecture-9-CNN-Architecture"><a href="#Lecture-9-CNN-Architecture" class="headerlink" title="Lecture 9. CNN Architecture"></a>Lecture 9. CNN Architecture</h2><p>这一节主要讲CNN的几个经典结构（AlexNet, VGG, GoogLeNet, ResNet），课外阅读材料相关网络对应的论文。</p><p>推荐阅读Justin ho对CNN architecture的介绍文章，链接在<a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">此</a>。</p><p><img src="/posts/notesofCS231N/9_1.JPG" alt="2019较2017加了最新的NAS技术，自动搜索最优结构网络"></p><p><strong>AlexNet</strong>: 开启CNN风潮的奠基之作，规划了CNN网络该有的部件（data augmentation, filter, maxpool, normalization, non-linearity activation function, fc, dropout, gpu加速等）。</p><p><img src="/posts/notesofCS231N/9_2.JPG" alt="AlexNet结构（5Conv+3FC），初版由于GPU显存小所以一半一半丢到两块GPU训练，最后合并"></p><p><strong>VGG</strong>: 引入小卷积核，一方面小卷积核堆叠拥有大卷积核同样的感受野，另一方面还可以减小参数，同时加深网络，提高拟合精度。后续的网络基本吸取了VGG网络中小卷积核（3x3, p=1, s=1）提取特征，mxpool (s=2) downsample 特征图，同时在缩小后的特征图上加倍filter数量保持时间复杂度。</p><p><img src="/posts/notesofCS231N/9_3.JPG" alt="现在以16和19层为主，且只有3x3卷积核，和s=2的maxpool缩小特征图尺寸"></p><p><strong>GoogLeNet</strong>: 受到<a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">Network in Network</a>很大启发（一个是跨通道融合的1x1卷积核带来的MLP功能，提高局部特征的抽象表达能力，另一个是Global Average Pooling (GAP)，单独利用特征图分类），同时提升网络的深度和宽度。</p><p><img src="/posts/notesofCS231N/9_4.jpg" alt="GoogLeNet--22层，三个分类器，FC前面利用AP减少参数"></p><p><img src="/posts/notesofCS231N/9_5.JPG" alt="采用inception module进行堆叠，并增加网络宽度，同时采用1x1卷积核降维特征图层数，减少参数并增加非线性"></p><blockquote><p>为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉。</p></blockquote><p>之后Google对inception进行改进到v4版本(详见<a href="https://my.oschina.net/u/876354/blog/1637819" target="_blank" rel="noopener">blog</a>)，v2, v3主要是改小卷积核，v4是加入了Resnet的残差连接思想，然后将basic block或者bottleneck中的conv组合换成inception module。</p><p>GoogLeNet团队发表的有关GoogLeNet的论文：《Going deeper with convolutions》、《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》、《Rethinking the Inception Architecture for Computer Vision》、《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》</p><p><strong>ResNet</strong>：加深网络可以获得更好的性能，但是采用常规方法堆叠卷积加深优化困难，实际效果不好。</p><p>恒等映射，深网络中有不少是对浅网络的缝补，深层网络中，如果保留那些浅层网络的性能，同时又考虑采用恒等映射，那么效果应该不会比单独用浅层网络更差。但是直接学习恒等映射不如学习输出与恒等映射的残差有效（redidual模块会明显减小模块中参数的值从而让网络中的参数对反向传导的损失值有更敏感的响应能力，虽然根本上没有解决回传的损失小的问题，但是却让参数减小，相对而言增加了回传损失的效果，也产生了一定的正则化作用），因此引入跳级连接，ResNet诞生（参考<a href="https://blog.csdn.net/weixin_43624538/article/details/85049699?from=timeline&amp;isappinstalled=0" target="_blank" rel="noopener">PRIS-SCMonkey</a>，<a href="https://zhuanlan.zhihu.com/p/54289848" target="_blank" rel="noopener">Pascal</a>）。</p><p><img src="/posts/notesofCS231N/9_6.JPG" alt="basic block和bottleneck结构应对不同深度的网络，bottleneck结构应该受到GoogLeNet启发进行降维减少参数"></p><p><img src="/posts/notesofCS231N/9_7.JPG" alt="不同深度网络结构参数配置"></p><p>后续的研究者一直在思考为什么ResNet会如此简洁高效，它到底在解决网络学习中的什么问题？1.degradation问题，梯度有效性被跳级连接解决（何恺明在论文中指出梯度消失爆炸问题已经被BN等normalization手段解决得很好了，跳级连接是为了深层的网络参数对梯度更加敏感，保持前面梯度的相关性，因为前向传播的时候将原始的信息带出一部分。当然ResNet也可以在一定程度上缓解梯度弥散的问题，但是这不是最主要的，详见<a href="https://www.zhihu.com/question/64494691" target="_blank" rel="noopener">这个知乎回答</a>），且必须是1系数。多个浅层子网络代表的函数的组合函数，并不是真正意义上的深层特征网络。</p><blockquote><p><strong>神经网络的退化才是难以训练深层网络根本原因所在，而不是梯度消散。</strong>虽然梯度范数大，但是如果网络的可用自由度对这些范数的贡献非常不均衡，<strong>也就是每个层中只有少量的隐藏单元对不同的输入改变它们的激活值，而大部分隐藏单元对不同的输入都是相同的反应，此时整个权重矩阵的秩不高。并且随着网络层数的增加，连乘后使得整个秩变的更低。</strong>这也是我们常说的网络退化问题，虽然是一个很高维的矩阵，但是大部分维度却没有信息，表达能力没有看起来那么强大。<strong>残差连接正是强制打破了网络的对称性。</strong>(来自Orhan A E, Pitkow X. Skip connections eliminate singularities[J]. arXiv preprint arXiv:1701.09175, 2017.)</p><p>来自知乎专栏：<a href="https://zhuanlan.zhihu.com/p/42833949" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/42833949</a></p></blockquote><p>2.ResNet中恒等变换的意义目前有多种解释，我目前倾向的<a href="https://www.zhihu.com/question/293243905" target="_blank" rel="noopener">解释</a>是这种残差结构是为了动态自适应拟合训练数据集网络深度的一种方法，它除了可以平稳甚至加速训练外（跳级连接让不同分辨率的feature组合，梯度的相关性得到了保持，后续的DenseNet则用了更多的不同分辨率的特征组合），就是可以根据你的问题去慢慢补充浅层网络解决的最主要的拟合之外的小边边角角，让最后的函数更加平滑。网络输出的残差$F(x)$部分可能在某个block接近0，可能某个部分比较大，所以是一种自适应复杂度。</p><blockquote><p>ResNet是完成一个微分方程的差分形式，dx(t)/dt=H(t)=&gt;x(t+1)=x(t)+H(t). 所以ResNet就是用微分方程来描述一个动态系统，寻找合适的H(t)来构造系统。采用这个结构的好处是H(t)具有某种自适应特性（相比于普通的CNN），对网络的深度不会过于敏感（自适应差分方程的步长），同时简化网络的结构，让网络结构更加均匀化，从几何角度看就是制造了一个更为平直的流形并在其上进行优化，导致系统的收敛性更好。</p><p>不同的网络结构实际上构造了不同的解空间的流形，并且部分的定义了其上的度量，而不同的度量引出不同的曲率分布，这对系统的收敛性能影响重大。所以不同网络结构的差异表现在：（1）网络结构是否和问题匹配，是否是一个包含了可能解的足够紧致的空间；（2）由网络结构所部分定义的度量是否导致一个比较平直的均匀的流形（度量还被所采用的代价函数所部分定义）。从这两个角度就可以定性和半定量的分析不同网络结构的特性和关系。</p><hr><p>残差模块并不是就是恒等啊，我觉得可以理解为:当网络需要这个模块是恒等时，它比较容易变成恒等。而传统的conv模块是很难通过学习变成恒等的，因为大家学过信号与系统都知道，恒等的话filter的冲激响应要为一个冲激函数，而神经网络本质是学概率分布 局部一层不太容易变成恒等，而resnet加入了这种模块给了神经网络学习恒等映射的能力。</p><p>所以我个人理解resnet除了减弱梯度消失外，我还理解为这是一种自适应深度，也就是网络可以自己调节层数的深浅，不需要太深时，中间恒等映射就多，需要时恒等映射就少。当然了，实际的神经网络并不会有这么强的局部特性，它的训练结果基本是一个整体，并不一定会出现我说的某些block就是恒等的情况</p><p>来自知乎回答：<a href="https://www.zhihu.com/question/293243905" target="_blank" rel="noopener">https://www.zhihu.com/question/293243905</a></p></blockquote><p>在论文<a href="https://arxiv.org/abs/1603.09382" target="_blank" rel="noopener">Deep Networks with Stochastic Depth</a>中，作者设置概率p让residual block随机失活（均匀概率与线性概率，线性概率更有效些，符合ResNet的设计思想），只保留shortcut，结果精度也没有怎么下降，反而提高了训练速度，这也说明Identity Mapping的正确性。</p><p>对ResNet的改进以及后续其他网络：</p><p><img src="/posts/notesofCS231N/9_8.jpg" alt="改进downsample部分，减小信息流失"></p><p><img src="/posts/notesofCS231N/9_9.JPG" alt="把激活函数和BN放在residual模块中，称其为full pre-activation，其效果最好。论文来自ResNet原班人马，Identity Mappings in Deep Residual Networks，在该论文中也分析讨论了有关ResNet的其他设置问题"></p><p><img src="/posts/notesofCS231N/9_10.JPG" alt="GoogLeNet引入ResNet结构，Inception-ResNet-v1"></p><p><a href="https://arxiv.org/pdf/1605.07146.pdf" target="_blank" rel="noopener">Wide Residual Networks</a>认为ResNet中有些block去掉也没有影响，所以residual是最主要的影响因素，作者尝试加倍block中的卷积核数量，而不是增加深度，结果发现效果更好，而且由于加深宽度可以由并行化计算解决，所以计算也更加快速。另外，作者也使用了卷积层上的dropout，结果也可以得到一些提升（论文指出何恺明他们是在identity part使用dropout效果不好，所以他们用在这，我想可能是因为拓宽了宽度，特征图增多，可能带来冗余信息，存在过拟合信息）。我没有仔细看论文，也没看源代码，不清楚作者是不是随机给特征图某个元素置0来进行的，实际上给卷积层做dropout的操作并不常见，一般都是BN就足够了，有部分研究是针对卷积层上或者特征图上的droput的，而且认为随机置某元素为0并不能起到作用，因为特征图一般是局部关联的，所以提出了区域置0，某个特征图置0等操作，见此知乎<a href="https://www.zhihu.com/question/52426832/answer/926104467" target="_blank" rel="noopener">回答</a>。</p><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Aggregated Residual Transformations for Deep Neural Networks</a>: ResNeXt，来自ResNet的作者，想法和上面的差不读，在一个block里采用多组重复卷积核操作，拓宽网络。</p><p><img src="/posts/notesofCS231N/9_11.JPG" alt="ResNet和ResNeXt结构差异"></p><p><a href="https://arxiv.org/pdf/1709.01507.pdf" target="_blank" rel="noopener">Squeeze-and-Excitation Networks</a> : SENet是在网络的特征通道之间进行建模，显示地通过FC去得到通道之间的依赖关系，然后利用最后的sigmoid激活函数给每个通道附一个新的权重，依照这个重要程度去提升有用的特征抑制用处不大的特征 (feature recalibration)。相关介绍见作者胡杰在机器之心的<a href="https://www.cnblogs.com/bonelee/p/9030092.html" target="_blank" rel="noopener">解释</a>。</p><p><img src="/posts/notesofCS231N/9_12.JPG" alt="在inception module和residual module中加入SE Block结构"></p><p><a href="https://arxiv.org/abs/1605.07648" target="_blank" rel="noopener">FractalNet: Ultra-Deep Neural Networks without Residuals</a>: 分形（从多个层次、多个角度、多个组成成分来共同解决一个问题）网络结构，类似于ResNet，但是认为深层网络的关键是浅层到深层的有效过度，residual representation不是必须的。分阶段组合不同分辨率特征，达到了类似于教师-学生机制、深度监督的效果。由于比较复杂，所以没有ResNet用的广泛，但是思想很有深度，其中最突出的contribution就是drop path。在下图的示意中，从输入到预测有很多条路可以走通，drop path机制就是随机失活某些路径，假如其中一条承担的作用是比较大，那么一旦被失活，其他网络就得承担全部的责任，加强其学习力度，所以一方面防止过拟合，另一方面让整个网络变得非常鲁棒，不会出现退化问题，参考此<a href="https://blog.csdn.net/wspba/article/details/73251731" target="_blank" rel="noopener">博客</a>。</p><p><img src="/posts/notesofCS231N/9_13.JPG" alt="Fractal architecture"></p><p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">Densely Connected Convolutional Networks</a>: CVPR2017 Best paper，写的也非常简洁通透。其dense block的设计是ResNet的“完全版”，不仅提高了特征的利用率，减小了参数，也减轻了梯度弥散，每一dense block单独产生的特征图层数也相对较少，也便于训练，具体解读见此<a href="https://blog.csdn.net/u014380165/article/details/75142664" target="_blank" rel="noopener">博客</a>。但是一开始的DenseNet虽然参数少，但是由于复杂的的skip connection和concatenate操作，中间量的存在会占用很多显存，作者后续论文<a href="https://arxiv.org/abs/1707.06990" target="_blank" rel="noopener">Memory-Efficient Implementation of DenseNets</a>减缓了这个问题，现在PyTorch等框架也应该自动解决了这个问题，在一定程度上。</p><p><img src="/posts/notesofCS231N/9_14.JPG" alt="Dense Block结构，前面每一层都会和后面每一层沟通"></p><p>MobileNet, ShuffleNet等轻量级计算网络，通过引入depthwise separable convolution, pointwise group convolution, channel shuffle (顺序重分配每组的channel到新的组，减少每个组单独流动信息的局限性)等操作改变传统卷积计算方式，来加速计算。详见知乎白裳的<a href="https://zhuanlan.zhihu.com/p/35405071" target="_blank" rel="noopener">博客随笔</a>和AI之路的<a href="https://blog.csdn.net/u014380165/article/details/75137111" target="_blank" rel="noopener">详解</a>。</p><p>Meta-learning，NAS自动搜索合适架构，开山之作是Google的<a href="https://arxiv.org/abs/1611.01578" target="_blank" rel="noopener">Neural Architecture Search with Reinforcement Learning </a>，通过RNN确定架构中每个元素的参数，然后利用reinforcement learning奖励精度高的，惩罚精度差的，以此来学习。NAS领域交叉知识多，暂时无法深入理解。</p><p><img src="/posts/notesofCS231N/9_15.JPG" alt="google最新的EfficientNet"></p><h2 id="Assignment-2"><a href="#Assignment-2" class="headerlink" title="Assignment 2"></a>Assignment 2</h2><p>宝藏作业，值得认真做。</p><p><a href="http://cs231n.github.io/assignments2019/assignment2/" target="_blank" rel="noopener">assignment page</a>;</p><p>reference：</p><ul><li><p><a href="https://github.com/Wangxb06/CS231n" target="_blank" rel="noopener">github-code</a>;</p></li><li><p>参考培训机构深度之眼的cs231n-camp的文档</p></li></ul><p>具体过程和结果参见这个<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments" target="_blank" rel="noopener">repository</a></p><p>在做CNN那一节时（ubuntu环境，如果是Windows环境可能需要安装python for c++等库），如果在fast_layer.py环节出现<code>CS231n A2: Global name &#39;col2im_6d_cython&#39; is not defined解决</code>，请关掉jupyter notebook，然后重新编译就可以了。</p><h2 id="Lecture-10-Recurrent-Neural-Networks"><a href="#Lecture-10-Recurrent-Neural-Networks" class="headerlink" title="Lecture 10. Recurrent Neural Networks"></a>Lecture 10. Recurrent Neural Networks</h2><p>这一节课主要讲RNN的网络结构（recursive neural network是树结构，recurrent neural network是chain结构），设计文本生成，image caption, visual question之类的网路，后面通过training问题提及了下LSTM。课程的目的在于希望NLP和CV两者结合，相辅相成，解决更为智能化的问题。</p><p>参考阅读材料方面主要是花书《deep learning》中的<a href="http://www.deeplearningbook.org/contents/rnn.html" target="_blank" rel="noopener">第十章</a>: Sequence Modeling: Recurrent and Recursive Nets. 其内容不难理解，也都是介绍性文字，大部分细节都是列出论文便于感兴趣读者去进一步阅读。（个人觉得看好Justin Johnson的<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture10.pdf" target="_blank" rel="noopener">课件</a>就差不多了，主要内容都是RNN结构和LSTM）</p><p><img src="/posts/notesofCS231N/10_1.JPG" alt="一个典型RNN结构，其中对每个类型变量的权重都是相同的，会重复使用"></p><p><img src="/posts/notesofCS231N/10_4.JPG" alt="多输出一个特征图的权重注意力分配信息"></p><p><img src="/posts/notesofCS231N/10_5.JPG" alt="VQA机制"></p><p><img src="/posts/notesofCS231N/10_6.JPG" alt="根据用户言语指导进行视觉导航"></p><p><img src="/posts/notesofCS231N/10_2.JPG" alt="原生RNN梯度回传会出现弥散或者爆炸现象，难以训练"></p><p><img src="/posts/notesofCS231N/10_3.JPG" alt="long-short-term-memory (LSTM)修正梯度回传问题"></p><p><img src="/posts/notesofCS231N/10_7.JPG" alt="LSTM处理方式示意"></p><p><img src="/posts/notesofCS231N/10_8.JPG" alt="LSTM可以很好缓解梯度问题，但是不能完全避免"></p><p>这里有一篇讲解LSTM的<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">blog</a>（嫌麻烦的可以直接看<a href="https://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="noopener">中文翻译版本</a>）。</p><p>有关Justin Johnson课上提到的RNN的一些fun application可以在karpathy的这篇<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">blog</a>找到。</p><p>目前，有关机器翻译，文本生成等NLP问题自谷歌的”Attention is all you need“之后，基本上都直接采用self-attention机制了，但这并不代表RNN没有用，已被淘汰了，其在处理依赖时间序列问题还是有很多用武之地的。</p><p><img src="/posts/notesofCS231N/10_9.JPG" alt="自Facebook的DETR检测模型开了transformers和CNN特征结合的先河之后，预测后续会有更多的想法出现"></p><h2 id="Lecture-11-Gnerative-Models"><a href="#Lecture-11-Gnerative-Models" class="headerlink" title="Lecture 11. Gnerative Models"></a>Lecture 11. Gnerative Models</h2><p>这节课没有官方推荐阅读材料，只有<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture11.pdf" target="_blank" rel="noopener">slides</a>首先介绍监督学习和分监督学习的一些区别（有无外部label带来的监督信号，前者着重利用数据做任务，后者着重学习数据内部的结构和分布等）。</p><p>2020新版增加了CS236的VAE的<a href="https://deepgenerativemodels.github.io/notes/vae/" target="_blank" rel="noopener">notes</a>和Ian Goodfellow在NIPS2016上的<a href="https://arxiv.org/pdf/1701.00160.pdf" target="_blank" rel="noopener">tutorial</a>.</p><p><img src="/posts/notesofCS231N/11_1.JPG" alt="supervised v.s. unsupervised learning"></p><p>后面主要讲无监督学习中的一个核心问题density estimation，介绍跟此问题有关的三类generative models：</p><p><img src="/posts/notesofCS231N/11_2.JPG" alt="PixelRCNN/CNN, VAE, GAN"></p><p>目前在生成式模型方面还未进行研究，所以暂时也没有啥思考，也就单纯把这节课当成一次introduction。课程中有关这三类模型的概念的公式也不搬过来了，直接看课件就好。</p><p><img src="/posts/notesofCS231N/11_3.JPG" alt="三类生成式模型比较"></p><p>后续会好好研究一下GAN，那时会再专门写笔记讨论体会心得，先放几个不错的resources链接：</p><ul><li>review-<a href="https://arxiv.org/abs/2001.06937" target="_blank" rel="noopener">A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications</a></li><li>paper list-<a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">the-gan-zoo</a></li><li>how-to-train-gan-<a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">ganhacks</a></li><li>知乎文章-<a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN</a></li><li>researcher-<a href="https://people.csail.mit.edu/junyanz/" target="_blank" rel="noopener">Jun-Yan-Zhu</a></li><li>Stanford-cs236-<a href="https://deepgenerativemodels.github.io/" target="_blank" rel="noopener">deep generative models</a></li><li>UCB-cs294-<a href="https://sites.google.com/view/berkeley-cs294-158-sp19/home" target="_blank" rel="noopener">deep unsupervised learning</a></li></ul><hr><p>2020.6更新：</p><p>普通的pixelRNN/CNNs是显示估计数据分布，即：$p_{\theta}(x)= \prod\limits _{i=1}^{n} p_{\theta}\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)$，虽然效果不错，但是计算很慢，VAE是利用一个中间的隐变量来进行，即：$p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x \mid z) d z$</p><p><img src="/posts/notesofCS231N/11_4.JPG" alt="利用中间变量进行降维，提取主要特征"></p><p><img src="/posts/notesofCS231N/11_5.JPG" alt="训练好的VAE还可以讲decoder去掉进行监督任务训练"></p><p>但是上述的VAE只能重建输入图片，如果想要重建满足输入数据分布下的另一个图片该怎么做？</p><p>我们可以选择$p(z)$为一个已知的比较简单的先验分布，比如高斯分布，$p(x|z)$是神经网络解码出的分布，但是前面的积分会使得整个过程intractable. 考虑后验概率，即$p_{\theta}(z \mid x)=p_{\theta}(x \mid z) p_{\theta}(z) / p_{\theta}(x)$，进一步地有：</p><p><img src="/posts/notesofCS231N/11_6.JPG" alt="丢掉最后一个KL散度，最大化data likelihood"></p><p>现在我们先让encoder去让后验概率去近似我们设定的先验概率$p(z)$，比如说是标准正态分布，然后让decoder从拟合的后验概率中采样去重建图片。训练时重建输入图片，用以反向传播，测试时可以直接丢掉encoder，利用decoder随机从先验概率分布中采一个数然后decoder出新的图片。非常漂亮的思想！</p><p><img src="/posts/notesofCS231N/11_7.JPG" alt="VAE的学习过程，两步近似"></p><p><img src="/posts/notesofCS231N/11_8.JPG" alt="测试时可以生成新的图片"></p><p>此外，隐变量$z$也是多维的，每个维度可以带便不同的图像特征，这样的话可以控制我们想要输出的图片内容。</p><p><img src="/posts/notesofCS231N/11_9.JPG" alt="不同维度的z编码了图片的不同因素"></p><p>以上的两种方法都在试图显示建立数据的分布，而GAN试图绕过复杂的数据分布建模，利用博弈的思想隐式建立数据分布，从而直接生成新的图片样本。</p><p><img src="/posts/notesofCS231N/11_10.JPG" alt="GAN将最大最小化改为双最大化，原因是在对生成器进行参数更新的时候，原始是希望最小化判别器对生成器的判断正确的能力，这样的话根据数学公式，导数-1/(1-x)前期变化慢，后期变化快，不利用学习，后来将其改成最大化判别器对生成器判断错误的能力，这样计算出的导数1/x，前期变化快，后期变化慢，符合一般的网络学习规律"></p><p><img src="/posts/notesofCS231N/11_11.JPG" alt="训练CNN结构GAN的建议"></p><h2 id="Lecture-12-Detection-and-Segmentation"><a href="#Lecture-12-Detection-and-Segmentation" class="headerlink" title="Lecture 12. Detection and Segmentation"></a>Lecture 12. Detection and Segmentation</h2><p>这节课介绍了计算机视觉中比较重要的四个任务：classification, semantic segmentation, object detection, instance segmentation, <a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture12.pdf" target="_blank" rel="noopener">slides</a>做的很不错。</p><p>此外，18版的课程里有个两个相关的discussion section：<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds06.pdf" target="_blank" rel="noopener">Practical Object Detection and Segmentation</a>，列出了经典的检测和分割结构，并给出了相应的代码实现链接和一些其他resources；<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds08.pdf" target="_blank" rel="noopener">Video Understanding</a>.</p><p>1.semantic segmentation主要通过FCN的结构介绍了downsampling—upsampling这种经典结构，重点引出一般的upsampling的方法，比如unpooling (一种是最近邻补值，一种是记下pooling的索引，然后其他补零)和转置卷积（利用权重矩阵乘以每个元素，然后得到大的特征图区域，overlap的地方累加&lt;累加只是一种处理手段，这方面或许可以有更好的办法恢复信息&gt;）</p><p>转置卷积transpose convolution： upsampling的一种 ，unpooling, max unpooling, 为什么叫转置卷积更好点，Justin Johnson在课上也给了数学解释。（<a href="https://www.zhihu.com/question/43609045/answer/132235276" target="_blank" rel="noopener">知乎问题</a>）</p><p><img src="/posts/notesofCS231N/12_1.JPG" alt="转置卷积"></p><p>空洞卷积Dilated Convolution：源自于语义分割领域，目前也大多用于语义分割领域。初衷是为了解决downsample丢失的信息会影响pixel-wise的分类。“dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息”（<a href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener">知乎问题</a>）</p><p>2.object detection和instance detection是交叉介绍的，因为后者是前者的集成，也顺带提及了human pose estimation，keypoint。这一部分的介绍相比2017版多了一些细节（尤其是RoI pooling&lt;浮点数两次转整型处理，丢失proposal原始像素信息&gt;和RoI Align&lt;保留浮点数，利用双线性插值得到每个max pooling小区域的四个点，对小目标更好&gt;）和scene graph, 3D检测，3D shape prediction等部分（3D Machine Learning应该是目前工业界比较看重的一个领域，在无人驾驶，增强现实等领域可以发挥作用）。其中object detection部分着重介绍了R-CNN系列，更详细的解释可以阅读另一篇<a href="http://densecollections.top/2019/11/30/RCNN-series-in-object-detection/">博客</a>。课程中也推荐一篇综述类型的paper: <a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="noopener">Speed/accuracy trade-offs for modern convolutional object detectors</a>便于进一步了解（在我的另一篇R-CNN检测专题<a href="http://densecollections.top/2020/01/10/RCNN-series-in-object-detection-续/">博客</a>会有所提及）。</p><p><img src="/posts/notesofCS231N/12_2.JPG" alt="ROI Align利用双线性插值提高ROI映射精度"></p><p>可变形卷积Deformable convolutional networks：受Andrew Zisserman的STN (Spatial Transformer Network) 启发，针对物体的形状自发的选择卷积的区域，而不是按照约定的矩形计算规则，更适合实际。在<a href="https://www.zhihu.com/question/57493889" target="_blank" rel="noopener">知乎回答</a>中，有人说可变性卷积是对空洞卷积的一种generalization，增大了感受野 。</p><h2 id="Lecture-13-Visualizing-and-Understanding"><a href="#Lecture-13-Visualizing-and-Understanding" class="headerlink" title="Lecture 13. Visualizing and Understanding"></a>Lecture 13. Visualizing and Understanding</h2><p>1.试图理解卷机神经网络是怎么做决策的，这些layers到底学了什么东西。因此引入了特征图可视化，看看网络对图像的那些区域或者blob甚至是pixel反应大些（感受野，遮挡，<strong>回传对pixel的梯度，gradient ascent</strong>等），激活的是哪些地方（这里面的针对像素的梯度上升方法与学习权重等参数的方法是对立统一的，固定神经元参数，给定zero 图像，加一些正则化方法，然后对图像进行优化，看看什么样的图像可以获得更高的类别分数），然后借此产生对抗样本adversarial example试图欺骗网络（17年的最后一课就是请Ian Goodfellow讲这个）。</p><p>2.利用可视化思想通过选定特定的layer对输入图像做gradient，然后更新图像，放大特征，产生<a href="https://github.com/google/deepdream" target="_blank" rel="noopener">DeepDream</a>效果；</p><p>3.Neural Texture Synthesis, 利用<em>Gram matrix</em>对小区域图像进行延展；</p><p>4.Neural Style Transfer, 对输入图像进行特定风格的渲染。这项技术现在已经被商业化了，有不少手机APP和其他应用都有此功能！Justin Johnson也有个torch的<a href="https://github.com/jcjohnson/neural-style" target="_blank" rel="noopener">实现</a>。</p><p><img src="/posts/notesofCS231N/13_1.JPG" alt="Neural Style Transfer model"></p><p>但是得到一张转换的图片很慢，因为有很多的forward和backword，因此J.J做了个<a href="https://github.com/jcjohnson/fast-neural-style" target="_blank" rel="noopener">Fast Style Transfer</a>, 训练另外一个网络来实现style transfer, 其中IN (instance norm)就是为了快速风格转换发明的。课程的<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture13.pdf" target="_blank" rel="noopener">slides</a>里也提供了很多相关的paper，对细节感兴趣的可以看看。</p><p><img src="/posts/notesofCS231N/13_2.JPG" alt></p><p>有关神经网络可视化，个人觉得是个很有意义也和好玩的一个领域，它在一定程度上解释了模型为什么work，也可以结合一些idea做出可消费的funny application出来。<del>Assignment3也有相关的作业，到时候应该可以深入学习理解下。</del>从assignment3来看，为了让输入图像的风格靠近目标图像 (style loss)，采用gram矩阵来编码模型各个层产生的通道之间的相关性，然后尽量让两幅图像的gram矩阵（以指定维度方向上的各个向量之间做内积）相近。gram矩阵代表图像特征中哪些特征是同时出现的，哪些是此消彼长，不相干的，而特征呢就相当于图像的某个特性，比如在修图的时候，我们会调调对比度，加些高光，调调饱和度等，为了得到我们想要的风格的图像，其中有些修图选项可以一起做，有些往大的值调，有些就得往小的调，gram矩阵也是类似的操作，缩小目标gram矩阵和输入gram矩阵之间的差异就等于是给图像加滤镜。此外，gram矩阵的对角线上还包含了每个特征图的原始信息，也就是说不仅有哪些特征（哪些修图选项），也有各个特征之间的联系（修图选项的调整值），因此可以大致代表图像的风格。（相关<a href="https://www.zhihu.com/question/49805962?sort=created" target="_blank" rel="noopener">知乎问题</a>)</p><p>相关blog: Hungryof的“谈谈图像的style transfer”系列—-<a href="https://blog.csdn.net/Hungryof/article/details/53981959" target="_blank" rel="noopener">1</a>, <a href="https://blog.csdn.net/Hungryof/article/details/71512406" target="_blank" rel="noopener">2</a>, <a href="https://blog.csdn.net/Hungryof/article/details/80310527" target="_blank" rel="noopener">3</a></p><h2 id="Discussion—Learning-on-videos"><a href="#Discussion—Learning-on-videos" class="headerlink" title="Discussion—Learning on videos"></a>Discussion—Learning on videos</h2><p>视频理解和3D machine learning 似乎成为现在CV中的两个新的热点。2020年的课程中也没有把视频理解放进前面的CV applications中，估计是考虑到视频理解中用的方法也是从那些分类，检测等基础任务中迁移过来的。从现在工业界短视频作为火热的媒介来看，视频理解的研究预计会出现比较大的需求和增长。</p><p><a href="http://cs231n.stanford.edu/slides/2020/section_8_video.pdf" target="_blank" rel="noopener">Discussion Section—-learning on videos</a></p><p>其中提到目前主流的三种模型：</p><ul><li><p>CNN + RNN: video understanding as sequence modeling</p></li><li><p>3D Convolution: embed temporal dimension to CNN</p></li><li><p>Two-stream: explicit model of motion (video = apperance + motion)</p></li></ul><h2 id="Lecture-14-Deep-Reinforcement-Learning"><a href="#Lecture-14-Deep-Reinforcement-Learning" class="headerlink" title="Lecture 14. Deep Reinforcement Learning"></a>Lecture 14. Deep Reinforcement Learning</h2><p>对于强化学习的印象只停留在一个框架的系统层面上：输入是状态（state），动作（action）和奖励（reward），输出时方案或者策略（policy），大意就是通过奖惩机制，在某个环境中为对象制定在初始状态实现目标的一系列策略。</p><p>强化学习的热度似乎没有神经网络，GAN等技术大，而且据我身边研究的朋友说，强化学习的学习难度有点高，具体没有细问，可能是模型，critic制作和训练的难度不低。一位bloger在他对强化学习简介的<a href="https://www.cnblogs.com/geniferology/p/what_is_reinforcement_learning.html" target="_blank" rel="noopener">blog</a>中说到：</p><blockquote><p>强化学习的领导研究者<a href="http://www.incompleteideas.net/" target="_blank" rel="noopener">Richard S. Sutton</a>认为，只有这种学习法才考虑到 <em>自主个体</em>、<em>环境</em>、<em>奖励</em> 等因素，所以它是人工智能中最 top-level 的 architecture，而其他人工智能的子系统，例如 logic 或 pattern recognition，都应该在它的控制之下，我觉得颇合理。</p></blockquote><p>个人认为，目前的强化学习方式确实是一个可以融合许多机器学习技术和模型的一个理论框架，视觉中的注意力机制可以用到，NAS中也可以用到，随着时间推移，可能会有更多的结合和应用会产生，甚至引导未来很多产业的落地。</p><p>一些相关resources:</p><ul><li><p><a href="https://www.zhihu.com/question/49230922/answer/115011594" target="_blank" rel="noopener">知乎—强化学习（reinforcement learning)有什么好的开源项目、网站、文章推荐一下</a></p></li><li><p><a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course" target="_blank" rel="noopener">Deep_reinforcement_learning_Course</a></p></li><li><p><a href="https://pathmind.com/wiki/deep-reinforcement-learning" target="_blank" rel="noopener">A Beginner’s Guide to Deep Reinforcement Learning</a></p></li><li><p><a href="http://www.incompleteideas.net/book/the-book.html" target="_blank" rel="noopener">textbook—Reinforcement Learning: An Introduction</a></p></li></ul><p>Besides，这<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture14.pdf" target="_blank" rel="noopener">课件</a>做得也挺好的，虽然Serena Yeung小姐姐讲的课让我一知半解，云里雾里的 :(</p><h2 id="Assignment-3"><a href="#Assignment-3" class="headerlink" title="Assignment 3"></a>Assignment 3</h2><p><a href="https://cs231n.github.io/assignments2019/assignment3/" target="_blank" rel="noopener">assignment page</a>;</p><p>reference：</p><ul><li><p><a href="https://github.com/FortiLeiZhang/cs231n/tree/master/code/cs231n/assignment3" target="_blank" rel="noopener">github-code</a>;</p></li><li><p>参考培训机构深度之眼的cs231n-camp的文档</p></li></ul><p>具体过程和结果参见这个<a href="https://github.com/Richardyu114/CS231N-notes-and-assignments" target="_blank" rel="noopener">repository</a></p><p>COCO_Caption数据集下载速度很能很慢，将链接放至迅雷可能快点，我这边是1～2M/s。</p><p>做<code>RNN_Caption.ipynb</code>时出现类似assignment1的无法导出<code>imread,imresize</code>问题，原因就是<code>scipy</code>包现在不支持这个函数了，直接将<code>scipy</code>从1.4退回到1.2.0即可。</p><p>在执行<code>Look at the data</code>部分时出现<code>[WinError 10054] 远程主机强迫关闭了一个现有的连接</code>错误，按照<a href="http://188.131.244.232/article/10" target="_blank" rel="noopener">此博客</a>的解释，可能是链接存在一些无法访问的情况，将“http”改成”https”即可，<code>url = url.replace(http://&quot;, &#39;https://&#39;)</code></p><p>改完之后可以顺利访问，但是出现<code>进程被占用</code>情况，解决方法同上面博客，修改<code>image_utils.py</code>中的<code>image_from_url</code>函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def image_from_url(url):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Read an image from a URL. Returns a numpy array with the pixel data.</span><br><span class="line">    We write the image to a temporary file then read it back. Kinda gross.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    try:</span><br><span class="line">        f = urllib.request.urlopen(url)</span><br><span class="line">        #_, fname = tempfile.mkstemp()</span><br><span class="line">        # 为了解决“另一个程序正在使用此文件，进程无法访问的错误&quot;</span><br><span class="line">        fd, fname = tempfile.mkstemp(dir=&quot;G:\CS231N&quot;)</span><br><span class="line">        with open(fname, &apos;wb&apos;) as ff:</span><br><span class="line">            ff.write(f.read())</span><br><span class="line">        img = imread(fname)</span><br><span class="line">        # 在删除临时文件前关闭该文件，防止被占用</span><br><span class="line">        os.close(fd)</span><br><span class="line">        os.remove(fname)</span><br><span class="line">        return img</span><br><span class="line">    except urllib.error.URLError as e:</span><br><span class="line">        print(&apos;URL Error: &apos;, e.reason, url)</span><br><span class="line">    except urllib.error.HTTPError as e:</span><br><span class="line">        print(&apos;HTTP Error: &apos;, e.code, url)</span><br></pre></td></tr></table></figure><p>在<code>NetworkVisualization-Pytorch.ipynb</code>的<code>load some ImageNet images</code>中，运行报错<code>Object arrays cannot be loaded when allow_pickle=False</code>，通过<a href="https://blog.csdn.net/weixin_42096901/article/details/89855804" target="_blank" rel="noopener">这篇博客</a>发现是自己<code>numpy</code>版本过高问题，将其回退到1.16.1或1.16.2即可解决</p><p>在使用pytorch时，要注意对<code>requires_grad=True</code>的Tensor和求梯度阶段需要用到的Tensor都不能用inplace operation，也就是不能直接对这些张量进行数值操作，最好加上.detach()曲线救国，见<a href="https://zhuanlan.zhihu.com/p/38475183" target="_blank" rel="noopener">此</a>。</p><p>在<code>Generative_Adversarial_Networks_PyTorch.ipynb</code>中下载MNIST可能很慢，可以直接找到数据网址自己下载好数据集，然后放到代码自动创建的存放数据的文件夹中的<code>raw</code>中，再次运行就可以不用在线下载，而是直接读取在process了。还有一种办法是改变源代码的中的urls到本地，详情可见<a href="https://blog.csdn.net/york1996/article/details/81780065" target="_blank" rel="noopener">here</a>.</p><h2 id="2017-Invited-Talk-Efficient-Methods-and-Hardware-for-Deep-Learning-Song-Han"><a href="#2017-Invited-Talk-Efficient-Methods-and-Hardware-for-Deep-Learning-Song-Han" class="headerlink" title="2017 Invited Talk-Efficient Methods and Hardware for Deep Learning, Song Han"></a>2017 Invited Talk-Efficient Methods and Hardware for Deep Learning, Song Han</h2><p>CS231N每年都会邀请不同的专家来做一些专题介绍，由于2017年以后都没release视频了，所以就直接看17年的talk了，正好17年的这两个topic都比较不错，在应用上都很重要，因此也很值得学习下。</p><ul><li><p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture15.pdf" target="_blank" rel="noopener">课件</a></p></li><li><p><a href="https://songhan.mit.edu/" target="_blank" rel="noopener">Song Han</a></p></li><li><p>17年的旷视也有一节课专门讲神经网络压缩，<a href="https://www.bilibili.com/video/BV1E7411t7ay?p=9" target="_blank" rel="noopener">链接</a></p></li></ul><p>韩松在该领域算是比较有名的一位，他在此节课从算法讲到硬件（分inference和training两部分），非常系统。虽然大多是overview，但是包含的内容很多，后续研究压缩领域也会从他主页中列举的资料来入门和学习。</p><p>韩松教授2017年的phd thesis: <a href="[https://stacks.stanford.edu/file/druid:qf934gh3708/EFFICIENT%20METHODS%20AND%20HARDWARE%20FOR%20DEEP%20LEARNING-augmented.pdf](https://stacks.stanford.edu/file/druid:qf934gh3708/EFFICIENT METHODS AND HARDWARE FOR DEEP LEARNING-augmented.pdf">EFFICIENT METHODS AND HARDWARE FOR DEEP LEARNING</a>)</p><p><a href="https://songhan.github.io/DSD/" target="_blank" rel="noopener">DSD (Dense-Sparse-Dense Training) Model Zoo</a></p><h2 id="2017-Invited-Talk-Adversarial-Examples-and-Adversarial-Training-Ian-Goodfellow"><a href="#2017-Invited-Talk-Adversarial-Examples-and-Adversarial-Training-Ian-Goodfellow" class="headerlink" title="2017 Invited Talk-Adversarial Examples and Adversarial Training, Ian Goodfellow"></a>2017 Invited Talk-Adversarial Examples and Adversarial Training, Ian Goodfellow</h2><p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture16.pdf" target="_blank" rel="noopener">课件</a></p><p>Ian Goodfellow的语速很慢，然而我整节课却没怎么听懂他在讲什么。</p><ul><li><p>知乎文章<a href="https://zhuanlan.zhihu.com/p/42667844" target="_blank" rel="noopener">对抗样本Adversarial Examples</a></p></li><li><p>知乎问题<a href="https://www.zhihu.com/question/49129585" target="_blank" rel="noopener">如何看待机器视觉的“对抗样本”问题，其原理是什么？</a></p></li></ul><h2 id="Lecture-17-Human-Centered-AI"><a href="#Lecture-17-Human-Centered-AI" class="headerlink" title="Lecture 17. Human-Centered AI"></a>Lecture 17. Human-Centered AI</h2><p>李飞飞最后的总结性课程，描绘AI对社会和人类的影响和蓝图，提出HAI的概念。</p><p><img src="/posts/notesofCS231N/17_1.JPG" alt="HAI"></p><h2 id="Lecture-18-Scene-Graphs"><a href="#Lecture-18-Scene-Graphs" class="headerlink" title="Lecture 18. Scene Graphs"></a>Lecture 18. Scene Graphs</h2><p>2020版新增了Scene Graphs and Graph Convolutions，<a href="http://cs231n.stanford.edu/slides/2020/lecture_18.pdf" target="_blank" rel="noopener">slides</a>.</p><p>这一部分应该是让计算机去更好的理解图片。分类，检测都是认识图片的一种手段，不是目的。</p><p><img src="/posts/notesofCS231N/18_1.JPG" alt></p><p><img src="/posts/notesofCS231N/18_2.JPG" alt></p><p><img src="/posts/notesofCS231N/18_3.JPG" alt></p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Richard YU</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://densecollections.top/posts/notesofCS231N/" title="Stanford CS231n 笔记">http://densecollections.top/posts/notesofCS231N/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/computer-vision/" rel="tag"><i class="fa fa-tag"></i> computer vision</a> <a href="/tags/public-course/" rel="tag"><i class="fa fa-tag"></i> public course</a> <a href="/tags/deep-learning/" rel="tag"><i class="fa fa-tag"></i> deep learning</a> <a href="/tags/Standford/" rel="tag"><i class="fa fa-tag"></i> Standford</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/megviidlcourse/" rel="next" title="旷视2017年深度学习实践课程"><i class="fa fa-chevron-left"></i> 旷视2017年深度学习实践课程</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/posts/4074/" rel="prev" title="[MIT]计算机科学课堂中学不到的知识">[MIT]计算机科学课堂中学不到的知识 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/header.jpg" alt="Richard YU"><p class="site-author-name" itemprop="name">Richard YU</p><div class="site-description motion-element" itemprop="description">Today everything exists to end in a photograph</div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">24</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">54</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/richardyu114" title="GitHub &rarr; https://github.com/richardyu114" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Yu1145635107" title="Twitter &rarr; https://twitter.com/Yu1145635107" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a> </span><span class="links-of-author-item"><a href="https://instagram.com/d.h.richard" title="Instagram &rarr; https://instagram.com/d.h.richard" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a> </span><span class="links-of-author-item"><a href="https://weibo.com/u/5211687990" title="Weibo &rarr; https://weibo.com/u/5211687990" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a> </span><span class="links-of-author-item"><a href="https://www.douban.com/people/161993653/" title="豆瓣 &rarr; https://www.douban.com/people/161993653/" rel="noopener" target="_blank"><i class="fa fa-fw fa-paperclip"></i>豆瓣</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#About"><span class="nav-number">1.</span> <span class="nav-text">About</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture1-Introduction"><span class="nav-number">2.</span> <span class="nav-text">Lecture1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture2-Image-Classification"><span class="nav-number">3.</span> <span class="nav-text">Lecture2. Image Classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture3-Loss-Functions-and-Optimization"><span class="nav-number">4.</span> <span class="nav-text">Lecture3. Loss Functions and Optimization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture4-Introduction-to-Neural-Networks"><span class="nav-number">5.</span> <span class="nav-text">Lecture4. Introduction to Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-5-Convolutional-Neural-Networks"><span class="nav-number">6.</span> <span class="nav-text">Lecture 5. Convolutional Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Assignment-1"><span class="nav-number">7.</span> <span class="nav-text">Assignment 1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-6-Deep-Learning-Hardware-and-Software"><span class="nav-number">8.</span> <span class="nav-text">Lecture 6. Deep Learning Hardware and Software</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-7—8-Training-Neural-Networks-I-amp-II"><span class="nav-number">9.</span> <span class="nav-text">Lecture 7—8. Training Neural Networks I &amp; II</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-9-CNN-Architecture"><span class="nav-number">10.</span> <span class="nav-text">Lecture 9. CNN Architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Assignment-2"><span class="nav-number">11.</span> <span class="nav-text">Assignment 2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-10-Recurrent-Neural-Networks"><span class="nav-number">12.</span> <span class="nav-text">Lecture 10. Recurrent Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-11-Gnerative-Models"><span class="nav-number">13.</span> <span class="nav-text">Lecture 11. Gnerative Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-12-Detection-and-Segmentation"><span class="nav-number">14.</span> <span class="nav-text">Lecture 12. Detection and Segmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-13-Visualizing-and-Understanding"><span class="nav-number">15.</span> <span class="nav-text">Lecture 13. Visualizing and Understanding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Discussion—Learning-on-videos"><span class="nav-number">16.</span> <span class="nav-text">Discussion—Learning on videos</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-14-Deep-Reinforcement-Learning"><span class="nav-number">17.</span> <span class="nav-text">Lecture 14. Deep Reinforcement Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Assignment-3"><span class="nav-number">18.</span> <span class="nav-text">Assignment 3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2017-Invited-Talk-Efficient-Methods-and-Hardware-for-Deep-Learning-Song-Han"><span class="nav-number">19.</span> <span class="nav-text">2017 Invited Talk-Efficient Methods and Hardware for Deep Learning, Song Han</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2017-Invited-Talk-Adversarial-Examples-and-Adversarial-Training-Ian-Goodfellow"><span class="nav-number">20.</span> <span class="nav-text">2017 Invited Talk-Adversarial Examples and Adversarial Training, Ian Goodfellow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-17-Human-Centered-AI"><span class="nav-number">21.</span> <span class="nav-text">Lecture 17. Human-Centered AI</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lecture-18-Scene-Graphs"><span class="nav-number">22.</span> <span class="nav-text">Lecture 18. Scene Graphs</span></a></li></ol></div></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Richard YU</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">268k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">4:03</span></div><div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.1</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script><script src="/lib/three/three.min.js"></script><script src="/lib/three/canvas_lines.min.js"></script><script src="/js/utils.js?v=7.1.1"></script><script src="/js/motion.js?v=7.1.1"></script><script src="/js/affix.js?v=7.1.1"></script><script src="/js/schemes/pisces.js?v=7.1.1"></script><script src="/js/scrollspy.js?v=7.1.1"></script><script src="/js/post-details.js?v=7.1.1"></script><script src="/js/next-boot.js?v=7.1.1"></script><script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>var GUEST=["nick","mail","link"],guest="nick,mail,link";guest=guest.split(",").filter(function(e){return-1<GUEST.indexOf(e)}),new Valine({el:"#comments",verify:!0,notify:!0,appId:"duGoywhUmLrx9pM6qQhmf47c-gzGzoHsz",appKey:"m7fc8w4Di5qnAXXaJ5Gp3Pgg",placeholder:"欢迎评论！请留下你的邮箱",avatar:"mm",meta:guest,pageSize:"10",visitor:!0,lang:"zh-cn"})</script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });</script><script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>$(".highlight").not(".gist .highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea"),o=window.pageYOffset||document.documentElement.scrollTop;n.style.top=o+"px",n.style.position="absolute",n.style.opacity="0",n.readOnly=!0,n.value=e,document.body.appendChild(n),n.select(),n.setSelectionRange(0,e.length),n.readOnly=!1,document.execCommand("copy")?$(this).text("复制成功"):$(this).text("复制失败"),n.blur(),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script></body></html>