<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>旷视2017年深度学习实践课程 | 自拙集</title><meta name="keywords" content="computer vision,deep learning,Megvii"><meta name="author" content="Richard YU"><meta name="copyright" content="Richard YU"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="About 视频; 课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取; 这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好">
<meta property="og:type" content="article">
<meta property="og:title" content="旷视2017年深度学习实践课程">
<meta property="og:url" content="http://densecollections.top/posts/megviidlcourse/index.html">
<meta property="og:site_name" content="自拙集">
<meta property="og:description" content="About 视频; 课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取; 这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images.unsplash.com/photo-1606870655612-8eacd813984d?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80">
<meta property="article:published_time" content="2020-02-25T03:57:44.000Z">
<meta property="article:modified_time" content="2020-07-08T12:52:29.799Z">
<meta property="article:author" content="Richard YU">
<meta property="article:tag" content="computer vision">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="Megvii">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images.unsplash.com/photo-1606870655612-8eacd813984d?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://densecollections.top/posts/megviidlcourse/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="preconnect" href="//zz.bdstatic.com"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Richard YU","link":"链接: ","source":"来源: 自拙集","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2020-07-08 20:52:29'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://raw.githubusercontent.com/Richardyu114/minds-thoughts-and-resources-about-research-/master/images/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">54</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/PaperStation/"><i class="fa-fw fas fa-edit"></i><span> PaperStation</span></a></div><div class="menus_item"><a class="site-page" href="/MindWandering/"><i class="fa-fw fas fa-paper-plane"></i><span> MindWandering</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://images.unsplash.com/photo-1606870655612-8eacd813984d?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1350&amp;q=80)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">自拙集</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/PaperStation/"><i class="fa-fw fas fa-edit"></i><span> PaperStation</span></a></div><div class="menus_item"><a class="site-page" href="/MindWandering/"><i class="fa-fw fas fa-paper-plane"></i><span> MindWandering</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">旷视2017年深度学习实践课程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-02-25T03:57:44.000Z" title="发表于 2020-02-25 11:57:44">2020-02-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-07-08T12:52:29.799Z" title="更新于 2020-07-08 20:52:29">2020-07-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/">课程记录</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av88056282/">视频</a>;</li>
<li>课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取;</li>
<li>这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好。</li>
</ul>
<a id="more"></a>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li><p>computer vision 在AI中的地位属于感知（perception）智能（还包括speech），另外一块是认知（cognitive），包括NLP和AGI（通用人工智能）；</p>
</li>
<li><p>人使用眼睛和大脑认识世界，电脑使用图像传感器和算力来视觉感知周围环境；</p>
</li>
<li><p>大脑皮层的出现，灵活，结构化，计算处理；</p>
</li>
<li><p>CV和AI的关系：其中非常重要的一个task/不同的研究工作和成果/作为关键应用；</p>
</li>
<li><p>现阶段的CV任务：classification (image)/ detection (region) /segmentation (pixel，实例，语义，全景)/ sequence (video，spatial+temporal)；</p>
</li>
<li><p>David Marr 的《vision》一书，这在visual SLAM中也十分重要，视觉知识的表示，part representation (拆成块，用各种模型表示，举例关键点检测);</p>
</li>
<li><p>part representation存在局限，有些不可分，引发了神经网络第二次复兴，Yann Lecun 的卷积神经网络应用于手写字体识别和人脸检测。由于当时难以复现，且懂的人不多，加上小规模数据和SVM等模型流行，神经网络出现衰落；</p>
</li>
<li><p>learning-based representation/ feature-based representation，特征工程+分类器（handcraft features engineering+SVM/Random Forest），浅层学习pipeline a short sequence；</p>
</li>
<li><p>端到端学习，所有参数联合优化 ，a long or very long sequence实现高维非线性映射；</p>
</li>
<li><p>受感知机启发的多层感知机（multilayer perceptron，MLP），利用backpropagation (BP) 梯度训练逼近（局部最优解）任意非线性函数；</p>
</li>
<li>90年代的神经网络成果：CNN/ autocoder/ boltzmann machine/ belief nets/ RNN；</li>
<li>复兴：data+computing+industry competition+a few breakthrough；</li>
</ul>
<p><img src="/.top//a_brief_history.JPG" alt="深度学习沉浮历史"></p>
<ul>
<li><p>Resnet的思想：由浅到深学习，保持梯度数值较大，防止梯度消失 ；</p>
</li>
<li><p>从以前的手工设计feature为重点到现在设计网络结构（2012-2017为止）为重点，不同的结构所需算力不同，现在轻量级网络是一个热点；</p>
</li>
<li>卷积核的方式：1x1，3x3，<a target="_blank" rel="noopener" href="https://blog.csdn.net/tintinetmilou/article/details/81607721">depthwise 3x3</a>等，网络layer连接的方式；</li>
</ul>
<h2 id="2-Math-in-DL-and-ML-Basics"><a href="#2-Math-in-DL-and-ML-Basics" class="headerlink" title="2. Math in DL and ML Basics"></a>2. Math in DL and ML Basics</h2><ul>
<li><p>深度学习的内涵：deep learning) representation learning) machine learning) AI;</p>
</li>
<li><p>Linear Algebra：</p>
<ul>
<li><p>向量，矩阵，集合，群，封闭性，矩阵乘法是为了表示一种变换关系，向量映射到另一个向量；</p>
<ul>
<li>方阵，正交矩阵，特征值，特征向量，实对称矩阵，二次型，正定矩阵，半正定矩阵，奇异值分解；</li>
</ul>
</li>
</ul>
</li>
<li><p>Probability：</p>
<ul>
<li><p>随机事件，随机变量，概率密度函数，联合分布，边缘分布，条件分布，独立变量；</p>
</li>
<li><p>贝叶斯法则，先验分布，后验分布，期望，方差，协方差矩阵（半正定）；</p>
</li>
<li><p>常见分布：二值分布，二项分布，多值/多相分布（图像分类问题），正态分布（高斯分布）；</p>
</li>
<li><p>信息熵（分类中的交叉熵损失函数，发生概率越大的事情信息越不值钱），交叉熵和KL-divergence，生成式模型中的wassertein distance；</p>
</li>
</ul>
</li>
<li><p>Optimize：</p>
<ul>
<li>minimization（最小化）— 梯度下降gradient descent（步长的选取很关键），stochastic gradient descent;</li>
</ul>
</li>
<li><p>机器学习基本知识（machine learning basics）— 定义，假设，模型，评估，supervised &amp; unsupervised learning (learning $p(y | x) \quad or \quad p(x,y)$，判别式模型，生成式模型&lt;目前都用判别式模型&gt;, learning $p(x)$，auto encoder，GAN)，“no free lunch theorem”（all learning algorithms are equal, but some algorithm are more equal than others），overfitting &amp; underfitting，model capacity vs. generalization error，regularization (正则项，数据增强，parameter reduce and tying);</p>
</li>
</ul>
<p><img src="/.top//supervised&amp;unsupervisedlearning.JPG" alt></p>
<h2 id="3-Neural-Networks-Basics-amp-Architecture-Design"><a href="#3-Neural-Networks-Basics-amp-Architecture-Design" class="headerlink" title="3. Neural Networks Basics &amp; Architecture Design"></a>3. Neural Networks Basics &amp; Architecture Design</h2><ul>
<li><p>Fundamental task in CV: classification, object detection, semantic segmentation, instance segmentation, keypoint detection, human pose estimation, VQA…</p>
</li>
<li><p>计算机识别图像的难点：图像内容的复杂性和多样性，比如姿势，光照，模糊等；</p>
</li>
<li><p>特征是计算机认识图像的一个灯塔，且应当使用非线性特征抽取器；</p>
</li>
<li><p>线性组合特征（kernel learning，boosting），缺点是需要大量的templates，对特征的利用性差；</p>
</li>
<li><p>特征层级组合，重复利用特征，更为高效 —-&gt; concepts reuse in DL，网络层级的特征也是由低到高，但是这样高度非线性的函数难以优化（目前采用收敛到局部最优值）；</p>
</li>
<li><p>key ideas of DL: nolinear system, learn it from data, feature hierarchies, end-to-end learning；</p>
</li>
<li><p>激活函数，神经元，全连接网络，训练决定网络参数（前向，反向，更新）；</p>
</li>
<li><p>针对图像的认识从locally-connected net到convolutional net的设计，参数共享；</p>
</li>
<li><p>卷积层的卷积操作，pooling layer等；</p>
</li>
<li>网络结构设计：网络拓扑结构，layer function，超参，优化算法等经验性的东西，手动/autoML；</li>
<li>简介AlexNet（包含LRN，加速收敛），VGG（发掘3x3小卷积核的显著作用，但并不代表最高效的做法），GoogleNet，ResNet（拟合残差而不是直接拟合原函数），Xception，ResNeXt (借鉴Xception在resnet基础改进)，ShuffleNet，DesneNet，SqueezeNet；</li>
<li>structure design: deeper and wider, ease of optimization, multi-path design, resdiual path, sparse connection；</li>
<li>简介部分layer design：SPP，batch normalization，parametric rectifiers，bilinear CNNs（做细粒度分类）；</li>
<li>针对特定任务的结构设计：Deepface (人脸识别)，Global Convolutional Networks (语义分割)，Hourglass Networks (沙漏结构，大的感受野，用于pose estimation或者关键点)；</li>
</ul>
<h2 id="4-Introduction-to-Computation-Technologies-in-Deep-Learning"><a href="#4-Introduction-to-Computation-Technologies-in-Deep-Learning" class="headerlink" title="4. Introduction to Computation Technologies in Deep Learning"></a>4. Introduction to Computation Technologies in Deep Learning</h2><p>该节课偏底层，听的不是很懂，权当了解。</p>
<ul>
<li><p>symbolic computation：</p>
<ul>
<li><p>深度学习框架overview—program, compilation, runtime mangement, kernels, hardware；</p>
</li>
<li><p>computing graph, graph structure—variable, operator, edge；</p>
</li>
<li><p>静态图和动态图； </p>
</li>
<li><p>执行和优化；</p>
</li>
</ul>
</li>
<li><p>dense numerical computation:</p>
<ul>
<li><p>CPU computation (机器码，流水线，超流水线，超标量，乱序执行/cache hierarchy/…)；</p>
</li>
<li><p>other computation devices (NVIDIA GPU&lt;单指令，多线程架构&gt;，Google TPU，Huawei NPU in Kirin 970，Mobile CPU+GPU+DSP)；</p>
</li>
<li><p>computation &amp; memory gap；</p>
</li>
</ul>
</li>
<li><p>distributed computation:</p>
<ul>
<li><p>system (communication，Remote Direct Memory Access);</p>
</li>
<li><p>optimization algorithm (synchronous SGD，asynchronous SGD)；</p>
</li>
<li><p>communication algorithm (MPI Primitives，An AllReduce Algorithm)；</p>
</li>
</ul>
</li>
</ul>
<h2 id="5-Neural-Network-Approximation-low-rank-sparsity-and-quantization"><a href="#5-Neural-Network-Approximation-low-rank-sparsity-and-quantization" class="headerlink" title="5. Neural Network Approximation(low rank, sparsity, and quantization)"></a>5. Neural Network Approximation(low rank, sparsity, and quantization)</h2><p>该节课着重神经网络压缩，for faster training，faster inference， smaller capacity;</p>
<p>convolution as matrix product，利用近似权重矩阵达到网络压缩的目的;</p>
<p>Low Rank (本质是对矩阵进行一系列分解变换近似操作，减小计算量和存储量):</p>
<ul>
<li><p>对权重矩阵进行奇异值分解，singular value decomposition；</p>
</li>
<li><p>SVD+Kronecker Product ——&gt; KSPD；</p>
</li>
<li><p>矩阵分解：C-HW-K====》C-HW-R-(1X1)-K，然后通过reshape进行重新分解，目前horizontal-vertical decomposition最好；</p>
</li>
<li><p>shared group convolution is a kronrcker layer；</p>
</li>
<li><p>CP-decomposition与depthwise；</p>
</li>
</ul>
<p><img src="/.top//CNN_decomposition.JPG" alt></p>
<p>Sparse Approximation:</p>
<ul>
<li><p>权重分布有点类似高斯分布 ，0附近很多，微调网络，weight pruning: 韩松博士的deepcompression，让为0的权重逐渐增多（掩模矩阵使权重为0），不让0附近的权重在训练时抖动，FC层效果压缩明显；</p>
</li>
<li><p>网络加速计算—稀疏矩阵计算，channel purning，sparse communication for distributed gradient descent；</p>
</li>
</ul>
<p>Quantization：</p>
<ul>
<li><p>用什么精度算；</p>
</li>
<li><p>参数的量化，激活的量化，梯度的量化；</p>
</li>
<li><p>二值化，binary network；</p>
</li>
<li><p>大容量模型利用小bit训练时掉点不明显，小容量模型视情况而定；</p>
</li>
</ul>
<p>主讲人<a target="_blank" rel="noopener" href="http://zsc.github.io/">周舒畅</a>推荐的几篇文章，其中XNOR-Net为课程阅读要求材料：</p>
<blockquote>
<p>Bit Neural Network<br>● Matthieu Courbariaux et al. BinaryConnect: Training Deep Neural Networks with binary<br>weights during propagations. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1511.00363">http://arxiv.org/abs/1511.00363</a>.<br>● Itay Hubara et al. Binarized Neural Networks <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.02505v3">https://arxiv.org/abs/1602.02505v3</a>.<br>● Matthieu Courbariaux et al. Binarized Neural Networks: Training Neural Networks with<br>Weights and Activations Constrained to +1 or -1. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1602.02830v3.pdf">http://arxiv.org/pdf/1602.02830v3.pdf</a>.<br>● Rastegari et al. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural<br>Networks <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1603.05279v1.pdf">http://arxiv.org/pdf/1603.05279v1.pdf</a>.<br>● Zhou et al. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with<br>Low Bitwidth Gradients <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.06160">https://arxiv.org/abs/1606.06160</a>.<br>● Hubara et al. Quantized Neural Networks: Training Neural Networks with Low Precision<br>Weights and Activations <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.07061">https://arxiv.org/abs/1609.07061</a>.</p>
</blockquote>
<h2 id="6-Modern-Object-Detection"><a href="#6-Modern-Object-Detection" class="headerlink" title="6. Modern Object Detection"></a>6. Modern Object Detection</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/62372897">anchor-free与anchor-based的交替轮回</a>;</p>
<p>Representation:</p>
<ul>
<li><p>Bounding-box: face detection, human detection, vehicle detection, text detection, general object detection;</p>
</li>
<li><p>Point: semantic segmentation (下一节课);</p>
</li>
<li><p>Keypoint: face landmark, human keypoint;</p>
</li>
</ul>
<p>Evaluation Criteria：</p>
<ul>
<li>precision (预测为真的里面真正是真的比例), recall (所有是真的里面预测为真的比例), Average Prcision (AP), mean AP (mAP)，IoU, mmAP(coco);</li>
</ul>
<p>Perform a detection:</p>
<ul>
<li><p>之前是手工特征+图像金字塔+滑动窗口+分类器（robust real-time detection; IJCV 2001）；</p>
</li>
<li><p>通过Fully Convolutional Network进行计算共享;</p>
</li>
</ul>
<p>Deep Learning for Object Detetcion:</p>
<ul>
<li><p>proposal and refine;</p>
</li>
<li><p>one stage:</p>
<ul>
<li><p>example: Densebox, YOLO, SSD, Retina Net…</p>
</li>
<li><p>keyword: anchor, divide and conquer, loss sampling;</p>
</li>
</ul>
</li>
<li><p>two stage:</p>
<ul>
<li>example: Faster R-CNN, RFCN, FPN, Mask R-CNN;</li>
</ul>
</li>
<li>keyword: speed, performance;</li>
</ul>
<p>One Stage:</p>
<p><strong>Densebox:</strong></p>
<ul>
<li><p>流程：图—&gt;图像金字塔—&gt;卷积神经网络—&gt;upsampling—&gt;卷积神经网络—&gt;（4+1）通道—&gt;预测+threshold+NMS；</p>
</li>
<li><p>输入：$m \times n \times 3$，输出：$m/4 \times n/4 \times 5$；</p>
</li>
<li><p>输出的feature map每个像素对应一个带分数的边框：</p>
</li>
</ul>
<p>$t_{i}=\left\{s_{i}, d x^{t}=x_{i}-x_{t}, d y^{t}=y_{i}-y_{t}, d x^{b}=x_{i}-x_{b}, d y^{b}=y_{i}-y_{b},\right\}$</p>
<p>其中t和b分别代表左上角和右下角坐标；</p>
<ul>
<li>问题：回归的L2损失函数选的不好（不同程度scale的object学习程度不同），GT assignment也存在问题，object比较拥挤的情况下，多个物体可能缩小在最后特征图上的一个点上，FP比较多，回归变量选取问题，误差较大；</li>
</ul>
<p>UnitBox：</p>
<ul>
<li>把L2 loss换成IoU loss = $-\ln IoU$;</li>
</ul>
<p>YOLO：</p>
<ul>
<li>$7 \times 7$的grid，加了fc层可以覆盖到一些更全局的context，但是受限于固定输入尺寸，运行速度虽快但是拥挤场景检测不是很work；</li>
</ul>
<p><strong>SSD</strong>：</p>
<ul>
<li><p>引入不同scale和aspect ratio的anchor；</p>
</li>
<li><p>回归GT与anchor的offset；</p>
</li>
<li>不同layer检测不同尺寸的物体，小物体浅层出，大物体深层出（但是并没有直接证据证明此法可靠）；</li>
<li>loss sampling和OHEM；</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_30815237/article/details/90292639">blog</a>；</li>
</ul>
<p>DSSD:</p>
<ul>
<li><p>SSD利用浅层检测小目标，但是浅层语义信息少；</p>
</li>
<li><p>利用upsampling和融合加强语义信息；</p>
</li>
</ul>
<p>RON:</p>
<ul>
<li><p>reverse connect (similar to FPN)；</p>
</li>
<li><p>loss sampling: objectness prior (先做二分类在再细分)；</p>
</li>
</ul>
<p>RetainaNet:</p>
<ul>
<li><p>引入Focal loss；</p>
</li>
<li><p>FPN结构；</p>
</li>
</ul>
<p>One Stage Detector: Summary</p>
<ul>
<li><p>Anchor：</p>
<ul>
<li><p>No anchor: YOLO, densebox/unitbox/east；</p>
</li>
<li><p>Anchor: YOLOv2, SSD, DSSD, RON, RetinaNet；</p>
</li>
</ul>
</li>
<li><p>Divide and conquer：</p>
<ul>
<li>SSD, DSSD, RON, RetinaNet；</li>
</ul>
</li>
<li>loss sample：<ul>
<li>all sample: densebox；</li>
<li>OHEM: SSD；</li>
<li>focal loss: RetinaNet；</li>
</ul>
</li>
</ul>
<p>Two Stage:</p>
<p>RCNN:</p>
<ul>
<li>selective search+分类proposal；</li>
</ul>
<p>Fast RCNN:</p>
<ul>
<li>selective search对应到特征图，通过RoI pooling去分类；</li>
</ul>
<p>Faster RCNN:</p>
<ul>
<li>用预设的anchor去找proposal；</li>
</ul>
<p>RFCN，Deformable Convolutional Networks，FPN，Mask RCNN…</p>
<p>Two Stages Detector-Summary:</p>
<ul>
<li>Speed：<ul>
<li>RCNN -&gt; Fast RCNN -&gt; Faster RCNN -&gt; RFCN；</li>
</ul>
</li>
<li>Performance：<ul>
<li>Divide and conquer：<ul>
<li>FPN；</li>
</ul>
</li>
<li>Deformable Pool/ROIAlign；</li>
<li>Deformable Conv；</li>
<li>Multi-task learning；</li>
</ul>
</li>
</ul>
<p>Open Problem in Detection：</p>
<ul>
<li>FP；</li>
<li>NMS (detection in crowd)；</li>
<li>GT assignment issue；</li>
<li>Detection in video：<ul>
<li>detect &amp; track in a network；</li>
</ul>
</li>
</ul>
<p>Human Keypoint Task:</p>
<ul>
<li><p>Single Person Skeleton：</p>
<ul>
<li><p>CPM；</p>
</li>
<li><p>Hourglass；</p>
</li>
</ul>
</li>
<li><p>Multiple-Person Skeleton：</p>
<ul>
<li><p>top down: </p>
<ul>
<li><p>detect-&gt;single person skeleton；</p>
</li>
<li><p>Depends on the detector：</p>
<ul>
<li><p>Fail in the crowd case；</p>
</li>
<li><p>Fail with partial observation；</p>
</li>
<li>can detect the small-scale human；</li>
</ul>
</li>
<li><p>More computation；</p>
</li>
<li><p>Better localization when the input-size of single person skeleton is large；</p>
</li>
</ul>
</li>
<li><p>bottom up: </p>
<ul>
<li>Deep/Deeper cut, OpenPose, Associative Embedding；</li>
<li>Fast computational speed；</li>
<li>good at localizing the human with partial observation；</li>
<li>Hard to assemble human；</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="7-Scene-Text-Detection-and-Recognition"><a href="#7-Scene-Text-Detection-and-Recognition" class="headerlink" title="7. Scene Text Detection and Recognition"></a>7. Scene Text Detection and Recognition</h2><p>Background:</p>
<ul>
<li><p>文字的重要性：文明标志，携带高层语义信息，作为visual recognition的线索；</p>
</li>
<li><p>problem: scene text detection+scene text recognition；</p>
</li>
<li><p>challenge: 比OCR更复杂，比如背景，颜色，字体，方向，文字混杂等；</p>
</li>
<li><p>application: card recognition，图片定位，产品搜索，自动驾驶，工业自动化等；</p>
</li>
</ul>
<p>conventional methods：</p>
<ul>
<li><p>detection before deep learning: MSER (maximally stable extremal regions)，SWT (stroke width transform)，Multi-Oriented；</p>
</li>
<li><p>recognition: Top-down and bottom-up cues（滑窗+统计特性），Tree-structured Model (DPM+CRF)，Label embedding (另辟蹊)；</p>
</li>
<li><p>统一检测和识别：Lexicon Driven；</p>
</li>
</ul>
<p>Deep learning methods：</p>
<p>包含传统辅助方法的：</p>
<ul>
<li><p>end-to-end-recognition: PhotoOCR，Deep Features，Reading Text；</p>
</li>
<li><p>detection: MSER Trees；</p>
</li>
</ul>
<p>不包含传统辅助方法的：</p>
<ul>
<li>detection: Holistic (当作语义分割来做)，<a target="_blank" rel="noopener" href="https://github.com/argman/EAST">EAST</a> (旷视CVPR2017，多任务学习)，Deep Direct Regression (与EAST相似)，SegLink (多尺度特征图)，Synthetic Data (在图片上产生文字)；</li>
</ul>
<p><img src="/.top//EAST.JPG" alt="EAST框架"></p>
<ul>
<li><p>recognition：$R^2AM$ (递归循环神经网络+soft-attention)，Visual Attention；</p>
</li>
<li><p>end-to-end recognition：<a target="_blank" rel="noopener" href="https://github.com/MichalBusta/DeepTextSpotter">Deep TextSpotter</a>；</p>
</li>
<li><p>summary: ideas from object detection and segmentation，end-to-end，use synthetic data；</p>
</li>
</ul>
<p>datasets and competitions：</p>
<ul>
<li>dataset: ICDAR 2103, MARA-TD500, ICDAR 2015, IIIT 5K-Word, COCO-Text, MLT, Total-Text；</li>
</ul>
<p>conclusion:</p>
<p><img src="/.top//7_conclusion.JPG" alt></p>
<p>challenges:</p>
<ul>
<li>Diversity of text: language, font, scale, orientation, arrangement, etc；</li>
<li>Complexity of background: virtually indistinguishable elements (signs, fences, bricks and grasses, etc.)；</li>
<li><p>Interferences: noise, blur, distortion, low resolution, nonuniform illumination, partial occlusion, etc；</p>
<p>Trends:</p>
</li>
<li><p>Stronger models (accuracy, efficiency, <strong>interpretability</strong>)；</p>
</li>
<li>Data synthesis；</li>
<li>Muiti-oriented text；</li>
<li>Curved text；</li>
<li>Muiti-language text；</li>
</ul>
<p>References:</p>
<ul>
<li><p>Survey:</p>
<ul>
<li>Ye et al.. Text Detection and Recognition in Imagery: A Survey. TPAMI, 2015.</li>
<li>Zhu et al.. Scene Text Detection and Recognition: Recent Advances and Future Trends. FCS, 2015.</li>
</ul>
</li>
<li><p>Conventional Methods:</p>
<ul>
<li>Epshtein et al.. Detecting Text in Natural Scenes with Stroke Width Transform. CVPR, 2010.</li>
<li>Neumann et al.. A method for text localization and recognition in real-world images. ACCV, 2010.</li>
<li>Yao et al.. Detecting Texts of Arbitrary Orientations in Natural Images. CVPR, 2012.</li>
<li>Wang et al.. End-to-End Scene Text Recognition. ICCV, 2011.</li>
<li>Mishra et al.. Scene Text Recognition using Higher Order Language Priors. BMVC, 2012.</li>
<li>Busta et al.. FASText: Efficient Unconstrained Scene Text Detector. ICCV 2015.</li>
</ul>
</li>
<li><p>Deep Learning Methods:</p>
<ul>
<li>Bissacco et al.. PhotoOCR: Reading Text in Uncontrolled Conditions. ICCV, 2013.</li>
<li>Jaderberg et al.. Deep Features for Text Spotting. ECCV, 2014.</li>
<li>Gupta et al.. Synthetic Data for Text Localisation in Natural Images. CVPR, 2016.</li>
<li>Zhou et al.. EAST: An Efficient and Accurate Scene Text Detector. CVPR, 2017.</li>
<li>Busta et al.. Deep TextSpotter: An End-To-End Trainable Scene Text Localization and Recognition Framework. ICCV, 2017.</li>
<li>Ghosh et al.. Visual attention models for scene text recognition. 2017. arXiv:1706.01487.</li>
<li>Cheng et al.. Focusing Attention: Towards Accurate Text Recognition in Natural Images. ICCV, 2017.</li>
</ul>
</li>
</ul>
<p>Useful Resources:</p>
<ul>
<li>Laboratories and Papers<br><a target="_blank" rel="noopener" href="https://github.com/chongyangtao/Awesome-Scene-Text-Recognition">https://github.com/chongyangtao/Awesome-Scene-Text-Recognition</a></li>
<li>Datasets and Codes<br><a target="_blank" rel="noopener" href="https://github.com/seungwooYoo/Curated-scene-text-recognition-analysis">https://github.com/seungwooYoo/Curated-scene-text-recognition-analysis</a></li>
<li>Projects and Products<br><a target="_blank" rel="noopener" href="https://github.com/wanghaisheng/awesome-ocr">https://github.com/wanghaisheng/awesome-ocr</a></li>
</ul>
<h2 id="8-Image-Segmentation"><a href="#8-Image-Segmentation" class="headerlink" title="8. Image Segmentation"></a>8. Image Segmentation</h2><p>semantic segmentaion, instace segmentation, scene parsing,  human parsing, stuff segmentation, UlrtraSound segmentation, selfie segmentation…</p>
<p>评价指标：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
{\operatorname{Accuracy}(\mathbf{y}, \hat{\mathbf{y}})=\sum_{i=0}^{n} \frac{I\left[y_{i}=\hat{y}_{i}\right]}{n}} \\
{\operatorname{mean} I O U(\mathbf{y}, \hat{\mathbf{y}})=\frac{\sum_{c}^{m} \sum_{i}^{n} I\left[y_{i}=c, \hat{y}_{i}=c\right]}{\sum_{c}^{m} \sum_{i}^{n} I\left[y_{i}=c \text { or } \hat{y}=c\right]}}
\end{array}</script><p>Semantic Segmantation:</p>
<ul>
<li><p>FCN: 第一篇语义分割工作；</p>
</li>
<li><p>Learning Deconvolution Network for Semantic Segmentation，引入unpool和反卷积deconvolution；</p>
</li>
<li><p>DeepLab，引入空洞卷积<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/f743bd9041b3">dilated-convolution</a>和DenseCRF；</p>
</li>
</ul>
<p><img src="/.top//CRF.JPG" alt></p>
<ul>
<li><p>CRF AS RNN;</p>
</li>
<li><p>Deeplab Attention;</p>
</li>
<li><p>PSPNet;</p>
</li>
<li><p>GCN (Global Convolutional Network，主讲人的工作，想要框住任意尺度的物体);</p>
</li>
<li><p>Deeplab V3;</p>
</li>
<li><p><strong>Deformable Convolution</strong>;</p>
</li>
</ul>
<p><img src="/.top//deformable_convolution.JPG" alt="deformable deconvolution"></p>
<p>Instance Segmentation:</p>
<p>Top-down pipeline (目前主流，依赖detection框架)：</p>
<ul>
<li><p>先detection再segmentation</p>
</li>
<li><p>FCIS (框得不准，但是分割依然准)；</p>
</li>
<li><p>Mask RCNN;</p>
</li>
</ul>
<p>Bottom-up pipeline (效果差，难实现，思考空间大):</p>
<ul>
<li><p>不出框分割;</p>
</li>
<li><p>Semantic instance segmentation via metric learning;</p>
</li>
</ul>
<hr>
<p>介绍旷视的框架：</p>
<ul>
<li>batch size in training:</li>
</ul>
<p>​       detection得batch size往往比分类小很多，主要是训练尺寸不同，另外可能一张图片会有很多proposal…</p>
<p>​       小batch size 会导致：unstable gradient，inaccurate BN statistics， extremely imbalanced data, very    </p>
<p>​       long training period…</p>
<ul>
<li><p>Multi-device BatchNorm;</p>
</li>
<li><p>Sublinear Memory;</p>
</li>
<li><p>Large Learning Rate;</p>
</li>
<li><p>打COCO instance segmentation比赛的一些tricks: precise RoI pooling, context extractor, mask generator；</p>
</li>
<li><p>keypoint比赛tricks；</p>
</li>
</ul>
<h2 id="9-Recurrent-Neural-Network"><a href="#9-Recurrent-Neural-Network" class="headerlink" title="9. Recurrent Neural Network"></a>9. Recurrent Neural Network</h2><p>RNN Bascis:</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://plato.stanford.edu/entries/turing-machine/">Turning Machine</a>, RNN is Turing Complete, Sequence Modeling；</p>
</li>
<li><p>RNN Diagram,$(h_{i}, y_{i}) = F(h_{i-1},x_{i},W)$ ；</p>
</li>
<li><p>根据input/output分类：many-to-many,  many-to-one, one-to-many, many-to-one+one-to-many;</p>
</li>
<li><p>many-to-many example: language model (predict next word by given previous words, tell story, write books in LaTex…);</p>
</li>
<li><p>many-to-one example: Sentiment analysis…</p>
</li>
<li><p>many-to-one+one_to_many exapmle: Neural Machine Translation (encoder+decoder)…</p>
</li>
<li><p>训练RNN，梯度爆炸和梯度消失: singular value &gt; 1 =&gt; explodes, singular value &lt; 1 =&gt; vanishes… LSTM (Long short-term memory) come to the resuce;</p>
</li>
<li><p>why LSTM works (input gate, forget gate, output gate, temp variable, memory cell); </p>
</li>
<li><p>GRU (similar to LSTM, let information flow without a separate memory cell);</p>
</li>
<li><p>Search for better RNN architecture;</p>
</li>
</ul>
<p>Simple RNN Extentsions:</p>
<ul>
<li><p>Bidirectional RNN (BDRNN)，预测未来；</p>
</li>
<li><p>2D-RNN: Pixel-RNN, each pixel depends on its top and left neighbor  (补图，segmentation);</p>
</li>
<li><p>Deep RNN (stack more of them, harder to train); </p>
</li>
</ul>
<p>RNN with Attention:</p>
<ul>
<li><p>attention: differentiate entities by its importance, spatial attention is related to location;  temporal attention is related to causality;</p>
</li>
<li><p>attention over input sequence: Neural Machine Translation (NMT);</p>
</li>
<li><p>Image Attention: Image Captioning (input image—&gt; Convolutional feature extraction—&gt;RNN with attention over the image—&gt;Word by word generation);</p>
</li>
</ul>
<p>RNN with External Memory:</p>
<ul>
<li>copy a sequence: Neural Turning Machines (NTM);</li>
</ul>
<p><img src="/.top//NTM.JPG" alt></p>
<p>More Applications:</p>
<ul>
<li><p>RNN without a sequence input: read house numbers from left to right, generate images of digits by learning to sequentially add color to canvas;</p>
</li>
<li><p>generalizing recurrence (a computation unit with shared parameter occurs at multiple places in the computation graph);</p>
</li>
<li><p>apply when there’s tree structure in data;</p>
</li>
<li><p>bottom-up aggregation of information;</p>
</li>
<li><p>speech recognition;</p>
</li>
<li><p>generating sequence;</p>
</li>
<li><p>question answering;</p>
</li>
<li><p>visual question answering;</p>
</li>
<li><p>combinatorial problems;</p>
</li>
<li><p>learning to excute;</p>
</li>
<li><p>compress image;</p>
</li>
<li><p>model architecture search;</p>
</li>
<li><p>meta-learning;</p>
</li>
</ul>
<p>…</p>
<p><img src="/.top//RNN_pros_cons.JPG" alt></p>
<p>RNN’s RIval:</p>
<ul>
<li><p>WaveNet: causal dilated convolution,  Oord, Aaron van den, et al. “Wavenet: A generative model for raw audio.” arXiv preprint arXiv:1609.03499 (2016).</p>
</li>
<li><p>Attention is All You Need (Transformer) ; </p>
</li>
</ul>
<h2 id="10-Introduction-to-Generative-Models-and-GANs"><a href="#10-Introduction-to-Generative-Models-and-GANs" class="headerlink" title="10. Introduction to Generative Models (and GANs)"></a>10. Introduction to Generative Models (and GANs)</h2><p>Basics:</p>
<ul>
<li><p>Generative Models: Learning the distributions;</p>
</li>
<li><p>Discriminative: learn the likelihood;</p>
</li>
<li><p>Generative: performs Density Estimation (learns the distribution) to allow sampling;</p>
</li>
<li><p>回归建模的话会取平均值，回归的是最可能情况的平均值，显得不真实，a driscrminative model just smoothes all possibilities, ambiguity and “blur” effect;</p>
</li>
<li><p>application of generative models: image generation from sketch, interactive editing, image to image translation;</p>
</li>
</ul>
<p>How to train generative models:</p>
<ul>
<li>给出一系列样本点，模型生成符合预期分布的输出；</li>
</ul>
<p><img src="/.top//taxonomy_of_generative_models.JPG" alt="从左往右方法逐渐work"></p>
<ul>
<li><p>exact model: NVP (non-volume preserving), real NVP: invertible no-linear transforms, 理论要求过于严格（Restriction on the source domain: must be of the <strong>same</strong> as the target.），效果不好（人脸稍微好点，因为其structure比较规矩）；</p>
</li>
<li><p>Variational Auto-Encoder (VAE): encoder 做density estimation的过程， decoder做sampling的过程。</p>
</li>
</ul>
<p><img src="/.top//VAE.JPG" alt></p>
<ul>
<li><p>Generative Adversarial Networks (GAN): 生成器和判别器相互学习进步，交替训练；</p>
</li>
<li><p>DCGAN: example of feature manipulation （人脸加眼镜，变性别之类的的操作）；</p>
</li>
<li><p>conditional, cross-domain generation (genenative adversarial text to image synthesis);</p>
</li>
<li><p>GAN training problems: unstable losses（训练时应该G和D应该处于动态平衡）, mini-batch fluctuation （每个batch之间生成的图像不同），model collapse (lack of diversity in generated results);</p>
</li>
<li><p>improve GAN training: label smoothing, <strong>Wasserstein GAN (WGAN)</strong> (stabilized taining curve, non-vanishing gradient), loss sensitive GAN (LS-GAN)… <a target="_blank" rel="noopener" href="https://github.com/hindupuravinash/the-gan-zoo">The GAN Zoo</a>;</p>
</li>
</ul>
<p><img src="/.top//WGAN.JPG" alt></p>
<p>举一些有名的GAN例子：</p>
<ul>
<li><p>zhu junyan—-Cycle GAN :correspondence from unpaired data;</p>
</li>
<li><p>DiscoGAN: cross-domain relation;</p>
</li>
<li><p>GeneGAN: shorter pathway improves training  (cross breeds and reproductions, 生成笑容)，object transfiguration (变发型)，interpolation in object subspace (改变发型方向)；</p>
</li>
</ul>
<p>Math behind Generative Models:</p>
<ul>
<li><p>formulation: sampling vs. density estimation;</p>
</li>
<li><p>RBM  (现在已经不怎么使用)；</p>
</li>
<li><p>from density to sample: 给定概率密度方程，无法有效采样；</p>
</li>
<li><p>from sample to density: 给定black-box sampler，是否可以估计概率密度（频率）；</p>
</li>
</ul>
<p>​        Given samples, some properties of the distribution can be learned, while others cannot.</p>
<p><img src="/.top//game_G_D.JPG" alt></p>
<ul>
<li><p>the future of GANs: guaranteed stabilization (new distance), broader application (apply adversarial loss in xx/ different type of data);</p>
</li>
<li><p>GAN tutorial from Ian Goodfellow: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.00160">https://arxiv.org/abs/1701.00160</a>;</p>
</li>
</ul>
<h2 id="11-Persom-Re-Identification"><a href="#11-Persom-Re-Identification" class="headerlink" title="11. Persom Re-Identification"></a>11. Persom Re-Identification</h2><p>ReID: from face to person;</p>
<ul>
<li><p>face recognition (verification, size: $32 \times 32$, horizontal: -30~30, vertical: -20~20, little occlusion);</p>
</li>
<li><p>person Re-Identification (trcaking in cameras, searching person in videos, clustering person in photos, challenges: inaccurate detection, misalignment, illumination difference, occlusion…);</p>
</li>
<li><p>common in FR &amp; ReID: deep metric learning, mutual learning, re-ranking;</p>
</li>
<li><p>special in ReID: feature alignment, ReID with pose estimation, ReID with human attributes;</p>
</li>
</ul>
<p>from classification to metric learning: </p>
<ul>
<li><p>classification network只能辨别那些“见过的”物体，没见过的物体就要重训练，对于人脸识别部署来说，不现实。为了克服这点，加入metric learning，拿pre-train过的classification网络在metric learning中finetune (similar feature);</p>
</li>
<li><p>有些工作是fusing intermediate feature maps, 但是计算量和存储都加大，拖慢了速度，不实用；</p>
</li>
</ul>
<p>Metric Learning: </p>
<ul>
<li><p>Learn a function that measures how similar two objects are. Compared to classification which works in a closed-word, metric learning deals with an open-world.</p>
</li>
<li><p>contrastive loss: $L_{\text {pairwise}}=\delta\left(I_{A}, I_{B}\right) \cdot\left|f_{A}-f_{B}\right|_{2}+\left(1-\delta\left(I_{A}, I_{B}\right)\right)\left(\alpha-\left|f_{A}-f_{B}\right|_{2}\right)_{+}$ （最后一项有focus困难样本的作用，$\delta$ is Kronecker Delta，$\alpha$ is the margin for different identities），让有相同identity的图像距离变小，反之变大，$\alpha$被用来略掉那些“naive”的negative pairs；</p>
</li>
<li><p>triplet loss: $L_{t r p}=\frac{1}{N}  \sum \limits ^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha\right)_{+}$ (The distance of A and A’ should be smaller than that of A and B. $\alpha$ is the margin between negative and positive pairs. Without  $\alpha$,  all distance converge to zero.);</p>
</li>
</ul>
<p><img src="/.top//closs_tloss.JPG" alt></p>
<ul>
<li><p>improved triplet loss: $ L_{i m t r p}=\frac{1}{N} \sum^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha\right)_{+} +\frac{1}{N} \sum^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\beta\right)_{+} $ ($\beta$ penalizes distance between features of $A$ and $A^{\prime}$), only consider image pairs with the same identity;</p>
</li>
<li><p>quadruplet loss: $\begin{aligned} L_{q u a d} &amp;=\frac{1}{N} \sum^{N}(\overbrace{\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha}^{\text {relative distance}}) \\ &amp;+\frac{1}{N} \sum^{N}(\overbrace{\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{C}-f_{B}\right|_{2}+\beta}^{\text {absolute distance }}) \end{aligned}$, 结合了triplet loss和pairwise loss，任何有着相同identity的image之间的distance都要比不同不同image之间的distance小；</p>
</li>
<li>triplet loss较contrastive loss提升明显，后面的quadruplet loss较triplet提升不多，而带来了计算量和搜索空间的提升，因此常用triplet loss;</li>
</ul>
<p>Hard Sample Mining:</p>
<ul>
<li><p>triplet hard loss: $ L_{\text {trihard}}=\frac{1}{N} \sum_{A \in \text {batch}}(\overbrace{\max _{A^{\prime}}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}\right)}^{\text {hard positive pair }} -\overbrace{\min \left(\left|f_{A}-f_{B}\right|_{2}\right)}^{\text {hard negative pair }}+\alpha) $, 找出矩阵中相同identity images中最不像的（the largest distance in the diagonal block）和不同identity images中最像的（The smallest distance in other places）;</p>
</li>
<li><p>soft  triplet hard loss: 不用一个个找出来，而是利用softmax自动去分配大权重给harder samples;</p>
</li>
<li><p>margin sample mining: $L_{e m l}=(\overbrace{\max _{A, A^{\prime}}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}\right)}^{\text {hardest positive pair }}-\overbrace{\min _{C, B}\left(\left|f_{C}-f_{B}\right|_{2}\right)}^{\text {hardest negative pair }}+\alpha)_{+}$;</p>
</li>
</ul>
<p><img src="/.top//conclu_dmetriclea.JPG" alt></p>
<p>Mutual Learning:</p>
<ul>
<li><p>knowledge distill: 知识蒸馏，学生网络学习老师网络的输出；</p>
</li>
<li><p>mutual learning: 几个学生网络自己相互学习，利用KL散度算各个网络output pro之间的接近程度；</p>
</li>
<li><p>metric mutual learning: $ L_{M}=\frac{1}{N^{2}} \sum_{i}^{N} \sum_{j}^{N} \left(\left[Z G\left(M_{i j}^{\theta_{1}}\right)-M_{i j}^{\theta_{2}}\right]^{2}+\left[M_{i j}^{\theta_{1}}-Z G\left(M_{i j}^{\theta_{2}}\right)\right]^{2}\right) $, ZG代表zero gradient，不计算梯度，不进行反向传播，学习distance matrix;</p>
</li>
</ul>
<p><img src="/.top//framework_MML.JPG" alt></p>
<ul>
<li>re-ranking: 对initial ranking list进行再ranking，使其smooth，on Supervised Smoothed Manifold/ by K-reciprocal Encoding;</li>
</ul>
<p>Person Re-Identification:</p>
<ul>
<li><p>difficulties: inaccurate detection, misalignment, illumination difference, occlusion, non-rigid body deformation, similar apperance…</p>
</li>
<li><p>evaluation criteria: CMC (Cumulative Math Characteristic)<rank-1, rank-5, rank-10>, mAP (based on rank);</rank-1,></p>
</li>
<li><p>datasets: Marke1501, CUHK03, DukeMTMC-reid, MARS;</p>
</li>
</ul>
<p>Feature Alignment:</p>
<p>motivations:</p>
<ul>
<li>Person is highly structured;</li>
<li>Local similarity plays a key role to decide the identity;</li>
</ul>
<p>methods:</p>
<ul>
<li>Local Features from local regions<ul>
<li>Traditional Methods (colors, texture…);</li>
<li>Deep Learning Methods;</li>
</ul>
</li>
<li>Local Feature Alignment<ul>
<li>Fusion by LSTM (RNN cannot fuse local features properly);</li>
<li>Alignment in PL-Net (Part Loss Network, unsupervised);</li>
<li>Alignment in AlignedReID (Face++出品，性能超越人类，global feature+7个local feature，代表人的7个部分，横向pool，只拿对应的边，使用动态规划);</li>
</ul>
</li>
</ul>
<p><img src="/.top//AlignedReID.JPG" alt></p>
<p>ReID with Extra Information:</p>
<p>ReID with Pose Estimation:</p>
<ul>
<li>Providing explicit guidance for alignment;</li>
<li>Global-Local Alignment Descriptor (GLAD);<ul>
<li>Vertical alignment by pose estimation;</li>
</ul>
</li>
<li>SpindleNet;<ul>
<li>Fusing local features from regions proposed by pose estimation;</li>
</ul>
</li>
</ul>
<p>ReID with Human Attributes:</p>
<ul>
<li>Attributes is critical in discriminating different persons;</li>
</ul>
<p><img src="/.top//11_conclusion.JPG" alt></p>
<h2 id="12-Shape-from-X-3D-reconstruction-传统和DL"><a href="#12-Shape-from-X-3D-reconstruction-传统和DL" class="headerlink" title="12. Shape from X (3D reconstruction: 传统和DL)"></a>12. Shape from X (3D reconstruction: 传统和DL)</h2><ul>
<li><p>Structure from Motion (SfM): the most easy-to-understand approach, triangulation gets depth;</p>
</li>
<li><p>triangulation: the epipolar constraint对极约束，单目；</p>
</li>
<li><p>stereo, rectification (更正), disparity (视差，depth): correspondence, 不能远距离测量；</p>
</li>
<li><p>3D point cloud: paper-building Rome in one day, 多视角图片SfM重建，3D geometry；</p>
</li>
<li><p>surface reconstruction: integration of oriented point;</p>
</li>
<li><p>SfM scanning: SLAM based, positioning, 华为手机发布会实现静止小熊猫玩偶重建；</p>
</li>
<li><p>depth sensing: active sensors, structured light, ToF (Time of Flight);</p>
</li>
<li><p>short baseline stereo: phase detection autofous;</p>
</li>
<li><p>shape from shading: shading as cue of 3D shape (the Lambertian law);</p>
</li>
<li><p>photometric stereo;</p>
</li>
<li><p>shape from texture, depth from focus, depth from defocus, shape from shadows, shape from sepcularities, object priors paper-<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.00603">A point set generation network for 3D object reconstruction from single image</a>;</p>
</li>
</ul>
<p>3D reconstruction from single image:</p>
<ul>
<li><p>the ShapeNet dataset;</p>
</li>
<li><p>depth map;</p>
</li>
<li><p>volumetric occupancy;</p>
</li>
<li><p>XML file;</p>
</li>
<li><p>ponit-based represenation;</p>
</li>
</ul>
<p>A neural method to stereo matching:</p>
<ul>
<li><p>Flownet &amp;  Dispnet (using raw left and right images as input, output disparity map);</p>
</li>
<li><p>stereo matching cost convolutional neural network—Yan lecun;</p>
</li>
<li><p>MRF (马尔可夫随机场) stereo methods;</p>
</li>
<li><p>global local stereo neural network;</p>
</li>
<li><p>PatchMatch Communication Layer;</p>
</li>
</ul>
<h2 id="13-Visual-Object-Tracking"><a href="#13-Visual-Object-Tracking" class="headerlink" title="13. Visual Object Tracking"></a>13. Visual Object Tracking</h2><p>Motion estimation/ Optical flow:</p>
<ul>
<li><p>motion field: the projection of the 3D motion onto a 2D image;</p>
</li>
<li><p>optical flwow: the pattern of apparent motion in images, $I(x, y, t)=I(x+d x, y+d y, t+d t)$, 在adjacent frames中像素的运动；</p>
</li>
<li><p>motion field与optical flow不是完全相等；</p>
</li>
<li><p>KLT feature tracker (找点，计算光流，更新点)，比较成熟，available in OpenCV；</p>
</li>
<li><p>optical flow with CNN: FlowNet / <a target="_blank" rel="noopener" href="https://github.com/lmb-freiburg/flownet2">FlowNet 2.0</a>, lack of training data (Flying Chairs / ChairsSDHom, Flying Things 3D);</p>
</li>
<li><p>optical flow长距离跟踪和复杂场景跟踪容易失效，不建议采用；</p>
</li>
</ul>
<p>Single object tracking:</p>
<ul>
<li><p>model free:  nothing but a single training example is provided by the bounding box in the first frame;</p>
</li>
<li><p>short term and subject to causality;</p>
</li>
</ul>
<p><img src="/.top//single_object_tracking.JPG" alt></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/foolwood/benchmark_results">paper list</a>;</p>
</li>
<li><p>correlation fiter: 模板匹配，similar to convolution;</p>
</li>
<li><p>MOSSE (Minimum Output Sum of Squared Error) Filter;</p>
</li>
<li><p>KCF (Kernelized Correlation Filter);</p>
</li>
</ul>
<p><img src="/.top//KCF.JPG" alt></p>
<ul>
<li><p>from KCF to Discriminative CF Trackers: Martin Danelljan, 从Deep SRDCF开始利用CNN feature; </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/martin-danelljan/Continuous-ConvOp">Continous-Convolution Operator Tracker</a>: very slow (~1fps) and easy to overfitting;</p>
</li>
<li><p>Efficient Convolution Operators: based (factorized convolution operator + Guassian mixture model) on C-COT, ~15fps on GPU;</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/HyeonseobNam/MDNet">Multi-Domain Convolutional Neural Network Tracker</a>: online tracking, bounding box regression, ~1fps;</p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://davheld.github.io/GOTURN/GOTURN.html">GOTURN</a>: ~100fps;</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/bertinetto/siamese-fc">SiameseFC</a>: ~60fps, a deep FCN is trained to address a more general similarity learning problem in an initial offline phase;</p>
</li>
<li><p>Benchmark: VOT (accuracy, robustness, EAO-expect average overlap), OTB(one pass evaluation, spatial robustness evaluation);</p>
</li>
</ul>
<p>Multiple object tracking:</p>
<p><img src="/.top//multi_object_tracking.JPG" alt></p>
<ul>
<li><p>tracking by detection: assocation based on location (IoU, L1/L2 distance), motion (modeling the movement of objects, Kalman filter), apperance (feature) ans so on;</p>
</li>
<li><p>association: </p>
</li>
</ul>
<p><img src="/.top//association.JPG" alt></p>
<ul>
<li><p>association as optimization: local method (Hungarian algorithm), global methods (clustering, network flow, minimum cost multi-cut problem), do optimization in a window (trade off speed against acc);</p>
</li>
<li><p>Benchmark: MOT, KITTI, ImageNet VID;</p>
</li>
<li><p>evaluation metrics: Multiple object tracking accuracy (MOTA);</p>
</li>
</ul>
<p>Other:</p>
<ul>
<li><p>fast moving object (FMO): an object that moves over a distance exceeding its size within the<br>exposure time;</p>
</li>
<li><p>multiple camera tracking;</p>
</li>
<li><p>tracking with multiple cues: with multiple detectors, with key points, with semantic segmentation, with RGBD camera;</p>
</li>
<li><p>multiple object tracking with NN: </p>
<ul>
<li>Milan, Anton, et al. “Online Multi-Target Tracking Using<br>Recurrent Neural Networks“. AAAI. 2017.</li>
<li>Son, Jeany, et al. “Multi-Object Tracking with Quadruplet<br>Convolutional Neural Networks.” CVPR. 2017.</li>
</ul>
</li>
</ul>
<h2 id="14-Neural-Network-in-Computer-Graphics"><a href="#14-Neural-Network-in-Computer-Graphics" class="headerlink" title="14. Neural Network in Computer Graphics"></a>14. Neural Network in Computer Graphics</h2><p>计算机视觉是将图像信息转换成抽象的语义信息等，而计算机图形学是将抽象的语义信息转换成图像信息。</p>
<ul>
<li><p>Graphics: rendering, 3D modeling, visual media retouching （图像修整）；</p>
</li>
<li><p>Neural Network for graphics: faster, better, more robust;</p>
</li>
<li><p>NN rendering: </p>
<ul>
<li><p>Monte Carlo ray tracing （光线追踪，寻找光源），paper-[SIGGRAPH17] Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings ( utilize CNN to predict de-noising kernels, thus enhance ray tracing rendering result);</p>
</li>
<li><p>volume rendering;</p>
</li>
<li>NN shading (real-time rendering), paper-Deep shading: Convolutional Neural Networks for Screen-space shading (2016);</li>
<li>goal is to accelerate, all training data can be gathered virtually;</li>
</ul>
</li>
<li><p>NN 3D modeling:</p>
<ul>
<li>shape understanding: <ul>
<li>3D ShapeNets: A Deep Representation for Volumetric Shapes (2015).</li>
<li>VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition (2015).</li>
<li>DeepPano: Deep Panoramic Representation for 3-D Shape Recognition (2015).</li>
<li>FusionNet: 3D Object Classification Using Multiple Data Representations (2016).</li>
<li>OctNet: Learning Deep 3D Representations at High Resolutions (2017).</li>
<li>O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis (2017).</li>
<li>Orientation-boosted voxel nets for 3D object recognition (2017).</li>
<li>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (2017).</li>
</ul>
</li>
<li>shape synthesis: 3D-conv, also use GAN</li>
<li>from 2D to 3D, data becomes harder to handle, design of mesh representation, high-resolution 3D problem;</li>
</ul>
</li>
<li><p>NN visual retouching:</p>
<ul>
<li>tone mapping:  paper-Deep Bilateral Learning for Real-Time Image Enhancement (2017), it can handle high-resolution images relatively fast;</li>
<li>automatic enhancement: paper- Exposure: A white-box Photo Post-processing Framework (2017);</li>
</ul>
</li>
</ul>
<p>Example-NN 3D Face:</p>
<ul>
<li><p>given a face RGB/RGBD still/sequence, reconstruct for each frame (intrinsic image or inverse rendering):</p>
<ul>
<li>Inner/outer camera matrix;</li>
<li>Face 3D pose;</li>
<li>Face shape;</li>
<li>Face expression;</li>
<li>Face albedo;</li>
<li>lighting;</li>
</ul>
</li>
<li><p>3D face priors: shape &amp; albedo, paper-A 3D Morphable Model learnt from 10,000 faces (2016);</p>
</li>
<li><p>3D face priors: expression: paper- FaceWarehouse: a 3D Facial Expression Database for Visual Computing (2012);</p>
</li>
<li><p>optimization: based 3D face fitting;</p>
</li>
<li><p>Coarse Net, Fine Net;</p>
</li>
<li><p>3D Face-without prior: paper-DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild(2017);</p>
</li>
</ul>
<p><img src="/.top//3D_face.JPG" alt></p>
<ul>
<li><p>render for CV:</p>
<ul>
<li>Synthesizing Training Data for Object Detection in Indoor Scenes, (2017);</li>
<li>Playing for Data: Ground Truth from Computer Games (2016)</li>
<li>Learning from Synthetic Humans (2017);</li>
</ul>
</li>
<li><p>demo: Face2Face, Real-Time high-fidelity facial performance capture, DenseReg;</p>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Richard YU</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://densecollections.top/posts/megviidlcourse/">http://densecollections.top/posts/megviidlcourse/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://densecollections.top" target="_blank">自拙集</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/computer-vision/">computer vision</a><a class="post-meta__tags" href="/tags/deep-learning/">deep learning</a><a class="post-meta__tags" href="/tags/Megvii/">Megvii</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5ff01204250e8048" async="async"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/notesofCS231N/"><img class="prev-cover" src="https://images.unsplash.com/photo-1595769393754-418cb8bf6b14?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1355&amp;q=80" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Stanford CS231n 笔记</div></div></a></div><div class="next-post pull-right"><a href="/posts/Summaryofthisyear-1/"><img class="next-cover" src="https://images.unsplash.com/photo-1608488426085-c8b7e9e4592f?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1351&amp;q=80" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">2019-2020:漫长的告别</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/RCNNseries-2/" title="RCNN-series-in-object-detection(续)"><img class="cover" src="https://images.unsplash.com/photo-1608153498891-e895052f0cbe?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-10</div><div class="title">RCNN-series-in-object-detection(续)</div></div></a></div><div><a href="/posts/BriefreviewofObjectdetection/" title="A Brief Review of Object Detection and Semantic Segmentation"><img class="cover" src="https://images.unsplash.com/photo-1595769393754-418cb8bf6b14?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1355&q=80" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-02-27</div><div class="title">A Brief Review of Object Detection and Semantic Segmentation</div></div></a></div><div><a href="/posts/ssd-refinedet-paper/" title="SSD-RefineDet论文阅读"><img class="cover" src="https://images.unsplash.com/photo-1606870655612-8eacd813984d?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-03</div><div class="title">SSD-RefineDet论文阅读</div></div></a></div><div><a href="/posts/yolopaperreading/" title="yolo系列论文阅读笔记"><img class="cover" src="https://images.unsplash.com/photo-1608971222449-15661fc7bdb6?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=675&q=80" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-04</div><div class="title">yolo系列论文阅读笔记</div></div></a></div><div><a href="/posts/worksummaryofintern/" title="实习痰涂片项目总结"><img class="cover" src="https://images.unsplash.com/photo-1608488426085-c8b7e9e4592f?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1351&q=80" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-10-04</div><div class="title">实习痰涂片项目总结</div></div></a></div><div><a href="/posts/notesofCS231N/" title="Stanford CS231n 笔记"><img class="cover" src="https://images.unsplash.com/photo-1595769393754-418cb8bf6b14?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1355&q=80" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-29</div><div class="title">Stanford CS231n 笔记</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://raw.githubusercontent.com/Richardyu114/minds-thoughts-and-resources-about-research-/master/images/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Richard YU</div><div class="author-info__description">Today everything exists to end in a photograph</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">54</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Richardyu114"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://twitter.com/Yu1145635107" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a><a class="social-icon" href="https://instagram.com/d.h.richard" target="_blank" title="Instagram"><i class="fab fa-instagram-square"></i></a><a class="social-icon" href="https://weibo.com/u/5211687990" target="_blank" title="Weibo"><i class="fab fa-weibo"></i></a><a class="social-icon" href="https://www.douban.com/people/161993653/" target="_blank" title="豆瓣"><i class="fas fa-bookmark"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#About"><span class="toc-number">1.</span> <span class="toc-text">About</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">2.</span> <span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Math-in-DL-and-ML-Basics"><span class="toc-number">3.</span> <span class="toc-text">2. Math in DL and ML Basics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Neural-Networks-Basics-amp-Architecture-Design"><span class="toc-number">4.</span> <span class="toc-text">3. Neural Networks Basics &amp; Architecture Design</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Introduction-to-Computation-Technologies-in-Deep-Learning"><span class="toc-number">5.</span> <span class="toc-text">4. Introduction to Computation Technologies in Deep Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Neural-Network-Approximation-low-rank-sparsity-and-quantization"><span class="toc-number">6.</span> <span class="toc-text">5. Neural Network Approximation(low rank, sparsity, and quantization)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Modern-Object-Detection"><span class="toc-number">7.</span> <span class="toc-text">6. Modern Object Detection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Scene-Text-Detection-and-Recognition"><span class="toc-number">8.</span> <span class="toc-text">7. Scene Text Detection and Recognition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Image-Segmentation"><span class="toc-number">9.</span> <span class="toc-text">8. Image Segmentation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Recurrent-Neural-Network"><span class="toc-number">10.</span> <span class="toc-text">9. Recurrent Neural Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-Introduction-to-Generative-Models-and-GANs"><span class="toc-number">11.</span> <span class="toc-text">10. Introduction to Generative Models (and GANs)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-Persom-Re-Identification"><span class="toc-number">12.</span> <span class="toc-text">11. Persom Re-Identification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-Shape-from-X-3D-reconstruction-%E4%BC%A0%E7%BB%9F%E5%92%8CDL"><span class="toc-number">13.</span> <span class="toc-text">12. Shape from X (3D reconstruction: 传统和DL)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-Visual-Object-Tracking"><span class="toc-number">14.</span> <span class="toc-text">13. Visual Object Tracking</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-Neural-Network-in-Computer-Graphics"><span class="toc-number">15.</span> <span class="toc-text">14. Neural Network in Computer Graphics</span></a></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/ssd-refinedet-paper/" title="SSD-RefineDet论文阅读"><img src="https://images.unsplash.com/photo-1606870655612-8eacd813984d?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1350&amp;q=80" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SSD-RefineDet论文阅读"/></a><div class="content"><a class="title" href="/posts/ssd-refinedet-paper/" title="SSD-RefineDet论文阅读">SSD-RefineDet论文阅读</a><time datetime="2020-08-03T13:25:33.000Z" title="发表于 2020-08-03 21:25:33">2020-08-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/houjieC++STL/" title="侯捷c++STL体系结构与内核分析"><img src="https://images.unsplash.com/photo-1608488426085-c8b7e9e4592f?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1351&amp;q=80" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="侯捷c++STL体系结构与内核分析"/></a><div class="content"><a class="title" href="/posts/houjieC++STL/" title="侯捷c++STL体系结构与内核分析">侯捷c++STL体系结构与内核分析</a><time datetime="2020-06-12T08:24:02.000Z" title="发表于 2020-06-12 16:24:02">2020-06-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/yolopaperreading/" title="yolo系列论文阅读笔记"><img src="https://images.unsplash.com/photo-1608971222449-15661fc7bdb6?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=675&amp;q=80" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="yolo系列论文阅读笔记"/></a><div class="content"><a class="title" href="/posts/yolopaperreading/" title="yolo系列论文阅读笔记">yolo系列论文阅读笔记</a><time datetime="2020-06-04T01:53:54.000Z" title="发表于 2020-06-04 09:53:54">2020-06-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/houjieC++/" title="侯捷C++面向对象程序设计"><img src="https://images.unsplash.com/photo-1608153498891-e895052f0cbe?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1350&amp;q=80" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="侯捷C++面向对象程序设计"/></a><div class="content"><a class="title" href="/posts/houjieC++/" title="侯捷C++面向对象程序设计">侯捷C++面向对象程序设计</a><time datetime="2020-05-28T08:48:27.000Z" title="发表于 2020-05-28 16:48:27">2020-05-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/loopdetectioninBipartitegraph/" title="二分图搜无向定长环"><img src="https://images.unsplash.com/photo-1608971222449-15661fc7bdb6?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=675&amp;q=80" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="二分图搜无向定长环"/></a><div class="content"><a class="title" href="/posts/loopdetectioninBipartitegraph/" title="二分图搜无向定长环">二分图搜无向定长环</a><time datetime="2020-05-08T02:41:32.000Z" title="发表于 2020-05-08 10:41:32">2020-05-08</time></div></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://images.unsplash.com/photo-1606870655612-8eacd813984d?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1350&amp;q=80)"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By Richard YU</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">It's hard to tell that the world we live in is either a reality or a dream.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.spacingElementById('content-inner')
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.spacingElementById('content-inner')
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    let initData = {
      el: '#vcomment',
      appId: 'duGoywhUmLrx9pM6qQhmf47c-gzGzoHsz',
      appKey: 'm7fc8w4Di5qnAXXaJ5Gp3Pgg',
      placeholder: '欢迎评论！请留下你的邮箱',
      avatar: 'robohash',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }
    
    if (false) {
      const otherData = false
      initData = Object.assign({}, initData, otherData)
    }
    
    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script>(function(){
  const bp = document.createElement('script');
  const curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  bp.dataset.pjax = ''
  const s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})()</script></div></body></html>