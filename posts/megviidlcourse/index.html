<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2"><meta name="google-site-verification" content="true"><meta name="baidu-site-verification" content="true"><link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1"><link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.1.1",sidebar:{position:"left",display:"post",offset:12,onmobile:!1,dimmer:!1},back2top:!0,back2top_sidebar:!1,fancybox:!0,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="baidu-site-verification" content="0bqk4mbBLD"><meta name="google-site-verification" content="FvMbHVgnH7FO0GFRzTmOevjSaFwvzIQZv94LdufMMPY"><meta name="description" content="About视频;课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取;这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好。"><meta name="keywords" content="computer vision,deep learning,Megvii"><meta property="og:type" content="article"><meta property="og:title" content="旷视2017年深度学习实践课程"><meta property="og:url" content="http://densecollections.top/posts/megviidlcourse/index.html"><meta property="og:site_name" content="自拙集"><meta property="og:description" content="About视频;课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取;这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/a_brief_history.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/supervised&unsupervisedlearning.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/CNN_decomposition.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/EAST.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/7_conclusion.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/CRF.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/deformable_convolution.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/NTM.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/RNN_pros_cons.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/taxonomy_of_generative_models.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/VAE.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/WGAN.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/game_G_D.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/closs_tloss.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/conclu_dmetriclea.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/framework_MML.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/AlignedReID.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/11_conclusion.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/single_object_tracking.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/KCF.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/multi_object_tracking.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/association.JPG"><meta property="og:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/3D_face.JPG"><meta property="og:updated_time" content="2020-07-08T09:50:05.065Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="旷视2017年深度学习实践课程"><meta name="twitter:description" content="About视频;课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取;这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好。"><meta name="twitter:image" content="http://densecollections.top/posts/megviidlcourse/旷视2017年深度学习实践课程/a_brief_history.JPG"><link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml"><link rel="canonical" href="http://densecollections.top/posts/megviidlcourse/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>旷视2017年深度学习实践课程 | 自拙集</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">自拙集</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Work cures everything</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-paperstation"><a href="/PaperStation/" rel="section"><i class="menu-item-icon fa fa-fw fa-edit"></i><br>PaperStation</a></li><li class="menu-item menu-item-mindwandering"><a href="/MindWandering/" rel="section"><i class="menu-item-icon fa fa-fw fa-paper-plane"></i><br>MindWandering</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://densecollections.top/posts/megviidlcourse/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Richard YU"><meta itemprop="description" content="Today everything exists to end in a photograph"><meta itemprop="image" content="/uploads/header.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="自拙集"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">旷视2017年深度学习实践课程</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-02-25 11:57:44" itemprop="dateCreated datePublished" datetime="2020-02-25T11:57:44+08:00">2020-02-25</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-07-08 17:50:05" itemprop="dateModified" datetime="2020-07-08T17:50:05+08:00">2020-07-08</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/课程记录/" itemprop="url" rel="index"><span itemprop="name">课程记录</span></a></span> </span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">评论数：</span> <a href="/posts/megviidlcourse/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/megviidlcourse/" itemprop="commentCount"></span> </a></span><span id="/posts/megviidlcourse/" class="leancloud_visitors" data-flag-title="旷视2017年深度学习实践课程"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">26k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">23 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><ul><li><a href="https://www.bilibili.com/video/av88056282/" target="_blank" rel="noopener">视频</a>;</li><li>课件关注“旷视研究院”公众号回复“深度学习实践PPT”可获取;</li><li>这套课程虽然是2017年的，但是涉及的内容还是很全面的，很多工作现在都发挥着不小的作用，所以并不过时，仍然值得一看。建议观看者是有过一定基础的人，刚入门者最好一开始不要看此系列课程，个人觉得第五课神经网络压缩，第六课基于DL的目标检测，第十课GAN，第十一课Person Re-Identification讲得比较好。</li></ul><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul><li><p>computer vision 在AI中的地位属于感知（perception）智能（还包括speech），另外一块是认知（cognitive），包括NLP和AGI（通用人工智能）；</p></li><li><p>人使用眼睛和大脑认识世界，电脑使用图像传感器和算力来视觉感知周围环境；</p></li><li><p>大脑皮层的出现，灵活，结构化，计算处理；</p></li><li><p>CV和AI的关系：其中非常重要的一个task/不同的研究工作和成果/作为关键应用；</p></li><li><p>现阶段的CV任务：classification (image)/ detection (region) /segmentation (pixel，实例，语义，全景)/ sequence (video，spatial+temporal)；</p></li><li><p>David Marr 的《vision》一书，这在visual SLAM中也十分重要，视觉知识的表示，part representation (拆成块，用各种模型表示，举例关键点检测);</p></li><li><p>part representation存在局限，有些不可分，引发了神经网络第二次复兴，Yann Lecun 的卷积神经网络应用于手写字体识别和人脸检测。由于当时难以复现，且懂的人不多，加上小规模数据和SVM等模型流行，神经网络出现衰落；</p></li><li><p>learning-based representation/ feature-based representation，特征工程+分类器（handcraft features engineering+SVM/Random Forest），浅层学习pipeline a short sequence；</p></li><li><p>端到端学习，所有参数联合优化 ，a long or very long sequence实现高维非线性映射；</p></li><li><p>受感知机启发的多层感知机（multilayer perceptron，MLP），利用backpropagation (BP) 梯度训练逼近（局部最优解）任意非线性函数；</p></li><li>90年代的神经网络成果：CNN/ autocoder/ boltzmann machine/ belief nets/ RNN；</li><li>复兴：data+computing+industry competition+a few breakthrough；</li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/a_brief_history.JPG" alt="深度学习沉浮历史"></p><ul><li><p>Resnet的思想：由浅到深学习，保持梯度数值较大，防止梯度消失 ；</p></li><li><p>从以前的手工设计feature为重点到现在设计网络结构（2012-2017为止）为重点，不同的结构所需算力不同，现在轻量级网络是一个热点；</p></li><li>卷积核的方式：1x1，3x3，<a href="https://blog.csdn.net/tintinetmilou/article/details/81607721" target="_blank" rel="noopener">depthwise 3x3</a>等，网络layer连接的方式；</li></ul><h2 id="2-Math-in-DL-and-ML-Basics"><a href="#2-Math-in-DL-and-ML-Basics" class="headerlink" title="2. Math in DL and ML Basics"></a>2. Math in DL and ML Basics</h2><ul><li><p>深度学习的内涵：deep learning) representation learning) machine learning) AI;</p></li><li><p>Linear Algebra：</p><ul><li><p>向量，矩阵，集合，群，封闭性，矩阵乘法是为了表示一种变换关系，向量映射到另一个向量；</p><ul><li>方阵，正交矩阵，特征值，特征向量，实对称矩阵，二次型，正定矩阵，半正定矩阵，奇异值分解；</li></ul></li></ul></li><li><p>Probability：</p><ul><li><p>随机事件，随机变量，概率密度函数，联合分布，边缘分布，条件分布，独立变量；</p></li><li><p>贝叶斯法则，先验分布，后验分布，期望，方差，协方差矩阵（半正定）；</p></li><li><p>常见分布：二值分布，二项分布，多值/多相分布（图像分类问题），正态分布（高斯分布）；</p></li><li><p>信息熵（分类中的交叉熵损失函数，发生概率越大的事情信息越不值钱），交叉熵和KL-divergence，生成式模型中的wassertein distance；</p></li></ul></li><li><p>Optimize：</p><ul><li>minimization（最小化）— 梯度下降gradient descent（步长的选取很关键），stochastic gradient descent;</li></ul></li><li><p>机器学习基本知识（machine learning basics）— 定义，假设，模型，评估，supervised &amp; unsupervised learning (learning $p(y | x) \quad or \quad p(x,y)$，判别式模型，生成式模型&lt;目前都用判别式模型&gt;, learning $p(x)$，auto encoder，GAN)，“no free lunch theorem”（all learning algorithms are equal, but some algorithm are more equal than others），overfitting &amp; underfitting，model capacity vs. generalization error，regularization (正则项，数据增强，parameter reduce and tying);</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/supervised&amp;unsupervisedlearning.JPG" alt></p><h2 id="3-Neural-Networks-Basics-amp-Architecture-Design"><a href="#3-Neural-Networks-Basics-amp-Architecture-Design" class="headerlink" title="3. Neural Networks Basics &amp; Architecture Design"></a>3. Neural Networks Basics &amp; Architecture Design</h2><ul><li><p>Fundamental task in CV: classification, object detection, semantic segmentation, instance segmentation, keypoint detection, human pose estimation, VQA…</p></li><li><p>计算机识别图像的难点：图像内容的复杂性和多样性，比如姿势，光照，模糊等；</p></li><li><p>特征是计算机认识图像的一个灯塔，且应当使用非线性特征抽取器；</p></li><li><p>线性组合特征（kernel learning，boosting），缺点是需要大量的templates，对特征的利用性差；</p></li><li><p>特征层级组合，重复利用特征，更为高效 —-&gt; concepts reuse in DL，网络层级的特征也是由低到高，但是这样高度非线性的函数难以优化（目前采用收敛到局部最优值）；</p></li><li><p>key ideas of DL: nolinear system, learn it from data, feature hierarchies, end-to-end learning；</p></li><li><p>激活函数，神经元，全连接网络，训练决定网络参数（前向，反向，更新）；</p></li><li><p>针对图像的认识从locally-connected net到convolutional net的设计，参数共享；</p></li><li><p>卷积层的卷积操作，pooling layer等；</p></li><li>网络结构设计：网络拓扑结构，layer function，超参，优化算法等经验性的东西，手动/autoML；</li><li>简介AlexNet（包含LRN，加速收敛），VGG（发掘3x3小卷积核的显著作用，但并不代表最高效的做法），GoogleNet，ResNet（拟合残差而不是直接拟合原函数），Xception，ResNeXt (借鉴Xception在resnet基础改进)，ShuffleNet，DesneNet，SqueezeNet；</li><li>structure design: deeper and wider, ease of optimization, multi-path design, resdiual path, sparse connection；</li><li>简介部分layer design：SPP，batch normalization，parametric rectifiers，bilinear CNNs（做细粒度分类）；</li><li>针对特定任务的结构设计：Deepface (人脸识别)，Global Convolutional Networks (语义分割)，Hourglass Networks (沙漏结构，大的感受野，用于pose estimation或者关键点)；</li></ul><h2 id="4-Introduction-to-Computation-Technologies-in-Deep-Learning"><a href="#4-Introduction-to-Computation-Technologies-in-Deep-Learning" class="headerlink" title="4. Introduction to Computation Technologies in Deep Learning"></a>4. Introduction to Computation Technologies in Deep Learning</h2><p>该节课偏底层，听的不是很懂，权当了解。</p><ul><li><p>symbolic computation：</p><ul><li><p>深度学习框架overview—program, compilation, runtime mangement, kernels, hardware；</p></li><li><p>computing graph, graph structure—variable, operator, edge；</p></li><li><p>静态图和动态图；</p></li><li><p>执行和优化；</p></li></ul></li><li><p>dense numerical computation:</p><ul><li><p>CPU computation (机器码，流水线，超流水线，超标量，乱序执行/cache hierarchy/…)；</p></li><li><p>other computation devices (NVIDIA GPU&lt;单指令，多线程架构&gt;，Google TPU，Huawei NPU in Kirin 970，Mobile CPU+GPU+DSP)；</p></li><li><p>computation &amp; memory gap；</p></li></ul></li><li><p>distributed computation:</p><ul><li><p>system (communication，Remote Direct Memory Access);</p></li><li><p>optimization algorithm (synchronous SGD，asynchronous SGD)；</p></li><li><p>communication algorithm (MPI Primitives，An AllReduce Algorithm)；</p></li></ul></li></ul><h2 id="5-Neural-Network-Approximation-low-rank-sparsity-and-quantization"><a href="#5-Neural-Network-Approximation-low-rank-sparsity-and-quantization" class="headerlink" title="5. Neural Network Approximation(low rank, sparsity, and quantization)"></a>5. Neural Network Approximation(low rank, sparsity, and quantization)</h2><p>该节课着重神经网络压缩，for faster training，faster inference， smaller capacity;</p><p>convolution as matrix product，利用近似权重矩阵达到网络压缩的目的;</p><p>Low Rank (本质是对矩阵进行一系列分解变换近似操作，减小计算量和存储量):</p><ul><li><p>对权重矩阵进行奇异值分解，singular value decomposition；</p></li><li><p>SVD+Kronecker Product ——&gt; KSPD；</p></li><li><p>矩阵分解：C-HW-K====》C-HW-R-(1X1)-K，然后通过reshape进行重新分解，目前horizontal-vertical decomposition最好；</p></li><li><p>shared group convolution is a kronrcker layer；</p></li><li><p>CP-decomposition与depthwise；</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/CNN_decomposition.JPG" alt></p><p>Sparse Approximation:</p><ul><li><p>权重分布有点类似高斯分布 ，0附近很多，微调网络，weight pruning: 韩松博士的deepcompression，让为0的权重逐渐增多（掩模矩阵使权重为0），不让0附近的权重在训练时抖动，FC层效果压缩明显；</p></li><li><p>网络加速计算—稀疏矩阵计算，channel purning，sparse communication for distributed gradient descent；</p></li></ul><p>Quantization：</p><ul><li><p>用什么精度算；</p></li><li><p>参数的量化，激活的量化，梯度的量化；</p></li><li><p>二值化，binary network；</p></li><li><p>大容量模型利用小bit训练时掉点不明显，小容量模型视情况而定；</p></li></ul><p>主讲人<a href="http://zsc.github.io/" target="_blank" rel="noopener">周舒畅</a>推荐的几篇文章，其中XNOR-Net为课程阅读要求材料：</p><blockquote><p>Bit Neural Network<br>● Matthieu Courbariaux et al. BinaryConnect: Training Deep Neural Networks with binary<br>weights during propagations. <a href="http://arxiv.org/abs/1511.00363" target="_blank" rel="noopener">http://arxiv.org/abs/1511.00363</a>.<br>● Itay Hubara et al. Binarized Neural Networks <a href="https://arxiv.org/abs/1602.02505v3" target="_blank" rel="noopener">https://arxiv.org/abs/1602.02505v3</a>.<br>● Matthieu Courbariaux et al. Binarized Neural Networks: Training Neural Networks with<br>Weights and Activations Constrained to +1 or -1. <a href="http://arxiv.org/pdf/1602.02830v3.pdf" target="_blank" rel="noopener">http://arxiv.org/pdf/1602.02830v3.pdf</a>.<br>● Rastegari et al. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural<br>Networks <a href="http://arxiv.org/pdf/1603.05279v1.pdf" target="_blank" rel="noopener">http://arxiv.org/pdf/1603.05279v1.pdf</a>.<br>● Zhou et al. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with<br>Low Bitwidth Gradients <a href="https://arxiv.org/abs/1606.06160" target="_blank" rel="noopener">https://arxiv.org/abs/1606.06160</a>.<br>● Hubara et al. Quantized Neural Networks: Training Neural Networks with Low Precision<br>Weights and Activations <a href="https://arxiv.org/abs/1609.07061" target="_blank" rel="noopener">https://arxiv.org/abs/1609.07061</a>.</p></blockquote><h2 id="6-Modern-Object-Detection"><a href="#6-Modern-Object-Detection" class="headerlink" title="6. Modern Object Detection"></a>6. Modern Object Detection</h2><p><a href="https://zhuanlan.zhihu.com/p/62372897" target="_blank" rel="noopener">anchor-free与anchor-based的交替轮回</a>;</p><p>Representation:</p><ul><li><p>Bounding-box: face detection, human detection, vehicle detection, text detection, general object detection;</p></li><li><p>Point: semantic segmentation (下一节课);</p></li><li><p>Keypoint: face landmark, human keypoint;</p></li></ul><p>Evaluation Criteria：</p><ul><li>precision (预测为真的里面真正是真的比例), recall (所有是真的里面预测为真的比例), Average Prcision (AP), mean AP (mAP)，IoU, mmAP(coco);</li></ul><p>Perform a detection:</p><ul><li><p>之前是手工特征+图像金字塔+滑动窗口+分类器（robust real-time detection; IJCV 2001）；</p></li><li><p>通过Fully Convolutional Network进行计算共享;</p></li></ul><p>Deep Learning for Object Detetcion:</p><ul><li><p>proposal and refine;</p></li><li><p>one stage:</p><ul><li><p>example: Densebox, YOLO, SSD, Retina Net…</p></li><li><p>keyword: anchor, divide and conquer, loss sampling;</p></li></ul></li><li><p>two stage:</p><ul><li>example: Faster R-CNN, RFCN, FPN, Mask R-CNN;</li></ul></li><li>keyword: speed, performance;</li></ul><p>One Stage:</p><p><strong>Densebox:</strong></p><ul><li><p>流程：图—&gt;图像金字塔—&gt;卷积神经网络—&gt;upsampling—&gt;卷积神经网络—&gt;（4+1）通道—&gt;预测+threshold+NMS；</p></li><li><p>输入：$m \times n \times 3$，输出：$m/4 \times n/4 \times 5$；</p></li><li><p>输出的feature map每个像素对应一个带分数的边框：</p></li></ul><p>$t_{i}=\left\{s_{i}, d x^{t}=x_{i}-x_{t}, d y^{t}=y_{i}-y_{t}, d x^{b}=x_{i}-x_{b}, d y^{b}=y_{i}-y_{b},\right\}$</p><p>其中t和b分别代表左上角和右下角坐标；</p><ul><li>问题：回归的L2损失函数选的不好（不同程度scale的object学习程度不同），GT assignment也存在问题，object比较拥挤的情况下，多个物体可能缩小在最后特征图上的一个点上，FP比较多，回归变量选取问题，误差较大；</li></ul><p>UnitBox：</p><ul><li>把L2 loss换成IoU loss = $-\ln IoU$;</li></ul><p>YOLO：</p><ul><li>$7 \times 7$的grid，加了fc层可以覆盖到一些更全局的context，但是受限于固定输入尺寸，运行速度虽快但是拥挤场景检测不是很work；</li></ul><p><strong>SSD</strong>：</p><ul><li><p>引入不同scale和aspect ratio的anchor；</p></li><li><p>回归GT与anchor的offset；</p></li><li>不同layer检测不同尺寸的物体，小物体浅层出，大物体深层出（但是并没有直接证据证明此法可靠）；</li><li>loss sampling和OHEM；</li><li><a href="https://blog.csdn.net/qq_30815237/article/details/90292639" target="_blank" rel="noopener">blog</a>；</li></ul><p>DSSD:</p><ul><li><p>SSD利用浅层检测小目标，但是浅层语义信息少；</p></li><li><p>利用upsampling和融合加强语义信息；</p></li></ul><p>RON:</p><ul><li><p>reverse connect (similar to FPN)；</p></li><li><p>loss sampling: objectness prior (先做二分类在再细分)；</p></li></ul><p>RetainaNet:</p><ul><li><p>引入Focal loss；</p></li><li><p>FPN结构；</p></li></ul><p>One Stage Detector: Summary</p><ul><li><p>Anchor：</p><ul><li><p>No anchor: YOLO, densebox/unitbox/east；</p></li><li><p>Anchor: YOLOv2, SSD, DSSD, RON, RetinaNet；</p></li></ul></li><li><p>Divide and conquer：</p><ul><li>SSD, DSSD, RON, RetinaNet；</li></ul></li><li>loss sample：<ul><li>all sample: densebox；</li><li>OHEM: SSD；</li><li>focal loss: RetinaNet；</li></ul></li></ul><p>Two Stage:</p><p>RCNN:</p><ul><li>selective search+分类proposal；</li></ul><p>Fast RCNN:</p><ul><li>selective search对应到特征图，通过RoI pooling去分类；</li></ul><p>Faster RCNN:</p><ul><li>用预设的anchor去找proposal；</li></ul><p>RFCN，Deformable Convolutional Networks，FPN，Mask RCNN…</p><p>Two Stages Detector-Summary:</p><ul><li>Speed：<ul><li>RCNN -&gt; Fast RCNN -&gt; Faster RCNN -&gt; RFCN；</li></ul></li><li>Performance：<ul><li>Divide and conquer：<ul><li>FPN；</li></ul></li><li>Deformable Pool/ROIAlign；</li><li>Deformable Conv；</li><li>Multi-task learning；</li></ul></li></ul><p>Open Problem in Detection：</p><ul><li>FP；</li><li>NMS (detection in crowd)；</li><li>GT assignment issue；</li><li>Detection in video：<ul><li>detect &amp; track in a network；</li></ul></li></ul><p>Human Keypoint Task:</p><ul><li><p>Single Person Skeleton：</p><ul><li><p>CPM；</p></li><li><p>Hourglass；</p></li></ul></li><li><p>Multiple-Person Skeleton：</p><ul><li><p>top down:</p><ul><li><p>detect-&gt;single person skeleton；</p></li><li><p>Depends on the detector：</p><ul><li><p>Fail in the crowd case；</p></li><li><p>Fail with partial observation；</p></li><li>can detect the small-scale human；</li></ul></li><li><p>More computation；</p></li><li><p>Better localization when the input-size of single person skeleton is large；</p></li></ul></li><li><p>bottom up:</p><ul><li>Deep/Deeper cut, OpenPose, Associative Embedding；</li><li>Fast computational speed；</li><li>good at localizing the human with partial observation；</li><li>Hard to assemble human；</li></ul></li></ul></li></ul><h2 id="7-Scene-Text-Detection-and-Recognition"><a href="#7-Scene-Text-Detection-and-Recognition" class="headerlink" title="7. Scene Text Detection and Recognition"></a>7. Scene Text Detection and Recognition</h2><p>Background:</p><ul><li><p>文字的重要性：文明标志，携带高层语义信息，作为visual recognition的线索；</p></li><li><p>problem: scene text detection+scene text recognition；</p></li><li><p>challenge: 比OCR更复杂，比如背景，颜色，字体，方向，文字混杂等；</p></li><li><p>application: card recognition，图片定位，产品搜索，自动驾驶，工业自动化等；</p></li></ul><p>conventional methods：</p><ul><li><p>detection before deep learning: MSER (maximally stable extremal regions)，SWT (stroke width transform)，Multi-Oriented；</p></li><li><p>recognition: Top-down and bottom-up cues（滑窗+统计特性），Tree-structured Model (DPM+CRF)，Label embedding (另辟蹊)；</p></li><li><p>统一检测和识别：Lexicon Driven；</p></li></ul><p>Deep learning methods：</p><p>包含传统辅助方法的：</p><ul><li><p>end-to-end-recognition: PhotoOCR，Deep Features，Reading Text；</p></li><li><p>detection: MSER Trees；</p></li></ul><p>不包含传统辅助方法的：</p><ul><li>detection: Holistic (当作语义分割来做)，<a href="https://github.com/argman/EAST" target="_blank" rel="noopener">EAST</a> (旷视CVPR2017，多任务学习)，Deep Direct Regression (与EAST相似)，SegLink (多尺度特征图)，Synthetic Data (在图片上产生文字)；</li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/EAST.JPG" alt="EAST框架"></p><ul><li><p>recognition：$R^2AM$ (递归循环神经网络+soft-attention)，Visual Attention；</p></li><li><p>end-to-end recognition：<a href="https://github.com/MichalBusta/DeepTextSpotter" target="_blank" rel="noopener">Deep TextSpotter</a>；</p></li><li><p>summary: ideas from object detection and segmentation，end-to-end，use synthetic data；</p></li></ul><p>datasets and competitions：</p><ul><li>dataset: ICDAR 2103, MARA-TD500, ICDAR 2015, IIIT 5K-Word, COCO-Text, MLT, Total-Text；</li></ul><p>conclusion:</p><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/7_conclusion.JPG" alt></p><p>challenges:</p><ul><li>Diversity of text: language, font, scale, orientation, arrangement, etc；</li><li>Complexity of background: virtually indistinguishable elements (signs, fences, bricks and grasses, etc.)；</li><li><p>Interferences: noise, blur, distortion, low resolution, nonuniform illumination, partial occlusion, etc；</p><p>Trends:</p></li><li><p>Stronger models (accuracy, efficiency, <strong>interpretability</strong>)；</p></li><li>Data synthesis；</li><li>Muiti-oriented text；</li><li>Curved text；</li><li>Muiti-language text；</li></ul><p>References:</p><ul><li><p>Survey:</p><ul><li>Ye et al.. Text Detection and Recognition in Imagery: A Survey. TPAMI, 2015.</li><li>Zhu et al.. Scene Text Detection and Recognition: Recent Advances and Future Trends. FCS, 2015.</li></ul></li><li><p>Conventional Methods:</p><ul><li>Epshtein et al.. Detecting Text in Natural Scenes with Stroke Width Transform. CVPR, 2010.</li><li>Neumann et al.. A method for text localization and recognition in real-world images. ACCV, 2010.</li><li>Yao et al.. Detecting Texts of Arbitrary Orientations in Natural Images. CVPR, 2012.</li><li>Wang et al.. End-to-End Scene Text Recognition. ICCV, 2011.</li><li>Mishra et al.. Scene Text Recognition using Higher Order Language Priors. BMVC, 2012.</li><li>Busta et al.. FASText: Efficient Unconstrained Scene Text Detector. ICCV 2015.</li></ul></li><li><p>Deep Learning Methods:</p><ul><li>Bissacco et al.. PhotoOCR: Reading Text in Uncontrolled Conditions. ICCV, 2013.</li><li>Jaderberg et al.. Deep Features for Text Spotting. ECCV, 2014.</li><li>Gupta et al.. Synthetic Data for Text Localisation in Natural Images. CVPR, 2016.</li><li>Zhou et al.. EAST: An Efficient and Accurate Scene Text Detector. CVPR, 2017.</li><li>Busta et al.. Deep TextSpotter: An End-To-End Trainable Scene Text Localization and Recognition Framework. ICCV, 2017.</li><li>Ghosh et al.. Visual attention models for scene text recognition. 2017. arXiv:1706.01487.</li><li>Cheng et al.. Focusing Attention: Towards Accurate Text Recognition in Natural Images. ICCV, 2017.</li></ul></li></ul><p>Useful Resources:</p><ul><li>Laboratories and Papers<br><a href="https://github.com/chongyangtao/Awesome-Scene-Text-Recognition" target="_blank" rel="noopener">https://github.com/chongyangtao/Awesome-Scene-Text-Recognition</a></li><li>Datasets and Codes<br><a href="https://github.com/seungwooYoo/Curated-scene-text-recognition-analysis" target="_blank" rel="noopener">https://github.com/seungwooYoo/Curated-scene-text-recognition-analysis</a></li><li>Projects and Products<br><a href="https://github.com/wanghaisheng/awesome-ocr" target="_blank" rel="noopener">https://github.com/wanghaisheng/awesome-ocr</a></li></ul><h2 id="8-Image-Segmentation"><a href="#8-Image-Segmentation" class="headerlink" title="8. Image Segmentation"></a>8. Image Segmentation</h2><p>semantic segmentaion, instace segmentation, scene parsing, human parsing, stuff segmentation, UlrtraSound segmentation, selfie segmentation…</p><p>评价指标：</p><script type="math/tex;mode=display">\begin{array}{l}
{\operatorname{Accuracy}(\mathbf{y}, \hat{\mathbf{y}})=\sum_{i=0}^{n} \frac{I\left[y_{i}=\hat{y}_{i}\right]}{n}} \\
{\operatorname{mean} I O U(\mathbf{y}, \hat{\mathbf{y}})=\frac{\sum_{c}^{m} \sum_{i}^{n} I\left[y_{i}=c, \hat{y}_{i}=c\right]}{\sum_{c}^{m} \sum_{i}^{n} I\left[y_{i}=c \text { or } \hat{y}=c\right]}}
\end{array}</script><p>Semantic Segmantation:</p><ul><li><p>FCN: 第一篇语义分割工作；</p></li><li><p>Learning Deconvolution Network for Semantic Segmentation，引入unpool和反卷积deconvolution；</p></li><li><p>DeepLab，引入空洞卷积<a href="https://www.jianshu.com/p/f743bd9041b3" target="_blank" rel="noopener">dilated-convolution</a>和DenseCRF；</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/CRF.JPG" alt></p><ul><li><p>CRF AS RNN;</p></li><li><p>Deeplab Attention;</p></li><li><p>PSPNet;</p></li><li><p>GCN (Global Convolutional Network，主讲人的工作，想要框住任意尺度的物体);</p></li><li><p>Deeplab V3;</p></li><li><p><strong>Deformable Convolution</strong>;</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/deformable_convolution.JPG" alt="deformable deconvolution"></p><p>Instance Segmentation:</p><p>Top-down pipeline (目前主流，依赖detection框架)：</p><ul><li><p>先detection再segmentation</p></li><li><p>FCIS (框得不准，但是分割依然准)；</p></li><li><p>Mask RCNN;</p></li></ul><p>Bottom-up pipeline (效果差，难实现，思考空间大):</p><ul><li><p>不出框分割;</p></li><li><p>Semantic instance segmentation via metric learning;</p></li></ul><hr><p>介绍旷视的框架：</p><ul><li>batch size in training:</li></ul><p>​ detection得batch size往往比分类小很多，主要是训练尺寸不同，另外可能一张图片会有很多proposal…</p><p>​ 小batch size 会导致：unstable gradient，inaccurate BN statistics， extremely imbalanced data, very</p><p>​ long training period…</p><ul><li><p>Multi-device BatchNorm;</p></li><li><p>Sublinear Memory;</p></li><li><p>Large Learning Rate;</p></li><li><p>打COCO instance segmentation比赛的一些tricks: precise RoI pooling, context extractor, mask generator；</p></li><li><p>keypoint比赛tricks；</p></li></ul><h2 id="9-Recurrent-Neural-Network"><a href="#9-Recurrent-Neural-Network" class="headerlink" title="9. Recurrent Neural Network"></a>9. Recurrent Neural Network</h2><p>RNN Bascis:</p><ul><li><p><a href="https://plato.stanford.edu/entries/turing-machine/" target="_blank" rel="noopener">Turning Machine</a>, RNN is Turing Complete, Sequence Modeling；</p></li><li><p>RNN Diagram,$(h_{i}, y_{i}) = F(h_{i-1},x_{i},W)$ ；</p></li><li><p>根据input/output分类：many-to-many, many-to-one, one-to-many, many-to-one+one-to-many;</p></li><li><p>many-to-many example: language model (predict next word by given previous words, tell story, write books in LaTex…);</p></li><li><p>many-to-one example: Sentiment analysis…</p></li><li><p>many-to-one+one_to_many exapmle: Neural Machine Translation (encoder+decoder)…</p></li><li><p>训练RNN，梯度爆炸和梯度消失: singular value &gt; 1 =&gt; explodes, singular value &lt; 1 =&gt; vanishes… LSTM (Long short-term memory) come to the resuce;</p></li><li><p>why LSTM works (input gate, forget gate, output gate, temp variable, memory cell);</p></li><li><p>GRU (similar to LSTM, let information flow without a separate memory cell);</p></li><li><p>Search for better RNN architecture;</p></li></ul><p>Simple RNN Extentsions:</p><ul><li><p>Bidirectional RNN (BDRNN)，预测未来；</p></li><li><p>2D-RNN: Pixel-RNN, each pixel depends on its top and left neighbor (补图，segmentation);</p></li><li><p>Deep RNN (stack more of them, harder to train);</p></li></ul><p>RNN with Attention:</p><ul><li><p>attention: differentiate entities by its importance, spatial attention is related to location; temporal attention is related to causality;</p></li><li><p>attention over input sequence: Neural Machine Translation (NMT);</p></li><li><p>Image Attention: Image Captioning (input image—&gt; Convolutional feature extraction—&gt;RNN with attention over the image—&gt;Word by word generation);</p></li></ul><p>RNN with External Memory:</p><ul><li>copy a sequence: Neural Turning Machines (NTM);</li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/NTM.JPG" alt></p><p>More Applications:</p><ul><li><p>RNN without a sequence input: read house numbers from left to right, generate images of digits by learning to sequentially add color to canvas;</p></li><li><p>generalizing recurrence (a computation unit with shared parameter occurs at multiple places in the computation graph);</p></li><li><p>apply when there’s tree structure in data;</p></li><li><p>bottom-up aggregation of information;</p></li><li><p>speech recognition;</p></li><li><p>generating sequence;</p></li><li><p>question answering;</p></li><li><p>visual question answering;</p></li><li><p>combinatorial problems;</p></li><li><p>learning to excute;</p></li><li><p>compress image;</p></li><li><p>model architecture search;</p></li><li><p>meta-learning;</p></li></ul><p>…</p><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/RNN_pros_cons.JPG" alt></p><p>RNN’s RIval:</p><ul><li><p>WaveNet: causal dilated convolution, Oord, Aaron van den, et al. “Wavenet: A generative model for raw audio.” arXiv preprint arXiv:1609.03499 (2016).</p></li><li><p>Attention is All You Need (Transformer) ;</p></li></ul><h2 id="10-Introduction-to-Generative-Models-and-GANs"><a href="#10-Introduction-to-Generative-Models-and-GANs" class="headerlink" title="10. Introduction to Generative Models (and GANs)"></a>10. Introduction to Generative Models (and GANs)</h2><p>Basics:</p><ul><li><p>Generative Models: Learning the distributions;</p></li><li><p>Discriminative: learn the likelihood;</p></li><li><p>Generative: performs Density Estimation (learns the distribution) to allow sampling;</p></li><li><p>回归建模的话会取平均值，回归的是最可能情况的平均值，显得不真实，a driscrminative model just smoothes all possibilities, ambiguity and “blur” effect;</p></li><li><p>application of generative models: image generation from sketch, interactive editing, image to image translation;</p></li></ul><p>How to train generative models:</p><ul><li>给出一系列样本点，模型生成符合预期分布的输出；</li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/taxonomy_of_generative_models.JPG" alt="从左往右方法逐渐work"></p><ul><li><p>exact model: NVP (non-volume preserving), real NVP: invertible no-linear transforms, 理论要求过于严格（Restriction on the source domain: must be of the <strong>same</strong> as the target.），效果不好（人脸稍微好点，因为其structure比较规矩）；</p></li><li><p>Variational Auto-Encoder (VAE): encoder 做density estimation的过程， decoder做sampling的过程。</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/VAE.JPG" alt></p><ul><li><p>Generative Adversarial Networks (GAN): 生成器和判别器相互学习进步，交替训练；</p></li><li><p>DCGAN: example of feature manipulation （人脸加眼镜，变性别之类的的操作）；</p></li><li><p>conditional, cross-domain generation (genenative adversarial text to image synthesis);</p></li><li><p>GAN training problems: unstable losses（训练时应该G和D应该处于动态平衡）, mini-batch fluctuation （每个batch之间生成的图像不同），model collapse (lack of diversity in generated results);</p></li><li><p>improve GAN training: label smoothing, <strong>Wasserstein GAN (WGAN)</strong> (stabilized taining curve, non-vanishing gradient), loss sensitive GAN (LS-GAN)… <a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">The GAN Zoo</a>;</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/WGAN.JPG" alt></p><p>举一些有名的GAN例子：</p><ul><li><p>zhu junyan—-Cycle GAN :correspondence from unpaired data;</p></li><li><p>DiscoGAN: cross-domain relation;</p></li><li><p>GeneGAN: shorter pathway improves training (cross breeds and reproductions, 生成笑容)，object transfiguration (变发型)，interpolation in object subspace (改变发型方向)；</p></li></ul><p>Math behind Generative Models:</p><ul><li><p>formulation: sampling vs. density estimation;</p></li><li><p>RBM (现在已经不怎么使用)；</p></li><li><p>from density to sample: 给定概率密度方程，无法有效采样；</p></li><li><p>from sample to density: 给定black-box sampler，是否可以估计概率密度（频率）；</p></li></ul><p>​ Given samples, some properties of the distribution can be learned, while others cannot.</p><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/game_G_D.JPG" alt></p><ul><li><p>the future of GANs: guaranteed stabilization (new distance), broader application (apply adversarial loss in xx/ different type of data);</p></li><li><p>GAN tutorial from Ian Goodfellow: <a href="https://arxiv.org/abs/1701.00160" target="_blank" rel="noopener">https://arxiv.org/abs/1701.00160</a>;</p></li></ul><h2 id="11-Persom-Re-Identification"><a href="#11-Persom-Re-Identification" class="headerlink" title="11. Persom Re-Identification"></a>11. Persom Re-Identification</h2><p>ReID: from face to person;</p><ul><li><p>face recognition (verification, size: $32 \times 32$, horizontal: -30~30, vertical: -20~20, little occlusion);</p></li><li><p>person Re-Identification (trcaking in cameras, searching person in videos, clustering person in photos, challenges: inaccurate detection, misalignment, illumination difference, occlusion…);</p></li><li><p>common in FR &amp; ReID: deep metric learning, mutual learning, re-ranking;</p></li><li><p>special in ReID: feature alignment, ReID with pose estimation, ReID with human attributes;</p></li></ul><p>from classification to metric learning:</p><ul><li><p>classification network只能辨别那些“见过的”物体，没见过的物体就要重训练，对于人脸识别部署来说，不现实。为了克服这点，加入metric learning，拿pre-train过的classification网络在metric learning中finetune (similar feature);</p></li><li><p>有些工作是fusing intermediate feature maps, 但是计算量和存储都加大，拖慢了速度，不实用；</p></li></ul><p>Metric Learning:</p><ul><li><p>Learn a function that measures how similar two objects are. Compared to classification which works in a closed-word, metric learning deals with an open-world.</p></li><li><p>contrastive loss: $L_{\text {pairwise}}=\delta\left(I_{A}, I_{B}\right) \cdot\left|f_{A}-f_{B}\right|_{2}+\left(1-\delta\left(I_{A}, I_{B}\right)\right)\left(\alpha-\left|f_{A}-f_{B}\right|_{2}\right)_{+}$ （最后一项有focus困难样本的作用，$\delta$ is Kronecker Delta，$\alpha$ is the margin for different identities），让有相同identity的图像距离变小，反之变大，$\alpha$被用来略掉那些“naive”的negative pairs；</p></li><li><p>triplet loss: $L_{t r p}=\frac{1}{N} \sum \limits ^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha\right)_{+}$ (The distance of A and A’ should be smaller than that of A and B. $\alpha$ is the margin between negative and positive pairs. Without $\alpha$, all distance converge to zero.);</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/closs_tloss.JPG" alt></p><ul><li><p>improved triplet loss: $ L_{i m t r p}=\frac{1}{N} \sum^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha\right)_{+} +\frac{1}{N} \sum^{N}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}-\beta\right)_{+} $ ($\beta$ penalizes distance between features of $A$ and $A^{\prime}$), only consider image pairs with the same identity;</p></li><li><p>quadruplet loss: $\begin{aligned} L_{q u a d} &amp;=\frac{1}{N} \sum^{N}(\overbrace{\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{A}-f_{B}\right|_{2}+\alpha}^{\text {relative distance}}) \\ &amp;+\frac{1}{N} \sum^{N}(\overbrace{\left|f_{A}-f_{A^{\prime}}\right|_{2}-\left|f_{C}-f_{B}\right|_{2}+\beta}^{\text {absolute distance }}) \end{aligned}$, 结合了triplet loss和pairwise loss，任何有着相同identity的image之间的distance都要比不同不同image之间的distance小；</p></li><li>triplet loss较contrastive loss提升明显，后面的quadruplet loss较triplet提升不多，而带来了计算量和搜索空间的提升，因此常用triplet loss;</li></ul><p>Hard Sample Mining:</p><ul><li><p>triplet hard loss: $ L_{\text {trihard}}=\frac{1}{N} \sum_{A \in \text {batch}}(\overbrace{\max _{A^{\prime}}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}\right)}^{\text {hard positive pair }} -\overbrace{\min \left(\left|f_{A}-f_{B}\right|_{2}\right)}^{\text {hard negative pair }}+\alpha) $, 找出矩阵中相同identity images中最不像的（the largest distance in the diagonal block）和不同identity images中最像的（The smallest distance in other places）;</p></li><li><p>soft triplet hard loss: 不用一个个找出来，而是利用softmax自动去分配大权重给harder samples;</p></li><li><p>margin sample mining: $L_{e m l}=(\overbrace{\max _{A, A^{\prime}}\left(\left|f_{A}-f_{A^{\prime}}\right|_{2}\right)}^{\text {hardest positive pair }}-\overbrace{\min _{C, B}\left(\left|f_{C}-f_{B}\right|_{2}\right)}^{\text {hardest negative pair }}+\alpha)_{+}$;</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/conclu_dmetriclea.JPG" alt></p><p>Mutual Learning:</p><ul><li><p>knowledge distill: 知识蒸馏，学生网络学习老师网络的输出；</p></li><li><p>mutual learning: 几个学生网络自己相互学习，利用KL散度算各个网络output pro之间的接近程度；</p></li><li><p>metric mutual learning: $ L_{M}=\frac{1}{N^{2}} \sum_{i}^{N} \sum_{j}^{N} \left(\left[Z G\left(M_{i j}^{\theta_{1}}\right)-M_{i j}^{\theta_{2}}\right]^{2}+\left[M_{i j}^{\theta_{1}}-Z G\left(M_{i j}^{\theta_{2}}\right)\right]^{2}\right) $, ZG代表zero gradient，不计算梯度，不进行反向传播，学习distance matrix;</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/framework_MML.JPG" alt></p><ul><li>re-ranking: 对initial ranking list进行再ranking，使其smooth，on Supervised Smoothed Manifold/ by K-reciprocal Encoding;</li></ul><p>Person Re-Identification:</p><ul><li><p>difficulties: inaccurate detection, misalignment, illumination difference, occlusion, non-rigid body deformation, similar apperance…</p></li><li><p>evaluation criteria: CMC (Cumulative Math Characteristic)<rank-1 , rank-5, rank-10>, mAP (based on rank);</rank-1></p></li><li><p>datasets: Marke1501, CUHK03, DukeMTMC-reid, MARS;</p></li></ul><p>Feature Alignment:</p><p>motivations:</p><ul><li>Person is highly structured;</li><li>Local similarity plays a key role to decide the identity;</li></ul><p>methods:</p><ul><li>Local Features from local regions<ul><li>Traditional Methods (colors, texture…);</li><li>Deep Learning Methods;</li></ul></li><li>Local Feature Alignment<ul><li>Fusion by LSTM (RNN cannot fuse local features properly);</li><li>Alignment in PL-Net (Part Loss Network, unsupervised);</li><li>Alignment in AlignedReID (Face++出品，性能超越人类，global feature+7个local feature，代表人的7个部分，横向pool，只拿对应的边，使用动态规划);</li></ul></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/AlignedReID.JPG" alt></p><p>ReID with Extra Information:</p><p>ReID with Pose Estimation:</p><ul><li>Providing explicit guidance for alignment;</li><li>Global-Local Alignment Descriptor (GLAD);<ul><li>Vertical alignment by pose estimation;</li></ul></li><li>SpindleNet;<ul><li>Fusing local features from regions proposed by pose estimation;</li></ul></li></ul><p>ReID with Human Attributes:</p><ul><li>Attributes is critical in discriminating different persons;</li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/11_conclusion.JPG" alt></p><h2 id="12-Shape-from-X-3D-reconstruction-传统和DL"><a href="#12-Shape-from-X-3D-reconstruction-传统和DL" class="headerlink" title="12. Shape from X (3D reconstruction: 传统和DL)"></a>12. Shape from X (3D reconstruction: 传统和DL)</h2><ul><li><p>Structure from Motion (SfM): the most easy-to-understand approach, triangulation gets depth;</p></li><li><p>triangulation: the epipolar constraint对极约束，单目；</p></li><li><p>stereo, rectification (更正), disparity (视差，depth): correspondence, 不能远距离测量；</p></li><li><p>3D point cloud: paper-building Rome in one day, 多视角图片SfM重建，3D geometry；</p></li><li><p>surface reconstruction: integration of oriented point;</p></li><li><p>SfM scanning: SLAM based, positioning, 华为手机发布会实现静止小熊猫玩偶重建；</p></li><li><p>depth sensing: active sensors, structured light, ToF (Time of Flight);</p></li><li><p>short baseline stereo: phase detection autofous;</p></li><li><p>shape from shading: shading as cue of 3D shape (the Lambertian law);</p></li><li><p>photometric stereo;</p></li><li><p>shape from texture, depth from focus, depth from defocus, shape from shadows, shape from sepcularities, object priors paper-<a href="https://arxiv.org/abs/1612.00603" target="_blank" rel="noopener">A point set generation network for 3D object reconstruction from single image</a>;</p></li></ul><p>3D reconstruction from single image:</p><ul><li><p>the ShapeNet dataset;</p></li><li><p>depth map;</p></li><li><p>volumetric occupancy;</p></li><li><p>XML file;</p></li><li><p>ponit-based represenation;</p></li></ul><p>A neural method to stereo matching:</p><ul><li><p>Flownet &amp; Dispnet (using raw left and right images as input, output disparity map);</p></li><li><p>stereo matching cost convolutional neural network—Yan lecun;</p></li><li><p>MRF (马尔可夫随机场) stereo methods;</p></li><li><p>global local stereo neural network;</p></li><li><p>PatchMatch Communication Layer;</p></li></ul><h2 id="13-Visual-Object-Tracking"><a href="#13-Visual-Object-Tracking" class="headerlink" title="13. Visual Object Tracking"></a>13. Visual Object Tracking</h2><p>Motion estimation/ Optical flow:</p><ul><li><p>motion field: the projection of the 3D motion onto a 2D image;</p></li><li><p>optical flwow: the pattern of apparent motion in images, $I(x, y, t)=I(x+d x, y+d y, t+d t)$, 在adjacent frames中像素的运动；</p></li><li><p>motion field与optical flow不是完全相等；</p></li><li><p>KLT feature tracker (找点，计算光流，更新点)，比较成熟，available in OpenCV；</p></li><li><p>optical flow with CNN: FlowNet / <a href="https://github.com/lmb-freiburg/flownet2" target="_blank" rel="noopener">FlowNet 2.0</a>, lack of training data (Flying Chairs / ChairsSDHom, Flying Things 3D);</p></li><li><p>optical flow长距离跟踪和复杂场景跟踪容易失效，不建议采用；</p></li></ul><p>Single object tracking:</p><ul><li><p>model free: nothing but a single training example is provided by the bounding box in the first frame;</p></li><li><p>short term and subject to causality;</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/single_object_tracking.JPG" alt></p><ul><li><p><a href="https://github.com/foolwood/benchmark_results" target="_blank" rel="noopener">paper list</a>;</p></li><li><p>correlation fiter: 模板匹配，similar to convolution;</p></li><li><p>MOSSE (Minimum Output Sum of Squared Error) Filter;</p></li><li><p>KCF (Kernelized Correlation Filter);</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/KCF.JPG" alt></p><ul><li><p>from KCF to Discriminative CF Trackers: Martin Danelljan, 从Deep SRDCF开始利用CNN feature;</p></li><li><p><a href="https://github.com/martin-danelljan/Continuous-ConvOp" target="_blank" rel="noopener">Continous-Convolution Operator Tracker</a>: very slow (~1fps) and easy to overfitting;</p></li><li><p>Efficient Convolution Operators: based (factorized convolution operator + Guassian mixture model) on C-COT, ~15fps on GPU;</p></li><li><p><a href="https://github.com/HyeonseobNam/MDNet" target="_blank" rel="noopener">Multi-Domain Convolutional Neural Network Tracker</a>: online tracking, bounding box regression, ~1fps;</p></li><li><p><a href="http://davheld.github.io/GOTURN/GOTURN.html" target="_blank" rel="noopener">GOTURN</a>: ~100fps;</p></li><li><p><a href="https://github.com/bertinetto/siamese-fc" target="_blank" rel="noopener">SiameseFC</a>: ~60fps, a deep FCN is trained to address a more general similarity learning problem in an initial offline phase;</p></li><li><p>Benchmark: VOT (accuracy, robustness, EAO-expect average overlap), OTB(one pass evaluation, spatial robustness evaluation);</p></li></ul><p>Multiple object tracking:</p><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/multi_object_tracking.JPG" alt></p><ul><li><p>tracking by detection: assocation based on location (IoU, L1/L2 distance), motion (modeling the movement of objects, Kalman filter), apperance (feature) ans so on;</p></li><li><p>association:</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/association.JPG" alt></p><ul><li><p>association as optimization: local method (Hungarian algorithm), global methods (clustering, network flow, minimum cost multi-cut problem), do optimization in a window (trade off speed against acc);</p></li><li><p>Benchmark: MOT, KITTI, ImageNet VID;</p></li><li><p>evaluation metrics: Multiple object tracking accuracy (MOTA);</p></li></ul><p>Other:</p><ul><li><p>fast moving object (FMO): an object that moves over a distance exceeding its size within the<br>exposure time;</p></li><li><p>multiple camera tracking;</p></li><li><p>tracking with multiple cues: with multiple detectors, with key points, with semantic segmentation, with RGBD camera;</p></li><li><p>multiple object tracking with NN:</p><ul><li>Milan, Anton, et al. “Online Multi-Target Tracking Using<br>Recurrent Neural Networks“. AAAI. 2017.</li><li>Son, Jeany, et al. “Multi-Object Tracking with Quadruplet<br>Convolutional Neural Networks.” CVPR. 2017.</li></ul></li></ul><h2 id="14-Neural-Network-in-Computer-Graphics"><a href="#14-Neural-Network-in-Computer-Graphics" class="headerlink" title="14. Neural Network in Computer Graphics"></a>14. Neural Network in Computer Graphics</h2><p>计算机视觉是将图像信息转换成抽象的语义信息等，而计算机图形学是将抽象的语义信息转换成图像信息。</p><ul><li><p>Graphics: rendering, 3D modeling, visual media retouching （图像修整）；</p></li><li><p>Neural Network for graphics: faster, better, more robust;</p></li><li><p>NN rendering:</p><ul><li><p>Monte Carlo ray tracing （光线追踪，寻找光源），paper-[SIGGRAPH17] Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings ( utilize CNN to predict de-noising kernels, thus enhance ray tracing rendering result);</p></li><li><p>volume rendering;</p></li><li>NN shading (real-time rendering), paper-Deep shading: Convolutional Neural Networks for Screen-space shading (2016);</li><li>goal is to accelerate, all training data can be gathered virtually;</li></ul></li><li><p>NN 3D modeling:</p><ul><li>shape understanding:<ul><li>3D ShapeNets: A Deep Representation for Volumetric Shapes (2015).</li><li>VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition (2015).</li><li>DeepPano: Deep Panoramic Representation for 3-D Shape Recognition (2015).</li><li>FusionNet: 3D Object Classification Using Multiple Data Representations (2016).</li><li>OctNet: Learning Deep 3D Representations at High Resolutions (2017).</li><li>O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis (2017).</li><li>Orientation-boosted voxel nets for 3D object recognition (2017).</li><li>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (2017).</li></ul></li><li>shape synthesis: 3D-conv, also use GAN</li><li>from 2D to 3D, data becomes harder to handle, design of mesh representation, high-resolution 3D problem;</li></ul></li><li><p>NN visual retouching:</p><ul><li>tone mapping: paper-Deep Bilateral Learning for Real-Time Image Enhancement (2017), it can handle high-resolution images relatively fast;</li><li>automatic enhancement: paper- Exposure: A white-box Photo Post-processing Framework (2017);</li></ul></li></ul><p>Example-NN 3D Face:</p><ul><li><p>given a face RGB/RGBD still/sequence, reconstruct for each frame (intrinsic image or inverse rendering):</p><ul><li>Inner/outer camera matrix;</li><li>Face 3D pose;</li><li>Face shape;</li><li>Face expression;</li><li>Face albedo;</li><li>lighting;</li></ul></li><li><p>3D face priors: shape &amp; albedo, paper-A 3D Morphable Model learnt from 10,000 faces (2016);</p></li><li><p>3D face priors: expression: paper- FaceWarehouse: a 3D Facial Expression Database for Visual Computing (2012);</p></li><li><p>optimization: based 3D face fitting;</p></li><li><p>Coarse Net, Fine Net;</p></li><li><p>3D Face-without prior: paper-DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild(2017);</p></li></ul><p><img src="/posts/megviidlcourse/旷视2017年深度学习实践课程/3D_face.JPG" alt></p><ul><li><p>render for CV:</p><ul><li>Synthesizing Training Data for Object Detection in Indoor Scenes, (2017);</li><li>Playing for Data: Ground Truth from Computer Games (2016)</li><li>Learning from Synthetic Humans (2017);</li></ul></li><li><p>demo: Face2Face, Real-Time high-fidelity facial performance capture, DenseReg;</p></li></ul></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Richard YU</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://densecollections.top/posts/megviidlcourse/" title="旷视2017年深度学习实践课程">http://densecollections.top/posts/megviidlcourse/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/computer-vision/" rel="tag"><i class="fa fa-tag"></i> computer vision</a> <a href="/tags/deep-learning/" rel="tag"><i class="fa fa-tag"></i> deep learning</a> <a href="/tags/Megvii/" rel="tag"><i class="fa fa-tag"></i> Megvii</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/Summaryofthisyear-1/" rel="next" title="2019-2020:漫长的告别"><i class="fa fa-chevron-left"></i> 2019-2020:漫长的告别</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/posts/notesofCS231N/" rel="prev" title="Stanford CS231n 笔记">Stanford CS231n 笔记 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/header.jpg" alt="Richard YU"><p class="site-author-name" itemprop="name">Richard YU</p><div class="site-description motion-element" itemprop="description">Today everything exists to end in a photograph</div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">24</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">54</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/richardyu114" title="GitHub &rarr; https://github.com/richardyu114" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Yu1145635107" title="Twitter &rarr; https://twitter.com/Yu1145635107" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a> </span><span class="links-of-author-item"><a href="https://instagram.com/d.h.richard" title="Instagram &rarr; https://instagram.com/d.h.richard" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a> </span><span class="links-of-author-item"><a href="https://weibo.com/u/5211687990" title="Weibo &rarr; https://weibo.com/u/5211687990" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a> </span><span class="links-of-author-item"><a href="https://www.douban.com/people/161993653/" title="豆瓣 &rarr; https://www.douban.com/people/161993653/" rel="noopener" target="_blank"><i class="fa fa-fw fa-paperclip"></i>豆瓣</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#About"><span class="nav-number">1.</span> <span class="nav-text">About</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-number">2.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Math-in-DL-and-ML-Basics"><span class="nav-number">3.</span> <span class="nav-text">2. Math in DL and ML Basics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Neural-Networks-Basics-amp-Architecture-Design"><span class="nav-number">4.</span> <span class="nav-text">3. Neural Networks Basics &amp; Architecture Design</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Introduction-to-Computation-Technologies-in-Deep-Learning"><span class="nav-number">5.</span> <span class="nav-text">4. Introduction to Computation Technologies in Deep Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Neural-Network-Approximation-low-rank-sparsity-and-quantization"><span class="nav-number">6.</span> <span class="nav-text">5. Neural Network Approximation(low rank, sparsity, and quantization)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Modern-Object-Detection"><span class="nav-number">7.</span> <span class="nav-text">6. Modern Object Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Scene-Text-Detection-and-Recognition"><span class="nav-number">8.</span> <span class="nav-text">7. Scene Text Detection and Recognition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Image-Segmentation"><span class="nav-number">9.</span> <span class="nav-text">8. Image Segmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-Recurrent-Neural-Network"><span class="nav-number">10.</span> <span class="nav-text">9. Recurrent Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-Introduction-to-Generative-Models-and-GANs"><span class="nav-number">11.</span> <span class="nav-text">10. Introduction to Generative Models (and GANs)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-Persom-Re-Identification"><span class="nav-number">12.</span> <span class="nav-text">11. Persom Re-Identification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-Shape-from-X-3D-reconstruction-传统和DL"><span class="nav-number">13.</span> <span class="nav-text">12. Shape from X (3D reconstruction: 传统和DL)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-Visual-Object-Tracking"><span class="nav-number">14.</span> <span class="nav-text">13. Visual Object Tracking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-Neural-Network-in-Computer-Graphics"><span class="nav-number">15.</span> <span class="nav-text">14. Neural Network in Computer Graphics</span></a></li></ol></div></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Richard YU</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">267k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">4:03</span></div><div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.1</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script><script src="/js/utils.js?v=7.1.1"></script><script src="/js/motion.js?v=7.1.1"></script><script src="/js/affix.js?v=7.1.1"></script><script src="/js/schemes/pisces.js?v=7.1.1"></script><script src="/js/scrollspy.js?v=7.1.1"></script><script src="/js/post-details.js?v=7.1.1"></script><script src="/js/next-boot.js?v=7.1.1"></script><script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>var GUEST=["nick","mail","link"],guest="nick,mail,link";guest=guest.split(",").filter(function(e){return-1<GUEST.indexOf(e)}),new Valine({el:"#comments",verify:!0,notify:!0,appId:"duGoywhUmLrx9pM6qQhmf47c-gzGzoHsz",appKey:"m7fc8w4Di5qnAXXaJ5Gp3Pgg",placeholder:"欢迎评论！请留下你的邮箱",avatar:"mm",meta:guest,pageSize:"10",visitor:!0,lang:"zh-cn"})</script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });</script><script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>$(".highlight").not(".gist .highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea"),o=window.pageYOffset||document.documentElement.scrollTop;n.style.top=o+"px",n.style.position="absolute",n.style.opacity="0",n.readOnly=!0,n.value=e,document.body.appendChild(n),n.select(),n.setSelectionRange(0,e.length),n.readOnly=!1,document.execCommand("copy")?$(this).text("复制成功"):$(this).text("复制失败"),n.blur(),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script></body></html>